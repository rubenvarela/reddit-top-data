{"kind": "Listing", "data": {"after": "t3_15ay13l", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Turns out databases are \"relational\" or something", "author_fullname": "t2_qlmkijv8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The data engineer came to me... tears in his eyes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 107, "top_awarded_type": null, "hide_score": false, "name": "t3_15ae6kp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 573, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 573, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/w5lBiBLnqC1xzIdkywBWCqB0YGYQeRWrqr08EMByD2s.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690397313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Turns out databases are &amp;quot;relational&amp;quot; or something&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1wm37l33vceb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1wm37l33vceb1.png?auto=webp&amp;s=8eb68cc0feab067c94c5d88ca54a7eda2cbfcfd2", "width": 1080, "height": 830}, "resolutions": [{"url": "https://preview.redd.it/1wm37l33vceb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=48652b42e4fe71b9c48eac0b2a2c6a0eaaad045b", "width": 108, "height": 83}, {"url": "https://preview.redd.it/1wm37l33vceb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=79d5b3781803dce41f5534196cb74c8f84aa1caa", "width": 216, "height": 166}, {"url": "https://preview.redd.it/1wm37l33vceb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=afdafe3ea62eae50680c1797cac89dd8c6010a0f", "width": 320, "height": 245}, {"url": "https://preview.redd.it/1wm37l33vceb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=992489262d93f2834aba10c908b504e4f58e328a", "width": 640, "height": 491}, {"url": "https://preview.redd.it/1wm37l33vceb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=97be176695471f82f58a8349a936a5d2c170eaf9", "width": 960, "height": 737}, {"url": "https://preview.redd.it/1wm37l33vceb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b45cda7bf48386140f347a041f725b62cbbe33b", "width": 1080, "height": 830}], "variants": {}, "id": "UwqiG5dVs8n80MCeWPA1--klG-ySKne2YEq3LyGRi8k"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "15ae6kp", "is_robot_indexable": true, "report_reasons": null, "author": "shed_antlers", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ae6kp/the_data_engineer_came_to_me_tears_in_his_eyes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1wm37l33vceb1.png", "subreddit_subscribers": 118416, "created_utc": 1690397313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering,\n\nWe recently ran an experiment at [Dozer](https://github.com/getdozer/dozer) that I think worth sharing. We processed in real-time 160m records spread across 4 tables and created APIs from the result. The operation consisted of 4 JOINs and 1 aggregation. Here is a link to the experiment:\n\n[https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce](https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce)\n\nWould love to get your feedback.  \n\n\nThanks  \nMatteo", "author_fullname": "t2_5efs1s7d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Processing 160m of records (with 4 JOINs and 1 aggregation) in real-time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15axba2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690450727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;We recently ran an experiment at &lt;a href=\"https://github.com/getdozer/dozer\"&gt;Dozer&lt;/a&gt; that I think worth sharing. We processed in real-time 160m records spread across 4 tables and created APIs from the result. The operation consisted of 4 JOINs and 1 aggregation. Here is a link to the experiment:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce\"&gt;https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to get your feedback.  &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;br/&gt;\nMatteo&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?auto=webp&amp;s=205a7ce9ef4cd41ab708aeb4c8b2f411ade2b5c6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b2137c62de69a82f790eca2cf323b4e9fa49a42", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e9e76b368d9bd2737828569495d8f080ea1ca66", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ecc4927d891da4391fe409958ef35c9d5e83965e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=19b42c63769cc4774b26d17ac2f1dbbcc10fd21a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ba1086ce1186414717e41899ed3393ff00818f4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f22dfc70fa64869063b5eedee17d4f7b841f7dc", "width": 1080, "height": 540}], "variants": {}, "id": "bsBU-V26yp9Cn213aetzEQxsmD1y-nrva8jMGPRMp5A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "15axba2", "is_robot_indexable": true, "report_reasons": null, "author": "matteopelati76", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15axba2/processing_160m_of_records_with_4_joins_and_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15axba2/processing_160m_of_records_with_4_joins_and_1/", "subreddit_subscribers": 118416, "created_utc": 1690450727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone used Microsoft Fabric yet? If your a Microsoft shop are you planning on adopting it? \n\nNot coming from any angle I just haven\u2019t talked with anyone who has been using it a lot.", "author_fullname": "t2_1n3qfa0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft Fabric - have you used it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15am3g2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690415941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone used Microsoft Fabric yet? If your a Microsoft shop are you planning on adopting it? &lt;/p&gt;\n\n&lt;p&gt;Not coming from any angle I just haven\u2019t talked with anyone who has been using it a lot.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15am3g2", "is_robot_indexable": true, "report_reasons": null, "author": "Culpgrant21", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15am3g2/microsoft_fabric_have_you_used_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15am3g2/microsoft_fabric_have_you_used_it/", "subreddit_subscribers": 118416, "created_utc": 1690415941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a cs software manager at a public company. I\u2019m not currently in a data engineering role, but I want to make a pivot into DE. I\u2019ve had a DE ticket open for several months for them just to scope and there\u2019s no end in sight. It could be several more months before they can even think about looking at our ticket. Suffice it to say our DE team is under resourced. \n\nThis is where I feel that maybe I\u2019m being naive: I feel like the request is simple. We want to create a daily job for some API reports and bring it into our data warehouse. \n\nHow many hoops does one need to jump through in a mid-level organization to write a script then push it to the warehouse? \n\nWhat level of difficulty is this normally? \n\nWould it be rude to write the script and hand it over to the DE team?\n\nLooking for advice to navigate the situation. Thanks!", "author_fullname": "t2_b7jxy4i2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I being naive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ad17i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690394712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a cs software manager at a public company. I\u2019m not currently in a data engineering role, but I want to make a pivot into DE. I\u2019ve had a DE ticket open for several months for them just to scope and there\u2019s no end in sight. It could be several more months before they can even think about looking at our ticket. Suffice it to say our DE team is under resourced. &lt;/p&gt;\n\n&lt;p&gt;This is where I feel that maybe I\u2019m being naive: I feel like the request is simple. We want to create a daily job for some API reports and bring it into our data warehouse. &lt;/p&gt;\n\n&lt;p&gt;How many hoops does one need to jump through in a mid-level organization to write a script then push it to the warehouse? &lt;/p&gt;\n\n&lt;p&gt;What level of difficulty is this normally? &lt;/p&gt;\n\n&lt;p&gt;Would it be rude to write the script and hand it over to the DE team?&lt;/p&gt;\n\n&lt;p&gt;Looking for advice to navigate the situation. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15ad17i", "is_robot_indexable": true, "report_reasons": null, "author": "Marble_Kween", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ad17i/am_i_being_naive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ad17i/am_i_being_naive/", "subreddit_subscribers": 118416, "created_utc": 1690394712.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If  you were to start over in the Data Engineering or would have to mentor someone, how would you do.   \nTaking into account all these new tools in today's tech stack.  \nI've researched all this, but TBH I don't have idea about 90% of these:  \n  \n\n**\u00b7 ETL and Scheduling or Orchestration or Jobs =**\n\n1. Open source Tools (Airflow(most used), Dagster, Argo, Prefect, Luigi)\n\n2. Traditional UI Tool (SSIS, Informatica, Talend, FiveTran)\n\n3. Cloud Tool (Azure Data Factory, Google Dataflow, AWS Glue)\n\n4. Modern Proprietary Tool (Databricks, Trifacta)\n\n5. Fivetran (just usually mostly used but a little old school) | MWAA (cost effective) | Apache Beam\n\n**\u00b7 Data Warehouse =** Snowflake or BigQuery | For Cloud = Amazon S3\n\n**\u00b7 Data Lakes =** DataBricks, Redshift\n\n**\u00b7 Data Transformation or Data Quality Control testing or Parallel Processing Tools =** dbt, Pandas, Apache Spark, \n\n**\u00b7 BI =** Power BI\n\n**\u00b7 Exploratory Data Analysis =** Snowflake, Jupyter, Alteryx, Snowflake Snowsight\n\n**\u00b7 Scaling Workers =** aws lambdas, kubernetes\n\n**\u00b7 Devops Methods =** Observability, alerting, incident management, ci/cd and good pr hygiene apply as much to DE as regular backend SE\n\n**\u00b7 Stream Processing =** Apache Kafka,\n\n**\u00b7 AWS Services =** RDS (to setup and scale db in cloud)", "author_fullname": "t2_3k9gevl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you learn DE if you had to start over ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15adu9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690396515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If  you were to start over in the Data Engineering or would have to mentor someone, how would you do.&lt;br/&gt;\nTaking into account all these new tools in today&amp;#39;s tech stack.&lt;br/&gt;\nI&amp;#39;ve researched all this, but TBH I don&amp;#39;t have idea about 90% of these:  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 ETL and Scheduling or Orchestration or Jobs =&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Open source Tools (Airflow(most used), Dagster, Argo, Prefect, Luigi)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Traditional UI Tool (SSIS, Informatica, Talend, FiveTran)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Cloud Tool (Azure Data Factory, Google Dataflow, AWS Glue)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Modern Proprietary Tool (Databricks, Trifacta)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Fivetran (just usually mostly used but a little old school) | MWAA (cost effective) | Apache Beam&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Data Warehouse =&lt;/strong&gt; Snowflake or BigQuery | For Cloud = Amazon S3&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Data Lakes =&lt;/strong&gt; DataBricks, Redshift&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Data Transformation or Data Quality Control testing or Parallel Processing Tools =&lt;/strong&gt; dbt, Pandas, Apache Spark, &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 BI =&lt;/strong&gt; Power BI&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Exploratory Data Analysis =&lt;/strong&gt; Snowflake, Jupyter, Alteryx, Snowflake Snowsight&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Scaling Workers =&lt;/strong&gt; aws lambdas, kubernetes&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Devops Methods =&lt;/strong&gt; Observability, alerting, incident management, ci/cd and good pr hygiene apply as much to DE as regular backend SE&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Stream Processing =&lt;/strong&gt; Apache Kafka,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 AWS Services =&lt;/strong&gt; RDS (to setup and scale db in cloud)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15adu9z", "is_robot_indexable": true, "report_reasons": null, "author": "Pillstyr", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15adu9z/how_would_you_learn_de_if_you_had_to_start_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15adu9z/how_would_you_learn_de_if_you_had_to_start_over/", "subreddit_subscribers": 118416, "created_utc": 1690396515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Reddit community!\r  \n\r  \nI came across this insightful blog post on SG Analytics about \"Data Trust: How to Build Data Trust.\" I thought it might be of great interest to all the data enthusiasts and professionals here, so I wanted to share it with you all.\r  \n\r  \n**URL:** [***Data Trust: How to Build Data Trust***](https://www.sganalytics.com/blog/data-trust-how-to-build-data-trust/)\r  \n\r  \nIn this article, SG Analytics discusses the essential steps and strategies for establishing data trust in any organization. With the increasing reliance on data-driven decision-making, ensuring data integrity and trustworthiness is crucial. The post covers topics like data governance, security measures, data quality, and more, making it a valuable resource for anyone dealing with data management.\r  \n\r  \nI found the insights in this blog post to be quite helpful, and I believe it can spark interesting discussions within our community. So take a look, and feel free to share your thoughts or experiences related to data trust and how you or your organization ensures the credibility of your data.\r  \n\r  \nLet's dive into the world of data trust and explore ways to harness the power of data while maintaining its accuracy and reliability! Happy reading!", "author_fullname": "t2_t1lpdh2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trust the Data: A Guide to Building Data Trust in Your Organization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15b0869", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690459849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Reddit community!&lt;/p&gt;\n\n&lt;p&gt;I came across this insightful blog post on SG Analytics about &amp;quot;Data Trust: How to Build Data Trust.&amp;quot; I thought it might be of great interest to all the data enthusiasts and professionals here, so I wanted to share it with you all.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;URL:&lt;/strong&gt; &lt;a href=\"https://www.sganalytics.com/blog/data-trust-how-to-build-data-trust/\"&gt;&lt;strong&gt;&lt;em&gt;Data Trust: How to Build Data Trust&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In this article, SG Analytics discusses the essential steps and strategies for establishing data trust in any organization. With the increasing reliance on data-driven decision-making, ensuring data integrity and trustworthiness is crucial. The post covers topics like data governance, security measures, data quality, and more, making it a valuable resource for anyone dealing with data management.&lt;/p&gt;\n\n&lt;p&gt;I found the insights in this blog post to be quite helpful, and I believe it can spark interesting discussions within our community. So take a look, and feel free to share your thoughts or experiences related to data trust and how you or your organization ensures the credibility of your data.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s dive into the world of data trust and explore ways to harness the power of data while maintaining its accuracy and reliability! Happy reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15b0869", "is_robot_indexable": true, "report_reasons": null, "author": "David_starc150", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b0869/trust_the_data_a_guide_to_building_data_trust_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b0869/trust_the_data_a_guide_to_building_data_trust_in/", "subreddit_subscribers": 118416, "created_utc": 1690459849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "50-60GB database on aurora, everythings fine on writes/updates and transactional operations. We hooked the db to be used as analytics dashboard pulling a fair amount of data but disk i/o is extremely slow from query plan around 5-15mb/s. Bumping up instance ram and loading all working database into shared_buffers cache everything works fine. So I came up to the following conclusions:\n\n- aws aurora is basically unusable for olap workloads as i/o timings are terribly slow (and couldn't find an official figure from aws)\n- aws aurora storage layer uses a storage attached network observed disk i/o is very low and there is no OS level cache so they recommend loading up all databse in memory\n- does aws rds postgres with provisioned ssd suffer from the same problem? iops and throughput are 100x better on paper\n- anyone had similar experiences with aurora and working with relatively large datasets?\n- considering moving to redshift", "author_fullname": "t2_b5vlra5h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Aurora not usable for OLAP workloads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15agi5q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690402621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;50-60GB database on aurora, everythings fine on writes/updates and transactional operations. We hooked the db to be used as analytics dashboard pulling a fair amount of data but disk i/o is extremely slow from query plan around 5-15mb/s. Bumping up instance ram and loading all working database into shared_buffers cache everything works fine. So I came up to the following conclusions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;aws aurora is basically unusable for olap workloads as i/o timings are terribly slow (and couldn&amp;#39;t find an official figure from aws)&lt;/li&gt;\n&lt;li&gt;aws aurora storage layer uses a storage attached network observed disk i/o is very low and there is no OS level cache so they recommend loading up all databse in memory&lt;/li&gt;\n&lt;li&gt;does aws rds postgres with provisioned ssd suffer from the same problem? iops and throughput are 100x better on paper&lt;/li&gt;\n&lt;li&gt;anyone had similar experiences with aurora and working with relatively large datasets?&lt;/li&gt;\n&lt;li&gt;considering moving to redshift&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15agi5q", "is_robot_indexable": true, "report_reasons": null, "author": "golangcafe", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15agi5q/aws_aurora_not_usable_for_olap_workloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15agi5q/aws_aurora_not_usable_for_olap_workloads/", "subreddit_subscribers": 118416, "created_utc": 1690402621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nSQL query tuning and optimization.\n\nSQL indexing.\n\nSQL integration.\n\nSQL reporting.\n\nSQL execution plans.", "author_fullname": "t2_97jhjawv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the right learning order for these SQL subjects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15a94v3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690385880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;SQL query tuning and optimization.&lt;/p&gt;\n\n&lt;p&gt;SQL indexing.&lt;/p&gt;\n\n&lt;p&gt;SQL integration.&lt;/p&gt;\n\n&lt;p&gt;SQL reporting.&lt;/p&gt;\n\n&lt;p&gt;SQL execution plans.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15a94v3", "is_robot_indexable": true, "report_reasons": null, "author": "TreatOk8778", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15a94v3/whats_the_right_learning_order_for_these_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15a94v3/whats_the_right_learning_order_for_these_sql/", "subreddit_subscribers": 118416, "created_utc": 1690385880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "dbt on Spark vs dbt on Trino\n\nHi, we're running a on-prem data platform solution with stacks of **Iceberg** (parquet), **Hive**, **Spark**, **MINIO**, etc. and planning to use **dbt** for our **SQL transformation** of tabular data (Dim Fact modeling, Data Mart, etc.), after the data has been processed by Spark (Ingestion, parsing, etc.). The processing data size is usually around 10-100s million records per batch\n\n* Should we use dbt on Spark or dbt on Trino. Our use cases are ETL, so what is the difference between the twos, regarding the performance, scalability, feature, etc.? \n   * I don't have much experience with Trino, so I'm not sure if we need to process heavy jobs, will Trino perform as good as Spark, given the same resources. Has anyone use Trino as the processing tool for ETL yet, rather than just a query engine for adhoc/interactive query? I've seen many blogs from Starburst of using Trino as a processing engine for ETL, but not sure if it's true.\n* For dbt on Spark, what should be the setup? Spark Thrift (default Spark Thrift Server or Apache Kyuubi?), Spark HTTP (using Apache Livy) or any other options? \n   * Spark Thrift: Spark Thrift Server seems to have many issues mentioned here ([https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi\\_vs\\_thriftserver.html#kyuubi-vs-spark-thrift-server](https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi_vs_thriftserver.html#kyuubi-vs-spark-thrift-server)), which make us think Apache Kyuubi would be a good choice. However, we couldn't find any document/post related to running dbt on Spark Apache Kyuubi\n   * dbt livy: provided by Cloudera, with very few updates. The Apache Livy project doesn't have much update recently, so I'm afraid we will go into another dead end with this approach\n\n&amp;#x200B;", "author_fullname": "t2_4j9omvnso", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Spark vs dbt Trino", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15av4jc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690443143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;dbt on Spark vs dbt on Trino&lt;/p&gt;\n\n&lt;p&gt;Hi, we&amp;#39;re running a on-prem data platform solution with stacks of &lt;strong&gt;Iceberg&lt;/strong&gt; (parquet), &lt;strong&gt;Hive&lt;/strong&gt;, &lt;strong&gt;Spark&lt;/strong&gt;, &lt;strong&gt;MINIO&lt;/strong&gt;, etc. and planning to use &lt;strong&gt;dbt&lt;/strong&gt; for our &lt;strong&gt;SQL transformation&lt;/strong&gt; of tabular data (Dim Fact modeling, Data Mart, etc.), after the data has been processed by Spark (Ingestion, parsing, etc.). The processing data size is usually around 10-100s million records per batch&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Should we use dbt on Spark or dbt on Trino. Our use cases are ETL, so what is the difference between the twos, regarding the performance, scalability, feature, etc.? \n\n&lt;ul&gt;\n&lt;li&gt;I don&amp;#39;t have much experience with Trino, so I&amp;#39;m not sure if we need to process heavy jobs, will Trino perform as good as Spark, given the same resources. Has anyone use Trino as the processing tool for ETL yet, rather than just a query engine for adhoc/interactive query? I&amp;#39;ve seen many blogs from Starburst of using Trino as a processing engine for ETL, but not sure if it&amp;#39;s true.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;For dbt on Spark, what should be the setup? Spark Thrift (default Spark Thrift Server or Apache Kyuubi?), Spark HTTP (using Apache Livy) or any other options? \n\n&lt;ul&gt;\n&lt;li&gt;Spark Thrift: Spark Thrift Server seems to have many issues mentioned here (&lt;a href=\"https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi_vs_thriftserver.html#kyuubi-vs-spark-thrift-server\"&gt;https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi_vs_thriftserver.html#kyuubi-vs-spark-thrift-server&lt;/a&gt;), which make us think Apache Kyuubi would be a good choice. However, we couldn&amp;#39;t find any document/post related to running dbt on Spark Apache Kyuubi&lt;/li&gt;\n&lt;li&gt;dbt livy: provided by Cloudera, with very few updates. The Apache Livy project doesn&amp;#39;t have much update recently, so I&amp;#39;m afraid we will go into another dead end with this approach&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "15av4jc", "is_robot_indexable": true, "report_reasons": null, "author": "chuqbach", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15av4jc/dbt_spark_vs_dbt_trino/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15av4jc/dbt_spark_vs_dbt_trino/", "subreddit_subscribers": 118416, "created_utc": 1690443143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Disclaimer: I have an okay amount of knowledge in containerization and docker, but certainly still learning.\n\nCurrently, I am developing an ETL job as a Docker application that reads parquet data from a data lake, performs transformations, and then loads the processed data back into the data lake. This process is basically read silver layer -&gt; performing transformations -&gt; load to gold layer. The ETL task handles new data that has been ingested within the last minute, where a message queue tells the ETL task which parquet file needs processed. To facilitate the transformations, some data needs to be persisted in an application database, enabling lookups during the transformation process. For instance, the database is used to join previously stored header data with new process data or to perform recursive lookups through parent-child relationships, seeking the \"origin\" for specific IDs. \n\nI'm treating it like a standard application architecture, with the ETL process acting as the \"front end,\" and the database holding the persisted data serving as the \"back end.\" The ETL front end, which is implemented in Python, reads the new data and executes multiple SQL queries against the back-end database (while also writing to it). These queries are needed to perform the necessary transformations. For the deployment, I intend to utilize Docker Compose and manage it as a multi-container application in production, incorporating a PostgreSQL database volume mount. \n\n Now, my primary concern is how to effectively test and develop the SQL queries that the ETL task \"front end\" container needs to execute on the PostgreSQL database backend, given that the database doesn't exist until after the application is up and running. I can't just randomly write the SQL queries throughout the python application and hope they will all work when I actually build and run the container. Just can't seem to wrap my head around this one. \n\nHope this makes sense, I can clarify and makes edits if needed. Thanks! ", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-Container Docker Application for ETL Task", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15arvak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690432327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer: I have an okay amount of knowledge in containerization and docker, but certainly still learning.&lt;/p&gt;\n\n&lt;p&gt;Currently, I am developing an ETL job as a Docker application that reads parquet data from a data lake, performs transformations, and then loads the processed data back into the data lake. This process is basically read silver layer -&amp;gt; performing transformations -&amp;gt; load to gold layer. The ETL task handles new data that has been ingested within the last minute, where a message queue tells the ETL task which parquet file needs processed. To facilitate the transformations, some data needs to be persisted in an application database, enabling lookups during the transformation process. For instance, the database is used to join previously stored header data with new process data or to perform recursive lookups through parent-child relationships, seeking the &amp;quot;origin&amp;quot; for specific IDs. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m treating it like a standard application architecture, with the ETL process acting as the &amp;quot;front end,&amp;quot; and the database holding the persisted data serving as the &amp;quot;back end.&amp;quot; The ETL front end, which is implemented in Python, reads the new data and executes multiple SQL queries against the back-end database (while also writing to it). These queries are needed to perform the necessary transformations. For the deployment, I intend to utilize Docker Compose and manage it as a multi-container application in production, incorporating a PostgreSQL database volume mount. &lt;/p&gt;\n\n&lt;p&gt;Now, my primary concern is how to effectively test and develop the SQL queries that the ETL task &amp;quot;front end&amp;quot; container needs to execute on the PostgreSQL database backend, given that the database doesn&amp;#39;t exist until after the application is up and running. I can&amp;#39;t just randomly write the SQL queries throughout the python application and hope they will all work when I actually build and run the container. Just can&amp;#39;t seem to wrap my head around this one. &lt;/p&gt;\n\n&lt;p&gt;Hope this makes sense, I can clarify and makes edits if needed. Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15arvak", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15arvak/multicontainer_docker_application_for_etl_task/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15arvak/multicontainer_docker_application_for_etl_task/", "subreddit_subscribers": 118416, "created_utc": 1690432327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_15ar04s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Q3g8NFhvddk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Q3g8NFhvddk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)\"&gt;&lt;/iframe&gt;", "author_name": "Dremio", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Q3g8NFhvddk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Dremio"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Q3g8NFhvddk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/15ar04s", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/UTHqpTlFHrrrFREZFZTlXAqR4y13Ig3P1bFS37PHAVQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690429699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/Q3g8NFhvddk", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YEosjDQSPs6x5ZEcqj1prjh4vJ4qmRJDPfsj24Cv3cQ.jpg?auto=webp&amp;s=58f6eb9c6da7e629e8d7d9552a11c526550b142f", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/YEosjDQSPs6x5ZEcqj1prjh4vJ4qmRJDPfsj24Cv3cQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=093b1485febcece41dbb6ab0d6c431d65c795dd9", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/YEosjDQSPs6x5ZEcqj1prjh4vJ4qmRJDPfsj24Cv3cQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5c3d43d042ab2f713542f19a17bdfa229b03d4a0", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/YEosjDQSPs6x5ZEcqj1prjh4vJ4qmRJDPfsj24Cv3cQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=88609eb050a6bf25a93e32b82ce82244d89e5855", "width": 320, "height": 240}], "variants": {}, "id": "8_alXOI9EvUESL8dnHyGGGOsh0IN9L4WwQtx6q-hGZk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15ar04s", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ar04s/ep26_versioning_data_in_the_data_lakehouse_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/Q3g8NFhvddk", "subreddit_subscribers": 118416, "created_utc": 1690429699.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Q3g8NFhvddk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)\"&gt;&lt;/iframe&gt;", "author_name": "Dremio", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Q3g8NFhvddk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Dremio"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an API endpoint that writes to a postgres database on each call. The writes are insert only, rows are not mutated, and can be partitioned by their write time.\n\nUltimately I want to replicate the database in Snowflake via S3, and I don't want to use something like Fivetran.\n\nS3 to Snowflake is straightforward enough. We would do this on a 30 minute schedule running a COPY INTO statement.\n\nThe decision I am stuck on is how to do Postgres to S3.\n\nAs far as I can tell, I have two options:\n\n- Copy directly from Postgres to s3 with the aws_s3 postgres extension as part of the batch job\n- Write to s3 after each API call as a background task in the app\n\nI'm unclear on the best option to go with.\n\nArguments in favor of aws_s3 extension: Copying directly from Postgres is safe for data integrity since the app code's transactions are atomic and reliable, whereas the background job can potentially fail even if a request and transaction succeeds. It also leads to simpler infrastructure. (I'd rather push more logic into batch jobs than the app code.)\n\nArguments in favor of app dumping directly to S3 from the app: Mainly I'm worried about having to manage performance of the Postgres instance. Performance wise, I'd rather just scale up the number of web app instances than have the Postgres database take a hit copying 30 minutes of data at a time.\n\nAny thoughts? What's the best way to go about this? I'm leaning toward the aws_s3 Postgres extension, but I'm not 100% sure.", "author_fullname": "t2_umwzpce8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "API -&gt; Postgres -&gt; S3 -- what is the best design pattern?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15amy4f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690418191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an API endpoint that writes to a postgres database on each call. The writes are insert only, rows are not mutated, and can be partitioned by their write time.&lt;/p&gt;\n\n&lt;p&gt;Ultimately I want to replicate the database in Snowflake via S3, and I don&amp;#39;t want to use something like Fivetran.&lt;/p&gt;\n\n&lt;p&gt;S3 to Snowflake is straightforward enough. We would do this on a 30 minute schedule running a COPY INTO statement.&lt;/p&gt;\n\n&lt;p&gt;The decision I am stuck on is how to do Postgres to S3.&lt;/p&gt;\n\n&lt;p&gt;As far as I can tell, I have two options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Copy directly from Postgres to s3 with the aws_s3 postgres extension as part of the batch job&lt;/li&gt;\n&lt;li&gt;Write to s3 after each API call as a background task in the app&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m unclear on the best option to go with.&lt;/p&gt;\n\n&lt;p&gt;Arguments in favor of aws_s3 extension: Copying directly from Postgres is safe for data integrity since the app code&amp;#39;s transactions are atomic and reliable, whereas the background job can potentially fail even if a request and transaction succeeds. It also leads to simpler infrastructure. (I&amp;#39;d rather push more logic into batch jobs than the app code.)&lt;/p&gt;\n\n&lt;p&gt;Arguments in favor of app dumping directly to S3 from the app: Mainly I&amp;#39;m worried about having to manage performance of the Postgres instance. Performance wise, I&amp;#39;d rather just scale up the number of web app instances than have the Postgres database take a hit copying 30 minutes of data at a time.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts? What&amp;#39;s the best way to go about this? I&amp;#39;m leaning toward the aws_s3 Postgres extension, but I&amp;#39;m not 100% sure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15amy4f", "is_robot_indexable": true, "report_reasons": null, "author": "riv3rtrip", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15amy4f/api_postgres_s3_what_is_the_best_design_pattern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15amy4f/api_postgres_s3_what_is_the_best_design_pattern/", "subreddit_subscribers": 118416, "created_utc": 1690418191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2cbhndmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is streaming SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_15al69d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Re0FrSKMKH7YzlSDIlhfvarfaaPUaILjQFPTJNoxxhQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690413596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arroyo.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.arroyo.dev/blog/streaming-sql-explained", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?auto=webp&amp;s=60cfb566bad70b9f2446d482dd485a7ce91f2f6a", "width": 4000, "height": 2250}, "resolutions": [{"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f8eecfb2808009fd734c4760b8b50c68a6d1aaa2", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1775b73296b89801e089b6019bae3538a3513fc1", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25901a5d50ac17e2eb59f6d17798ef81011aec78", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f31c8d445f2be0eec7b36cbc3f7c1d8aa6fdc1f5", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d643256c055bda777359846c4365c23c6a18a6c", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=feab0d107309110d3ed7f808565aa6c0ce73add5", "width": 1080, "height": 607}], "variants": {}, "id": "6IvzkDLHQGPnq8tOrsS9svvlhoQh6aVrBV8elvIdwPc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15al69d", "is_robot_indexable": true, "report_reasons": null, "author": "mwylde_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15al69d/what_is_streaming_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.arroyo.dev/blog/streaming-sql-explained", "subreddit_subscribers": 118416, "created_utc": 1690413596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org uses kafka to ingest data from source teams into snowflake, but then uses dbt + airflow to do nearly every model that happens downstream of raw data. I had a recent conversation with a coworker about how it's kindof a shame that we have real-time data, but don't actually serve it on our data platform except for once or a few times per day. \n\nIs it generally recommended to push everything to stream processing (if it can be)? \n\nFor example, suppose you're a retail company and a typical use case could be serving data for downstream about the price of products. You might maintain: \n\n* the latest price of the product in a dimensional table - `dim_product_current`\n* the historical prices of the product in a type 2 scd history table - `dim_product_history`\n* a daily snapshot of the prices for all of your products on a given day\n\nOf the three, the current use-case seems most appropriate (and easiest to implement) for real-time. Does it even make sense to enable maintaining scd type 2 for streaming data?\n\nCurious how other teams/orgs approach these problems", "author_fullname": "t2_5e0ue", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming vs Batch modeling discussion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ak8si", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690411292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org uses kafka to ingest data from source teams into snowflake, but then uses dbt + airflow to do nearly every model that happens downstream of raw data. I had a recent conversation with a coworker about how it&amp;#39;s kindof a shame that we have real-time data, but don&amp;#39;t actually serve it on our data platform except for once or a few times per day. &lt;/p&gt;\n\n&lt;p&gt;Is it generally recommended to push everything to stream processing (if it can be)? &lt;/p&gt;\n\n&lt;p&gt;For example, suppose you&amp;#39;re a retail company and a typical use case could be serving data for downstream about the price of products. You might maintain: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the latest price of the product in a dimensional table - &lt;code&gt;dim_product_current&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;the historical prices of the product in a type 2 scd history table - &lt;code&gt;dim_product_history&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;a daily snapshot of the prices for all of your products on a given day&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Of the three, the current use-case seems most appropriate (and easiest to implement) for real-time. Does it even make sense to enable maintaining scd type 2 for streaming data?&lt;/p&gt;\n\n&lt;p&gt;Curious how other teams/orgs approach these problems&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15ak8si", "is_robot_indexable": true, "report_reasons": null, "author": "harrytrumanprimate", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ak8si/streaming_vs_batch_modeling_discussion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ak8si/streaming_vs_batch_modeling_discussion/", "subreddit_subscribers": 118416, "created_utc": 1690411292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't know if anyone else was confused about the multitude of ETL tools out there.  With the help of ChatGPT I came up with this table.  It really helped turn a light bulb on for me.  It would be great if there could be a public list like this that we could keep updated.\n\nEdit: updated table to add Airbyte\n\n&amp;#x200B;\n\n|Product Name|Description|Specialization|\n|:-|:-|:-|\n|dbt|dbt (data build tool) is a popular open-source data transformation tool that enables data analysts and engineers to transform, analyze, and document data.|Transform|\n|Apache Airflow|Apache Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. It is commonly used for orchestrating data pipelines.|Orchestrator|\n|Apache Spark|Apache Spark is an open-source distributed data processing engine that provides fast and general-purpose cluster computing for big data processing.|Transform|\n|Apache Kafka|Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and streaming applications.|Extract, Transform|\n|Talend|Talend is a popular data integration and data quality platform that supports data engineering, data integration, data profiling, and more.|Extract, Transform, Load|\n|Stitch|Stitch is a cloud-based data integration service that replicates data from various sources into data warehouses for analytics purposes.|Extract, Load|\n|Fivetran|Fivetran is a fully managed data integration service that continuously syncs data from source systems to data warehouses, making it easy to centralize data.|Extract, Load|\n|AWS Glue|AWS Glue is a fully managed extract, transform, and load (ETL) service provided by Amazon Web Services, making it easy to prepare and load data for analytics.|Extract, Transform, Load|\n|Google Cloud Dataflow|Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines. It supports both batch and stream processing on Google Cloud Platform.|Transform|\n|Databricks|Databricks is a unified data analytics platform that provides a collaborative workspace for data engineering and data science tasks, built on Apache Spark.\u00a0\u00a0|Transform|\n|Matillion|Matillion is a cloud-native ETL and data integration platform that simplifies the process of loading, transforming, and joining data on cloud data warehouses.|Extract, Transform, Load|\n|Airbyte|Open source ELT tool.   Full table and incremental via change data capture. Integrate deeply with Kubernetes, Airflow, Dagster, Prefect, and dbt.\u00a0 |ELT|\n\n^(Table) ^(formatting) ^(brought) ^(to) ^(you) ^(by) [^(ExcelToReddit)](https://xl2reddit.github.io/)", "author_fullname": "t2_j15uu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "List of ETL tools, short description, and their primary purpose", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15aeh6s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690404884.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690398006.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know if anyone else was confused about the multitude of ETL tools out there.  With the help of ChatGPT I came up with this table.  It really helped turn a light bulb on for me.  It would be great if there could be a public list like this that we could keep updated.&lt;/p&gt;\n\n&lt;p&gt;Edit: updated table to add Airbyte&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Product Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Description&lt;/th&gt;\n&lt;th align=\"left\"&gt;Specialization&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dbt&lt;/td&gt;\n&lt;td align=\"left\"&gt;dbt (data build tool) is a popular open-source data transformation tool that enables data analysts and engineers to transform, analyze, and document data.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Transform&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Apache Airflow&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. It is commonly used for orchestrating data pipelines.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Orchestrator&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Apache Spark&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache Spark is an open-source distributed data processing engine that provides fast and general-purpose cluster computing for big data processing.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Transform&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Apache Kafka&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and streaming applications.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Extract, Transform&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Talend&lt;/td&gt;\n&lt;td align=\"left\"&gt;Talend is a popular data integration and data quality platform that supports data engineering, data integration, data profiling, and more.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Extract, Transform, Load&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Stitch&lt;/td&gt;\n&lt;td align=\"left\"&gt;Stitch is a cloud-based data integration service that replicates data from various sources into data warehouses for analytics purposes.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Extract, Load&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Fivetran&lt;/td&gt;\n&lt;td align=\"left\"&gt;Fivetran is a fully managed data integration service that continuously syncs data from source systems to data warehouses, making it easy to centralize data.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Extract, Load&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;AWS Glue&lt;/td&gt;\n&lt;td align=\"left\"&gt;AWS Glue is a fully managed extract, transform, and load (ETL) service provided by Amazon Web Services, making it easy to prepare and load data for analytics.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Extract, Transform, Load&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Google Cloud Dataflow&lt;/td&gt;\n&lt;td align=\"left\"&gt;Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines. It supports both batch and stream processing on Google Cloud Platform.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Transform&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Databricks&lt;/td&gt;\n&lt;td align=\"left\"&gt;Databricks is a unified data analytics platform that provides a collaborative workspace for data engineering and data science tasks, built on Apache Spark.\u00a0\u00a0&lt;/td&gt;\n&lt;td align=\"left\"&gt;Transform&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Matillion&lt;/td&gt;\n&lt;td align=\"left\"&gt;Matillion is a cloud-native ETL and data integration platform that simplifies the process of loading, transforming, and joining data on cloud data warehouses.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Extract, Transform, Load&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Airbyte&lt;/td&gt;\n&lt;td align=\"left\"&gt;Open source ELT tool.   Full table and incremental via change data capture. Integrate deeply with Kubernetes, Airflow, Dagster, Prefect, and dbt.\u00a0&lt;/td&gt;\n&lt;td align=\"left\"&gt;ELT&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;sup&gt;Table&lt;/sup&gt; &lt;sup&gt;formatting&lt;/sup&gt; &lt;sup&gt;brought&lt;/sup&gt; &lt;sup&gt;to&lt;/sup&gt; &lt;sup&gt;you&lt;/sup&gt; &lt;sup&gt;by&lt;/sup&gt; &lt;a href=\"https://xl2reddit.github.io/\"&gt;&lt;sup&gt;ExcelToReddit&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15aeh6s", "is_robot_indexable": true, "report_reasons": null, "author": "jbrune", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15aeh6s/list_of_etl_tools_short_description_and_their/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15aeh6s/list_of_etl_tools_short_description_and_their/", "subreddit_subscribers": 118416, "created_utc": 1690398006.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Seems like Redshift would be good to learn, but the pricing I see on it scares the c#@p out of me. Does anyone know a good tutorial which also includes how much will be spent by following along and also shows thorough instructions on how to shut down and destroy the resources when finished?", "author_fullname": "t2_io9vf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scared of Redshift - Any Good Resources to Learn with Low Cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15acq1h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690394010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seems like Redshift would be good to learn, but the pricing I see on it scares the c#@p out of me. Does anyone know a good tutorial which also includes how much will be spent by following along and also shows thorough instructions on how to shut down and destroy the resources when finished?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15acq1h", "is_robot_indexable": true, "report_reasons": null, "author": "Scalar_Mikeman", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15acq1h/scared_of_redshift_any_good_resources_to_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15acq1h/scared_of_redshift_any_good_resources_to_learn/", "subreddit_subscribers": 118416, "created_utc": 1690394010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm assisting a non-profit in a food supply chain project and wanting to understand the cheapest, most efficient way to track the lat/longs of their trucks over time, including real-time. Only about \\~10 trucks and we don't need anything fancy. Just truck+datetime+lat+long.\n\nWhat hardware/tools would be cheapest to obtain this type of information? Options I've considered, but each has their downsides:\n\n1. My mind went to Airtags first for simplicity purposes, but accessing history seems possibly achievable, but frustrating.\n2. Verizon Connect Fleet Tracking - Ability to access historical data, but I didn't see lat/long in their video. Also probably too expensive, but they're too vague on pricing on their site.\n3. A quick \"GPS Fleet Tracking\" search yields a ton of different company offerings, but many seem like overkill (and would likely price as such). I'm looking for the simplest offering, not the most robust.\n\nFor what it's worth, I'm relatively new to data engineering and brand new to ingestion of lat/longs, so apologies if this is a stupid question!", "author_fullname": "t2_pmps7fj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easiest way to ingest lat/long data via GPS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15a8yck", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690385459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m assisting a non-profit in a food supply chain project and wanting to understand the cheapest, most efficient way to track the lat/longs of their trucks over time, including real-time. Only about ~10 trucks and we don&amp;#39;t need anything fancy. Just truck+datetime+lat+long.&lt;/p&gt;\n\n&lt;p&gt;What hardware/tools would be cheapest to obtain this type of information? Options I&amp;#39;ve considered, but each has their downsides:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;My mind went to Airtags first for simplicity purposes, but accessing history seems possibly achievable, but frustrating.&lt;/li&gt;\n&lt;li&gt;Verizon Connect Fleet Tracking - Ability to access historical data, but I didn&amp;#39;t see lat/long in their video. Also probably too expensive, but they&amp;#39;re too vague on pricing on their site.&lt;/li&gt;\n&lt;li&gt;A quick &amp;quot;GPS Fleet Tracking&amp;quot; search yields a ton of different company offerings, but many seem like overkill (and would likely price as such). I&amp;#39;m looking for the simplest offering, not the most robust.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For what it&amp;#39;s worth, I&amp;#39;m relatively new to data engineering and brand new to ingestion of lat/longs, so apologies if this is a stupid question!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15a8yck", "is_robot_indexable": true, "report_reasons": null, "author": "hornfan87", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15a8yck/easiest_way_to_ingest_latlong_data_via_gps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15a8yck/easiest_way_to_ingest_latlong_data_via_gps/", "subreddit_subscribers": 118416, "created_utc": 1690385459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are currently working on replacing one of our online management systems with a new one and one of the requirements for the migration is to sort out how we are going to do this on a DW side.\n\nI know that my role is to figure out what the ETL/ELT process is and make sure that the data pipelines are feeding in but the biggest concern I have is understanding whether my job involves getting the business logic for field transformation directly or whether this is something which the BI team should do ? I've worked as a BI analyst in the business and I've always thought that this should be a BI job to understand how each column is calculated at least on a high end level and then work with the DE to figure out whether there's any extra details to any columns. One of the arguments for it being a DE job for business logic requirements is that there is already backend logic to this which I think is fair but not all the column is mapped 1-1 and the systems totally different.\n\nHaven't asked my manager but was hoping to grab some 2nd opinions before this is raised up as it's definitely making me frustrated. As context I've work in BI for 4 years and DE for 2.", "author_fullname": "t2_2ahdonrr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE or BI to figure out business logic requirements ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15a7d7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690381714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently working on replacing one of our online management systems with a new one and one of the requirements for the migration is to sort out how we are going to do this on a DW side.&lt;/p&gt;\n\n&lt;p&gt;I know that my role is to figure out what the ETL/ELT process is and make sure that the data pipelines are feeding in but the biggest concern I have is understanding whether my job involves getting the business logic for field transformation directly or whether this is something which the BI team should do ? I&amp;#39;ve worked as a BI analyst in the business and I&amp;#39;ve always thought that this should be a BI job to understand how each column is calculated at least on a high end level and then work with the DE to figure out whether there&amp;#39;s any extra details to any columns. One of the arguments for it being a DE job for business logic requirements is that there is already backend logic to this which I think is fair but not all the column is mapped 1-1 and the systems totally different.&lt;/p&gt;\n\n&lt;p&gt;Haven&amp;#39;t asked my manager but was hoping to grab some 2nd opinions before this is raised up as it&amp;#39;s definitely making me frustrated. As context I&amp;#39;ve work in BI for 4 years and DE for 2.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15a7d7y", "is_robot_indexable": true, "report_reasons": null, "author": "taafpxd", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15a7d7y/de_or_bi_to_figure_out_business_logic_requirements/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15a7d7y/de_or_bi_to_figure_out_business_logic_requirements/", "subreddit_subscribers": 118416, "created_utc": 1690381714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_kgkprqpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Observability: The Next Frontier of Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15a6h78", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1690379554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dasca.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dasca.org/world-of-big-data/article/data-observability-the-next-frontier-of-data-engineering", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15a6h78", "is_robot_indexable": true, "report_reasons": null, "author": "Emily-joe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15a6h78/data_observability_the_next_frontier_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dasca.org/world-of-big-data/article/data-observability-the-next-frontier-of-data-engineering", "subreddit_subscribers": 118416, "created_utc": 1690379554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nWe are doing a POC for DB right now and it's going pretty well.  One thing though is we don't have access to the photon engine for querying, only as the executor for Spark SQL.  It's been performant compared to what we have now, but I guess I get paranoid when someone tells me I can't see something they want me to buy.  \n\nMy concern is more around whether there is lag or hanging at all when doing SQL or the BI layer based on some of the behavior I saw with the Spark side.  About 80% of our usage is through the BI tools so that is significant.  Has anyone had any issues with that or any other issues with Photon you think I should be aware of?\n\nAppreciate any feedback.", "author_fullname": "t2_117fpt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Photon engine question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15b159i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690462447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;We are doing a POC for DB right now and it&amp;#39;s going pretty well.  One thing though is we don&amp;#39;t have access to the photon engine for querying, only as the executor for Spark SQL.  It&amp;#39;s been performant compared to what we have now, but I guess I get paranoid when someone tells me I can&amp;#39;t see something they want me to buy.  &lt;/p&gt;\n\n&lt;p&gt;My concern is more around whether there is lag or hanging at all when doing SQL or the BI layer based on some of the behavior I saw with the Spark side.  About 80% of our usage is through the BI tools so that is significant.  Has anyone had any issues with that or any other issues with Photon you think I should be aware of?&lt;/p&gt;\n\n&lt;p&gt;Appreciate any feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15b159i", "is_robot_indexable": true, "report_reasons": null, "author": "Gators1992", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b159i/databricks_photon_engine_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b159i/databricks_photon_engine_question/", "subreddit_subscribers": 118416, "created_utc": 1690462447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my new position, I am putting together robotic workflows to pump out experimental data on multiple local hard drives in my lab. Unfortunately, this new lab had not previously put much effort into standardizing their sample and experimental data structures and are manually moving output files and copypasting them together in spreadsheets to analyze them. Additionally, i am the only one with the experience needed to standardize the data structures, write scripts, and come up with a strategy to replace the manual file transfers, copypasting and spreadsheets.  Together with my actual position as automation engineer, its a lot of work for one person. \n\nIn a past job, I was on a team of data scientists/engineers that set up a stack with a dedicated network PC monitoring all the lab hard drives for new files to trigger file transfers to Google Drive (or Cloud Storage, not sure), development/testing in Saturn Cloud, pipelines stored in GitHub and set up to trigger Google Cloudbuilds on commits of new .yaml files, with data being stored in Google Drive (may have been Google Cloud Storage).\n\nTheres no way i can manage to set all that stuff up myself and also do the job i was hired for. I also doubt that at this time, my current company needs something that sophistocated as we are doing basic and predictable data transformations, merging, and analyses of relatively small data sets. \n\nIt seems like if i set up local network monitoring to send data to Google Drive/Cloud Storage, and develop/test scripts in Google Colab for Google Cloud Functions to trigger when the new data lands in Drive/Storage, that might be a relatively simple route to ultimately cure my lab of manual file transfers, transformations, and analyses. \n\nI'd love any input regarding limitations of this general approach, whether its completely off base and im missing some glaring detail, or if anyone has an alternative suggestion for a simple way to acheive this goal. For the record, i am proficient in R and Python. Thanks in advance!", "author_fullname": "t2_fwinz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Assistance needed - method for simple automated data ingestion/analyses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15b0t80", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690461498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my new position, I am putting together robotic workflows to pump out experimental data on multiple local hard drives in my lab. Unfortunately, this new lab had not previously put much effort into standardizing their sample and experimental data structures and are manually moving output files and copypasting them together in spreadsheets to analyze them. Additionally, i am the only one with the experience needed to standardize the data structures, write scripts, and come up with a strategy to replace the manual file transfers, copypasting and spreadsheets.  Together with my actual position as automation engineer, its a lot of work for one person. &lt;/p&gt;\n\n&lt;p&gt;In a past job, I was on a team of data scientists/engineers that set up a stack with a dedicated network PC monitoring all the lab hard drives for new files to trigger file transfers to Google Drive (or Cloud Storage, not sure), development/testing in Saturn Cloud, pipelines stored in GitHub and set up to trigger Google Cloudbuilds on commits of new .yaml files, with data being stored in Google Drive (may have been Google Cloud Storage).&lt;/p&gt;\n\n&lt;p&gt;Theres no way i can manage to set all that stuff up myself and also do the job i was hired for. I also doubt that at this time, my current company needs something that sophistocated as we are doing basic and predictable data transformations, merging, and analyses of relatively small data sets. &lt;/p&gt;\n\n&lt;p&gt;It seems like if i set up local network monitoring to send data to Google Drive/Cloud Storage, and develop/test scripts in Google Colab for Google Cloud Functions to trigger when the new data lands in Drive/Storage, that might be a relatively simple route to ultimately cure my lab of manual file transfers, transformations, and analyses. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love any input regarding limitations of this general approach, whether its completely off base and im missing some glaring detail, or if anyone has an alternative suggestion for a simple way to acheive this goal. For the record, i am proficient in R and Python. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15b0t80", "is_robot_indexable": true, "report_reasons": null, "author": "Pelecabra", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b0t80/assistance_needed_method_for_simple_automated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b0t80/assistance_needed_method_for_simple_automated/", "subreddit_subscribers": 118416, "created_utc": 1690461498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm playing with Spark Thrift Server and want to save a dataframe to it via the JDBC port it opens, but I'm facing an error with the SQL code generated by Spark.  \n\n\nI've started STS with `spark-submit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --name \"Thrift JDBC/ODBC Server\"` and I'm using spark-shell (started with `spark-shell` only) to read a csv file and write it to STS, using the following command `df.write.format(\"delta\") .mode(\"overwrite\") .jdbc(\"jdbc:hive2://localhost:10000\",\"movies\", connectionProperties)`.\n\n&amp;#x200B;\n\nThis is the error I'm facing\n\nhttps://preview.redd.it/c8ti6ffq3ieb1.png?width=1847&amp;format=png&amp;auto=webp&amp;s=ca2b330bc10b5ae5b776a14edf1f44d92e3ec144\n\nI've searched a lot but wasn't able to find an answer to this problem, i hope someone here can help me with this.", "author_fullname": "t2_6iu8m5m6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "need help to write spark dataframe in spark thrift server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 34, "top_awarded_type": null, "hide_score": true, "media_metadata": {"c8ti6ffq3ieb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 26, "x": 108, "u": "https://preview.redd.it/c8ti6ffq3ieb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6af34f9ed2e9fb4af485292127abb02ef04093a"}, {"y": 53, "x": 216, "u": "https://preview.redd.it/c8ti6ffq3ieb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b00822f50d48888bd1fc45b2a4bf2f23f051e7ff"}, {"y": 78, "x": 320, "u": "https://preview.redd.it/c8ti6ffq3ieb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=24b3b40bea26e4c8ce5d0306e498fd8174e9d365"}, {"y": 157, "x": 640, "u": "https://preview.redd.it/c8ti6ffq3ieb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f7157812f1d4427c634351e07201b75b89a54307"}, {"y": 235, "x": 960, "u": "https://preview.redd.it/c8ti6ffq3ieb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1ab29d534c4e729071e4412ed6c81c204502a4c0"}, {"y": 265, "x": 1080, "u": "https://preview.redd.it/c8ti6ffq3ieb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1857366a8f166f606d55542c94ca69d4ccaebcb2"}], "s": {"y": 454, "x": 1847, "u": "https://preview.redd.it/c8ti6ffq3ieb1.png?width=1847&amp;format=png&amp;auto=webp&amp;s=ca2b330bc10b5ae5b776a14edf1f44d92e3ec144"}, "id": "c8ti6ffq3ieb1"}}, "name": "t3_15b0kiv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BfbgmjAjzRm8OO8y41i3xweFCzmOfy412dqLV0Vc2Tg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690460821.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m playing with Spark Thrift Server and want to save a dataframe to it via the JDBC port it opens, but I&amp;#39;m facing an error with the SQL code generated by Spark.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve started STS with &lt;code&gt;spark-submit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --name &amp;quot;Thrift JDBC/ODBC Server&amp;quot;&lt;/code&gt; and I&amp;#39;m using spark-shell (started with &lt;code&gt;spark-shell&lt;/code&gt; only) to read a csv file and write it to STS, using the following command &lt;code&gt;df.write.format(&amp;quot;delta&amp;quot;) .mode(&amp;quot;overwrite&amp;quot;) .jdbc(&amp;quot;jdbc:hive2://localhost:10000&amp;quot;,&amp;quot;movies&amp;quot;, connectionProperties)&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This is the error I&amp;#39;m facing&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/c8ti6ffq3ieb1.png?width=1847&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca2b330bc10b5ae5b776a14edf1f44d92e3ec144\"&gt;https://preview.redd.it/c8ti6ffq3ieb1.png?width=1847&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca2b330bc10b5ae5b776a14edf1f44d92e3ec144&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve searched a lot but wasn&amp;#39;t able to find an answer to this problem, i hope someone here can help me with this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15b0kiv", "is_robot_indexable": true, "report_reasons": null, "author": "gss-", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b0kiv/need_help_to_write_spark_dataframe_in_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b0kiv/need_help_to_write_spark_dataframe_in_spark/", "subreddit_subscribers": 118416, "created_utc": 1690460821.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How A Database Get Rid of OOM Crashes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 24, "top_awarded_type": null, "hide_score": true, "name": "t3_15b03up", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8p0me-lxGI0ye38ln9SAqjHamG7fEJnx56anle2Onzw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690459507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/p/d3e91fb39ea5", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?auto=webp&amp;s=c6b255ae65df4e7d855e996be1a18b3913b3c16b", "width": 1200, "height": 209}, "resolutions": [{"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6cc6b77f5b127ce5b4abb15d72981db560ebbffe", "width": 108, "height": 18}, {"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2b715ad9378ef133ba8e99571b574cb8c07fc2d3", "width": 216, "height": 37}, {"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4df7804d72f0502bcb05ec9aa64513ae94c3310a", "width": 320, "height": 55}, {"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bb89db914e3a13bc2dd9ce16225c0763d1c3b03", "width": 640, "height": 111}, {"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=13e7a15f1e837741b7e5bfbd5962ea9fb98b8ca1", "width": 960, "height": 167}, {"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eb8f46c3e3028df7b04e8f1b8d393a66bf6c2c91", "width": 1080, "height": 188}], "variants": {}, "id": "e-9zVxHoiwpkzfG7uqCmLN2rjt6CjbVDqkj47Avtxoo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15b03up", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b03up/how_a_database_get_rid_of_oom_crashes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/p/d3e91fb39ea5", "subreddit_subscribers": 118416, "created_utc": 1690459507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fesy5456p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A hypothesis that the Federal Reserve can set interest rates based on the movements of the planet Mars. Here I have data going back to 1896 that shows how the Dow Jones performed when Mars was within 30 degrees of the lunar node.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_15azrwj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ufsg-uun1xaQlE61fHuJoi3qCcNaKJIt_k3VzIKSLGw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690458554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "academia.edu", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.academia.edu/42243993/A_hypothesis_that_the_Federal_Reserve_can_set_interest_rates_based_on_the_movements_of_the_planet_Mars_Here_I_have_data_going_back_to_1896_that_shows_how_the_Dow_Jones_performed_when_Mars_was_within_30_degrees_of_the_lunar_node_from_appendix_of_Ares_Le_Mandat_4th_ed_", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lu8NhtJOUgHqqyYcKduymZVmYs5CzX1ZypRdadhK53Y.jpg?auto=webp&amp;s=4044cf9262375528c6c5b13faec0d4309f97dace", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/lu8NhtJOUgHqqyYcKduymZVmYs5CzX1ZypRdadhK53Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bc9b56bb720581fe5bc2b4328ca1ff86069f2e0e", "width": 108, "height": 108}], "variants": {}, "id": "zCZRl5IHUtOgLP6kzFh5KDelz3UJxd-L1dGiQwqBJPA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15azrwj", "is_robot_indexable": true, "report_reasons": null, "author": "AnthonyofBoston", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15azrwj/a_hypothesis_that_the_federal_reserve_can_set/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.academia.edu/42243993/A_hypothesis_that_the_Federal_Reserve_can_set_interest_rates_based_on_the_movements_of_the_planet_Mars_Here_I_have_data_going_back_to_1896_that_shows_how_the_Dow_Jones_performed_when_Mars_was_within_30_degrees_of_the_lunar_node_from_appendix_of_Ares_Le_Mandat_4th_ed_", "subreddit_subscribers": 118416, "created_utc": 1690458554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in the process of learning and improvising designing systems and solutions around data engineering usecases. Would like to know better tools and references for learning well-architected solutions (just like AWS Solution Architect - Learning Path), but generalizer and data engineering specific", "author_fullname": "t2_2fcmcq98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which are good tools and references to learn design and architecting data engineering solutions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ay13l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690453100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in the process of learning and improvising designing systems and solutions around data engineering usecases. Would like to know better tools and references for learning well-architected solutions (just like AWS Solution Architect - Learning Path), but generalizer and data engineering specific&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15ay13l", "is_robot_indexable": true, "report_reasons": null, "author": "rockeyjam", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ay13l/which_are_good_tools_and_references_to_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ay13l/which_are_good_tools_and_references_to_learn/", "subreddit_subscribers": 118416, "created_utc": 1690453100.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}