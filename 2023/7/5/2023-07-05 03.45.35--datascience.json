{"kind": "Listing", "data": {"after": null, "dist": 10, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been working for about half a year now as a junior machine learning engineer. I feel like I have gained more skills/experience making my own project than what I have in the industry.\n\nI want to stay relevant in the field and continue to progress my career and eventually move the ladder.\n\n&amp;#x200B;\n\nHow do you guys stay relevant, hone your skills and master your craft?", "author_fullname": "t2_16t67m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to stay relevant in the field?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qmqj2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688494781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working for about half a year now as a junior machine learning engineer. I feel like I have gained more skills/experience making my own project than what I have in the industry.&lt;/p&gt;\n\n&lt;p&gt;I want to stay relevant in the field and continue to progress my career and eventually move the ladder.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How do you guys stay relevant, hone your skills and master your craft?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14qmqj2", "is_robot_indexable": true, "report_reasons": null, "author": "Mighty__hammer", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14qmqj2/how_to_stay_relevant_in_the_field/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14qmqj2/how_to_stay_relevant_in_the_field/", "subreddit_subscribers": 938463, "created_utc": 1688494781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently a newcomer learning data science and i was given a task in an assignment question to do multivariate analysis on the whole dataset. The data set that was given to me is i assumed to be a mock dataset of a food delivery service with multiple variables both numerical and categorical. The question doesn't specify what are the values i need to analyze, it just told me to do multivariate analysis to explore the relationships between the variables in that dataset. To put it simply, i think it tasked me to do multivariate EDA. However, i didn't realize how many possible combinations of potentially correlated variables the dataframe had with each other. Do i really need to do a highly detailed analysis on each possible correlation for each variable, specifying a detailed description of each relationship, or can i do a more broad and general analysis? I have to admit, I am getting really tired of doing this, not to mention my lack of expertise in using pandas in making the proper dataframe to present the data properly.", "author_fullname": "t2_32iq704k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I have to check for every single possible correlation between each variable while doing multivariate data analysis?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qad4a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688480945.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688462926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a newcomer learning data science and i was given a task in an assignment question to do multivariate analysis on the whole dataset. The data set that was given to me is i assumed to be a mock dataset of a food delivery service with multiple variables both numerical and categorical. The question doesn&amp;#39;t specify what are the values i need to analyze, it just told me to do multivariate analysis to explore the relationships between the variables in that dataset. To put it simply, i think it tasked me to do multivariate EDA. However, i didn&amp;#39;t realize how many possible combinations of potentially correlated variables the dataframe had with each other. Do i really need to do a highly detailed analysis on each possible correlation for each variable, specifying a detailed description of each relationship, or can i do a more broad and general analysis? I have to admit, I am getting really tired of doing this, not to mention my lack of expertise in using pandas in making the proper dataframe to present the data properly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14qad4a", "is_robot_indexable": true, "report_reasons": null, "author": "mega_lova_nia", "discussion_type": null, "num_comments": 64, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14qad4a/do_i_have_to_check_for_every_single_possible/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14qad4a/do_i_have_to_check_for_every_single_possible/", "subreddit_subscribers": 938463, "created_utc": 1688462926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have my Excel 2019 associate certificate (MO 200) and am working on the word certificate and the excel specialist certificate. Is being a report writer the best job to get before being a data analyst? Or is data entry better? I understand that it will take a lot of self learning to be a data analyst. I just want to know what is a good starting job to work while preparing to be a data analyst. Also is a report writer an entry level job or do I need to do data entry before being a report writer? Or are there other options that are equally as good or better?\n\nAlso I have an associates degree in comp sci.", "author_fullname": "t2_dwqc3c21a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best entry level job to lead up to being a data analyst?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qx8td", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688526162.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688522287.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have my Excel 2019 associate certificate (MO 200) and am working on the word certificate and the excel specialist certificate. Is being a report writer the best job to get before being a data analyst? Or is data entry better? I understand that it will take a lot of self learning to be a data analyst. I just want to know what is a good starting job to work while preparing to be a data analyst. Also is a report writer an entry level job or do I need to do data entry before being a report writer? Or are there other options that are equally as good or better?&lt;/p&gt;\n\n&lt;p&gt;Also I have an associates degree in comp sci.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14qx8td", "is_robot_indexable": true, "report_reasons": null, "author": "Larrysc6", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14qx8td/what_is_the_best_entry_level_job_to_lead_up_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14qx8td/what_is_the_best_entry_level_job_to_lead_up_to/", "subreddit_subscribers": 938463, "created_utc": 1688522287.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_ebz9hcabk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best feature engineering book (in python)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qx9as", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688522327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14qx9as", "is_robot_indexable": true, "report_reasons": null, "author": "cho_odama_rasengan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14qx9as/best_feature_engineering_book_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14qx9as/best_feature_engineering_book_in_python/", "subreddit_subscribers": 938463, "created_utc": 1688522327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\nHey! \nI'm currently building my data science portfolio and would love your feedback about it (Attached link). Your feedback will be immensely helpful as I strive to enhance my portfolio and increase my chances of landing a data science job. Futhermore, I'm on the hunt for my first data science or data analysis job, and I would greatly appreciate any advice or tips you may have to offer. If you have any insights or suggestions on how to secure my first job in the field, I would be incredibly grateful for your guidance.\n\nThank you in advance for your valuable input!", "author_fullname": "t2_t1grjisj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feedback on Data Science Portfolio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qyhnl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1688526143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! \nI&amp;#39;m currently building my data science portfolio and would love your feedback about it (Attached link). Your feedback will be immensely helpful as I strive to enhance my portfolio and increase my chances of landing a data science job. Futhermore, I&amp;#39;m on the hunt for my first data science or data analysis job, and I would greatly appreciate any advice or tips you may have to offer. If you have any insights or suggestions on how to secure my first job in the field, I would be incredibly grateful for your guidance.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your valuable input!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/lpalfonsoa/Data-Analysis", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14qyhnl", "is_robot_indexable": true, "report_reasons": null, "author": "Sae-nathra", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14qyhnl/feedback_on_data_science_portfolio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/lpalfonsoa/Data-Analysis", "subreddit_subscribers": 938463, "created_utc": 1688526143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As a beginner, I'm working on these libraries, numpy, pandas, matplotlib, seaborn, TensorFlow, sci-kit learn, and nltk.\n\nWhat else should I be working on?", "author_fullname": "t2_dg9d2w5zh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I need advanced Excel and oop for data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qyf4y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688525947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a beginner, I&amp;#39;m working on these libraries, numpy, pandas, matplotlib, seaborn, TensorFlow, sci-kit learn, and nltk.&lt;/p&gt;\n\n&lt;p&gt;What else should I be working on?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14qyf4y", "is_robot_indexable": true, "report_reasons": null, "author": "Luo-yi-", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14qyf4y/do_i_need_advanced_excel_and_oop_for_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14qyf4y/do_i_need_advanced_excel_and_oop_for_data_science/", "subreddit_subscribers": 938463, "created_utc": 1688525947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Not even sure they\u2019re anything more than Frobes hype driven FOMO for business leaders, but maybe a proper regression analysis might be an appropriate source for these thing\u2026\n\nI\u2019m tasked to declare a set of \u201cleading KPIs\u201d as a team leader in analytics. These are supposed to be  for a very specific initiative (technically a knee jerk reaction to not hitting anywhere close to where our company needs to be in terms of our annual goals for the year). \n\nBasically, they want more conversions...\n\nProblem, they have absolutely no idea how to get them\u2026\n\nIgnoring all the details, Ive agreed to provide them with some very rudimentary optimization algorithm that aims to improve marketing response rate. Let\u2019s leave it at that for now.\n\nThey want a leading KPI for this\u2026 A leading KPI\u2026 for an optimization algorithm (basically just multi armed bandit application to segmented prospect groups for specific ads on the topic of conversion). We\u2019re not even sure the ads will work, but that\u2019s another optimization problem. \n\nAs I think about this, I realize all of data science aims to eliminate the need for these \u201cleading KPIs\u201d and instead build solutions that either:\n\n* Identify \u201cleading KPIs\u201d for manual operations in the form of regression terms and coefficients\n* Do away with manual decision making all together and let algos go brrrr refining business operations automatically, doing away with the needs for a human interpretable \u201cleading KPI\u201d\n* Predict future values of time series processes that may or may not ignore other independent variables, more often just decomposing a process into its additive forms and extrapolating potential future value ranges and probability distributions for those  - like, model says 68% chance of rainfall between 1\u201d and 3\u201d over next 30 days kind of thing, do with this what you will\n\nThe entire concept of a \u201cleading KPI\u201d is very sale sand operations productivity oriented, right? Hold time for a call center leads customer satisfaction. How much wood a wood chuck can chick leads whatever wood chucks do with the wood. A roofer can lay 3 shingles per minute when he\u2019s rested and 1 per minute when he is exhausted, therefore a roof can be finished in X time given Y breaks. A sales team can produce 10 leads per hour if they wear suits and 100 per hour if they go to the bar with a group of potential clients.\n\nHow does this even translate to data science? At most, I can think of things like VPN bandwidth, data warehouse performance, training hardware specs, cloud budget, team size, WIP, ad hoc request load, interruption frequency, vintage of teams last contemporary technology training, workstation specs, etc. \n\nThe rest really depends on deployment and utilization - like the marketing team does or does not ignore model results, or advertising is fully automated so marketing team can\u2019t ignore the results.\n\nIt\u2019s not like the number of models in production is any indicator of company performance. Quality of models, maybe\u2026 But really its about what aspects of a work day contribute to a productive team that is not burdened by distractions and allowed to do their work in stable, repeatable, unbiased ways that operate on theories based on observable realities. \n\nThat\u2019s it. \n\nGive me more people with better training and experience, leave us alone, define problems well and allow us the agency of telling you if what you want is feasible and actually something that can realistically be done. Give us enough budget to build and deploy the solution. Give us people to support and maintain it. Then we monitor \u201clagging KPIs\u201d in the range of profit, revenue, expenses, and how variants model performance measures contribute to those.\n\n&amp;#x200B;\n\nUnless I\u2019m supposed to say MRSE or F1 are my \u201cleading KPIs?\u201d My perspective is that they are not and are completely dependent on if the model even gets deployed or may even dictate if the model is deployed. I can\u2019t just say, \u201coh, all these hypothetical models I might build will absolutely hit an MRSE of 0.0012345 and that will contribute to increased conversion rate.\u201d Or am I viewing this wrong? ", "author_fullname": "t2_w100sesa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The concept of \u201cLeading KPIs\u201d is not compatible with Data Science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qvre5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688517661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not even sure they\u2019re anything more than Frobes hype driven FOMO for business leaders, but maybe a proper regression analysis might be an appropriate source for these thing\u2026&lt;/p&gt;\n\n&lt;p&gt;I\u2019m tasked to declare a set of \u201cleading KPIs\u201d as a team leader in analytics. These are supposed to be  for a very specific initiative (technically a knee jerk reaction to not hitting anywhere close to where our company needs to be in terms of our annual goals for the year). &lt;/p&gt;\n\n&lt;p&gt;Basically, they want more conversions...&lt;/p&gt;\n\n&lt;p&gt;Problem, they have absolutely no idea how to get them\u2026&lt;/p&gt;\n\n&lt;p&gt;Ignoring all the details, Ive agreed to provide them with some very rudimentary optimization algorithm that aims to improve marketing response rate. Let\u2019s leave it at that for now.&lt;/p&gt;\n\n&lt;p&gt;They want a leading KPI for this\u2026 A leading KPI\u2026 for an optimization algorithm (basically just multi armed bandit application to segmented prospect groups for specific ads on the topic of conversion). We\u2019re not even sure the ads will work, but that\u2019s another optimization problem. &lt;/p&gt;\n\n&lt;p&gt;As I think about this, I realize all of data science aims to eliminate the need for these \u201cleading KPIs\u201d and instead build solutions that either:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Identify \u201cleading KPIs\u201d for manual operations in the form of regression terms and coefficients&lt;/li&gt;\n&lt;li&gt;Do away with manual decision making all together and let algos go brrrr refining business operations automatically, doing away with the needs for a human interpretable \u201cleading KPI\u201d&lt;/li&gt;\n&lt;li&gt;Predict future values of time series processes that may or may not ignore other independent variables, more often just decomposing a process into its additive forms and extrapolating potential future value ranges and probability distributions for those  - like, model says 68% chance of rainfall between 1\u201d and 3\u201d over next 30 days kind of thing, do with this what you will&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The entire concept of a \u201cleading KPI\u201d is very sale sand operations productivity oriented, right? Hold time for a call center leads customer satisfaction. How much wood a wood chuck can chick leads whatever wood chucks do with the wood. A roofer can lay 3 shingles per minute when he\u2019s rested and 1 per minute when he is exhausted, therefore a roof can be finished in X time given Y breaks. A sales team can produce 10 leads per hour if they wear suits and 100 per hour if they go to the bar with a group of potential clients.&lt;/p&gt;\n\n&lt;p&gt;How does this even translate to data science? At most, I can think of things like VPN bandwidth, data warehouse performance, training hardware specs, cloud budget, team size, WIP, ad hoc request load, interruption frequency, vintage of teams last contemporary technology training, workstation specs, etc. &lt;/p&gt;\n\n&lt;p&gt;The rest really depends on deployment and utilization - like the marketing team does or does not ignore model results, or advertising is fully automated so marketing team can\u2019t ignore the results.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s not like the number of models in production is any indicator of company performance. Quality of models, maybe\u2026 But really its about what aspects of a work day contribute to a productive team that is not burdened by distractions and allowed to do their work in stable, repeatable, unbiased ways that operate on theories based on observable realities. &lt;/p&gt;\n\n&lt;p&gt;That\u2019s it. &lt;/p&gt;\n\n&lt;p&gt;Give me more people with better training and experience, leave us alone, define problems well and allow us the agency of telling you if what you want is feasible and actually something that can realistically be done. Give us enough budget to build and deploy the solution. Give us people to support and maintain it. Then we monitor \u201clagging KPIs\u201d in the range of profit, revenue, expenses, and how variants model performance measures contribute to those.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Unless I\u2019m supposed to say MRSE or F1 are my \u201cleading KPIs?\u201d My perspective is that they are not and are completely dependent on if the model even gets deployed or may even dictate if the model is deployed. I can\u2019t just say, \u201coh, all these hypothetical models I might build will absolutely hit an MRSE of 0.0012345 and that will contribute to increased conversion rate.\u201d Or am I viewing this wrong? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14qvre5", "is_robot_indexable": true, "report_reasons": null, "author": "Dry-Sir-5932", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14qvre5/the_concept_of_leading_kpis_is_not_compatible/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14qvre5/the_concept_of_leading_kpis_is_not_compatible/", "subreddit_subscribers": 938463, "created_utc": 1688517661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nFor a project I am doing I want to identify the top x topics/issues discussed in [r/changemyview](https://www.reddit.com/r/changemyview/). For example I may find the most common topics are\n\n1. Affirmative Action\n2. Gun Control\n3. etc ...\n\nI am familiar with using praw to retrieve post titles from the sub. What are some techniques to identify the topic/issue each post is addressing. For example in the post: \"CMV: The 2nd Amendment enables the police state, it does not protect our other rights.\" the topic is 2nd Amendment. Is the best way to do this to define several topics and classify each post into one of the pre defined topics? Another method I saw online is using \"Bag of Words\" or \"Term Frequency-Inverse Document Frequency\" both of these methods take into account the frequency and importance of a word. I am not familiar with these two methods but I was thinking I could find the most frequently occurring words to identify the most frequent topics as well.\n\nTLDR: How to parse [r/changemyview](https://www.reddit.com/r/changemyview/) in order to identify the most frequently occurring topics.", "author_fullname": "t2_jagpllod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to identify topics using r/changemyview post titles?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qml8v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688494430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;For a project I am doing I want to identify the top x topics/issues discussed in &lt;a href=\"https://www.reddit.com/r/changemyview/\"&gt;r/changemyview&lt;/a&gt;. For example I may find the most common topics are&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Affirmative Action&lt;/li&gt;\n&lt;li&gt;Gun Control&lt;/li&gt;\n&lt;li&gt;etc ...&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I am familiar with using praw to retrieve post titles from the sub. What are some techniques to identify the topic/issue each post is addressing. For example in the post: &amp;quot;CMV: The 2nd Amendment enables the police state, it does not protect our other rights.&amp;quot; the topic is 2nd Amendment. Is the best way to do this to define several topics and classify each post into one of the pre defined topics? Another method I saw online is using &amp;quot;Bag of Words&amp;quot; or &amp;quot;Term Frequency-Inverse Document Frequency&amp;quot; both of these methods take into account the frequency and importance of a word. I am not familiar with these two methods but I was thinking I could find the most frequently occurring words to identify the most frequent topics as well.&lt;/p&gt;\n\n&lt;p&gt;TLDR: How to parse &lt;a href=\"https://www.reddit.com/r/changemyview/\"&gt;r/changemyview&lt;/a&gt; in order to identify the most frequently occurring topics.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14qml8v", "is_robot_indexable": true, "report_reasons": null, "author": "JigglyBooii", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14qml8v/how_to_identify_topics_using_rchangemyview_post/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14qml8v/how_to_identify_topics_using_rchangemyview_post/", "subreddit_subscribers": 938463, "created_utc": 1688494430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys\n\nI am still kinda new to this data analytics industry but in a nutshell,\n\nI am working for a company and my personal project is auto populating some columns based on one column. I can't give you specifics since the information is confidential but basically, I am using description column to auto populate engine hours, model year and etc of farming equipment. For example, the description columns are usually like \"John Deere 2019, it has 1500 hours. it is in good condition, it also has Hydraulics and AC. Front dual. it has enclosed cap\" something like this. And I wrote almost all the codes that are needed for this on SQLite except that I faced an issue where I cannot use regex properly in the where clause (it does not produce the right results) and also  the instring function in SQLite does not support specifying a range of numbers. and it can only search for a specific substring within a string. But I found out that I could solve the issue by using PATINDEX function that I could not solve by using SQLite.\n\n&amp;#x200B;\n\nSo I decided to use SQL server instead and tried to find a way to automate uploading process. However, I also realized that I did not want to convert all the codes into SQL server form since it will take a long time.. I used Chat GPT and it converts a lot of stuff wrong. Also, SQL requires a little more manipulation in SQL import wizard than SQLite import function in terms of importing the csv data. I want users to be able to use the program easily. Right now, I have codes for SQLite since I solved the issue in a different way finally but I am not sure if making users to install SQLite and import and run the codes is the best way.\n\n&amp;#x200B;\n\nI thought about SSIS ETL process but I am not sure if this process is what I want.  What I want is make users to easily upload the csv data and get the data based on the algorithms on which I wrote the codes for the process of autopopulating the columns based on the one column. So I looked into visual basic studio 2019. and I also used chat gpt to figure this out. The chat gpt says that I could use [ASP.net](https://ASP.net) to allow users to easily upload the csv file and get the columns autopopulate on a website by using and inserting the SQL codes. However, certainly, I do not know how to do this specifically. I was able to make a webpage but there are multiple pages I need to fill out to make this work in the visual studio ASP project. The chat gpt does not help the specifications since the codes are usually kinda wrong.\n\n&amp;#x200B;\n\nDo any of the data scientists or analysts have suggestions how to go about this? Also, I am pretty sure what I am doing is kinda close to machine learning since I am building an algorithms for almost every case I can think of. Are there anyways that I can teach the machine to learn how to build different cases for itself so I don't have to input new codes for new patterns when a new case rises?", "author_fullname": "t2_cnor8yse9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Autopopulating program", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qld2h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688493432.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688491580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys&lt;/p&gt;\n\n&lt;p&gt;I am still kinda new to this data analytics industry but in a nutshell,&lt;/p&gt;\n\n&lt;p&gt;I am working for a company and my personal project is auto populating some columns based on one column. I can&amp;#39;t give you specifics since the information is confidential but basically, I am using description column to auto populate engine hours, model year and etc of farming equipment. For example, the description columns are usually like &amp;quot;John Deere 2019, it has 1500 hours. it is in good condition, it also has Hydraulics and AC. Front dual. it has enclosed cap&amp;quot; something like this. And I wrote almost all the codes that are needed for this on SQLite except that I faced an issue where I cannot use regex properly in the where clause (it does not produce the right results) and also  the instring function in SQLite does not support specifying a range of numbers. and it can only search for a specific substring within a string. But I found out that I could solve the issue by using PATINDEX function that I could not solve by using SQLite.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So I decided to use SQL server instead and tried to find a way to automate uploading process. However, I also realized that I did not want to convert all the codes into SQL server form since it will take a long time.. I used Chat GPT and it converts a lot of stuff wrong. Also, SQL requires a little more manipulation in SQL import wizard than SQLite import function in terms of importing the csv data. I want users to be able to use the program easily. Right now, I have codes for SQLite since I solved the issue in a different way finally but I am not sure if making users to install SQLite and import and run the codes is the best way.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I thought about SSIS ETL process but I am not sure if this process is what I want.  What I want is make users to easily upload the csv data and get the data based on the algorithms on which I wrote the codes for the process of autopopulating the columns based on the one column. So I looked into visual basic studio 2019. and I also used chat gpt to figure this out. The chat gpt says that I could use &lt;a href=\"https://ASP.net\"&gt;ASP.net&lt;/a&gt; to allow users to easily upload the csv file and get the columns autopopulate on a website by using and inserting the SQL codes. However, certainly, I do not know how to do this specifically. I was able to make a webpage but there are multiple pages I need to fill out to make this work in the visual studio ASP project. The chat gpt does not help the specifications since the codes are usually kinda wrong.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do any of the data scientists or analysts have suggestions how to go about this? Also, I am pretty sure what I am doing is kinda close to machine learning since I am building an algorithms for almost every case I can think of. Are there anyways that I can teach the machine to learn how to build different cases for itself so I don&amp;#39;t have to input new codes for new patterns when a new case rises?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tOcHqen1kUsOwwy69KXemKWBNWHsA4cbiSO3JrFVitg.jpg?auto=webp&amp;v=enabled&amp;s=a626a7e9d320fbdfe2116b03f85e52dc18fce2e3", "width": 238, "height": 238}, "resolutions": [{"url": "https://external-preview.redd.it/tOcHqen1kUsOwwy69KXemKWBNWHsA4cbiSO3JrFVitg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93819bfca99048541e71bba5f90386fdbbba7844", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/tOcHqen1kUsOwwy69KXemKWBNWHsA4cbiSO3JrFVitg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d43106cb2f5d8a9a58fe523b46fa1bafbfd024d", "width": 216, "height": 216}], "variants": {}, "id": "TtFVmEs53hWc_tUPscFPQzT0bv04t5CPuG5-SNnw3mo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14qld2h", "is_robot_indexable": true, "report_reasons": null, "author": "Money_Working_9702", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14qld2h/autopopulating_program/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14qld2h/autopopulating_program/", "subreddit_subscribers": 938463, "created_utc": 1688491580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\nWe're hosting a free-to-enter, virtual hackathon, All Inclusive Hacks, focused on developing creative solutions to strengthen inclusivity in modern AI products. It will be hosted during mid to late August and we expect the prize pool to be in the range of **$200k to $300k.** We\u00a0currently in sponsorship talks with major tech corporations, including **Google, OpenAI, and Microsoft**. This is\u00a0**open to all** aged 13+ regardless of experience in programming!\n\n**Why Should You Join This Hackathon?**\n\nThis hackathon is a great opportunity to...\n\n1. Win cash and in-kind prizes!\n2. Meet peers with similar interests\n3. Improve one's resume for jobs, internships, and college or grad school admissions\n4. Improve one's grasp of artificial intelligence and its industry applications\n5. Learn from our workshops that will be hosted by leading figures in artificial intelligence\n\nParticipants are not expected to develop complex AI products or models! Rather, we expect simple more fundamental and abstract solutions that tackle some of the issues listed below:\u00a0\n\n\\- Language Preservation\n\n\\- Stereotype/Bias Identification\n\n\\- Disability Access\n\n\\- Ethics\n\n\\- Explainability\u00a0\n\n\\-\u00a0*A Relevant Issue of Your Choice!*\n\nSign Up Link: [https://all-inclusive-hacks.devpost.com/](https://all-inclusive-hacks.devpost.com/)", "author_fullname": "t2_dfcjm8gnk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Projected $200,000 in Prizes AI Hackathon (Free to Enter)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qwcj2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688519445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re hosting a free-to-enter, virtual hackathon, All Inclusive Hacks, focused on developing creative solutions to strengthen inclusivity in modern AI products. It will be hosted during mid to late August and we expect the prize pool to be in the range of &lt;strong&gt;$200k to $300k.&lt;/strong&gt; We\u00a0currently in sponsorship talks with major tech corporations, including &lt;strong&gt;Google, OpenAI, and Microsoft&lt;/strong&gt;. This is\u00a0&lt;strong&gt;open to all&lt;/strong&gt; aged 13+ regardless of experience in programming!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why Should You Join This Hackathon?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This hackathon is a great opportunity to...&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Win cash and in-kind prizes!&lt;/li&gt;\n&lt;li&gt;Meet peers with similar interests&lt;/li&gt;\n&lt;li&gt;Improve one&amp;#39;s resume for jobs, internships, and college or grad school admissions&lt;/li&gt;\n&lt;li&gt;Improve one&amp;#39;s grasp of artificial intelligence and its industry applications&lt;/li&gt;\n&lt;li&gt;Learn from our workshops that will be hosted by leading figures in artificial intelligence&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Participants are not expected to develop complex AI products or models! Rather, we expect simple more fundamental and abstract solutions that tackle some of the issues listed below:\u00a0&lt;/p&gt;\n\n&lt;p&gt;- Language Preservation&lt;/p&gt;\n\n&lt;p&gt;- Stereotype/Bias Identification&lt;/p&gt;\n\n&lt;p&gt;- Disability Access&lt;/p&gt;\n\n&lt;p&gt;- Ethics&lt;/p&gt;\n\n&lt;p&gt;- Explainability\u00a0&lt;/p&gt;\n\n&lt;p&gt;-\u00a0&lt;em&gt;A Relevant Issue of Your Choice!&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Sign Up Link: &lt;a href=\"https://all-inclusive-hacks.devpost.com/\"&gt;https://all-inclusive-hacks.devpost.com/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/b1QDuKROgpRC3HTpJSHW5JrFzQyvW-Uvc2VpPQjO0sU.jpg?auto=webp&amp;v=enabled&amp;s=6704dfe78599a146e6a4116e366a624a91f5a53c", "width": 518, "height": 478}, "resolutions": [{"url": "https://external-preview.redd.it/b1QDuKROgpRC3HTpJSHW5JrFzQyvW-Uvc2VpPQjO0sU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=220d04d3513568acaa174bd86f214c0e2431bd80", "width": 108, "height": 99}, {"url": "https://external-preview.redd.it/b1QDuKROgpRC3HTpJSHW5JrFzQyvW-Uvc2VpPQjO0sU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=800259dad38ece532e00e0a47ab37d305b7a8c8b", "width": 216, "height": 199}, {"url": "https://external-preview.redd.it/b1QDuKROgpRC3HTpJSHW5JrFzQyvW-Uvc2VpPQjO0sU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a712b9286be9ecec816bc6f7780dd12ed3a3be8", "width": 320, "height": 295}], "variants": {}, "id": "aq3DlaWNARS2L2mhnH3TdUlfYvK4Gj48uHsY8cSiQxE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14qwcj2", "is_robot_indexable": true, "report_reasons": null, "author": "AllInclusiveHacks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14qwcj2/projected_200000_in_prizes_ai_hackathon_free_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14qwcj2/projected_200000_in_prizes_ai_hackathon_free_to/", "subreddit_subscribers": 938463, "created_utc": 1688519445.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}