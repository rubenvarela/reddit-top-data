{"kind": "Listing", "data": {"after": "t3_14qltvp", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been looking all over for a solution to this problem, but haven't found anything that I would call concrete.  I'm looking to build a pretty large media storage array (500TB) and based on what I'm seeing, I'm going to need a 24+ bay enclosure.  I also have an existing, overly capable transcoding server that I would like to keep standalone and have the new storage array connect over external SAS cables as a JBOD unit.  I'm REALLY trying to avoid a separate or replacement build with server motherboard/CPUs.  Right now I'm looking at getting either a NetApp DS4246 or a Supermicro CSE-847E16-RJBOD1.  \n\nI have no experience with external SAS storage arrays and am not sure what to expect in regards to managing it vs internal SATA storage or even a NAS.  I'd love to hear from any folks using external SAS storage for their Plex/Emby/Jellyfin servers, how they have it set up, any pros and cons, etc.  \n\nThanks for your time :) ", "author_fullname": "t2_3zitg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Options for external 24+ bay SAS storage enclosures for Plex library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qtric", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 77, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 77, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688511880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been looking all over for a solution to this problem, but haven&amp;#39;t found anything that I would call concrete.  I&amp;#39;m looking to build a pretty large media storage array (500TB) and based on what I&amp;#39;m seeing, I&amp;#39;m going to need a 24+ bay enclosure.  I also have an existing, overly capable transcoding server that I would like to keep standalone and have the new storage array connect over external SAS cables as a JBOD unit.  I&amp;#39;m REALLY trying to avoid a separate or replacement build with server motherboard/CPUs.  Right now I&amp;#39;m looking at getting either a NetApp DS4246 or a Supermicro CSE-847E16-RJBOD1.  &lt;/p&gt;\n\n&lt;p&gt;I have no experience with external SAS storage arrays and am not sure what to expect in regards to managing it vs internal SATA storage or even a NAS.  I&amp;#39;d love to hear from any folks using external SAS storage for their Plex/Emby/Jellyfin servers, how they have it set up, any pros and cons, etc.  &lt;/p&gt;\n\n&lt;p&gt;Thanks for your time :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qtric", "is_robot_indexable": true, "report_reasons": null, "author": "cockpitlove", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qtric/options_for_external_24_bay_sas_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qtric/options_for_external_24_bay_sas_storage/", "subreddit_subscribers": 691156, "created_utc": 1688511880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I love collecting mature videos and love the idea of collecting a copy of an entire series.  With a long weekend I decided to focus on 1 series.  There is a studio that has multiple series so I decided to focus on the \"MyHotGirl\" series (actual name is different of course).\n\nHere is the workflow I came up with.\n\nCATALOG\n\nThe first problem with a 'collection' is you need to know all the possible titles.\n\nI went to the official studio website and discovered they have separate web pages for each series as a catalog/index to the videos. Great. I wanted to create a local copy of these pages so I could scan for titles, dates, actors, etc. at my leisure.\n\nFor series that occupy 2-5 web pages I could simply go to the page, right-clicked and did \"Save As..\" and saved each page as a single file with .html extension.  The \"MyHotGirl\" series has been around for years and had 45 web pages.  So I wrote a simple Python script that calculated the Page 1,2,3 URL, did a Request and saved each to a local Catalog file.\n\nThen I wrote another Python script to use BeautifulSoup. It opens each Catalog file and searches for redirects or \"a href\" .  There are tons but eventually I discover the pattern that the pages use for a video and made my python script only print these.\n\nBUT - I don't want to link/play the video on the studio site.  I want the Release date, title AROUND each title.  I am trying to build a Catalog.\n\nKnowing one of the URL's I went back to the Catalog site and right-clicked on the page and did \"View Source\".  Then I searched for one of the URL's.\n\nNow - I look above and below the url in the source of the web page.  I am NOT a web page designer but I could see rows with the Title of the video and the Release Date for the video.\n\nMy goal was to derive the likely file name for each title in this format:\n\nMyHotGirl.YY.MM.DD.Title Title Title Actor Actor Actor\n\nI fought with BeautifulSoup to get it to extract all 3 parts of the information but failed.  I could get the URL's of the videos. I could get all Date strings or I could get all text that included video titles - but a lot of other crap as well.\n\nUSING CHATGPT\n\nEach video was encased in SPAN, CLASS, DIV and other mark up. I copied and pasted all the enclosing HTML into a text editor.\n\nI then changed the website name to something other than the Porn site and also cleaned up the Title strings so it was not adult.\n\nThen I fired up ChatGPT and asked it this question:\n\n&gt;I need to use Python3 and BeautifulSoup to extract the Date, Title and Series name from this HTML:  (Paste my cleaned up snippet here)\n\nTo my shock and delight - It spit out some Python code that could actually work.\n\nThen I added:\n\n&gt;I need the code above to handle multiple items on the web page and extract all the dates titles and series.\n\nAgain - it spit out code like this that now included a loop:\n\n    soup = BeautifulSoup(url, 'html.parser')\n    redirects = []\n    video_items = soup.select('.thumb.item')\n    \n    for item in video_items:\n        a_tag = item.select_one('a.thumb__top')\n        video_url = a_tag['href']\n        img_tag = item.select_one('.thumb__img img')\n        alt_text = img_tag['alt']\n        date_text = item.select_one(.thumb__date')\n        #print(\"Video URL:\", video_url)\n        #print(\"Alt Text:\", alt_text)\n        #print(\"Date Text:\", date_text)\n\nSuddenly I could now get the Series, Date and Title as a set for each video on each catalog page.\n\nI ran the code in a loop to print out a line of text for each that looked like this to what I call a \"Catalog File\"\n\n    MyHotGirl.15.03.11.Title Title Title|N      &lt;-- 'N' means Not in my collection\n    MyHotGirl.15.04.03.Title Title Title|N\n    ...\n\nNow I had a text file that should match the file names I already had.  I found a list of most of my videos and searched for \"MyHotGirl\". For each row I had - I found that row in my Catalog file and changed \"|N\" to \"|Y\" to indicate I had the video in my collection.\n\n&amp;#x200B;\n\nQUESTION\n\nIs this post interesting to the group? It is very nerdy and technical.  But I thought the workflow or order of operations might be of interest.  The same workflow could be used for many other studios and series.\n\nI will continue if there is interest.\n\n&amp;#x200B;\n\n==============================================\n\nPart 2 - FILE DOWNLOADS\n\nI went to several sites and searched for \"MyHotGirl\" series of videos. But there were problems:\n\n* Some of the sites had horrible search engines. I could type in the name of an actor but it would not show me all the videos with the series name.\n* Some of the sites only contained 10 minute clips\n* Some of the sites would not work with JDownloader2 which is my favorite download engine.\n\nEventually I discovered a \"Porn Try\" site that had full clips, the search engine only returned the videos I wanted and the Title of each video page is usually the \"Title Title Title\" from each video.\n\nFor the first search result page I would right-click and \"Open in new tab\" to bring up the individual video page. I would highlight the URL and hit Ctrl-C.  JDownloaders 'watches' the clipboard and it scanned the page, found the video and used the title of the page as the file name.\n\nThe other important point: It 'remembers' the URLs and if I tried to download a second time it marks these in RED.  When I trigger the Download - it asks if I want to ignore duplicates. Very cool and one of the reasons I love this tool.\n\nSo I told it to download all the videos from the first search result page with the \"Open in new Tab\" and Ctrl-C trick. That got about the first 50 videos downloading.\n\nThen I got smart.  I took the URL of the second result page and saw it said \"/2/\" for the second page and \"/3/\" for the third.\n\nI wrote another quick Python script to calc the URL for all 14 search result pages - and I saved these to 14 flat files.\n\nI then opened one of the video pages in my browser. I copied the video URL.\n\nI looked at the search result page 2 and searched and found the Video URL.\n\nI again took a snippet and asked ChatGPT to help me find these URLS.\n\nSuddenly I had a script that would find all the Video URL's from each flat file search result. Great.\n\nI can now highlight 20+ URLS and hit Ctrl-C.  JDownloader then spent a few minutes and found all the video files.  I then told it to start downloading.\n\nI planned to iterate through each search result page.\n\n&amp;#x200B;\n\nPART 3 - FILE NAMES\n\nI copied all the videos I had downloaded from the Page 1 search results to a new folder.\n\nI wrote a python script to:\n\nRead the video names from my Catalog file:  MyHotGirl.YY.MM.DD.Aaaa Bbbb Cccc Dddd|N\n\nKeep the name but extract just the \"Aaaa Bbbb Cccc Dddd\" part.\n\nLoop through the video files and see if \"Aaaa Bbbb Cccc Dddd\" was in the file name.\n\nIf I found a match - I renamed the file using the full name: MyHotGirl.YY.MM.DD.Aaaa Bbbb Cccc Dddd.mp4\n\nI got about 20% of the video files renamed this way.\n\n&amp;#x200B;\n\nBut that means 80% of the video files still did not have the correct \"MyHotGirl.YY.MM.DD\" prefix to the file name.  \n\nSo I made a second rename pass. But this time I did the following:\n\nI read all the rows from my Catalog file.\n\nI stripped the series and YY.MM.DD from the string to leave just the title.\n\nI lower-cased and removed all the non-alphanumeric characters from the title to give me: aaaabbbbccccddd.\n\n&amp;#x200B;\n\nThen I looped through all the actual video files and lower-cased the names and removed the non-alphanumeric characters.  This gave me: \"ddddffffggggaaaa720\" for each video file. \n\nSuddenly - I was able to find a lot more matches once spaces, dashes, etc as I looked for the sub-string in the file name.  This allowed me to rename nearly all the video files.\n\n&amp;#x200B;\n\nPART 4 - Do I have the video\n\nI wrote yet another script.  This time I read all the video file names but I just took \"MyHotGirl.YY.MM.DD\" from the file name.\n\nI then read my Catalog file to find a match. If that row ended in \"|N\" meaning not in my collection, I changed the \"|N\" to \"|Y\" and wrote it back out.\n\n&amp;#x200B;\n\nNow when I read my Catalog file I print out the total row counts and the rows that end in \"Y\" to show this:\n\nSeries (mhg) Row count: 483\n\nSeries (mhg) In Collection: 69\n\n&amp;#x200B;\n\nThis tells me I have about 14% of the total possible \"MyHotGirl\" series. Put this is only after downloading 1 search result page. The others are still processing so this percentage will soon go up.\n\nFINAL THOUGHTS\n\nOnce all the files download, are re-named by my script and the few others renamed by hand, I will see how close to 100% I got.\n\nThe Catalog file will now also tell me what titles I need to find or buy to complete the collection.\n\nOverall it was a fun 1-2 day project that gave me both a lot of satisfaction to write.\n\nTOOLS LIST:\n\n* Python3 with Requests and BeautifulSoup packages\n* JDownloader to do the actual work.\n* BulkRename - to do some occasional removing funny chars or double spaces\n* ChatGPT - to analyze snippets of HTML and tell me how to extract stuff from the various different websites with BeautifulSoup.\n\nI do NOT have a Github repo for my scripts.  Before I do this I want to re-write things because there is a lot of \"Try this, try that, etc\" code. Also some of my variable names need work. \"I use Prefix/Catalog/Index\" in an inconsistent fashion. \n\nI DO have a simple ascii menu system so my code is just 2 files but the menu system lets me run different scripts at different times.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_1jno0tn5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My long weekend data hoard project - workflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qli4g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": 1688521807.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688491905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I love collecting mature videos and love the idea of collecting a copy of an entire series.  With a long weekend I decided to focus on 1 series.  There is a studio that has multiple series so I decided to focus on the &amp;quot;MyHotGirl&amp;quot; series (actual name is different of course).&lt;/p&gt;\n\n&lt;p&gt;Here is the workflow I came up with.&lt;/p&gt;\n\n&lt;p&gt;CATALOG&lt;/p&gt;\n\n&lt;p&gt;The first problem with a &amp;#39;collection&amp;#39; is you need to know all the possible titles.&lt;/p&gt;\n\n&lt;p&gt;I went to the official studio website and discovered they have separate web pages for each series as a catalog/index to the videos. Great. I wanted to create a local copy of these pages so I could scan for titles, dates, actors, etc. at my leisure.&lt;/p&gt;\n\n&lt;p&gt;For series that occupy 2-5 web pages I could simply go to the page, right-clicked and did &amp;quot;Save As..&amp;quot; and saved each page as a single file with .html extension.  The &amp;quot;MyHotGirl&amp;quot; series has been around for years and had 45 web pages.  So I wrote a simple Python script that calculated the Page 1,2,3 URL, did a Request and saved each to a local Catalog file.&lt;/p&gt;\n\n&lt;p&gt;Then I wrote another Python script to use BeautifulSoup. It opens each Catalog file and searches for redirects or &amp;quot;a href&amp;quot; .  There are tons but eventually I discover the pattern that the pages use for a video and made my python script only print these.&lt;/p&gt;\n\n&lt;p&gt;BUT - I don&amp;#39;t want to link/play the video on the studio site.  I want the Release date, title AROUND each title.  I am trying to build a Catalog.&lt;/p&gt;\n\n&lt;p&gt;Knowing one of the URL&amp;#39;s I went back to the Catalog site and right-clicked on the page and did &amp;quot;View Source&amp;quot;.  Then I searched for one of the URL&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;Now - I look above and below the url in the source of the web page.  I am NOT a web page designer but I could see rows with the Title of the video and the Release Date for the video.&lt;/p&gt;\n\n&lt;p&gt;My goal was to derive the likely file name for each title in this format:&lt;/p&gt;\n\n&lt;p&gt;MyHotGirl.YY.MM.DD.Title Title Title Actor Actor Actor&lt;/p&gt;\n\n&lt;p&gt;I fought with BeautifulSoup to get it to extract all 3 parts of the information but failed.  I could get the URL&amp;#39;s of the videos. I could get all Date strings or I could get all text that included video titles - but a lot of other crap as well.&lt;/p&gt;\n\n&lt;p&gt;USING CHATGPT&lt;/p&gt;\n\n&lt;p&gt;Each video was encased in SPAN, CLASS, DIV and other mark up. I copied and pasted all the enclosing HTML into a text editor.&lt;/p&gt;\n\n&lt;p&gt;I then changed the website name to something other than the Porn site and also cleaned up the Title strings so it was not adult.&lt;/p&gt;\n\n&lt;p&gt;Then I fired up ChatGPT and asked it this question:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;I need to use Python3 and BeautifulSoup to extract the Date, Title and Series name from this HTML:  (Paste my cleaned up snippet here)&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;To my shock and delight - It spit out some Python code that could actually work.&lt;/p&gt;\n\n&lt;p&gt;Then I added:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;I need the code above to handle multiple items on the web page and extract all the dates titles and series.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Again - it spit out code like this that now included a loop:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;soup = BeautifulSoup(url, &amp;#39;html.parser&amp;#39;)\nredirects = []\nvideo_items = soup.select(&amp;#39;.thumb.item&amp;#39;)\n\nfor item in video_items:\n    a_tag = item.select_one(&amp;#39;a.thumb__top&amp;#39;)\n    video_url = a_tag[&amp;#39;href&amp;#39;]\n    img_tag = item.select_one(&amp;#39;.thumb__img img&amp;#39;)\n    alt_text = img_tag[&amp;#39;alt&amp;#39;]\n    date_text = item.select_one(.thumb__date&amp;#39;)\n    #print(&amp;quot;Video URL:&amp;quot;, video_url)\n    #print(&amp;quot;Alt Text:&amp;quot;, alt_text)\n    #print(&amp;quot;Date Text:&amp;quot;, date_text)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Suddenly I could now get the Series, Date and Title as a set for each video on each catalog page.&lt;/p&gt;\n\n&lt;p&gt;I ran the code in a loop to print out a line of text for each that looked like this to what I call a &amp;quot;Catalog File&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;MyHotGirl.15.03.11.Title Title Title|N      &amp;lt;-- &amp;#39;N&amp;#39; means Not in my collection\nMyHotGirl.15.04.03.Title Title Title|N\n...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Now I had a text file that should match the file names I already had.  I found a list of most of my videos and searched for &amp;quot;MyHotGirl&amp;quot;. For each row I had - I found that row in my Catalog file and changed &amp;quot;|N&amp;quot; to &amp;quot;|Y&amp;quot; to indicate I had the video in my collection.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;QUESTION&lt;/p&gt;\n\n&lt;p&gt;Is this post interesting to the group? It is very nerdy and technical.  But I thought the workflow or order of operations might be of interest.  The same workflow could be used for many other studios and series.&lt;/p&gt;\n\n&lt;p&gt;I will continue if there is interest.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;&lt;/h1&gt;\n\n&lt;p&gt;Part 2 - FILE DOWNLOADS&lt;/p&gt;\n\n&lt;p&gt;I went to several sites and searched for &amp;quot;MyHotGirl&amp;quot; series of videos. But there were problems:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Some of the sites had horrible search engines. I could type in the name of an actor but it would not show me all the videos with the series name.&lt;/li&gt;\n&lt;li&gt;Some of the sites only contained 10 minute clips&lt;/li&gt;\n&lt;li&gt;Some of the sites would not work with JDownloader2 which is my favorite download engine.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Eventually I discovered a &amp;quot;Porn Try&amp;quot; site that had full clips, the search engine only returned the videos I wanted and the Title of each video page is usually the &amp;quot;Title Title Title&amp;quot; from each video.&lt;/p&gt;\n\n&lt;p&gt;For the first search result page I would right-click and &amp;quot;Open in new tab&amp;quot; to bring up the individual video page. I would highlight the URL and hit Ctrl-C.  JDownloaders &amp;#39;watches&amp;#39; the clipboard and it scanned the page, found the video and used the title of the page as the file name.&lt;/p&gt;\n\n&lt;p&gt;The other important point: It &amp;#39;remembers&amp;#39; the URLs and if I tried to download a second time it marks these in RED.  When I trigger the Download - it asks if I want to ignore duplicates. Very cool and one of the reasons I love this tool.&lt;/p&gt;\n\n&lt;p&gt;So I told it to download all the videos from the first search result page with the &amp;quot;Open in new Tab&amp;quot; and Ctrl-C trick. That got about the first 50 videos downloading.&lt;/p&gt;\n\n&lt;p&gt;Then I got smart.  I took the URL of the second result page and saw it said &amp;quot;/2/&amp;quot; for the second page and &amp;quot;/3/&amp;quot; for the third.&lt;/p&gt;\n\n&lt;p&gt;I wrote another quick Python script to calc the URL for all 14 search result pages - and I saved these to 14 flat files.&lt;/p&gt;\n\n&lt;p&gt;I then opened one of the video pages in my browser. I copied the video URL.&lt;/p&gt;\n\n&lt;p&gt;I looked at the search result page 2 and searched and found the Video URL.&lt;/p&gt;\n\n&lt;p&gt;I again took a snippet and asked ChatGPT to help me find these URLS.&lt;/p&gt;\n\n&lt;p&gt;Suddenly I had a script that would find all the Video URL&amp;#39;s from each flat file search result. Great.&lt;/p&gt;\n\n&lt;p&gt;I can now highlight 20+ URLS and hit Ctrl-C.  JDownloader then spent a few minutes and found all the video files.  I then told it to start downloading.&lt;/p&gt;\n\n&lt;p&gt;I planned to iterate through each search result page.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;PART 3 - FILE NAMES&lt;/p&gt;\n\n&lt;p&gt;I copied all the videos I had downloaded from the Page 1 search results to a new folder.&lt;/p&gt;\n\n&lt;p&gt;I wrote a python script to:&lt;/p&gt;\n\n&lt;p&gt;Read the video names from my Catalog file:  MyHotGirl.YY.MM.DD.Aaaa Bbbb Cccc Dddd|N&lt;/p&gt;\n\n&lt;p&gt;Keep the name but extract just the &amp;quot;Aaaa Bbbb Cccc Dddd&amp;quot; part.&lt;/p&gt;\n\n&lt;p&gt;Loop through the video files and see if &amp;quot;Aaaa Bbbb Cccc Dddd&amp;quot; was in the file name.&lt;/p&gt;\n\n&lt;p&gt;If I found a match - I renamed the file using the full name: MyHotGirl.YY.MM.DD.Aaaa Bbbb Cccc Dddd.mp4&lt;/p&gt;\n\n&lt;p&gt;I got about 20% of the video files renamed this way.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But that means 80% of the video files still did not have the correct &amp;quot;MyHotGirl.YY.MM.DD&amp;quot; prefix to the file name.  &lt;/p&gt;\n\n&lt;p&gt;So I made a second rename pass. But this time I did the following:&lt;/p&gt;\n\n&lt;p&gt;I read all the rows from my Catalog file.&lt;/p&gt;\n\n&lt;p&gt;I stripped the series and YY.MM.DD from the string to leave just the title.&lt;/p&gt;\n\n&lt;p&gt;I lower-cased and removed all the non-alphanumeric characters from the title to give me: aaaabbbbccccddd.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Then I looped through all the actual video files and lower-cased the names and removed the non-alphanumeric characters.  This gave me: &amp;quot;ddddffffggggaaaa720&amp;quot; for each video file. &lt;/p&gt;\n\n&lt;p&gt;Suddenly - I was able to find a lot more matches once spaces, dashes, etc as I looked for the sub-string in the file name.  This allowed me to rename nearly all the video files.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;PART 4 - Do I have the video&lt;/p&gt;\n\n&lt;p&gt;I wrote yet another script.  This time I read all the video file names but I just took &amp;quot;MyHotGirl.YY.MM.DD&amp;quot; from the file name.&lt;/p&gt;\n\n&lt;p&gt;I then read my Catalog file to find a match. If that row ended in &amp;quot;|N&amp;quot; meaning not in my collection, I changed the &amp;quot;|N&amp;quot; to &amp;quot;|Y&amp;quot; and wrote it back out.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Now when I read my Catalog file I print out the total row counts and the rows that end in &amp;quot;Y&amp;quot; to show this:&lt;/p&gt;\n\n&lt;p&gt;Series (mhg) Row count: 483&lt;/p&gt;\n\n&lt;p&gt;Series (mhg) In Collection: 69&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This tells me I have about 14% of the total possible &amp;quot;MyHotGirl&amp;quot; series. Put this is only after downloading 1 search result page. The others are still processing so this percentage will soon go up.&lt;/p&gt;\n\n&lt;p&gt;FINAL THOUGHTS&lt;/p&gt;\n\n&lt;p&gt;Once all the files download, are re-named by my script and the few others renamed by hand, I will see how close to 100% I got.&lt;/p&gt;\n\n&lt;p&gt;The Catalog file will now also tell me what titles I need to find or buy to complete the collection.&lt;/p&gt;\n\n&lt;p&gt;Overall it was a fun 1-2 day project that gave me both a lot of satisfaction to write.&lt;/p&gt;\n\n&lt;p&gt;TOOLS LIST:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Python3 with Requests and BeautifulSoup packages&lt;/li&gt;\n&lt;li&gt;JDownloader to do the actual work.&lt;/li&gt;\n&lt;li&gt;BulkRename - to do some occasional removing funny chars or double spaces&lt;/li&gt;\n&lt;li&gt;ChatGPT - to analyze snippets of HTML and tell me how to extract stuff from the various different websites with BeautifulSoup.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I do NOT have a Github repo for my scripts.  Before I do this I want to re-write things because there is a lot of &amp;quot;Try this, try that, etc&amp;quot; code. Also some of my variable names need work. &amp;quot;I use Prefix/Catalog/Index&amp;quot; in an inconsistent fashion. &lt;/p&gt;\n\n&lt;p&gt;I DO have a simple ascii menu system so my code is just 2 files but the menu system lets me run different scripts at different times.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qli4g", "is_robot_indexable": true, "report_reasons": null, "author": "ShamBawk33", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qli4g/my_long_weekend_data_hoard_project_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qli4g/my_long_weekend_data_hoard_project_workflow/", "subreddit_subscribers": 691156, "created_utc": 1688491905.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Saw this offer on Amazon. Anyone has experience with this drive?", "author_fullname": "t2_2res6mzu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a good drive for the price?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_14qngms", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mgs0JN5BSPv7eJ6YzxuwPng0zbfN9xl6t8QqHpg1riQ.jpg", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688496431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saw this offer on Amazon. Anyone has experience with this drive?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ri0spihsuz9b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ri0spihsuz9b1.jpg?auto=webp&amp;v=enabled&amp;s=a24f83f6d5f7d588bbb24c50e8a6531eafdc3d2f", "width": 1064, "height": 3031}, "resolutions": [{"url": "https://preview.redd.it/ri0spihsuz9b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=891cb7e819e75bea255dbfb0f2f897cd7a1ee4e5", "width": 108, "height": 216}, {"url": "https://preview.redd.it/ri0spihsuz9b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4592f4441f965ea784d55344a0e9441b3918dd6c", "width": 216, "height": 432}, {"url": "https://preview.redd.it/ri0spihsuz9b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4ec8ae106784ff80b6e05f600e9e443e2e11e9a", "width": 320, "height": 640}, {"url": "https://preview.redd.it/ri0spihsuz9b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe9778a013d8ab5d2ac2909ef33a8c04b6de4cd6", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/ri0spihsuz9b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9b72ae33cc4895359315442757e22a29fa26752", "width": 960, "height": 1920}], "variants": {}, "id": "YNpqV0JG5PnX2k7w2CcVqECtDx4cbT_ah2MQUTyvfFI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "150 TB in Google Drive ", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qngms", "is_robot_indexable": true, "report_reasons": null, "author": "tempoguyx", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14qngms/is_it_a_good_drive_for_the_price/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ri0spihsuz9b1.jpg", "subreddit_subscribers": 691156, "created_utc": 1688496431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Im trying to get an Excel sheet of all my Liked Videos, before I delete my YouTube Account.\n\nThe method in this video seems to work, however, YouTube have removed the option to make Liked Videos playlist public, so this won't work for Liked videos.\n\nDoes anyone know of a way around this?  \n\n\n[https://www.youtube.com/watch?v=jZUw6qIHl5o](https://www.youtube.com/watch?v=jZUw6qIHl5o)", "author_fullname": "t2_113aya", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "YouTube \"Liked Videos\" Playlist to Excel workaround?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14r8wmx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688559136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im trying to get an Excel sheet of all my Liked Videos, before I delete my YouTube Account.&lt;/p&gt;\n\n&lt;p&gt;The method in this video seems to work, however, YouTube have removed the option to make Liked Videos playlist public, so this won&amp;#39;t work for Liked videos.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of a way around this?  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=jZUw6qIHl5o\"&gt;https://www.youtube.com/watch?v=jZUw6qIHl5o&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RlwOmp3M6RaL3tMtcjdtjcz5_U7zPdAxoU9YePfBf50.jpg?auto=webp&amp;v=enabled&amp;s=f3e85e543b98296d2c12569b98b7d8afd54acd3f", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/RlwOmp3M6RaL3tMtcjdtjcz5_U7zPdAxoU9YePfBf50.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=213227310a5a173b73d3f80b2f9e8c6b6b485d08", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/RlwOmp3M6RaL3tMtcjdtjcz5_U7zPdAxoU9YePfBf50.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c31138d7d1f33dc8101ff2eea10a5144175b2ad0", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/RlwOmp3M6RaL3tMtcjdtjcz5_U7zPdAxoU9YePfBf50.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5019fd759e6095e7ba7171465c8799f9dd5b56e4", "width": 320, "height": 240}], "variants": {}, "id": "s2PYoMHJ4S7DqCQs5dpl7N2-crdqv-gJteQwtCHxrzA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14r8wmx", "is_robot_indexable": true, "report_reasons": null, "author": "terrycells", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14r8wmx/youtube_liked_videos_playlist_to_excel_workaround/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14r8wmx/youtube_liked_videos_playlist_to_excel_workaround/", "subreddit_subscribers": 691156, "created_utc": 1688559136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I edit videos, so I have a lot of clips and images in a bunch of different folders (spanning years) that I need to organise, are there any software recommendations where I can tag, give descriptions, etc to my media? Also a decent search is also important.\n\nI looked at diffractor which is good but the dev doesn't seem active anymore and it will probably start to break after a few years with new formats.\n\nThis is not for movies or TV, but camera footage, images and audio.", "author_fullname": "t2_n7c4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Media organisation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qt2qf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688510020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I edit videos, so I have a lot of clips and images in a bunch of different folders (spanning years) that I need to organise, are there any software recommendations where I can tag, give descriptions, etc to my media? Also a decent search is also important.&lt;/p&gt;\n\n&lt;p&gt;I looked at diffractor which is good but the dev doesn&amp;#39;t seem active anymore and it will probably start to break after a few years with new formats.&lt;/p&gt;\n\n&lt;p&gt;This is not for movies or TV, but camera footage, images and audio.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qt2qf", "is_robot_indexable": true, "report_reasons": null, "author": "Buzstringer", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qt2qf/media_organisation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qt2qf/media_organisation/", "subreddit_subscribers": 691156, "created_utc": 1688510020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have about 300 gigs of Steam game demos I want to put on Archive.org. Are there any files that could contain personal details linking back to my account within the files?\n\nI'd like to stress these are literally **just** the demos. These aren't the full games.", "author_fullname": "t2_w6lyzny8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there anything I should remove from Steam demos before uploading to archive.org?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qmghg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688494123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have about 300 gigs of Steam game demos I want to put on Archive.org. Are there any files that could contain personal details linking back to my account within the files?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to stress these are literally &lt;strong&gt;just&lt;/strong&gt; the demos. These aren&amp;#39;t the full games.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qmghg", "is_robot_indexable": true, "report_reasons": null, "author": "JebryyathHS", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qmghg/is_there_anything_i_should_remove_from_steam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qmghg/is_there_anything_i_should_remove_from_steam/", "subreddit_subscribers": 691156, "created_utc": 1688494123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I remember reading about the internet archive making a copy of all of their data in 2017, and storing the storage units somewhere... but I can't find a single article on this now. Can someone help me find this?", "author_fullname": "t2_4i9ed589", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a physical copy of the entire internet archive somewhere?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qlzj9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688493057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I remember reading about the internet archive making a copy of all of their data in 2017, and storing the storage units somewhere... but I can&amp;#39;t find a single article on this now. Can someone help me find this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "what is a hard drive", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qlzj9", "is_robot_indexable": true, "report_reasons": null, "author": "Anoyint", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14qlzj9/is_there_a_physical_copy_of_the_entire_internet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qlzj9/is_there_a_physical_copy_of_the_entire_internet/", "subreddit_subscribers": 691156, "created_utc": 1688493057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've normally have done dual HD in RAID 1 in my computers in the past. I'm looking at possible doing a 5 bay set up but was wondering if I could just get 3 more of the drives I currently have and put them in and set the box up so I could move the data over and then wipe the two drives in my computer and add them to the box and have a 5 drive at the end. I know Drobo could do something like that, but wasn't sure if other companies could.", "author_fullname": "t2_1ftqbdxt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First External Backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qn7s1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688495900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve normally have done dual HD in RAID 1 in my computers in the past. I&amp;#39;m looking at possible doing a 5 bay set up but was wondering if I could just get 3 more of the drives I currently have and put them in and set the box up so I could move the data over and then wipe the two drives in my computer and add them to the box and have a 5 drive at the end. I know Drobo could do something like that, but wasn&amp;#39;t sure if other companies could.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qn7s1", "is_robot_indexable": true, "report_reasons": null, "author": "shadow1013", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qn7s1/first_external_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qn7s1/first_external_backup/", "subreddit_subscribers": 691156, "created_utc": 1688495900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm in the process of migrating a \\~3TB video library from an external hard drive to my new server, and there are several files which cannot be copied.\n\n`sync -ruhv --info=progress2 --exclude .Trash-1000 /src_dir /dest_dir`\n\nproduced the following error while running:\n\n`path/to/file.mp4`  \n`53.99G   2%   59.27MB/s    0:14:28 (xfr#22, ir-chk=1628/2101)`  \n`rsync: read errors mapping \"path/to/file.mp4\": Input/output error (5)`\n\nThen at the end of excecution produces this error:\n\n`WARNING: path/to/file.mp4 failed verification -- update discarded (will try again).`\n\nCopying the files with \\`cp\\` or using a file browser also fails.\n\nAttempting to play back the videos works fine for awhile, but 30-60 min in depending on the file the media player crashes.  I assume this means the file has become corrupted somehow and/or there are physical problems with the drive.  Fortunately this is all media which I purchased and can re-download, but I am alarmed that so many files have become corrupted without warning.\n\nI have 2 questions I'm hoping y'all might have some input on:\n\n1. Are there periodic file system/drive integrity checks I should be running that would have caught this earlier?  How do you monitor for and mitigate data corruption in your archives?\n2. Now I am in this position, is there anything to be done except try \\`fsck\\` and see if it helps or restore from a backup?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_35o6p4z4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Preventing/Mitigating Data Corruption?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qpz6v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688502372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in the process of migrating a ~3TB video library from an external hard drive to my new server, and there are several files which cannot be copied.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sync -ruhv --info=progress2 --exclude .Trash-1000 /src_dir /dest_dir&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;produced the following error while running:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;path/to/file.mp4&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;53.99G   2%   59.27MB/s    0:14:28 (xfr#22, ir-chk=1628/2101)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;rsync: read errors mapping &amp;quot;path/to/file.mp4&amp;quot;: Input/output error (5)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Then at the end of excecution produces this error:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;WARNING: path/to/file.mp4 failed verification -- update discarded (will try again).&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Copying the files with `cp` or using a file browser also fails.&lt;/p&gt;\n\n&lt;p&gt;Attempting to play back the videos works fine for awhile, but 30-60 min in depending on the file the media player crashes.  I assume this means the file has become corrupted somehow and/or there are physical problems with the drive.  Fortunately this is all media which I purchased and can re-download, but I am alarmed that so many files have become corrupted without warning.&lt;/p&gt;\n\n&lt;p&gt;I have 2 questions I&amp;#39;m hoping y&amp;#39;all might have some input on:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Are there periodic file system/drive integrity checks I should be running that would have caught this earlier?  How do you monitor for and mitigate data corruption in your archives?&lt;/li&gt;\n&lt;li&gt;Now I am in this position, is there anything to be done except try `fsck` and see if it helps or restore from a backup?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qpz6v", "is_robot_indexable": true, "report_reasons": null, "author": "BuilderOfDragons", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qpz6v/preventingmitigating_data_corruption/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qpz6v/preventingmitigating_data_corruption/", "subreddit_subscribers": 691156, "created_utc": 1688502372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "since the application support for LTFS on linux only for IBM, can I use LTFS via virtualbox to access the tape drive?\n\nor LTFS software can run using wine?", "author_fullname": "t2_7bbojdkz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTFS HP StoreEver Tape Drive via VirtualBox", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14r2h2v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688541834.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688539246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;since the application support for LTFS on linux only for IBM, can I use LTFS via virtualbox to access the tape drive?&lt;/p&gt;\n\n&lt;p&gt;or LTFS software can run using wine?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14r2h2v", "is_robot_indexable": true, "report_reasons": null, "author": "kokizzu2", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14r2h2v/ltfs_hp_storeever_tape_drive_via_virtualbox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14r2h2v/ltfs_hp_storeever_tape_drive_via_virtualbox/", "subreddit_subscribers": 691156, "created_utc": 1688539246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Best free software for ongoing backup from one external HDD to another?\n\nIn short, I have one external 4TB HDD that I use for Google Drive and a few other things, and I'd like to have an ongoing exact copy on a second external HDD. \n\nI tried iDrive since I'm using it for a cloud backup, but it put the files in an odd format so that they weren't browsable. I also tried AOMEI, and it did make a backup that was browsable, but something didn't feel reliable about it. Hard to explain.\n\nI just want an exact copy on the second drive, updated either continuously or maybe daily on a schedule, and only updated with the files that changed. \n\nAny suggestions?\n\nAlso, yes, HDDs are outdated, I know. Someday I'll replace both of these with SSDs. Someday... lol", "author_fullname": "t2_6yksj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best free software for ongoing backup from one external HDD to another?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qrt9d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688506781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Best free software for ongoing backup from one external HDD to another?&lt;/p&gt;\n\n&lt;p&gt;In short, I have one external 4TB HDD that I use for Google Drive and a few other things, and I&amp;#39;d like to have an ongoing exact copy on a second external HDD. &lt;/p&gt;\n\n&lt;p&gt;I tried iDrive since I&amp;#39;m using it for a cloud backup, but it put the files in an odd format so that they weren&amp;#39;t browsable. I also tried AOMEI, and it did make a backup that was browsable, but something didn&amp;#39;t feel reliable about it. Hard to explain.&lt;/p&gt;\n\n&lt;p&gt;I just want an exact copy on the second drive, updated either continuously or maybe daily on a schedule, and only updated with the files that changed. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n\n&lt;p&gt;Also, yes, HDDs are outdated, I know. Someday I&amp;#39;ll replace both of these with SSDs. Someday... lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qrt9d", "is_robot_indexable": true, "report_reasons": null, "author": "visodd", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qrt9d/best_free_software_for_ongoing_backup_from_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qrt9d/best_free_software_for_ongoing_backup_from_one/", "subreddit_subscribers": 691156, "created_utc": 1688506781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Recently I have started to collect some Xbox, Xbox 360 and Xbox one game discs. Since they are all used, some of them may have some minor scratches on them.\n\nSo I need to check some of them to see if they are 100% readable.\n\nThe best way to do this is to calculate the MD5Sum of the discs and compare it against other peoples' MD5Sums at redump. \n\nOne way of calculating the MD5Sum of the disc is to make an .iso image file of the disc and then calculate the MD5Sum of the .iso file. There are plenty of programs that can do this.\n\nMy question is, Can I skip making an .iso image file and directly calculate the MD5Sum of the discs ? is there a utility for that on Windows ?\n\n&amp;#x200B;\n\nAny other way to make sure the discs are 100% readable ? ", "author_fullname": "t2_cb7fdihh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to make sure an Xbox disc is 100% readable ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qnmfl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688496794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently I have started to collect some Xbox, Xbox 360 and Xbox one game discs. Since they are all used, some of them may have some minor scratches on them.&lt;/p&gt;\n\n&lt;p&gt;So I need to check some of them to see if they are 100% readable.&lt;/p&gt;\n\n&lt;p&gt;The best way to do this is to calculate the MD5Sum of the discs and compare it against other peoples&amp;#39; MD5Sums at redump. &lt;/p&gt;\n\n&lt;p&gt;One way of calculating the MD5Sum of the disc is to make an .iso image file of the disc and then calculate the MD5Sum of the .iso file. There are plenty of programs that can do this.&lt;/p&gt;\n\n&lt;p&gt;My question is, Can I skip making an .iso image file and directly calculate the MD5Sum of the discs ? is there a utility for that on Windows ?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any other way to make sure the discs are 100% readable ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qnmfl", "is_robot_indexable": true, "report_reasons": null, "author": "l3gi0n0fH3ll", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qnmfl/how_to_make_sure_an_xbox_disc_is_100_readable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qnmfl/how_to_make_sure_an_xbox_disc_is_100_readable/", "subreddit_subscribers": 691156, "created_utc": 1688496794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have 3 of these drives and having to reach around all the time is a pain in the butt to unplug when I am done using them\n\nI dont want to use a hub since transferring from drive to drive on a hub is going to be really slow. So I am wanting to run them one by one into my motherboard since I have enough usb ports avail.\n\nDo they make single switches for each drive?\n\nAny other ideas on how to do this?", "author_fullname": "t2_lzwh7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Usb Seagate Expansion Drive help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qnazx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688496097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 3 of these drives and having to reach around all the time is a pain in the butt to unplug when I am done using them&lt;/p&gt;\n\n&lt;p&gt;I dont want to use a hub since transferring from drive to drive on a hub is going to be really slow. So I am wanting to run them one by one into my motherboard since I have enough usb ports avail.&lt;/p&gt;\n\n&lt;p&gt;Do they make single switches for each drive?&lt;/p&gt;\n\n&lt;p&gt;Any other ideas on how to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qnazx", "is_robot_indexable": true, "report_reasons": null, "author": "cmdrmcgarrett", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qnazx/usb_seagate_expansion_drive_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qnazx/usb_seagate_expansion_drive_help/", "subreddit_subscribers": 691156, "created_utc": 1688496097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I am looking for a high endurance ssd that performs well. When comparing a Samsung 870 EVO 2TB to a Samsung PM893 1TB (roughly the same price) I noticed that the EVO is a 3-bit MLC vs the data center oriented PM893 is a TLC drive. I was under the impression that MLC is superior to TLC in terms of endurance so now I am confused even more on which to go for? Can anyone shed some light on what I am missing?\n\n[https://i.imgur.com/LVrYZsR.png](https://i.imgur.com/LVrYZsR.png)\n\nscreenshot of the side by side comparison.", "author_fullname": "t2_bguqd9mp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Samsung mlc vs tlc endurance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qiqxo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688485612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am looking for a high endurance ssd that performs well. When comparing a Samsung 870 EVO 2TB to a Samsung PM893 1TB (roughly the same price) I noticed that the EVO is a 3-bit MLC vs the data center oriented PM893 is a TLC drive. I was under the impression that MLC is superior to TLC in terms of endurance so now I am confused even more on which to go for? Can anyone shed some light on what I am missing?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/LVrYZsR.png\"&gt;https://i.imgur.com/LVrYZsR.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;screenshot of the side by side comparison.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?auto=webp&amp;v=enabled&amp;s=046a5c68a1f16470e50806b5a1717fe921bf70e0", "width": 1793, "height": 831}, "resolutions": [{"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9f056401726a3f040163ee68b91ade4c6239999", "width": 108, "height": 50}, {"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e1ac6bd93f601adc19e2d54808d2f2791e06e9b", "width": 216, "height": 100}, {"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=053cf9c643253c65b682d035fa5c5472986fb42f", "width": 320, "height": 148}, {"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c20f62007aaa7126b2306056e9fb5be5bd33080a", "width": 640, "height": 296}, {"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=759ceaf00b147db4a0e932718980d1347f297fc9", "width": 960, "height": 444}, {"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b65a3a8c91b3a98a3f3d2c82c217e20e1ac3f86", "width": 1080, "height": 500}], "variants": {}, "id": "DoBVfJeD_491-95tHhIp2KtEPAk_VBKtIhriezCU3YQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qiqxo", "is_robot_indexable": true, "report_reasons": null, "author": "itidi0t", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qiqxo/samsung_mlc_vs_tlc_endurance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qiqxo/samsung_mlc_vs_tlc_endurance/", "subreddit_subscribers": 691156, "created_utc": 1688485612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone. I'm in the process of backing up some photos that are extremely important to me. The capacity is around 10 terabytes, and for now, I've settled on using raid 6 with snapraid for the purposes of avoiding bit rot and protecting against two drive failures. The current plan is to regularly scrub the drives for errors (i.e. bitflips, bit rot). I plan to buy 5x 4tb WD Red Pro HDDs (CMR), as I've heard non-CMR drives can become a headache as sync/scrub/rebuild times are much worse. This backup will not be a NAS and will probably remain as individually stored internal HDDs in a housing of some kind. Will this setup be realistic for adding photos later and doing regular sync/scrub operations? I'm a photographer, so the capacity will grow over time. Ideally, I want to create a setup that doesn't take a long time to sync/scrub (4+ hours a week). I was also wondering if this setup would work well with 8tb drives. \n\nI plan to maybe duplicate this setup for an offsite backup. Does this plan look decent overall?\n\nThank you for your time and assistance!", "author_fullname": "t2_gk2se", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Protecting irreplaceable photos with 5x 4tb RAID 6", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14rb6e8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688564844.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I&amp;#39;m in the process of backing up some photos that are extremely important to me. The capacity is around 10 terabytes, and for now, I&amp;#39;ve settled on using raid 6 with snapraid for the purposes of avoiding bit rot and protecting against two drive failures. The current plan is to regularly scrub the drives for errors (i.e. bitflips, bit rot). I plan to buy 5x 4tb WD Red Pro HDDs (CMR), as I&amp;#39;ve heard non-CMR drives can become a headache as sync/scrub/rebuild times are much worse. This backup will not be a NAS and will probably remain as individually stored internal HDDs in a housing of some kind. Will this setup be realistic for adding photos later and doing regular sync/scrub operations? I&amp;#39;m a photographer, so the capacity will grow over time. Ideally, I want to create a setup that doesn&amp;#39;t take a long time to sync/scrub (4+ hours a week). I was also wondering if this setup would work well with 8tb drives. &lt;/p&gt;\n\n&lt;p&gt;I plan to maybe duplicate this setup for an offsite backup. Does this plan look decent overall?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time and assistance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14rb6e8", "is_robot_indexable": true, "report_reasons": null, "author": "AngelicBread", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14rb6e8/protecting_irreplaceable_photos_with_5x_4tb_raid_6/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14rb6e8/protecting_irreplaceable_photos_with_5x_4tb_raid_6/", "subreddit_subscribers": 691156, "created_utc": 1688564844.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to digitize all of my family photos in a way that won't take me many hours...  \n\n\nCan anyone recommend a photo scanner that allows you to quickly scan standard-size photos?   \n\n\nReally appreciate any advice or recommendations!", "author_fullname": "t2_8l0m6sg0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can anyone recommend me a \"fast\" scanner to digitize/convert hundreds of family photos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14raj29", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688563289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to digitize all of my family photos in a way that won&amp;#39;t take me many hours...  &lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend a photo scanner that allows you to quickly scan standard-size photos?   &lt;/p&gt;\n\n&lt;p&gt;Really appreciate any advice or recommendations!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14raj29", "is_robot_indexable": true, "report_reasons": null, "author": "Significant_Ad1953", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14raj29/can_anyone_recommend_me_a_fast_scanner_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14raj29/can_anyone_recommend_me_a_fast_scanner_to/", "subreddit_subscribers": 691156, "created_utc": 1688563289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just found my old Kingston Micro SD card from maybe 2012. I'm 100% sure that it contains old photos from then.\n\nThe problem is when I try to use it on my card reader or even on my phone, it just won't work. Neither the phone nor PC detects it. It's not physically damaged, I know it was working fine back then.\n\nAny tips on what should I do to get those photos back? I would appreciate it alot.", "author_fullname": "t2_ipt4q4p5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My old Micro SD Card stopped working?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14r863c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688557391.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688556991.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just found my old Kingston Micro SD card from maybe 2012. I&amp;#39;m 100% sure that it contains old photos from then.&lt;/p&gt;\n\n&lt;p&gt;The problem is when I try to use it on my card reader or even on my phone, it just won&amp;#39;t work. Neither the phone nor PC detects it. It&amp;#39;s not physically damaged, I know it was working fine back then.&lt;/p&gt;\n\n&lt;p&gt;Any tips on what should I do to get those photos back? I would appreciate it alot.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14r863c", "is_robot_indexable": true, "report_reasons": null, "author": "Za6olo", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14r863c/my_old_micro_sd_card_stopped_working/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14r863c/my_old_micro_sd_card_stopped_working/", "subreddit_subscribers": 691156, "created_utc": 1688556991.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Will be used daily for games, storage, and backups. Feel free to recommend anything else around the $200 price point. These 2 have caught my interest because reliability is my number one concern. I'm just concerned about why the larger drive with the more impressive specs is so much cheaper. Both clearly say they are in new condition.\n\n[https://www.amazon.com/Western-Digital-14TB-Ultrastar-HC530/dp/B07KPL474H](https://www.amazon.com/Western-Digital-14TB-Ultrastar-HC530/dp/B07KPL474H)  \nThe 14 TB WD Ultrastar DC HC530 has 512 MB of cache, is larger in capacity, and is much cheaper at $160. Plus, since it's larger it has the HelioSeal technology.\n\n[https://www.amazon.com/Gold-Enterprise-Class-Internal-Drive/dp/B07XGDNZXT](https://www.amazon.com/Gold-Enterprise-Class-Internal-Drive/dp/B07XGDNZXT)  \nThe 8 TB WD Gold only has 256 MB of cache, is smaller in capacity, and is more expensive at $210. The same capacity 14 TB WD Gold would cost $320.\n\nThanks in advance.", "author_fullname": "t2_7upnbpdh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On the search for a new \u2265 8TB 3.5\" SATA 7200 RPM hard drive. Need advice.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qyy9t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688527587.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Will be used daily for games, storage, and backups. Feel free to recommend anything else around the $200 price point. These 2 have caught my interest because reliability is my number one concern. I&amp;#39;m just concerned about why the larger drive with the more impressive specs is so much cheaper. Both clearly say they are in new condition.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/Western-Digital-14TB-Ultrastar-HC530/dp/B07KPL474H\"&gt;https://www.amazon.com/Western-Digital-14TB-Ultrastar-HC530/dp/B07KPL474H&lt;/a&gt;&lt;br/&gt;\nThe 14 TB WD Ultrastar DC HC530 has 512 MB of cache, is larger in capacity, and is much cheaper at $160. Plus, since it&amp;#39;s larger it has the HelioSeal technology.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/Gold-Enterprise-Class-Internal-Drive/dp/B07XGDNZXT\"&gt;https://www.amazon.com/Gold-Enterprise-Class-Internal-Drive/dp/B07XGDNZXT&lt;/a&gt;&lt;br/&gt;\nThe 8 TB WD Gold only has 256 MB of cache, is smaller in capacity, and is more expensive at $210. The same capacity 14 TB WD Gold would cost $320.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qyy9t", "is_robot_indexable": true, "report_reasons": null, "author": "idkwhatimdoing1208", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qyy9t/on_the_search_for_a_new_8tb_35_sata_7200_rpm_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qyy9t/on_the_search_for_a_new_8tb_35_sata_7200_rpm_hard/", "subreddit_subscribers": 691156, "created_utc": 1688527587.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_7xnlre8a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "USB 3 to sata controller trouble", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_14qqq53", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xDLFMxID6gQDTvySuATKCrid1qx1YiihiWWU-lzSiGs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688504162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/wp2b98orh0ab1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/wp2b98orh0ab1.jpg?auto=webp&amp;v=enabled&amp;s=6f8122ba50ff21eebbc7206f1ba81752c9af405a", "width": 3024, "height": 4032}, "resolutions": [{"url": "https://preview.redd.it/wp2b98orh0ab1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20089dd9271e74aca905f34075888d9c88e2bf1e", "width": 108, "height": 144}, {"url": "https://preview.redd.it/wp2b98orh0ab1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5df38b8c35b4b8a9cd7376975ea26525d8fc4f7d", "width": 216, "height": 288}, {"url": "https://preview.redd.it/wp2b98orh0ab1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=768672c14407f5f8e5265fb5bbb9fd3b06a1768b", "width": 320, "height": 426}, {"url": "https://preview.redd.it/wp2b98orh0ab1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92fc98ef17d2e58eb3cb9481d21d1f3046dbac6d", "width": 640, "height": 853}, {"url": "https://preview.redd.it/wp2b98orh0ab1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7cac4a704d550fddda31ff5fc160f2bbaaef191", "width": 960, "height": 1280}, {"url": "https://preview.redd.it/wp2b98orh0ab1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a160f0baf790e68a13a1d00f512845baea74c345", "width": 1080, "height": 1440}], "variants": {}, "id": "0askq5PjiOpRpx50W7jh5I7wS10kFqVTpijJh-oHBnE"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qqq53", "is_robot_indexable": true, "report_reasons": null, "author": "Ancient-Stress-2525", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qqq53/usb_3_to_sata_controller_trouble/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/wp2b98orh0ab1.jpg", "subreddit_subscribers": 691156, "created_utc": 1688504162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently I have a laptop in the basement that runs Ubuntu for AFP, SMB and Plex, through a DVD to SATA adapter I have a 500GB SSD and a 1TB HDD. It only supports 2.5 inch. I am trying to get at least a few more terabytes as I rip home DVDs and I wanted to get your opinions. \n\nI'm trying to do this as cheaply as possible. I was thinking of either getting 2 4TB harddrives for around $240 or getting 2 4TB SSDs for $300. I looked into a 4-Bay NAS but the cost would be about $300 just for the NAS, but a potential upside would be that I can expand it further later and the drives would potentially be cheaper. \n\nWhat do you all think? Open to suggestions.", "author_fullname": "t2_3lurx74i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expanding my storage, what is best?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qpzxq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688502420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I have a laptop in the basement that runs Ubuntu for AFP, SMB and Plex, through a DVD to SATA adapter I have a 500GB SSD and a 1TB HDD. It only supports 2.5 inch. I am trying to get at least a few more terabytes as I rip home DVDs and I wanted to get your opinions. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to do this as cheaply as possible. I was thinking of either getting 2 4TB harddrives for around $240 or getting 2 4TB SSDs for $300. I looked into a 4-Bay NAS but the cost would be about $300 just for the NAS, but a potential upside would be that I can expand it further later and the drives would potentially be cheaper. &lt;/p&gt;\n\n&lt;p&gt;What do you all think? Open to suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.5TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qpzxq", "is_robot_indexable": true, "report_reasons": null, "author": "LavaCreeperBOSSB", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14qpzxq/expanding_my_storage_what_is_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qpzxq/expanding_my_storage_what_is_best/", "subreddit_subscribers": 691156, "created_utc": 1688502420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I know there are a lot of posts about keeping an archive of personal emails. I've read through many of them and think I have found a solution that will work for me. However, I'm just knowledgeable enough to figure out a solution, but not knowledgeable enough to have good context on the practicality of the solution or if I've missed something that will cause me problems down the road. I'd be super appreciative if anyone can offer any insight into if the goals/ solutions shown below make sense. Thanks in advance!  \n\n\nGoals: \n\nI want to archive my personal emails from the last 15 years or so across a gmail account and a few personal accounts I have through a personal website. The goal is to keep these old emails incase I want to reference them and maybe eventually organize them further, off of my computer/ servers to save disk space/ server space, and easily accessible/ searchable on an ssd drive. Additionally, I want to have the emails that are currently not archived to be accessible on my normal devices.   \n\n\nMy Solution/ process: \n\nI downloaded Thunderbird and saved it to a fairly decent sized SSD drive.   \nI connected my email account to Thunderbird with IMAP.  \nIn Thunderbird, I'll create local subfolders for each Email account I want to archive.  \nWhen I wan't to archive a bunch of emails, I'll drag/ drop them from the inbox to the local folder. \n\nI haven't dug into it, but maybe I can set up an archiving rule in Thunderbird to transfer emails older than a certain time period to the local folder. I'm assuming this would automatically happen when I open Thunderbird after being disconnected for some time.  \n\n\nThis seems fairly straight forward, but would love to know if I'm missing anything important.   \n\n\nThanks again!", "author_fullname": "t2_7xu217zo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Portable Thunderbird Email Archive - Any Red Flags?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qmkb6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688494368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I know there are a lot of posts about keeping an archive of personal emails. I&amp;#39;ve read through many of them and think I have found a solution that will work for me. However, I&amp;#39;m just knowledgeable enough to figure out a solution, but not knowledgeable enough to have good context on the practicality of the solution or if I&amp;#39;ve missed something that will cause me problems down the road. I&amp;#39;d be super appreciative if anyone can offer any insight into if the goals/ solutions shown below make sense. Thanks in advance!  &lt;/p&gt;\n\n&lt;p&gt;Goals: &lt;/p&gt;\n\n&lt;p&gt;I want to archive my personal emails from the last 15 years or so across a gmail account and a few personal accounts I have through a personal website. The goal is to keep these old emails incase I want to reference them and maybe eventually organize them further, off of my computer/ servers to save disk space/ server space, and easily accessible/ searchable on an ssd drive. Additionally, I want to have the emails that are currently not archived to be accessible on my normal devices.   &lt;/p&gt;\n\n&lt;p&gt;My Solution/ process: &lt;/p&gt;\n\n&lt;p&gt;I downloaded Thunderbird and saved it to a fairly decent sized SSD drive.&lt;br/&gt;\nI connected my email account to Thunderbird with IMAP.&lt;br/&gt;\nIn Thunderbird, I&amp;#39;ll create local subfolders for each Email account I want to archive.&lt;br/&gt;\nWhen I wan&amp;#39;t to archive a bunch of emails, I&amp;#39;ll drag/ drop them from the inbox to the local folder. &lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t dug into it, but maybe I can set up an archiving rule in Thunderbird to transfer emails older than a certain time period to the local folder. I&amp;#39;m assuming this would automatically happen when I open Thunderbird after being disconnected for some time.  &lt;/p&gt;\n\n&lt;p&gt;This seems fairly straight forward, but would love to know if I&amp;#39;m missing anything important.   &lt;/p&gt;\n\n&lt;p&gt;Thanks again!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qmkb6", "is_robot_indexable": true, "report_reasons": null, "author": "JacksonIsBillCarson", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qmkb6/portable_thunderbird_email_archive_any_red_flags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qmkb6/portable_thunderbird_email_archive_any_red_flags/", "subreddit_subscribers": 691156, "created_utc": 1688494368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently, google for education announced changes in storage space. I now need to move all my files from my .edu account to my normal .gmail account. \n\nI have tried a few things:\n\n1. Download and upload: This does not work for google files, it saves as a .gdoc file and when it's uploaded, the file cannot be viewed or access.\n2. Transfer ownership: This also doesn't work and I get the following message: Sorry, cannot transfer ownership to x@gmail.com. Ownership can only be transferred to another user in the same organization as the current owner.\n\nPlease help :( ", "author_fullname": "t2_6por8pt9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RIP Google Education Unlimited Storage: How to transfer ownership of google files (GSheets, GDocs) to another organization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14r4w90", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688547105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently, google for education announced changes in storage space. I now need to move all my files from my .edu account to my normal .gmail account. &lt;/p&gt;\n\n&lt;p&gt;I have tried a few things:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download and upload: This does not work for google files, it saves as a .gdoc file and when it&amp;#39;s uploaded, the file cannot be viewed or access.&lt;/li&gt;\n&lt;li&gt;Transfer ownership: This also doesn&amp;#39;t work and I get the following message: Sorry, cannot transfer ownership to &lt;a href=\"mailto:x@gmail.com\"&gt;x@gmail.com&lt;/a&gt;. Ownership can only be transferred to another user in the same organization as the current owner.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Please help :( &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14r4w90", "is_robot_indexable": true, "report_reasons": null, "author": "beanutbutterbanana", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14r4w90/rip_google_education_unlimited_storage_how_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14r4w90/rip_google_education_unlimited_storage_how_to/", "subreddit_subscribers": 691156, "created_utc": 1688547105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Im curios if there is an program where u can like sort files to a specific \u201ebinder\u201c\nLike it shows a picture, than u can say where it can go and than it shows the next picture and so on. And u can say like u have buttons that u can connect to a specific binder that goes like \u201edog\u201c goes to dog folder, shows next picture and u can select again :)", "author_fullname": "t2_5gu0r67y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Picture/Video Sorter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14r7beb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688554496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im curios if there is an program where u can like sort files to a specific \u201ebinder\u201c\nLike it shows a picture, than u can say where it can go and than it shows the next picture and so on. And u can say like u have buttons that u can connect to a specific binder that goes like \u201edog\u201c goes to dog folder, shows next picture and u can select again :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14r7beb", "is_robot_indexable": true, "report_reasons": null, "author": "LittleThunderX", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14r7beb/picturevideo_sorter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14r7beb/picturevideo_sorter/", "subreddit_subscribers": 691156, "created_utc": 1688554496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently bought 3x WD 16gb ultrastar (renewed) drives from amazon $169 ea..  seems like a pretty solid deal, they came nicely packed and have reconditioned printed on side of the drive..  the smart table info is zero'd out on them, i figure its maybe like used drives from elsewhere.   needless to say i am doing the full burn-in/testing recommended by truenas before i put anything on them...\n\nI am definitely skeptical about the origin of these..  Anyone else using these and can share their experience? Anything I should watch out for?  Thanks.", "author_fullname": "t2_1wjo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Opinions on renewed HDDs from Amazon?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qmzcb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688495336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently bought 3x WD 16gb ultrastar (renewed) drives from amazon $169 ea..  seems like a pretty solid deal, they came nicely packed and have reconditioned printed on side of the drive..  the smart table info is zero&amp;#39;d out on them, i figure its maybe like used drives from elsewhere.   needless to say i am doing the full burn-in/testing recommended by truenas before i put anything on them...&lt;/p&gt;\n\n&lt;p&gt;I am definitely skeptical about the origin of these..  Anyone else using these and can share their experience? Anything I should watch out for?  Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qmzcb", "is_robot_indexable": true, "report_reasons": null, "author": "dss", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qmzcb/opinions_on_renewed_hdds_from_amazon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qmzcb/opinions_on_renewed_hdds_from_amazon/", "subreddit_subscribers": 691156, "created_utc": 1688495336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking at getting a NAS going and had been planing on RAID5 all the time. I jsut used a 4x3TB NAS before with RAID5 without any issues. But now I've been seeing so many warnings about large disks and RAID5 and that you should be doing RAID6. \n\nI had been thinking of 16TB disks and 6 bay at this time. Should I settle with RAID6 and 64TB?", "author_fullname": "t2_10fiz2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the consensus on setting up a NAS with large disks, is RAID5 really obsolete with them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qltvp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688492682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking at getting a NAS going and had been planing on RAID5 all the time. I jsut used a 4x3TB NAS before with RAID5 without any issues. But now I&amp;#39;ve been seeing so many warnings about large disks and RAID5 and that you should be doing RAID6. &lt;/p&gt;\n\n&lt;p&gt;I had been thinking of 16TB disks and 6 bay at this time. Should I settle with RAID6 and 64TB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qltvp", "is_robot_indexable": true, "report_reasons": null, "author": "Boogertwilliams", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qltvp/whats_the_consensus_on_setting_up_a_nas_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qltvp/whats_the_consensus_on_setting_up_a_nas_with/", "subreddit_subscribers": 691156, "created_utc": 1688492682.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}