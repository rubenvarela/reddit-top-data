{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Gfycat](https://gfycat.com/) is apparently being permanently discontinued, according to its front page:\n\n&gt;The Gfycat service is being discontinued. Please save or delete your Gfycat content by visiting [https://www.gfycat.com](https://www.gfycat.com) and logging in to your account. After September 1, 2023, all Gfycat content and data will be deleted from gfycat.com\n\nI haven't seen much news about this, granted most of the current surface content is findable elsewhere but it's still a shame. Its days were clearly numbered. The owners already announced the deletion of a lot of content in 2019, which [Archive Team acted on](https://twitter.com/textfiles/status/1192518085997137920), with Gfycat [threatening to sue](https://www.reddit.com/r/DataHoarder/comments/13mjaw5/comment/jkw0h01/?utm_source=share&amp;utm_medium=web2x&amp;context=3) before they decided to [work with the team](https://wiki.archiveteam.org/index.php/Gfycat). Last month the [HTTPS certificate expired](https://www.reddit.com/r/DataHoarder/comments/13mjaw5/rip_gfycat_site_does_not_look_to_be_maintained/) briefly, but its provider [renewed](https://www.vice.com/en/article/dy37d7/gfycat-down-tls-security-certificate-expired) it after a few days, guess that was just foreshadowing its real death.\n\nAn r/gfycat post from yesterday: [https://www.reddit.com/r/gfycat/comments/14nf5r2/gfycat\\_is\\_shutting\\_down\\_on\\_the\\_1st\\_of\\_september/](https://www.reddit.com/r/gfycat/comments/14nf5r2/gfycat_is_shutting_down_on_the_1st_of_september/)", "author_fullname": "t2_yz4rz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gfycat is shutting down in September", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14o5u5d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 291, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 291, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688245843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://gfycat.com/\"&gt;Gfycat&lt;/a&gt; is apparently being permanently discontinued, according to its front page:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The Gfycat service is being discontinued. Please save or delete your Gfycat content by visiting &lt;a href=\"https://www.gfycat.com\"&gt;https://www.gfycat.com&lt;/a&gt; and logging in to your account. After September 1, 2023, all Gfycat content and data will be deleted from gfycat.com&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I haven&amp;#39;t seen much news about this, granted most of the current surface content is findable elsewhere but it&amp;#39;s still a shame. Its days were clearly numbered. The owners already announced the deletion of a lot of content in 2019, which &lt;a href=\"https://twitter.com/textfiles/status/1192518085997137920\"&gt;Archive Team acted on&lt;/a&gt;, with Gfycat &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13mjaw5/comment/jkw0h01/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;threatening to sue&lt;/a&gt; before they decided to &lt;a href=\"https://wiki.archiveteam.org/index.php/Gfycat\"&gt;work with the team&lt;/a&gt;. Last month the &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13mjaw5/rip_gfycat_site_does_not_look_to_be_maintained/\"&gt;HTTPS certificate expired&lt;/a&gt; briefly, but its provider &lt;a href=\"https://www.vice.com/en/article/dy37d7/gfycat-down-tls-security-certificate-expired\"&gt;renewed&lt;/a&gt; it after a few days, guess that was just foreshadowing its real death.&lt;/p&gt;\n\n&lt;p&gt;An &lt;a href=\"/r/gfycat\"&gt;r/gfycat&lt;/a&gt; post from yesterday: &lt;a href=\"https://www.reddit.com/r/gfycat/comments/14nf5r2/gfycat_is_shutting_down_on_the_1st_of_september/\"&gt;https://www.reddit.com/r/gfycat/comments/14nf5r2/gfycat_is_shutting_down_on_the_1st_of_september/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?auto=webp&amp;v=enabled&amp;s=197ee160d951682c6c0923bd3452e34a367cba2f", "width": 1112, "height": 1112}, "resolutions": [{"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b53a728ee6a9aa5fc5e63bff233e9d4bba7d5c57", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34626711083be36d145e367ae860a804462d995a", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46940425f27bf2e5573bd249d4df1818ae11e1a8", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e01a0d7743d0100648fed90e77ab6e3a6837d94b", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b054cf3c5f782f2b1c6d45eb6fcecf5ea2467f3", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d1664d345739dd6b5ff669deef461b863f3b710d", "width": 1080, "height": 1080}], "variants": {}, "id": "BqPyhyADuPOou7QJXixclbY9Kh6b4lY0i-ZqXVZOrLo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14o5u5d", "is_robot_indexable": true, "report_reasons": null, "author": "minindo", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14o5u5d/gfycat_is_shutting_down_in_september/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14o5u5d/gfycat_is_shutting_down_in_september/", "subreddit_subscribers": 690687, "created_utc": 1688245843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is an article I wrote about some challenges our archive project faces, which I believe may resonate with many of you. Its the first article I have written, so feedback is appreciated!", "author_fullname": "t2_few9393b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Digital Paradox: How the Internet Age is Erasing our Past", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_14ny22h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/vPsxmNBkYXmAunsVedf8A75tl78GaSZYLtgkeUZUxk4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688225804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is an article I wrote about some challenges our archive project faces, which I believe may resonate with many of you. Its the first article I have written, so feedback is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@avenstewart/the-digital-paradox-how-the-internet-age-is-erasing-our-past-5a367ecf42f0", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?auto=webp&amp;v=enabled&amp;s=305e568c784b704078d9e4d7714755382bc2ac51", "width": 1200, "height": 847}, "resolutions": [{"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff5a00630be5b2d65486c6ab5990461f93fcf87b", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ddcd9713fdbcd6edf184405f303aed0a9529b48e", "width": 216, "height": 152}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=538c88dba59703a953a9afb57171ed4043a0b050", "width": 320, "height": 225}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f29633e0477d93433d360d014bc32f05a0a56f0b", "width": 640, "height": 451}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d017b1f3e34d8d7bd2f95821d1c8b93e0d62ccd", "width": 960, "height": 677}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=951f3eb45588a5881eb69d13818cb9306e1f8e87", "width": 1080, "height": 762}], "variants": {}, "id": "mWw6L_GlywXuGnjjl0mRiUxsvMKC7FTgCCQ4lXCfims"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14ny22h", "is_robot_indexable": true, "report_reasons": null, "author": "SciencePlaceArchives", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ny22h/the_digital_paradox_how_the_internet_age_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@avenstewart/the-digital-paradox-how-the-internet-age-is-erasing-our-past-5a367ecf42f0", "subreddit_subscribers": 690687, "created_utc": 1688225804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**Results are shown in separate posts below because this exceeds the 40000 Reddit Character Limit. Replies only accept 10000 character limit. Sorry about that. Permalinks are provided at the bottom of this post to the test results.**\n\n**EDIT: tl;dr at the bottom of this post:** \n\n**EDIT 2:** Just FYI, I do not advocate for using SMR disks in RAID arrays nor in general. But this was simply a curiosity experiment and I had access to a number of disks for testing.\n\n-----------------\n\nI am posting this here but you can also view video showing the results (https://youtu.be/y0-VhMfUWwI), or on my blog (https://htwingnut.com/2023/07/01/smr-hard-drive-perforamnce-rebuild-with-parity-raid/) which is basically the exact same info presented here. The YouTube video is chaptered so you can skip to sections you want.\n\nIt is essentially the same content whether in this post or video, except I go into more detail explaining what SMR is in the video and has graphical charts instead of text results. There is a lot of data here and did my best to keep it concise and organized. There's so many ways to slice it, but feel free to digest it however you see fit. I don't blame you if you don't want to hear my nasally voice ramble on for an hour.\n\nThis started as a simple curiosity experiment but ballooned into a much larger test set and took much longer than expected. Through multiple iterations, troubleshooting, learning new things, tests run as time permitted, took me well over a year to complete.\n\nBasically testing that nobody asked for and probably don't care to know, but I did it anyhow, LOL.\n\nWhile most people here probably know what SMR is, I'll just give my two cent summary.\n\nShingled Magnetic Recording technology, abbreviated as SMR, allows for disk drives to store more capacity per platter than a traditional hard drive. The technology is intended to reduce costs, because more data per platter means fewer platters and read/write heads. But because of how they store data the tradeoff is that write performance may degrade over time after enough data has been written to, deleted and then overwritten on the disk.\n\nSMR drives are really intended for archival data, or data that is not frequently deleted or changed. However, since Western Digital decided to change their bottom tier WD RED NAS line of disk from CMR to SMR a few years back without full disclosure, many users were not happy with this change, as it had adverse effects greatly increasing rebuild times primarily of ZFS arrays. This got me wondering how much of an effect it really has in a parity RAID situation in other configurations.\n\n**DRIVES and TEST CONFIGS:**\n\nThree different SMR disks were tested to see how they would fare in popular RAID configurations. \n\nThe SMR test disks were all 2TB 3.5\" SATA drives:\n\n* Seagate Barracuda Compute ST2000DM008\n* WD Red WD20EFAX\n* WD Blue WD20EZAZ\n\nSeveral control CMR test disks were also utilized. The control set of disks were Seagate ST2000DM001 2TB 7200 RPM hard drives. The following 2TB CMR disks were also tested for comparison:\n\n* Seagate Barracuda ST2000DM001\n* WD Red Plus WD20EFZX\n* Seagate Skyhawk ST2000VX008\n\nWhy were 2TB disks used? Bottom line, time and money. Some disks were already on hand, others were purchased solely for this test. Ultimately a Seagate Exos drive would have been preferable instead of Skyhawk, but at time of purchase, the Skyhawk was appreciably less expensive ($30 vs $80). 2TB also is the smallest capacity in 3.5\" form factor that comes in SMR as well as CMR. I also didn't want to have to fill up more data than needed because writing multiple TB's of data through dozens of tests is time consuming enough. Anything  more than 2TB would have extended the test time considerably.\n\nThe following configurations were tested for single disk REBUILD times:\n\n* OMV mdadm 4 disk RAID 5\n* OMV ZFS 4 disk RAID Z1\n* OMV SnapRAID Data &amp; Parity disk\n* Synology 4 disk SHR-1\n* QNAP TR-004 4 disk \"hardware\" RAID 5\n* UnRAID Data &amp; Parity Disk\n* Linux EXT4 Single Disk with and without TRIM\n* Windows NTFS Single Disk with and without TRIM\n* Controller: LSI 9211-8i vs Marvell 9215 vs Intel H77 MDADM RAID 5\n* Controller: LSI 9211-8i vs Marvell 9215 vs Intel H77 ZFS RAID Z1\n\nThe configurations that were tested both WRITE and READ:\n\n* NTFS Windows 10 Single Disk - tested over local SATA\n* EXT4 OMV Single Disk - tested over 1GbE SMB\n* XFS UnRAID single parity - tested over 1GbE SMB\n* MDADM 4x 2TB RAID 5 OMV, formatted EXT4, tested over 1GbE SMB\n* ZFS 4x 2TB RAID Z1 OMV, formatted ZFS (of course), tested over 1GbE SMB\n* Synology BTRFS/MDADM 4x 2TB RAID 5 with DS920+, formatted BTRFS, tested over 1GbE SMB\n* QNAP TR-004 DAS \"Hardware\" 4x 2TB RAID 5, formatted NTFS, tested over USB\n\n**HARDWARE SETUP**\n\nI made use of older hardware I had on hand. The PC config that was used for software NAS setup (i.e. OMV, UnRAID):\n\n* CPU: Intel Core i5-3570 3.4GHz 4 core / 4 thread\n* OS SSD: Sandisk Extreme 240GB\n* Motherboard: Asus P8H77-I\n* RAM: 2x8GB DDR3 1600\n* PSU for PC: Seasonic S12II 430W 80plus Bronze\n* PSU for HDD's: FSP Group FSP270-60LE 270W\n* SATA Controller: LSI 9211-8i\n* LAN: Onboard Realtek RTL8111F Gigabit\n* Disk Rack: Sans Digital HDD Rack 5 (https://www.sansdigital.com/hddrack5.html)\n\nNote: the onboard SATA controller (Intel H77 Express) and also PCIe SATA controller (Marvell 9215) were utilized to compare performance between controllers in a few scenarios\n\nThe PC that ran the test programs to perform send and receive of file over ethernet and USB was configured as:\n\n* OS: Windows 10 Pro\n* Test Files SSD: Muskin Reactor 1TB 2.5\" SATA (READ FROM SSD WRITE TO TEST ARRAY)\n* Test Files SSD: Crucial MX500 1TB 2.5\" SATA (WRITE TO SSD FROM TEST ARRAY)\n* CPU: Intel Xeon E5-2630 v3 2.4GHz 8 core / 16 thread\n* Motherboard: ASRock X99 Extreme4/3.1\n* RAM: 2x16GB DDR4 1866 ECC\n* SATA controller: onboard Intel X99\n* LAN: onboard Intel 1218V 1GbE\n* USB: onboard USB 3.1 5Gbps\n\n**TEST METHODOLOGY**\n\nDisks were not formatted or wiped between tests. All data from previous tests were left on the disk. Prior to each test, the disk was \"cleared\" or \"formatted\" using the minimal recommend process. For example in OMV, just running a  \"quick wipe\" before adding the disk to a RAID 5 array.\n\nAll default settings for setting up an array were used. No TRIM or DISCARD settings were manually implemented except for the few specific TRIM tests.\n\nFor RAID array tests, 4x ST2000DM001 hard drives were used as control level to start, build array, add filler data, and then complete WRITE/READ test. Then one ST2000DM001 disk was then replaced with a test disk, rebuild the array, and then initiate the next read/write test for that disk. Subsequent tests would just swap out the test disk with another test disk, rebuild, and perform write/read testing until all disks were tested for that particular array. So 3x ST2000DM001 disks would always remain in the array with the fourth disk being a test disk.\n\nFor each initial array setup, the array was filled with random size and content data leaving approximately 1200 to 1500 GB free on a RAID array (out of ~ 6GB in a four disk single parity setup), approximately 60-70% filled capacity. This is considered the filler data and is not touched after the initial write to the disk or array.\n\nFor single disk tests, the filler data was filled to about 60-70% capacity, leaving about 800GB free for testing.\n\nSample random file distribution:\n\n        Total Files: 9979\n      File Size Min: 3KB\n      File Size Max: 2098998KB (~ 2.1GB)\n      File Size Avg: 154379 (~ 154MB)\n    \n            % Files &lt; 1MB:  1.74%\n      % Files 1MB to 10MB:  9.59%\n    % Files 10MB to 100MB: 45.59%\n     % Files 100MB to 1GB: 41.48%\n            % Files &gt; 1GB:  1.60%\n\n\nAll test files were written from an SSD, and an actual file copy was executed from the Windows PC SSD over 1GbE or USB (depending on the test) using a Powershell script.\n\nFor RAID tests, gigabit ethernet was used because it is a typical use case to copy from a PC over SMB to a NAS. Any performance issues due to SMR should result in performance well below 1GbE speeds of about 112 MB/sec (realistic speeds).\n\nA Powershell script was written and utilized to automate the tasks of file fill and measuring the per file transfer performance of the test data.\n\nRandom data was generated using RNGCryptoServiceProvider:\n\n    $rnd10 = (Get-Random 10) + 1\n    $rndmax = [int64]((Get-Random (2GB - 1))/$rnd10)\n    $bytes = (Get-Random $rndmax)\n    [System.Security.Cryptography.RNGCryptoServiceProvider] $rng = New-Object System.Security.Cryptography.RNGCryptoServiceProvider\n    $rndbytes = New-Object byte[] $bytes\n    $rng.GetBytes($rndbytes)\n\n\nFile transfer performance was measured using the `StopWatch` command:\n\n    $StopWatch=[system.diagnostics.stopwatch]::startnew()\n    Copy-Item \"$spath\\$f\\$file\" \"$dpath\\TEST\\$f\"\n    $SecondsElapsed=$StopWatch.Elapsed.TotalSeconds\n    $StopWatch.Stop()\n\nThe average transfer speeds were computed using total size of files copied divided by summation of all time elapsed from above script.\n\nSetting up a proper test to mitigate skewed results due to regular fragmentation was of high consideration. The following two test scenarios were devised which should highlight any effects of SMR degradation while minimizing any effect of fragmentation:\n\n**Test 1:** Mixed Size Write / Read\n\n1. Write random size/content data to remaining full capacity of disk/array from Windows test PC.\n2. Use \"delete\" command to delete all random data just written.\n3. Write a test set of 620GB: 20x 10GB, 200x 1GB, 2000x 100MB, 20000x 1MB files from SSD and measure results per file.\n4. Read back half of files and measure results.\n5. Use \"delete\" command to delete test set of 620GB from test array.\n6. Immediately start next Scenario.\n\n**Test 2:** Alternating 10MB / 1GB files\n\n1. Write 10MB size files to remaining full capacity of disk/array from Windows test PC.\n2. Use \"delete\" command to delete EVERY OTHER 10MB file, so half the 10MB files just written.\n3. Write 800x 1GB sized files to disk/array from SSD and measure results per file.\n4. Read back half of files and measure results per file.\n5. Use \"delete\" command to delete all 10MB and 1GB test files.\n6. Remove disk and start next disk test.\n7. Shut down, remove test disk and replace with next test disk.\n\nBoth scenarios take the disk to its fullest capacity and then immediately delete and then write data back to the disk with no idle time.\n\nThe second scenario should really bring forward any issues pertaining to SMR. Alternating 10MB file deletion should remove big chunks of data from each SMR zone that would require a rewrite of data to fill each SMR zone back.\n\n**BASELINE PERFORMANCE**\n\nEvery disk was subjected to several baseline performance tests to ensure they were performing as intended:\n\n* CrystalDiskMark 5x 1GB test set\n* ATTO 512b to 64MB I/O with 1GB file size\n* Hard Disk Sentinel Full Disk WRITE and READ\n\nThe CrystalDiskMark and ATTO benchmarks are omitted from this post, only because compiling the results will be quite tedious and honestly not very value added. However, I may provide them as raw image data in a future update to this blog.\n\nThat being said, here are baseline results from the Hard Disk Sentinel Full Disk WRITE and READ tests for reference, which are probably more relevant anyhow: https://imgur.com/a/SdW5B9h\n\nThe two SSD's used to send and receive data were tested with HD Sentinel with results shown below:\n\n* Test SSD read from to the test array Mushkin Reactor 1TB 2.5\" SATA (475MB/sec read speed)\n* Test SSD write to from the test array Crucial MX500 1TB 2.5\" SATA (350MB/sec write speed)\n\n**FAILURES**\n\nThroughout testing, three of five WD Red WD20EFAX (SMR) had failures, all at different times in the testing. One became completely non responsive, it would power up but was never detected. Another started having increased failing sectors. The third one would fail out of a ZFS RAID Z1 rebuild despite having clean SMART and no other apparent issues. It was exchanged in hopes that the replacement would fare better. Each disk was successfully RMA'd promptly with a replacement within two weeks.\n\nAn SG Barracuda Compute ST2000DM008 (SMR) had to be RMA'd for being non-responsive and would hang the system periodically.\n\nEach problematic disk went through a thorough troubleshooting process, changing PC's, changing power supplies, changing cables, validating PSU voltages, etc before submitting for RMA. In each case, the replacement disk solved the issue completely (with the exception of the one instance with ZFS rebuild errors). Is this a result of SMR? Bad luck? Maybe some unknown other issue? I don't know, but each of the replacement disks soldiered on through the rest of the testing without a hitch.\n\nThe SG Barracuda Compute ST2000DM008 (SMR) was not recognized as a device supporting TRIM despite in Linux desite it running TRIM just fine with NTFS in Windows. This is why there was an N/A TRIM in the TRIM results for the Seagate Barracuda Compute ST2000DM008 drive.\n\nThere were four WD RED Disks utilized. Three were WD20EFAX-68B2RN1 and one was WD20EFAX-68FB5N0. The 68FB5N0 would not TRIM in Linux despite it running TRIM just fine in NTFS Windows. 68B2RN1 drives would accept TRIM commands fine in Linux and Windows. This is why the TRIM test only included 3x WD RED instead of 4x WD RED.\n\n**Results are shown in separate posts below because this exceeds the 40000 Reddit Character Limit. Replies only accept 10000 character limit. Sorry about that.**\n\n**Permalinks for Test Results:**\n\n**REBUILD TIMES:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xlb2/\n\n**TEST 1 RESULTS: WRITE 620GB MIXED FILES SIZES AFTER DISK FILL &amp; DELETE:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xwwd/\n\n**TEST 1 RESULTS: READ 620GB MIXED FILES SIZES AFTER DISK FILL &amp; DELETE:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9y96s/\n\n**TEST 2 RESULTS: WRITE &amp; READ 1GB FILES AFTER 10MB DISK FILL AND DELETE EVERY OTHER 10MB FILE:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yeqo/\n\n**TEST 3 RESULTS: WRITE PERFORMANCE AFTER TRIM:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9ymhv/\n\n**TEST 3 RESULTS: READ PERFORMANCE AFTER TRIM:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yq1u/\n\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n\n**tl;dr** - This is hard to summarize and make generalities, but I'll do my best.\n\nCheck out these percentage difference charts here which may help: https://imgur.com/a/FdDITyR\n\nOn the linked charts, 1GB alternating are results from \"Test 2\" (write 10MB files, delete every other 10MB file, write 1GB files), the other file sizes are from \"Test 1\" (fill with random data, delete, then write 1MB, 100MB, 1GB, 10GB files / 620GB total).\n\nTest 1, SMR disks did perform worse than CMR across the board with file writes. The Seagate Barracuda Compute ST2000DM008 performing the worse, by about 50%. The rest pretty much had less than a 30% performance penalty. TRIM had minimal impact on improving performance for Test 1. My thoughts on this are because it was mass deletion of files and the disks possibly have some level of intelligence to realize freed ups SMR zones it just overwrites them.\n\nTest 2, all SMR disks had a write performance penalty of 30-90%, but it was mitigated with a TRIM command and 4 hrs idle time. It seems TRIM does its job given enough time. Whether 4 hours is needed, that amount of time was not tested, just used 4 hrs to give it ample time to do its job.\n\nTest 3, TRIM results - as noted above, TRIM had minimal impact on Test 1, then again most test the performance wasn't degraded significantly compared with CMR disks. TRIM after file deletion during Test 2 had a significant performance improvement, nearing CMR level of performance.\n\nTest 4 REBUILD - There was no significant performance penalty during rebuild times except for the Seagate Barracuda Compute with ZFS RAID Z1 where it took over 500 minutes vs about 200 minutes for the other disks. It also faltered a bit with the SnapRAID EXT4 test Where it took 160 minutes vs about 90 minutes with the other disks. WD Red also faltered a bit with SnapRAID EXT4 rebuild running 123 minutes vs about 90 minutes.\n\nCONTROLLER REBUILD Difference - LSI 9211-8i PCIe vs Intel H77 SATA onboard vs Marvell 9215 SATA PCIe : LSI &amp; Intel were within 10% of each other, Marvell was about 25% slower than the LSI.\n\nREBUILD Swap all CMR with all SMR: Swapping 4x ST2000DM001 CMR with 4x WD20EFAX SMR single parity RAID (RAID 5) one at a time with QNAP TR-004 Hardware RAID, Synology DS920+ SHR-1, Linux MDADM RAID 5. There was no significant difference in performance.\n\nSurprisingly the WD Blue seemed to fare best out of the three SMR disks.", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extensive Testing - SMR Results with RAID Rebuild and File Transfers Compared with CMR", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nz7ow", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1688272806.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688228768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Results are shown in separate posts below because this exceeds the 40000 Reddit Character Limit. Replies only accept 10000 character limit. Sorry about that. Permalinks are provided at the bottom of this post to the test results.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT: tl;dr at the bottom of this post:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT 2:&lt;/strong&gt; Just FYI, I do not advocate for using SMR disks in RAID arrays nor in general. But this was simply a curiosity experiment and I had access to a number of disks for testing.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;I am posting this here but you can also view video showing the results (&lt;a href=\"https://youtu.be/y0-VhMfUWwI\"&gt;https://youtu.be/y0-VhMfUWwI&lt;/a&gt;), or on my blog (&lt;a href=\"https://htwingnut.com/2023/07/01/smr-hard-drive-perforamnce-rebuild-with-parity-raid/\"&gt;https://htwingnut.com/2023/07/01/smr-hard-drive-perforamnce-rebuild-with-parity-raid/&lt;/a&gt;) which is basically the exact same info presented here. The YouTube video is chaptered so you can skip to sections you want.&lt;/p&gt;\n\n&lt;p&gt;It is essentially the same content whether in this post or video, except I go into more detail explaining what SMR is in the video and has graphical charts instead of text results. There is a lot of data here and did my best to keep it concise and organized. There&amp;#39;s so many ways to slice it, but feel free to digest it however you see fit. I don&amp;#39;t blame you if you don&amp;#39;t want to hear my nasally voice ramble on for an hour.&lt;/p&gt;\n\n&lt;p&gt;This started as a simple curiosity experiment but ballooned into a much larger test set and took much longer than expected. Through multiple iterations, troubleshooting, learning new things, tests run as time permitted, took me well over a year to complete.&lt;/p&gt;\n\n&lt;p&gt;Basically testing that nobody asked for and probably don&amp;#39;t care to know, but I did it anyhow, LOL.&lt;/p&gt;\n\n&lt;p&gt;While most people here probably know what SMR is, I&amp;#39;ll just give my two cent summary.&lt;/p&gt;\n\n&lt;p&gt;Shingled Magnetic Recording technology, abbreviated as SMR, allows for disk drives to store more capacity per platter than a traditional hard drive. The technology is intended to reduce costs, because more data per platter means fewer platters and read/write heads. But because of how they store data the tradeoff is that write performance may degrade over time after enough data has been written to, deleted and then overwritten on the disk.&lt;/p&gt;\n\n&lt;p&gt;SMR drives are really intended for archival data, or data that is not frequently deleted or changed. However, since Western Digital decided to change their bottom tier WD RED NAS line of disk from CMR to SMR a few years back without full disclosure, many users were not happy with this change, as it had adverse effects greatly increasing rebuild times primarily of ZFS arrays. This got me wondering how much of an effect it really has in a parity RAID situation in other configurations.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;DRIVES and TEST CONFIGS:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Three different SMR disks were tested to see how they would fare in popular RAID configurations. &lt;/p&gt;\n\n&lt;p&gt;The SMR test disks were all 2TB 3.5&amp;quot; SATA drives:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Seagate Barracuda Compute ST2000DM008&lt;/li&gt;\n&lt;li&gt;WD Red WD20EFAX&lt;/li&gt;\n&lt;li&gt;WD Blue WD20EZAZ&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Several control CMR test disks were also utilized. The control set of disks were Seagate ST2000DM001 2TB 7200 RPM hard drives. The following 2TB CMR disks were also tested for comparison:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Seagate Barracuda ST2000DM001&lt;/li&gt;\n&lt;li&gt;WD Red Plus WD20EFZX&lt;/li&gt;\n&lt;li&gt;Seagate Skyhawk ST2000VX008&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Why were 2TB disks used? Bottom line, time and money. Some disks were already on hand, others were purchased solely for this test. Ultimately a Seagate Exos drive would have been preferable instead of Skyhawk, but at time of purchase, the Skyhawk was appreciably less expensive ($30 vs $80). 2TB also is the smallest capacity in 3.5&amp;quot; form factor that comes in SMR as well as CMR. I also didn&amp;#39;t want to have to fill up more data than needed because writing multiple TB&amp;#39;s of data through dozens of tests is time consuming enough. Anything  more than 2TB would have extended the test time considerably.&lt;/p&gt;\n\n&lt;p&gt;The following configurations were tested for single disk REBUILD times:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OMV mdadm 4 disk RAID 5&lt;/li&gt;\n&lt;li&gt;OMV ZFS 4 disk RAID Z1&lt;/li&gt;\n&lt;li&gt;OMV SnapRAID Data &amp;amp; Parity disk&lt;/li&gt;\n&lt;li&gt;Synology 4 disk SHR-1&lt;/li&gt;\n&lt;li&gt;QNAP TR-004 4 disk &amp;quot;hardware&amp;quot; RAID 5&lt;/li&gt;\n&lt;li&gt;UnRAID Data &amp;amp; Parity Disk&lt;/li&gt;\n&lt;li&gt;Linux EXT4 Single Disk with and without TRIM&lt;/li&gt;\n&lt;li&gt;Windows NTFS Single Disk with and without TRIM&lt;/li&gt;\n&lt;li&gt;Controller: LSI 9211-8i vs Marvell 9215 vs Intel H77 MDADM RAID 5&lt;/li&gt;\n&lt;li&gt;Controller: LSI 9211-8i vs Marvell 9215 vs Intel H77 ZFS RAID Z1&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The configurations that were tested both WRITE and READ:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;NTFS Windows 10 Single Disk - tested over local SATA&lt;/li&gt;\n&lt;li&gt;EXT4 OMV Single Disk - tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;XFS UnRAID single parity - tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;MDADM 4x 2TB RAID 5 OMV, formatted EXT4, tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;ZFS 4x 2TB RAID Z1 OMV, formatted ZFS (of course), tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;Synology BTRFS/MDADM 4x 2TB RAID 5 with DS920+, formatted BTRFS, tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;QNAP TR-004 DAS &amp;quot;Hardware&amp;quot; 4x 2TB RAID 5, formatted NTFS, tested over USB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;HARDWARE SETUP&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I made use of older hardware I had on hand. The PC config that was used for software NAS setup (i.e. OMV, UnRAID):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CPU: Intel Core i5-3570 3.4GHz 4 core / 4 thread&lt;/li&gt;\n&lt;li&gt;OS SSD: Sandisk Extreme 240GB&lt;/li&gt;\n&lt;li&gt;Motherboard: Asus P8H77-I&lt;/li&gt;\n&lt;li&gt;RAM: 2x8GB DDR3 1600&lt;/li&gt;\n&lt;li&gt;PSU for PC: Seasonic S12II 430W 80plus Bronze&lt;/li&gt;\n&lt;li&gt;PSU for HDD&amp;#39;s: FSP Group FSP270-60LE 270W&lt;/li&gt;\n&lt;li&gt;SATA Controller: LSI 9211-8i&lt;/li&gt;\n&lt;li&gt;LAN: Onboard Realtek RTL8111F Gigabit&lt;/li&gt;\n&lt;li&gt;Disk Rack: Sans Digital HDD Rack 5 (&lt;a href=\"https://www.sansdigital.com/hddrack5.html\"&gt;https://www.sansdigital.com/hddrack5.html&lt;/a&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Note: the onboard SATA controller (Intel H77 Express) and also PCIe SATA controller (Marvell 9215) were utilized to compare performance between controllers in a few scenarios&lt;/p&gt;\n\n&lt;p&gt;The PC that ran the test programs to perform send and receive of file over ethernet and USB was configured as:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OS: Windows 10 Pro&lt;/li&gt;\n&lt;li&gt;Test Files SSD: Muskin Reactor 1TB 2.5&amp;quot; SATA (READ FROM SSD WRITE TO TEST ARRAY)&lt;/li&gt;\n&lt;li&gt;Test Files SSD: Crucial MX500 1TB 2.5&amp;quot; SATA (WRITE TO SSD FROM TEST ARRAY)&lt;/li&gt;\n&lt;li&gt;CPU: Intel Xeon E5-2630 v3 2.4GHz 8 core / 16 thread&lt;/li&gt;\n&lt;li&gt;Motherboard: ASRock X99 Extreme4/3.1&lt;/li&gt;\n&lt;li&gt;RAM: 2x16GB DDR4 1866 ECC&lt;/li&gt;\n&lt;li&gt;SATA controller: onboard Intel X99&lt;/li&gt;\n&lt;li&gt;LAN: onboard Intel 1218V 1GbE&lt;/li&gt;\n&lt;li&gt;USB: onboard USB 3.1 5Gbps&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST METHODOLOGY&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Disks were not formatted or wiped between tests. All data from previous tests were left on the disk. Prior to each test, the disk was &amp;quot;cleared&amp;quot; or &amp;quot;formatted&amp;quot; using the minimal recommend process. For example in OMV, just running a  &amp;quot;quick wipe&amp;quot; before adding the disk to a RAID 5 array.&lt;/p&gt;\n\n&lt;p&gt;All default settings for setting up an array were used. No TRIM or DISCARD settings were manually implemented except for the few specific TRIM tests.&lt;/p&gt;\n\n&lt;p&gt;For RAID array tests, 4x ST2000DM001 hard drives were used as control level to start, build array, add filler data, and then complete WRITE/READ test. Then one ST2000DM001 disk was then replaced with a test disk, rebuild the array, and then initiate the next read/write test for that disk. Subsequent tests would just swap out the test disk with another test disk, rebuild, and perform write/read testing until all disks were tested for that particular array. So 3x ST2000DM001 disks would always remain in the array with the fourth disk being a test disk.&lt;/p&gt;\n\n&lt;p&gt;For each initial array setup, the array was filled with random size and content data leaving approximately 1200 to 1500 GB free on a RAID array (out of ~ 6GB in a four disk single parity setup), approximately 60-70% filled capacity. This is considered the filler data and is not touched after the initial write to the disk or array.&lt;/p&gt;\n\n&lt;p&gt;For single disk tests, the filler data was filled to about 60-70% capacity, leaving about 800GB free for testing.&lt;/p&gt;\n\n&lt;p&gt;Sample random file distribution:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    Total Files: 9979\n  File Size Min: 3KB\n  File Size Max: 2098998KB (~ 2.1GB)\n  File Size Avg: 154379 (~ 154MB)\n\n        % Files &amp;lt; 1MB:  1.74%\n  % Files 1MB to 10MB:  9.59%\n% Files 10MB to 100MB: 45.59%\n % Files 100MB to 1GB: 41.48%\n        % Files &amp;gt; 1GB:  1.60%\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;All test files were written from an SSD, and an actual file copy was executed from the Windows PC SSD over 1GbE or USB (depending on the test) using a Powershell script.&lt;/p&gt;\n\n&lt;p&gt;For RAID tests, gigabit ethernet was used because it is a typical use case to copy from a PC over SMB to a NAS. Any performance issues due to SMR should result in performance well below 1GbE speeds of about 112 MB/sec (realistic speeds).&lt;/p&gt;\n\n&lt;p&gt;A Powershell script was written and utilized to automate the tasks of file fill and measuring the per file transfer performance of the test data.&lt;/p&gt;\n\n&lt;p&gt;Random data was generated using RNGCryptoServiceProvider:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$rnd10 = (Get-Random 10) + 1\n$rndmax = [int64]((Get-Random (2GB - 1))/$rnd10)\n$bytes = (Get-Random $rndmax)\n[System.Security.Cryptography.RNGCryptoServiceProvider] $rng = New-Object System.Security.Cryptography.RNGCryptoServiceProvider\n$rndbytes = New-Object byte[] $bytes\n$rng.GetBytes($rndbytes)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;File transfer performance was measured using the &lt;code&gt;StopWatch&lt;/code&gt; command:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$StopWatch=[system.diagnostics.stopwatch]::startnew()\nCopy-Item &amp;quot;$spath\\$f\\$file&amp;quot; &amp;quot;$dpath\\TEST\\$f&amp;quot;\n$SecondsElapsed=$StopWatch.Elapsed.TotalSeconds\n$StopWatch.Stop()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The average transfer speeds were computed using total size of files copied divided by summation of all time elapsed from above script.&lt;/p&gt;\n\n&lt;p&gt;Setting up a proper test to mitigate skewed results due to regular fragmentation was of high consideration. The following two test scenarios were devised which should highlight any effects of SMR degradation while minimizing any effect of fragmentation:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Test 1:&lt;/strong&gt; Mixed Size Write / Read&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Write random size/content data to remaining full capacity of disk/array from Windows test PC.&lt;/li&gt;\n&lt;li&gt;Use &amp;quot;delete&amp;quot; command to delete all random data just written.&lt;/li&gt;\n&lt;li&gt;Write a test set of 620GB: 20x 10GB, 200x 1GB, 2000x 100MB, 20000x 1MB files from SSD and measure results per file.&lt;/li&gt;\n&lt;li&gt;Read back half of files and measure results.&lt;/li&gt;\n&lt;li&gt;Use &amp;quot;delete&amp;quot; command to delete test set of 620GB from test array.&lt;/li&gt;\n&lt;li&gt;Immediately start next Scenario.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Test 2:&lt;/strong&gt; Alternating 10MB / 1GB files&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Write 10MB size files to remaining full capacity of disk/array from Windows test PC.&lt;/li&gt;\n&lt;li&gt;Use &amp;quot;delete&amp;quot; command to delete EVERY OTHER 10MB file, so half the 10MB files just written.&lt;/li&gt;\n&lt;li&gt;Write 800x 1GB sized files to disk/array from SSD and measure results per file.&lt;/li&gt;\n&lt;li&gt;Read back half of files and measure results per file.&lt;/li&gt;\n&lt;li&gt;Use &amp;quot;delete&amp;quot; command to delete all 10MB and 1GB test files.&lt;/li&gt;\n&lt;li&gt;Remove disk and start next disk test.&lt;/li&gt;\n&lt;li&gt;Shut down, remove test disk and replace with next test disk.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Both scenarios take the disk to its fullest capacity and then immediately delete and then write data back to the disk with no idle time.&lt;/p&gt;\n\n&lt;p&gt;The second scenario should really bring forward any issues pertaining to SMR. Alternating 10MB file deletion should remove big chunks of data from each SMR zone that would require a rewrite of data to fill each SMR zone back.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;BASELINE PERFORMANCE&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Every disk was subjected to several baseline performance tests to ensure they were performing as intended:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CrystalDiskMark 5x 1GB test set&lt;/li&gt;\n&lt;li&gt;ATTO 512b to 64MB I/O with 1GB file size&lt;/li&gt;\n&lt;li&gt;Hard Disk Sentinel Full Disk WRITE and READ&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The CrystalDiskMark and ATTO benchmarks are omitted from this post, only because compiling the results will be quite tedious and honestly not very value added. However, I may provide them as raw image data in a future update to this blog.&lt;/p&gt;\n\n&lt;p&gt;That being said, here are baseline results from the Hard Disk Sentinel Full Disk WRITE and READ tests for reference, which are probably more relevant anyhow: &lt;a href=\"https://imgur.com/a/SdW5B9h\"&gt;https://imgur.com/a/SdW5B9h&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The two SSD&amp;#39;s used to send and receive data were tested with HD Sentinel with results shown below:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Test SSD read from to the test array Mushkin Reactor 1TB 2.5&amp;quot; SATA (475MB/sec read speed)&lt;/li&gt;\n&lt;li&gt;Test SSD write to from the test array Crucial MX500 1TB 2.5&amp;quot; SATA (350MB/sec write speed)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;FAILURES&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Throughout testing, three of five WD Red WD20EFAX (SMR) had failures, all at different times in the testing. One became completely non responsive, it would power up but was never detected. Another started having increased failing sectors. The third one would fail out of a ZFS RAID Z1 rebuild despite having clean SMART and no other apparent issues. It was exchanged in hopes that the replacement would fare better. Each disk was successfully RMA&amp;#39;d promptly with a replacement within two weeks.&lt;/p&gt;\n\n&lt;p&gt;An SG Barracuda Compute ST2000DM008 (SMR) had to be RMA&amp;#39;d for being non-responsive and would hang the system periodically.&lt;/p&gt;\n\n&lt;p&gt;Each problematic disk went through a thorough troubleshooting process, changing PC&amp;#39;s, changing power supplies, changing cables, validating PSU voltages, etc before submitting for RMA. In each case, the replacement disk solved the issue completely (with the exception of the one instance with ZFS rebuild errors). Is this a result of SMR? Bad luck? Maybe some unknown other issue? I don&amp;#39;t know, but each of the replacement disks soldiered on through the rest of the testing without a hitch.&lt;/p&gt;\n\n&lt;p&gt;The SG Barracuda Compute ST2000DM008 (SMR) was not recognized as a device supporting TRIM despite in Linux desite it running TRIM just fine with NTFS in Windows. This is why there was an N/A TRIM in the TRIM results for the Seagate Barracuda Compute ST2000DM008 drive.&lt;/p&gt;\n\n&lt;p&gt;There were four WD RED Disks utilized. Three were WD20EFAX-68B2RN1 and one was WD20EFAX-68FB5N0. The 68FB5N0 would not TRIM in Linux despite it running TRIM just fine in NTFS Windows. 68B2RN1 drives would accept TRIM commands fine in Linux and Windows. This is why the TRIM test only included 3x WD RED instead of 4x WD RED.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Results are shown in separate posts below because this exceeds the 40000 Reddit Character Limit. Replies only accept 10000 character limit. Sorry about that.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Permalinks for Test Results:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;REBUILD TIMES:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xlb2/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xlb2/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 1 RESULTS: WRITE 620GB MIXED FILES SIZES AFTER DISK FILL &amp;amp; DELETE:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xwwd/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xwwd/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 1 RESULTS: READ 620GB MIXED FILES SIZES AFTER DISK FILL &amp;amp; DELETE:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9y96s/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9y96s/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 2 RESULTS: WRITE &amp;amp; READ 1GB FILES AFTER 10MB DISK FILL AND DELETE EVERY OTHER 10MB FILE:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yeqo/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yeqo/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 3 RESULTS: WRITE PERFORMANCE AFTER TRIM:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9ymhv/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9ymhv/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 3 RESULTS: READ PERFORMANCE AFTER TRIM:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yq1u/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yq1u/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; - This is hard to summarize and make generalities, but I&amp;#39;ll do my best.&lt;/p&gt;\n\n&lt;p&gt;Check out these percentage difference charts here which may help: &lt;a href=\"https://imgur.com/a/FdDITyR\"&gt;https://imgur.com/a/FdDITyR&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;On the linked charts, 1GB alternating are results from &amp;quot;Test 2&amp;quot; (write 10MB files, delete every other 10MB file, write 1GB files), the other file sizes are from &amp;quot;Test 1&amp;quot; (fill with random data, delete, then write 1MB, 100MB, 1GB, 10GB files / 620GB total).&lt;/p&gt;\n\n&lt;p&gt;Test 1, SMR disks did perform worse than CMR across the board with file writes. The Seagate Barracuda Compute ST2000DM008 performing the worse, by about 50%. The rest pretty much had less than a 30% performance penalty. TRIM had minimal impact on improving performance for Test 1. My thoughts on this are because it was mass deletion of files and the disks possibly have some level of intelligence to realize freed ups SMR zones it just overwrites them.&lt;/p&gt;\n\n&lt;p&gt;Test 2, all SMR disks had a write performance penalty of 30-90%, but it was mitigated with a TRIM command and 4 hrs idle time. It seems TRIM does its job given enough time. Whether 4 hours is needed, that amount of time was not tested, just used 4 hrs to give it ample time to do its job.&lt;/p&gt;\n\n&lt;p&gt;Test 3, TRIM results - as noted above, TRIM had minimal impact on Test 1, then again most test the performance wasn&amp;#39;t degraded significantly compared with CMR disks. TRIM after file deletion during Test 2 had a significant performance improvement, nearing CMR level of performance.&lt;/p&gt;\n\n&lt;p&gt;Test 4 REBUILD - There was no significant performance penalty during rebuild times except for the Seagate Barracuda Compute with ZFS RAID Z1 where it took over 500 minutes vs about 200 minutes for the other disks. It also faltered a bit with the SnapRAID EXT4 test Where it took 160 minutes vs about 90 minutes with the other disks. WD Red also faltered a bit with SnapRAID EXT4 rebuild running 123 minutes vs about 90 minutes.&lt;/p&gt;\n\n&lt;p&gt;CONTROLLER REBUILD Difference - LSI 9211-8i PCIe vs Intel H77 SATA onboard vs Marvell 9215 SATA PCIe : LSI &amp;amp; Intel were within 10% of each other, Marvell was about 25% slower than the LSI.&lt;/p&gt;\n\n&lt;p&gt;REBUILD Swap all CMR with all SMR: Swapping 4x ST2000DM001 CMR with 4x WD20EFAX SMR single parity RAID (RAID 5) one at a time with QNAP TR-004 Hardware RAID, Synology DS920+ SHR-1, Linux MDADM RAID 5. There was no significant difference in performance.&lt;/p&gt;\n\n&lt;p&gt;Surprisingly the WD Blue seemed to fare best out of the three SMR disks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?auto=webp&amp;v=enabled&amp;s=a93ffd455f3489bc77ec3d13e7c75810e8ebad7a", "width": 3963, "height": 2230}, "resolutions": [{"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63194afa5000a7878ea279f0c201eb331bec8712", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f637559669ff57373e38461489bbc2676d8ea35e", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b64c32ad58a594e9094304d9b29977b2a2be870", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=805b211d783e8bb5ef6011192400c522a70c6558", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e2a476bb967ad6fa6dd3ebf145c54ee12077651", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=642beb2a64abcde2c0d264317cd3994570bb0676", "width": 1080, "height": 607}], "variants": {}, "id": "Uz_t8KVSq-Mkp85blH-_19UMIuSwthpYy-MMFKHvHgM"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 250, "id": "award_c8503d66-6450-40c5-963f-35ced99bd361", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/Respect_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/Respect_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/Respect_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/Respect_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/Respect_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/Respect_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Tip of my hat to you", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Respect", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/anog86pfyh471_Respect.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=e56ad76b9e711337f46ae69af291e984bf6dd7b1", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/anog86pfyh471_Respect.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=b5adaa02054f8972a11162db7880f717ab4532e0", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/anog86pfyh471_Respect.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=74eeefc55c481d40a60d83c5fa047299890e3d8d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/anog86pfyh471_Respect.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=d20ca628a887d55f1e7ad05aaee33a405737b1c6", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/anog86pfyh471_Respect.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=b1bde27d7935331234a88caf1cd1fbb76c48372b", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 512, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/anog86pfyh471_Respect.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14nz7ow", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/", "subreddit_subscribers": 690687, "created_utc": 1688228768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I mainly just want to rip my anime Blurays to watch on PC/Oculus and watch 3D movies (not really 4K as i dont even have a 4k tv) via said Oculus. I would like to just insert disk and watch too but I hear you need a paid software to do that. I would rather save that as a last resort, but if its too complicated then feel free to recommend a good pc player and I'll just stick with that. So I've been looking for a bluray reader for my pc to rip disks. I already know about Makemkv. I just need the right hardware\n\nProblem is I can only seem to find stuff related to 4k/UHD and I am not interested in that. Aside from that i'll see stuff like \"get this model its great for what you need!\" only to find out that i cant get JUST that. I have to buy that, an enclosure for the drive, and some cables to hook it up. And there were a few times where I would see listings for an external bluray player only for the description to say it supports only DVDs.\n\nIs there something out there that already has the necessary parts that I can hook up and rip my blurays/ 3D blurays without getting an unnecessary UHD/4K ripper?", "author_fullname": "t2_txn5u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to understand how to get to where i can rip Bluray disks for backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14o1jya", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688235197.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688234806.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I mainly just want to rip my anime Blurays to watch on PC/Oculus and watch 3D movies (not really 4K as i dont even have a 4k tv) via said Oculus. I would like to just insert disk and watch too but I hear you need a paid software to do that. I would rather save that as a last resort, but if its too complicated then feel free to recommend a good pc player and I&amp;#39;ll just stick with that. So I&amp;#39;ve been looking for a bluray reader for my pc to rip disks. I already know about Makemkv. I just need the right hardware&lt;/p&gt;\n\n&lt;p&gt;Problem is I can only seem to find stuff related to 4k/UHD and I am not interested in that. Aside from that i&amp;#39;ll see stuff like &amp;quot;get this model its great for what you need!&amp;quot; only to find out that i cant get JUST that. I have to buy that, an enclosure for the drive, and some cables to hook it up. And there were a few times where I would see listings for an external bluray player only for the description to say it supports only DVDs.&lt;/p&gt;\n\n&lt;p&gt;Is there something out there that already has the necessary parts that I can hook up and rip my blurays/ 3D blurays without getting an unnecessary UHD/4K ripper?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14o1jya", "is_robot_indexable": true, "report_reasons": null, "author": "Dullapan", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14o1jya/trying_to_understand_how_to_get_to_where_i_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14o1jya/trying_to_understand_how_to_get_to_where_i_can/", "subreddit_subscribers": 690687, "created_utc": 1688234806.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I have found several old CD-R, DVD-R and DVD+R discs. Since they provide only little storage by modern standards, I wonder what I can for example record to these CD-R/DVD-R discs.\n\nIf they are from a brand that doesn't exist anymore or are they a 650 MB variety (which are practically not made anymore), are they worth more? What about slightly scratched disks?", "author_fullname": "t2_rej6q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to do with several old CD-R/DVD-R blank discs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ntca7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688212792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have found several old CD-R, DVD-R and DVD+R discs. Since they provide only little storage by modern standards, I wonder what I can for example record to these CD-R/DVD-R discs.&lt;/p&gt;\n\n&lt;p&gt;If they are from a brand that doesn&amp;#39;t exist anymore or are they a 650 MB variety (which are practically not made anymore), are they worth more? What about slightly scratched disks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ntca7", "is_robot_indexable": true, "report_reasons": null, "author": "smsaczek", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ntca7/what_to_do_with_several_old_cdrdvdr_blank_discs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14ntca7/what_to_do_with_several_old_cdrdvdr_blank_discs/", "subreddit_subscribers": 690687, "created_utc": 1688212792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using USB caddy/hard drive enclosure to recover browser bookmarks, etc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14ol7ma", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_dyeb9", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "datarecovery", "selftext": "Is it possible to do that with these? I know you can recover old files, pics, etc. but does that extend to this level as well?", "author_fullname": "t2_dyeb9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using USB caddy/hard drive enclosure to recover browser bookmarks, etc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datarecovery", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14ol5be", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688294159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datarecovery", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to do that with these? I know you can recover old files, pics, etc. but does that extend to this level as well?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qpkw", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14ol5be", "is_robot_indexable": true, "report_reasons": null, "author": "tehWoodcock", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datarecovery/comments/14ol5be/using_usb_caddyhard_drive_enclosure_to_recover/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datarecovery/comments/14ol5be/using_usb_caddyhard_drive_enclosure_to_recover/", "subreddit_subscribers": 16289, "created_utc": 1688294159.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1688294407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datarecovery", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/datarecovery/comments/14ol5be/using_usb_caddyhard_drive_enclosure_to_recover/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ol7ma", "is_robot_indexable": true, "report_reasons": null, "author": "tehWoodcock", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_14ol5be", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ol7ma/using_usb_caddyhard_drive_enclosure_to_recover/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/datarecovery/comments/14ol5be/using_usb_caddyhard_drive_enclosure_to_recover/", "subreddit_subscribers": 690687, "created_utc": 1688294407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First one is 8 bay, the other is 6. Can't find much online, especially about the terramaster.\n\nI'm planning to use 5 Seagate 18TB drives since they seems to offer the best value for money, still not sure about one or two drives for parity (two migh be overkill tbh).\n\nQNAP is more futureproof with 2 more bays, but I don't think i'll need those in the near future. It also has 2 120mm fans, which means they're generally quieter than 80mm (i think i'll be swapping those out for Noctua anyway).\n\nTerraMaster cost less and should be good enough for my needs since i'll be getting the same amount of storage early on with 5 drives. Has the same connectivity with 10gbps USB and looks like is well built. I don't like 80mm fans as I said, in case i'll be swapping those too but from experience, 120mm fans are quieter so, that's a minor point to it. The main issue is that there is nothing online beside announcements. No review, no opinions, nothing.\n\n&amp;#x200B;\n\nIf you have suggestions, or even other models to suggest, please feel free to do so.", "author_fullname": "t2_k99xt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "USB HDD enclosure: QNAP TL-D800C vs TerraMaster D6-320", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nz3xf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688228498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First one is 8 bay, the other is 6. Can&amp;#39;t find much online, especially about the terramaster.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to use 5 Seagate 18TB drives since they seems to offer the best value for money, still not sure about one or two drives for parity (two migh be overkill tbh).&lt;/p&gt;\n\n&lt;p&gt;QNAP is more futureproof with 2 more bays, but I don&amp;#39;t think i&amp;#39;ll need those in the near future. It also has 2 120mm fans, which means they&amp;#39;re generally quieter than 80mm (i think i&amp;#39;ll be swapping those out for Noctua anyway).&lt;/p&gt;\n\n&lt;p&gt;TerraMaster cost less and should be good enough for my needs since i&amp;#39;ll be getting the same amount of storage early on with 5 drives. Has the same connectivity with 10gbps USB and looks like is well built. I don&amp;#39;t like 80mm fans as I said, in case i&amp;#39;ll be swapping those too but from experience, 120mm fans are quieter so, that&amp;#39;s a minor point to it. The main issue is that there is nothing online beside announcements. No review, no opinions, nothing.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you have suggestions, or even other models to suggest, please feel free to do so.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nz3xf", "is_robot_indexable": true, "report_reasons": null, "author": "NaXter24R", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nz3xf/usb_hdd_enclosure_qnap_tld800c_vs_terramaster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nz3xf/usb_hdd_enclosure_qnap_tld800c_vs_terramaster/", "subreddit_subscribers": 690687, "created_utc": 1688228498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there ways to reuse random hard drive you got for free without much work? I'd image a small computer, you find a free hard drive, attach it to the computer and it will be automatically added to the Nas as a raid or so?", "author_fullname": "t2_jp3bv4xk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ghetto-NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nwaqu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688221205.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there ways to reuse random hard drive you got for free without much work? I&amp;#39;d image a small computer, you find a free hard drive, attach it to the computer and it will be automatically added to the Nas as a raid or so?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nwaqu", "is_robot_indexable": true, "report_reasons": null, "author": "CryptographerOdd299", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nwaqu/ghettonas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nwaqu/ghettonas/", "subreddit_subscribers": 690687, "created_utc": 1688221205.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there, I recently upgraded from a M5015 to a Adaptec 71605. I have the new card connected to a IBM X3400 SAS Expander. The problem I'm having is that the new card does not detect the drives connected to the expander. 2 drives connected directly to the card via backplane are detected. The old card with the expander did and still does show all drives when reconnected. I have attached an image of the cards boot screen showing it detects the expander but finds no logical drives. There also appears to be a no bios warning but I'm not sure if that's relevant. Any help that you can provide would be appreciated, Thanks.\n\nhttps://preview.redd.it/yx9bf9s0tc9b1.jpg?width=1851&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=2443fa2e91da638387a2aeeb9a0903f9c6940c19", "author_fullname": "t2_zm7dz8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adaptec 71605 + ServeRAID X3400 HDD Detection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "media_metadata": {"yx9bf9s0tc9b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 75, "x": 108, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5dd973a07f3d3404327497dcf8df18df791604e9"}, {"y": 151, "x": 216, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdd64a9184cd125321cae0109d41c0f7bb807d1a"}, {"y": 224, "x": 320, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=375773e9db8f6e60828e377e563b2d63697be333"}, {"y": 449, "x": 640, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=173a9de5d16c07498c6183c509b3826a64a20c34"}, {"y": 673, "x": 960, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a7946c3a27212e91b9e4f2589e5eaf5ca574f21"}, {"y": 757, "x": 1080, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4aaefe93f24cb16e6239efe08e6876dbb999fab9"}], "s": {"y": 1299, "x": 1851, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=1851&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=2443fa2e91da638387a2aeeb9a0903f9c6940c19"}, "id": "yx9bf9s0tc9b1"}}, "name": "t3_14nuwfj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fD3BXNmxibK2pJO_WwL5hBfguRsIiOTUPcZVOU_orpo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688217333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I recently upgraded from a M5015 to a Adaptec 71605. I have the new card connected to a IBM X3400 SAS Expander. The problem I&amp;#39;m having is that the new card does not detect the drives connected to the expander. 2 drives connected directly to the card via backplane are detected. The old card with the expander did and still does show all drives when reconnected. I have attached an image of the cards boot screen showing it detects the expander but finds no logical drives. There also appears to be a no bios warning but I&amp;#39;m not sure if that&amp;#39;s relevant. Any help that you can provide would be appreciated, Thanks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=1851&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2443fa2e91da638387a2aeeb9a0903f9c6940c19\"&gt;https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=1851&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2443fa2e91da638387a2aeeb9a0903f9c6940c19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nuwfj", "is_robot_indexable": true, "report_reasons": null, "author": "smorgisborg1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nuwfj/adaptec_71605_serveraid_x3400_hdd_detection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nuwfj/adaptec_71605_serveraid_x3400_hdd_detection/", "subreddit_subscribers": 690687, "created_utc": 1688217333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a tool that can automatically scan &amp; remove any URLs or tags (indicated by square brackets) in filenames?", "author_fullname": "t2_addo5jzj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "file name scanner to remove URLs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ojrnx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688289238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a tool that can automatically scan &amp;amp; remove any URLs or tags (indicated by square brackets) in filenames?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ojrnx", "is_robot_indexable": true, "report_reasons": null, "author": "milkygirl21", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ojrnx/file_name_scanner_to_remove_urls/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14ojrnx/file_name_scanner_to_remove_urls/", "subreddit_subscribers": 690687, "created_utc": 1688289238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone. This sub amazes me- you are really doing the Lords work here. I'm looking to find an old cached version of a Facebook page. It was started about a decade ago and while the page still exists, the comments have unfortunately been deleted. \nWayback machine and its counterparts had no records, and Google's only cached version is recent. And I can't get a response from the page admin. \nHoping someone will have a silver bullet. But look forward to any advice (or hard truths, if there's nothing to be done about it).", "author_fullname": "t2_7proj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help finding a cached Facebook page", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ojmcv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688288719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. This sub amazes me- you are really doing the Lords work here. I&amp;#39;m looking to find an old cached version of a Facebook page. It was started about a decade ago and while the page still exists, the comments have unfortunately been deleted. \nWayback machine and its counterparts had no records, and Google&amp;#39;s only cached version is recent. And I can&amp;#39;t get a response from the page admin. \nHoping someone will have a silver bullet. But look forward to any advice (or hard truths, if there&amp;#39;s nothing to be done about it).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ojmcv", "is_robot_indexable": true, "report_reasons": null, "author": "dablor", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ojmcv/help_finding_a_cached_facebook_page/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14ojmcv/help_finding_a_cached_facebook_page/", "subreddit_subscribers": 690687, "created_utc": 1688288719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Line: gallery-dl [https://twitter.com/walfieee](https://twitter.com/walfieee) \\--write-metadata -o skip=true\n\n    C:\\Users\\Yusei Fudo&gt;gallery-dl https://twitter.com/walfieee --write-metadata -o skip=true -v [gallery-dl][debug] Version 1.25.4 [gallery-dl][debug] Python 3.11.3 - Windows-10-10.0.22631-SP0 [gallery-dl][debug] requests 2.30.0 - urllib3 2.0.2 [gallery-dl][debug] Configuration Files [] [gallery-dl][debug] Starting DownloadJob for 'https://twitter.com/walfieee' [twitter][debug] Using TwitterTimelineExtractor for 'https://twitter.com/walfieee' [urllib3.connectionpool][debug] Starting new HTTPS connection (1): api.twitter.com:443 [urllib3.connectionpool][debug] https://api.twitter.com:443 \"GET /graphql/k26ASEiniqy4eXMdknTSoQ/UserByScreenName?variables=%7B%22screen_name%22%3A%22walfieee%22%2C%22withSafetyModeUserFields%22%3Atrue%7D&amp;features=%7B%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%7D HTTP/1.1\" 404 0 [twitter][error] 404 Not Found ()", "author_fullname": "t2_6dq3ykqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Need Help Archiving Twitter Account] Using Gallery-dl but It's not downloading anything. It keeps getting 404", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ojii6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688288367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Line: gallery-dl &lt;a href=\"https://twitter.com/walfieee\"&gt;https://twitter.com/walfieee&lt;/a&gt; --write-metadata -o skip=true&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;C:\\Users\\Yusei Fudo&amp;gt;gallery-dl https://twitter.com/walfieee --write-metadata -o skip=true -v [gallery-dl][debug] Version 1.25.4 [gallery-dl][debug] Python 3.11.3 - Windows-10-10.0.22631-SP0 [gallery-dl][debug] requests 2.30.0 - urllib3 2.0.2 [gallery-dl][debug] Configuration Files [] [gallery-dl][debug] Starting DownloadJob for &amp;#39;https://twitter.com/walfieee&amp;#39; [twitter][debug] Using TwitterTimelineExtractor for &amp;#39;https://twitter.com/walfieee&amp;#39; [urllib3.connectionpool][debug] Starting new HTTPS connection (1): api.twitter.com:443 [urllib3.connectionpool][debug] https://api.twitter.com:443 &amp;quot;GET /graphql/k26ASEiniqy4eXMdknTSoQ/UserByScreenName?variables=%7B%22screen_name%22%3A%22walfieee%22%2C%22withSafetyModeUserFields%22%3Atrue%7D&amp;amp;features=%7B%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%7D HTTP/1.1&amp;quot; 404 0 [twitter][error] 404 Not Found ()\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ojii6", "is_robot_indexable": true, "report_reasons": null, "author": "AnImEpRo3609", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ojii6/need_help_archiving_twitter_account_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14ojii6/need_help_archiving_twitter_account_using/", "subreddit_subscribers": 690687, "created_utc": 1688288367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I started downloading porn from Twitter accounts and I'm hitting rate limiting. When I visit any twitter account I get \"Something went wrong. Try reloading.\". In the \"Network\" tab I can see that the server replies with 429.\n\nI started browsing on Friday evening, started getting errors on Saturday afternoon. Decided to take a break. Woke up this (Sunday) morning, started browsing again, and after an hour, maybe two, started getting 429 responses again.\n\nAny advice? I'm not using any automatic scripts, just clicking around.", "author_fullname": "t2_l4230wn9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Twitter rate limiting?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ojczw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688287797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started downloading porn from Twitter accounts and I&amp;#39;m hitting rate limiting. When I visit any twitter account I get &amp;quot;Something went wrong. Try reloading.&amp;quot;. In the &amp;quot;Network&amp;quot; tab I can see that the server replies with 429.&lt;/p&gt;\n\n&lt;p&gt;I started browsing on Friday evening, started getting errors on Saturday afternoon. Decided to take a break. Woke up this (Sunday) morning, started browsing again, and after an hour, maybe two, started getting 429 responses again.&lt;/p&gt;\n\n&lt;p&gt;Any advice? I&amp;#39;m not using any automatic scripts, just clicking around.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ojczw", "is_robot_indexable": true, "report_reasons": null, "author": "TwinkForAHairyBear", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ojczw/twitter_rate_limiting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14ojczw/twitter_rate_limiting/", "subreddit_subscribers": 690687, "created_utc": 1688287797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have 5 backup drives in total, and used a dedupe tool on my Main drive to remove all the dupes. \n\nNow I want my 2\u207f\u1d48 to 5\u1d57\u02b0 drive to mirror exactly what my Main drive has, without me manually going through each file again. It's a 8TB drive, so efficiency is key. \n\nI do have Air Explorer Pro but not sure whether it's a good enough tool for comparing differences.\n\nWhat tool can I use to achieve this?", "author_fullname": "t2_addo5jzj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best open source tool to compare difference across Drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ojbxx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688287685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 5 backup drives in total, and used a dedupe tool on my Main drive to remove all the dupes. &lt;/p&gt;\n\n&lt;p&gt;Now I want my 2\u207f\u1d48 to 5\u1d57\u02b0 drive to mirror exactly what my Main drive has, without me manually going through each file again. It&amp;#39;s a 8TB drive, so efficiency is key. &lt;/p&gt;\n\n&lt;p&gt;I do have Air Explorer Pro but not sure whether it&amp;#39;s a good enough tool for comparing differences.&lt;/p&gt;\n\n&lt;p&gt;What tool can I use to achieve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ojbxx", "is_robot_indexable": true, "report_reasons": null, "author": "milkygirl21", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ojbxx/best_open_source_tool_to_compare_difference/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14ojbxx/best_open_source_tool_to_compare_difference/", "subreddit_subscribers": 690687, "created_utc": 1688287685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Feels like a lot of things have been shutting down lately, but [Filethis is also shutting down](https://filethis.com/consumer/shutdown-notice/) on Oct 1, although the site will function normally only until Sep 1. \n\nFilethis has been really helpful over the years for automatically downloading bank statements, utility bills, and the like. Does anyone know of any alternatives?", "author_fullname": "t2_dd3ki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Filethis is shutting down on Oct 1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14og7w5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688276489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Feels like a lot of things have been shutting down lately, but &lt;a href=\"https://filethis.com/consumer/shutdown-notice/\"&gt;Filethis is also shutting down&lt;/a&gt; on Oct 1, although the site will function normally only until Sep 1. &lt;/p&gt;\n\n&lt;p&gt;Filethis has been really helpful over the years for automatically downloading bank statements, utility bills, and the like. Does anyone know of any alternatives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14og7w5", "is_robot_indexable": true, "report_reasons": null, "author": "bsizzle13", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14og7w5/filethis_is_shutting_down_on_oct_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14og7w5/filethis_is_shutting_down_on_oct_1/", "subreddit_subscribers": 690687, "created_utc": 1688276489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have purchased a WD Elements 10TB and 14TB (not at the same time, 10TB earlier) both new, and the 10TB is very noisy like a fast CPU fan when the disk is running, whereas the 14TB is silent. They both look the same, so I am not sure what the difference is.\n\nIs it purely a coincidence (that is, there are silent 10TB and noisy 14TB), or did WD improve the noise on the 14TB or later? \n\nI am asking this, because I am considering buying a new WD Elmeents, and I am wondering if I have to buy 14TB or larger, if I want a silent disk.", "author_fullname": "t2_nvkhy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My WD Elements 14TB noticeably quieter than my 10TB. Coincidence?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14of5jy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688272873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have purchased a WD Elements 10TB and 14TB (not at the same time, 10TB earlier) both new, and the 10TB is very noisy like a fast CPU fan when the disk is running, whereas the 14TB is silent. They both look the same, so I am not sure what the difference is.&lt;/p&gt;\n\n&lt;p&gt;Is it purely a coincidence (that is, there are silent 10TB and noisy 14TB), or did WD improve the noise on the 14TB or later? &lt;/p&gt;\n\n&lt;p&gt;I am asking this, because I am considering buying a new WD Elmeents, and I am wondering if I have to buy 14TB or larger, if I want a silent disk.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14of5jy", "is_robot_indexable": true, "report_reasons": null, "author": "evolution2015", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14of5jy/my_wd_elements_14tb_noticeably_quieter_than_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14of5jy/my_wd_elements_14tb_noticeably_quieter_than_my/", "subreddit_subscribers": 690687, "created_utc": 1688272873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know if there's a good resource for soccer videos? I'm looking for stuff like player/team documentaries and official highlight videos, not full matches. \n\nThanks.", "author_fullname": "t2_ob33k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a tracker or site for soccer videos? (documentaries and highlight releases, not matches)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14of3wt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688272718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know if there&amp;#39;s a good resource for soccer videos? I&amp;#39;m looking for stuff like player/team documentaries and official highlight videos, not full matches. &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14of3wt", "is_robot_indexable": true, "report_reasons": null, "author": "ps1981", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14of3wt/is_there_a_tracker_or_site_for_soccer_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14of3wt/is_there_a_tracker_or_site_for_soccer_videos/", "subreddit_subscribers": 690687, "created_utc": 1688272718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've seen similar questions many times looking through this sub but need advice for my specific situation so any advice would help.\n\nExternal ssd or hdd?\n\nI have an old (8-10 yr) seagate external hard drive that has served me well, but it has been whirring for the past year and occasionally loses connection mid transfers. I use it mostly for storing video and access it maybe 5-6 times in a month, sometimes with many months of gaps in between.\n\nI've read keeping an ssd without power for too long can lead to data loss, but would a couple months also be a problem?\n\nI want something that'll last me another decade.\nCurrently considering Samsung T7 for ssd and either Seagate or WD for hdd.\n\nAny advice for my usage?\n\nEdit: Need a minimum 2TB", "author_fullname": "t2_5t4nbhds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to buy for an average user?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nz277", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688231498.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688228377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen similar questions many times looking through this sub but need advice for my specific situation so any advice would help.&lt;/p&gt;\n\n&lt;p&gt;External ssd or hdd?&lt;/p&gt;\n\n&lt;p&gt;I have an old (8-10 yr) seagate external hard drive that has served me well, but it has been whirring for the past year and occasionally loses connection mid transfers. I use it mostly for storing video and access it maybe 5-6 times in a month, sometimes with many months of gaps in between.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read keeping an ssd without power for too long can lead to data loss, but would a couple months also be a problem?&lt;/p&gt;\n\n&lt;p&gt;I want something that&amp;#39;ll last me another decade.\nCurrently considering Samsung T7 for ssd and either Seagate or WD for hdd.&lt;/p&gt;\n\n&lt;p&gt;Any advice for my usage?&lt;/p&gt;\n\n&lt;p&gt;Edit: Need a minimum 2TB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nz277", "is_robot_indexable": true, "report_reasons": null, "author": "IncorrectCoffee", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nz277/what_to_buy_for_an_average_user/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nz277/what_to_buy_for_an_average_user/", "subreddit_subscribers": 690687, "created_utc": 1688228377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, \n\nI have a Panasonic NV-FJ610 which has [these connections on the back](https://imgur.com/CaZWtZN) and I also have a BlackMagic Intensity Shuffle USB 3.0. \n\nCan you please help me understand what wires/equipment I need in order to capture my VHS tapes and make them digital? \n\nI was following [this tutorial on Youtube](https://www.youtube.com/watch?v=_3QH_-Tzzxk&amp;t=613s) but it's unclear how he had it set up. I know there's SCART to HDMI converters but I would want the best possible quality when capturing and from what I've read online, they can decrease quality as well as change the resolution.\n\nThere's an S video input on the BlackMagic shuffle so would it be worth getting a SCART to S video converter/cable and then capturing that way or that a similar situation with the HDMI converters?\n\nI have home movies that are over 30 years old that I want to capture and digitise and share with family as I know the shelf live on these tapes doesn't last forever so I don't want to lose memories from when I was a kid.\n\n&amp;#x200B;", "author_fullname": "t2_ce61udt7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to capture VHS footage with the setup I have? - Help me!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14o10o6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688233454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I have a Panasonic NV-FJ610 which has &lt;a href=\"https://imgur.com/CaZWtZN\"&gt;these connections on the back&lt;/a&gt; and I also have a BlackMagic Intensity Shuffle USB 3.0. &lt;/p&gt;\n\n&lt;p&gt;Can you please help me understand what wires/equipment I need in order to capture my VHS tapes and make them digital? &lt;/p&gt;\n\n&lt;p&gt;I was following &lt;a href=\"https://www.youtube.com/watch?v=_3QH_-Tzzxk&amp;amp;t=613s\"&gt;this tutorial on Youtube&lt;/a&gt; but it&amp;#39;s unclear how he had it set up. I know there&amp;#39;s SCART to HDMI converters but I would want the best possible quality when capturing and from what I&amp;#39;ve read online, they can decrease quality as well as change the resolution.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s an S video input on the BlackMagic shuffle so would it be worth getting a SCART to S video converter/cable and then capturing that way or that a similar situation with the HDMI converters?&lt;/p&gt;\n\n&lt;p&gt;I have home movies that are over 30 years old that I want to capture and digitise and share with family as I know the shelf live on these tapes doesn&amp;#39;t last forever so I don&amp;#39;t want to lose memories from when I was a kid.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nrSvQmkh_7gNq1mIySpnqf4OP4JxP6iB2DljfNuW0lo.jpg?auto=webp&amp;v=enabled&amp;s=04702e486a120242a94c1ab5292e5d5e98d2a838", "width": 600, "height": 315}, "resolutions": [{"url": "https://external-preview.redd.it/nrSvQmkh_7gNq1mIySpnqf4OP4JxP6iB2DljfNuW0lo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ad86628185d9c06006d9ba34c9b0c0b1e7dee17", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/nrSvQmkh_7gNq1mIySpnqf4OP4JxP6iB2DljfNuW0lo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a2e6e375154e7d3b162580242c241bd61940e40", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/nrSvQmkh_7gNq1mIySpnqf4OP4JxP6iB2DljfNuW0lo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=960aa84ff93ccf912f66f3e701dc953038e13142", "width": 320, "height": 168}], "variants": {}, "id": "eipK41Wyx27nBiHp4iV66587rz3Sc7KhL8MK072Dumc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14o10o6", "is_robot_indexable": true, "report_reasons": null, "author": "Uncle_Beanpole", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14o10o6/whats_the_best_way_to_capture_vhs_footage_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14o10o6/whats_the_best_way_to_capture_vhs_footage_with/", "subreddit_subscribers": 690687, "created_utc": 1688233454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm thinking of taking the plunge into ZFS coming from Windows 11. I routinely use the Arr apps and would like help/suggestions on whether to go with Ubuntu on ZFS or to go with Unraid or similar with BTRFS?\n\nMy present setup is a z790 chipset 13th gen Intel with onboard graphics.\n\n Is there any suggested reading or guides available for someone like me who has zero experience with Linux and wants to switch to utilize the benefits of ZFS? \n\nI currently have a 16TB, 18TB, 18TB enterprise hard drives installed internally which contain data that I would like to have on my new ZFS system. I am booting from NVMe SSD. \n\nProblem is, all of my data is on NTFS partitions. I do have external HDD's (also NTFS) which contain the same data so I can format the above three HDD's to create the raid group.\n\nI am considering buying a 3rd 18tb HDD so I have equal size drives to create my RAIDZ2 group starting with just 3 drives and not including my 16tb to start with. \n\nWould it be best for me to have equal sized drives in creating this group? Can I expand the group when I buy more drives or need more space? How will I handle copying my files from a drive with NTFS to a ZFS pool?\n\nIs it wise to create more than one pool or manage just one pool? I've read that others do daily short SMART tests, weekly long tests, and bi-weekly/monthly scrubs. Would this be sufficient? \n\nThe purpose of this build will be to do one thing. To operate the Arr apps. (Sonarr, Prowlarr, Overseerr, etc.) I will be installing Plex and using it as a HTPC - and little to nothing else. \n\nThe reason I am looking to switch to ZFS is due to it detecting/repairing bitrot. I have thought about Unraid but became confused on whether to use Ubuntu or Unraid. The idea of parity drives kind of confused me when it comes to Unraid.\n\n I am unclear on what would be best for my usage scenario. What would you recommend for ease of implementation - taking into consideration that my primary usage scenario is the Arr apps and Plex? ", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guides or tips for setting up first ZFS file/plex server?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nwq7w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688222356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of taking the plunge into ZFS coming from Windows 11. I routinely use the Arr apps and would like help/suggestions on whether to go with Ubuntu on ZFS or to go with Unraid or similar with BTRFS?&lt;/p&gt;\n\n&lt;p&gt;My present setup is a z790 chipset 13th gen Intel with onboard graphics.&lt;/p&gt;\n\n&lt;p&gt;Is there any suggested reading or guides available for someone like me who has zero experience with Linux and wants to switch to utilize the benefits of ZFS? &lt;/p&gt;\n\n&lt;p&gt;I currently have a 16TB, 18TB, 18TB enterprise hard drives installed internally which contain data that I would like to have on my new ZFS system. I am booting from NVMe SSD. &lt;/p&gt;\n\n&lt;p&gt;Problem is, all of my data is on NTFS partitions. I do have external HDD&amp;#39;s (also NTFS) which contain the same data so I can format the above three HDD&amp;#39;s to create the raid group.&lt;/p&gt;\n\n&lt;p&gt;I am considering buying a 3rd 18tb HDD so I have equal size drives to create my RAIDZ2 group starting with just 3 drives and not including my 16tb to start with. &lt;/p&gt;\n\n&lt;p&gt;Would it be best for me to have equal sized drives in creating this group? Can I expand the group when I buy more drives or need more space? How will I handle copying my files from a drive with NTFS to a ZFS pool?&lt;/p&gt;\n\n&lt;p&gt;Is it wise to create more than one pool or manage just one pool? I&amp;#39;ve read that others do daily short SMART tests, weekly long tests, and bi-weekly/monthly scrubs. Would this be sufficient? &lt;/p&gt;\n\n&lt;p&gt;The purpose of this build will be to do one thing. To operate the Arr apps. (Sonarr, Prowlarr, Overseerr, etc.) I will be installing Plex and using it as a HTPC - and little to nothing else. &lt;/p&gt;\n\n&lt;p&gt;The reason I am looking to switch to ZFS is due to it detecting/repairing bitrot. I have thought about Unraid but became confused on whether to use Ubuntu or Unraid. The idea of parity drives kind of confused me when it comes to Unraid.&lt;/p&gt;\n\n&lt;p&gt;I am unclear on what would be best for my usage scenario. What would you recommend for ease of implementation - taking into consideration that my primary usage scenario is the Arr apps and Plex? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nwq7w", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nwq7w/guides_or_tips_for_setting_up_first_zfs_fileplex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nwq7w/guides_or_tips_for_setting_up_first_zfs_fileplex/", "subreddit_subscribers": 690687, "created_utc": 1688222356.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\n&amp;#x200B;\n\nI have a Google workspace enterprise account with 10tb of used storage.  Paying \u00a315 a month and over my limit by 5tb.\n\n&amp;#x200B;\n\nHave data backed up on a HD at home.\n\n&amp;#x200B;\n\nData in cloud is for my Plex server using netdrive on a Windows server hosted in the cloud.\n\n&amp;#x200B;\n\nMy options are.\n\n&amp;#x200B;\n\nAdd another user and pay a total of \u00a330 a month for Google storage.\n\n&amp;#x200B;\n\nOr\n\n&amp;#x200B;\n\nUse my existing office 365 account, set up 5 users with 1 tb of OneDrive\n\n&amp;#x200B;\n\nMake several drives on my server from the OneDrive accounts using netdrive.\n\n&amp;#x200B;\n\nCopy over 1tb of data per account using multcloud.\n\n&amp;#x200B;\n\nThen create a Plex library combing the drives together.\n\n&amp;#x200B;\n\nGet another office 365 account and set up another 5 users to handle another 5TB of data.\n\n&amp;#x200B;\n\nThat way I'll spend \u00a3100 a year on 2x office 365 accounts and 12tb of OneDrive storage instead over \u00a3300 with Google for 10tb storage.\n\n&amp;#x200B;\n\nI've done a test using the OneDrive scenario and it seems to work ok with Plex.\n\n&amp;#x200B;\n\nHas anyone done this and any disadvantages apart from having to manage several OneDrive accounts?", "author_fullname": "t2_rco24", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just want to check before I implement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nvoru", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688219542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have a Google workspace enterprise account with 10tb of used storage.  Paying \u00a315 a month and over my limit by 5tb.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Have data backed up on a HD at home.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Data in cloud is for my Plex server using netdrive on a Windows server hosted in the cloud.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My options are.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Add another user and pay a total of \u00a330 a month for Google storage.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Or&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Use my existing office 365 account, set up 5 users with 1 tb of OneDrive&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Make several drives on my server from the OneDrive accounts using netdrive.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Copy over 1tb of data per account using multcloud.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Then create a Plex library combing the drives together.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Get another office 365 account and set up another 5 users to handle another 5TB of data.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;That way I&amp;#39;ll spend \u00a3100 a year on 2x office 365 accounts and 12tb of OneDrive storage instead over \u00a3300 with Google for 10tb storage.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done a test using the OneDrive scenario and it seems to work ok with Plex.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anyone done this and any disadvantages apart from having to manage several OneDrive accounts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nvoru", "is_robot_indexable": true, "report_reasons": null, "author": "richarduklon", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nvoru/just_want_to_check_before_i_implement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nvoru/just_want_to_check_before_i_implement/", "subreddit_subscribers": 690687, "created_utc": 1688219542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone!\n\nJust happy to share that I made a desktop app called CaptureGem which is great for data hoarding if you're into live streams. It lets you record from a variety of adult streaming sites. It also uses native code to do all this and is multi-threaded, so can record/download as much as your network connection can handle. Link to see more details: [https://www.capturegem.com](https://www.capturegem.com)\n\nKey features:\n\n* Supports a variety of sites [(see full list)](https://www.capturegem.com/#sites-supported)\n* Record multiple models simultaneously\n* Automatically start recordings when models come online\n* Auto-restart recordings after network timeouts or disconnects\n* Multi-threaded for high performance, with no limit on the number of live models you can record at once\n* Fully customizable settings, including video duration, network timeout, wait time variability, offline retry wait time, and error retry wait time\n* Records both VR and Non-VR streams\n* Saves all videos into .mp4 format\n\nLink to subreddit: [r/capturegem](https://www.reddit.com/r/capturegem/)", "author_fullname": "t2_ad2ew0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just sharing that I made a new desktop app called CaptureGem that lets you record cam models from a variety of sites. Also it's multi-threaded and high performance so you can record a lot of streams at once.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nsp47", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688210675.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;Just happy to share that I made a desktop app called CaptureGem which is great for data hoarding if you&amp;#39;re into live streams. It lets you record from a variety of adult streaming sites. It also uses native code to do all this and is multi-threaded, so can record/download as much as your network connection can handle. Link to see more details: &lt;a href=\"https://www.capturegem.com\"&gt;https://www.capturegem.com&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Key features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Supports a variety of sites &lt;a href=\"https://www.capturegem.com/#sites-supported\"&gt;(see full list)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Record multiple models simultaneously&lt;/li&gt;\n&lt;li&gt;Automatically start recordings when models come online&lt;/li&gt;\n&lt;li&gt;Auto-restart recordings after network timeouts or disconnects&lt;/li&gt;\n&lt;li&gt;Multi-threaded for high performance, with no limit on the number of live models you can record at once&lt;/li&gt;\n&lt;li&gt;Fully customizable settings, including video duration, network timeout, wait time variability, offline retry wait time, and error retry wait time&lt;/li&gt;\n&lt;li&gt;Records both VR and Non-VR streams&lt;/li&gt;\n&lt;li&gt;Saves all videos into .mp4 format&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Link to subreddit: &lt;a href=\"https://www.reddit.com/r/capturegem/\"&gt;r/capturegem&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6nwCwmBxDH8w2E3vYeVQFdSlI3AIwQmBaNf1CU6m0Sk.jpg?auto=webp&amp;v=enabled&amp;s=3dd9d4e1a324b5ad64f29c1582f33da7f64ed8ae", "width": 1528, "height": 1356}, "resolutions": [{"url": "https://external-preview.redd.it/6nwCwmBxDH8w2E3vYeVQFdSlI3AIwQmBaNf1CU6m0Sk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5154acff9800a7cc8e4352cbb98fa8cae6c85bd3", "width": 108, "height": 95}, {"url": "https://external-preview.redd.it/6nwCwmBxDH8w2E3vYeVQFdSlI3AIwQmBaNf1CU6m0Sk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=697da873c58152ede36829690fda533d96c86a32", "width": 216, "height": 191}, {"url": "https://external-preview.redd.it/6nwCwmBxDH8w2E3vYeVQFdSlI3AIwQmBaNf1CU6m0Sk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b6d1ec3b5c63cdd6d64757bb3dfad93faba149c", "width": 320, "height": 283}, {"url": "https://external-preview.redd.it/6nwCwmBxDH8w2E3vYeVQFdSlI3AIwQmBaNf1CU6m0Sk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=044b2f1e7c0cb82a901eab921ea59455b12741e0", "width": 640, "height": 567}, {"url": "https://external-preview.redd.it/6nwCwmBxDH8w2E3vYeVQFdSlI3AIwQmBaNf1CU6m0Sk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7659d603c29270304387ab0a27a6b006a98a52cc", "width": 960, "height": 851}, {"url": "https://external-preview.redd.it/6nwCwmBxDH8w2E3vYeVQFdSlI3AIwQmBaNf1CU6m0Sk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e86610040795584b1ff2a893745d9ee912588a3c", "width": 1080, "height": 958}], "variants": {}, "id": "QZgOj0Mz-MgALv9K8p7W4E_nCs1-P63VTUrJFnuEjyI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nsp47", "is_robot_indexable": true, "report_reasons": null, "author": "xyzzy8", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nsp47/just_sharing_that_i_made_a_new_desktop_app_called/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nsp47/just_sharing_that_i_made_a_new_desktop_app_called/", "subreddit_subscribers": 690687, "created_utc": 1688210675.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am using FDM, does it automatically assign a contiguous block of storage? Or does it keep on writing stuff on the fly to whatever small blocks it finds and basically results in a highly fragmented disc? All my movies range from 50 to 80 GB. I've got 3 of them downloading simultaneously for about an hour now. Any info on this topic is really appreciated, thanks!", "author_fullname": "t2_5b1mdb8x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am downloading multiple UHD movies simultaneously on my external HDD, will it result in disc fragmentation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nzqjj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.11, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688230131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using FDM, does it automatically assign a contiguous block of storage? Or does it keep on writing stuff on the fly to whatever small blocks it finds and basically results in a highly fragmented disc? All my movies range from 50 to 80 GB. I&amp;#39;ve got 3 of them downloading simultaneously for about an hour now. Any info on this topic is really appreciated, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14nzqjj", "is_robot_indexable": true, "report_reasons": null, "author": "theADDMIN", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nzqjj/i_am_downloading_multiple_uhd_movies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nzqjj/i_am_downloading_multiple_uhd_movies/", "subreddit_subscribers": 690687, "created_utc": 1688230131.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}