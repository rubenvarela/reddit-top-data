{"kind": "Listing", "data": {"after": null, "dist": 11, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I'm curious. What do you usually do while your code or ML models are running for 2, 5, 10 minutes, or even longer? Personally, I often find myself getting distracted by social networks (especially picking up my cellphone, since I work remotely), and end up wasting twice as much time on non-productive activities.\n\nDoes anyone have any tricks/tips that work for them to stay productive during those idle times? I tend to waste a couple of hours per day this way. I was thinking of picking up a book, but with so many interruptions, it feels more like an attempt to feel productive rather than actually reading and comprehending the book.\n\nSo, as a data scientist/programmer, what do you usually do during those idle times between running code?", "author_fullname": "t2_3vzap1d5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you do while your code is running?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14o4s6b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 98, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 98, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688243215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious. What do you usually do while your code or ML models are running for 2, 5, 10 minutes, or even longer? Personally, I often find myself getting distracted by social networks (especially picking up my cellphone, since I work remotely), and end up wasting twice as much time on non-productive activities.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any tricks/tips that work for them to stay productive during those idle times? I tend to waste a couple of hours per day this way. I was thinking of picking up a book, but with so many interruptions, it feels more like an attempt to feel productive rather than actually reading and comprehending the book.&lt;/p&gt;\n\n&lt;p&gt;So, as a data scientist/programmer, what do you usually do during those idle times between running code?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14o4s6b", "is_robot_indexable": true, "report_reasons": null, "author": "conlake", "discussion_type": null, "num_comments": 102, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14o4s6b/what_do_you_do_while_your_code_is_running/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14o4s6b/what_do_you_do_while_your_code_is_running/", "subreddit_subscribers": 936279, "created_utc": 1688243215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Let's take a moment to dive into a lesser-discussed but equally compelling career path within the realm of data: data analysis. While the spotlight often shines on data science, I want to shed some light on the invaluable contributions of data analysts in today's fast-paced tech landscape.\n\nSure, data science is captivating, with its complex algorithms and endless possibilities. It's no wonder many of us are drawn to it, enticed by the allure of solving intricate problems and pushing the boundaries of knowledge. But let's not forget the unsung heroes who wield SQL like a masterful sword, create dazzling visualizations, and extract meaningful insights from raw data.\n\nData analysts are the wizards behind the scenes, wielding their expertise to generate reports, craft informative dashboards, and provide actionable recommendations. In many organizations, they are the driving force behind data-informed decision-making, helping stakeholders understand complex datasets and uncover hidden patterns.\n\nThe truth is, while data science may offer a more sophisticated approach to analytics, the demand for such expertise often outstrips the appetite of many tech companies and start-ups. These organizations are laser-focused on rapid growth, and that's where the data analyst truly shines. Their ability to swiftly analyze and present data-driven insights is often the catalyst for making informed business decisions that accelerate growth.\n\nSo, if you find yourself struggling to break into the world of data science, I urge you to consider the realm of analytics. Not only can you still earn a handsome salary (think six figures, especially in product analytics), but it also opens up a natural pathway for future growth in data science.\n\nThe beauty of starting your journey as a data analyst lies in the breadth of opportunities it presents. You'll gain invaluable experience working closely with diverse datasets, honing your skills in data manipulation, visualization, and storytelling. Armed with this knowledge, you'll be well-equipped to transition seamlessly into data science when the time is right, bringing a unique perspective to the table.\n\nBy embracing analytics, you'll discover a world where you can make an immediate impact, influence key decisions, and drive meaningful change. Your expertise in translating raw data into actionable insights will be in high demand, and you'll be the go-to person for extracting value from vast amounts of information.\n\nSo, my fellow data aficionados, don't rule out the power of data analysis. Embrace this exciting field as an entry point to a rewarding career that bridges the gap between raw data and impactful outcomes. Whether you choose to continue on the path of analytics or venture into the realm of data science, remember that you have options, and your skills are in demand.\n\nLet's give a round of applause to the unsung heroes of analytics\u2014the data analysts\u2014who bring clarity and meaning to the data-driven world we live in. Together, let's unlock the true potential of data!\n\nEndless possibilities await. \ud83d\ude80", "author_fullname": "t2_1n8ec852", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unleashing the Power of Data Analysts: The Unsung Heroes of Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14okyuk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688293518.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s take a moment to dive into a lesser-discussed but equally compelling career path within the realm of data: data analysis. While the spotlight often shines on data science, I want to shed some light on the invaluable contributions of data analysts in today&amp;#39;s fast-paced tech landscape.&lt;/p&gt;\n\n&lt;p&gt;Sure, data science is captivating, with its complex algorithms and endless possibilities. It&amp;#39;s no wonder many of us are drawn to it, enticed by the allure of solving intricate problems and pushing the boundaries of knowledge. But let&amp;#39;s not forget the unsung heroes who wield SQL like a masterful sword, create dazzling visualizations, and extract meaningful insights from raw data.&lt;/p&gt;\n\n&lt;p&gt;Data analysts are the wizards behind the scenes, wielding their expertise to generate reports, craft informative dashboards, and provide actionable recommendations. In many organizations, they are the driving force behind data-informed decision-making, helping stakeholders understand complex datasets and uncover hidden patterns.&lt;/p&gt;\n\n&lt;p&gt;The truth is, while data science may offer a more sophisticated approach to analytics, the demand for such expertise often outstrips the appetite of many tech companies and start-ups. These organizations are laser-focused on rapid growth, and that&amp;#39;s where the data analyst truly shines. Their ability to swiftly analyze and present data-driven insights is often the catalyst for making informed business decisions that accelerate growth.&lt;/p&gt;\n\n&lt;p&gt;So, if you find yourself struggling to break into the world of data science, I urge you to consider the realm of analytics. Not only can you still earn a handsome salary (think six figures, especially in product analytics), but it also opens up a natural pathway for future growth in data science.&lt;/p&gt;\n\n&lt;p&gt;The beauty of starting your journey as a data analyst lies in the breadth of opportunities it presents. You&amp;#39;ll gain invaluable experience working closely with diverse datasets, honing your skills in data manipulation, visualization, and storytelling. Armed with this knowledge, you&amp;#39;ll be well-equipped to transition seamlessly into data science when the time is right, bringing a unique perspective to the table.&lt;/p&gt;\n\n&lt;p&gt;By embracing analytics, you&amp;#39;ll discover a world where you can make an immediate impact, influence key decisions, and drive meaningful change. Your expertise in translating raw data into actionable insights will be in high demand, and you&amp;#39;ll be the go-to person for extracting value from vast amounts of information.&lt;/p&gt;\n\n&lt;p&gt;So, my fellow data aficionados, don&amp;#39;t rule out the power of data analysis. Embrace this exciting field as an entry point to a rewarding career that bridges the gap between raw data and impactful outcomes. Whether you choose to continue on the path of analytics or venture into the realm of data science, remember that you have options, and your skills are in demand.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s give a round of applause to the unsung heroes of analytics\u2014the data analysts\u2014who bring clarity and meaning to the data-driven world we live in. Together, let&amp;#39;s unlock the true potential of data!&lt;/p&gt;\n\n&lt;p&gt;Endless possibilities await. \ud83d\ude80&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14okyuk", "is_robot_indexable": true, "report_reasons": null, "author": "Abaddon55156", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14okyuk/unleashing_the_power_of_data_analysts_the_unsung/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14okyuk/unleashing_the_power_of_data_analysts_the_unsung/", "subreddit_subscribers": 936279, "created_utc": 1688293518.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I currently have a bachelor's in math and have been working the same job for roughly the same income ($80,000/yr) for the past two years, since graduating college. I haven't been promoted, haven't been offered anything except for some shares of the company that everyone got. \n\nMy job responsibilities honestly don't have much to do with data analysis, even though that's what I was hired on for. I work a lot with the company's AI framework (extending it, adding features). I'm not qualified to be doing actual AI work, but that's what I've been tasked with. \n\nI'm wondering what the best move should be for advancing forward in my career. I'm looking at job listings, but data science jobs want at least a master's, and data analyst jobs pay about the same as what I'm currently getting. Other jobs in this general area (e.g. ML engineer) seem to require a lot more software background/skill than I currently have. \n\nI'm working on a project to demonstrate proficiency with AI and to skill build. I'm also trying to find ways to get better at my current job so that I can be deemed worthy of a raise or promotion of some sort. I'm looking into taking online courses, but ultimately, I feel lost as to how I should really be going about all this.", "author_fullname": "t2_4lr3dhfo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've been working at a startup as a solutions engineer (hired as a data analyst) for 2 years now, but I feel stagnant in my career and want a higher salary-- what should I do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14o7srz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688250913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have a bachelor&amp;#39;s in math and have been working the same job for roughly the same income ($80,000/yr) for the past two years, since graduating college. I haven&amp;#39;t been promoted, haven&amp;#39;t been offered anything except for some shares of the company that everyone got. &lt;/p&gt;\n\n&lt;p&gt;My job responsibilities honestly don&amp;#39;t have much to do with data analysis, even though that&amp;#39;s what I was hired on for. I work a lot with the company&amp;#39;s AI framework (extending it, adding features). I&amp;#39;m not qualified to be doing actual AI work, but that&amp;#39;s what I&amp;#39;ve been tasked with. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering what the best move should be for advancing forward in my career. I&amp;#39;m looking at job listings, but data science jobs want at least a master&amp;#39;s, and data analyst jobs pay about the same as what I&amp;#39;m currently getting. Other jobs in this general area (e.g. ML engineer) seem to require a lot more software background/skill than I currently have. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on a project to demonstrate proficiency with AI and to skill build. I&amp;#39;m also trying to find ways to get better at my current job so that I can be deemed worthy of a raise or promotion of some sort. I&amp;#39;m looking into taking online courses, but ultimately, I feel lost as to how I should really be going about all this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14o7srz", "is_robot_indexable": true, "report_reasons": null, "author": "PathalogicalObject", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14o7srz/ive_been_working_at_a_startup_as_a_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14o7srz/ive_been_working_at_a_startup_as_a_solutions/", "subreddit_subscribers": 936279, "created_utc": 1688250913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm curious if anyone here is using Hex or DeepNote and if they have any thoughts on these tools. Curious why they might have chosen Hex or DeepNote or Count, etc. vs. Google Colab, etc. I'm also curious if there are any downsides to using tools like these over a standard Jupyter Notebook running on my laptop.\n\nRecently saw Hex also introduced AI features like debugging and writing queries using natural language but like many other tools doing this I believe for complex queries that are generally written the accuracy of these features will be a key challenge has anyone tried it if yes how useful have you found them?  \n\n\n*(Apologies if this has been asked earlier but wanted to get some opinions out there as I want to adopt a few of such tools in my org so trying to gather some strong points around the same.)*", "author_fullname": "t2_a7ri9r5h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone here using tools like Hex, DeepNote, or Count?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ohi3o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688280965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious if anyone here is using Hex or DeepNote and if they have any thoughts on these tools. Curious why they might have chosen Hex or DeepNote or Count, etc. vs. Google Colab, etc. I&amp;#39;m also curious if there are any downsides to using tools like these over a standard Jupyter Notebook running on my laptop.&lt;/p&gt;\n\n&lt;p&gt;Recently saw Hex also introduced AI features like debugging and writing queries using natural language but like many other tools doing this I believe for complex queries that are generally written the accuracy of these features will be a key challenge has anyone tried it if yes how useful have you found them?  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;(Apologies if this has been asked earlier but wanted to get some opinions out there as I want to adopt a few of such tools in my org so trying to gather some strong points around the same.)&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14ohi3o", "is_robot_indexable": true, "report_reasons": null, "author": "Comprehensive-Pay530", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14ohi3o/is_anyone_here_using_tools_like_hex_deepnote_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14ohi3o/is_anyone_here_using_tools_like_hex_deepnote_or/", "subreddit_subscribers": 936279, "created_utc": 1688280965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, I've been pondering a concept lately, and I'm curious to know if it sounds reasonable to anyone else. Specifically, I'm curious about the idea of 'supervised feature clustering'. My idea revolves around clustering features in a manner that's driven by a particular outcome (e.g., a binary outcome).\n\nThis concept reminds me somewhat of the contrast between PLS (Partial Least Squares) and PCA (Principal Component Analysis), as with PLS, the outcome directly influences the generation of the principal components.\n\nIn my search for further details on this topic, I've struggled to find extensive information online, which is why I'm reaching out to ya'll.\n\nMoreover, I'm curious about the potential of Conditional (aka. observational) SHAP values in this context. Essentially, these values offer a way to distribute feature importance among not only to those the model is using, but also correlated features (vs. the typical SHAP which would attribute feature importance only to the features the model is directly using). (paper here: [https://arxiv.org/abs/2006.16234](https://arxiv.org/abs/2006.16234))\n\nHere's an idea: Imagine if we trained a model, calculated the Conditional SHAP values, and then clustered the features based on these conditional SHAP values. This would, theoretically, create a form of supervised feature clustering.\n\nI'd love to hear your thoughts on this approach or if you have any insights or resources to share!", "author_fullname": "t2_4cbqp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Approach to Supervised Feature Clustering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14om3ga", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688297971.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688297486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;ve been pondering a concept lately, and I&amp;#39;m curious to know if it sounds reasonable to anyone else. Specifically, I&amp;#39;m curious about the idea of &amp;#39;supervised feature clustering&amp;#39;. My idea revolves around clustering features in a manner that&amp;#39;s driven by a particular outcome (e.g., a binary outcome).&lt;/p&gt;\n\n&lt;p&gt;This concept reminds me somewhat of the contrast between PLS (Partial Least Squares) and PCA (Principal Component Analysis), as with PLS, the outcome directly influences the generation of the principal components.&lt;/p&gt;\n\n&lt;p&gt;In my search for further details on this topic, I&amp;#39;ve struggled to find extensive information online, which is why I&amp;#39;m reaching out to ya&amp;#39;ll.&lt;/p&gt;\n\n&lt;p&gt;Moreover, I&amp;#39;m curious about the potential of Conditional (aka. observational) SHAP values in this context. Essentially, these values offer a way to distribute feature importance among not only to those the model is using, but also correlated features (vs. the typical SHAP which would attribute feature importance only to the features the model is directly using). (paper here: &lt;a href=\"https://arxiv.org/abs/2006.16234\"&gt;https://arxiv.org/abs/2006.16234&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an idea: Imagine if we trained a model, calculated the Conditional SHAP values, and then clustered the features based on these conditional SHAP values. This would, theoretically, create a form of supervised feature clustering.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear your thoughts on this approach or if you have any insights or resources to share!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?auto=webp&amp;v=enabled&amp;s=757c00601aa4ffb984c87000927a0610d04c3845", "width": 1200, "height": 700}, "resolutions": [{"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=586089b93aa59ebd86bb3b273ad1fb0c73e45ab7", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=00869aa5692fb9c8aa11f48ed92bff8db4f47293", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72f6ae2c0800df8a56c3fc74afb033bf37cc16a9", "width": 320, "height": 186}, {"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cfcb5f9f66743f2e26952e5edff4dfed984af692", "width": 640, "height": 373}, {"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=821ed287940b59a56b2643dcaf6a356ccfdc4eb5", "width": 960, "height": 560}, {"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f101972ffc7ec2e3eedefa45eaa677e4d9024520", "width": 1080, "height": 630}], "variants": {}, "id": "q3evP6JeDpAC2MdSQHWYxnCYTqbJkElIQsLFqVSdkss"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14om3ga", "is_robot_indexable": true, "report_reasons": null, "author": "Rictoo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14om3ga/approach_to_supervised_feature_clustering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14om3ga/approach_to_supervised_feature_clustering/", "subreddit_subscribers": 936279, "created_utc": 1688297486.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nMean Squarred Error has fast convergence capabilities while discarding low errors as noise, but has stability issues.\n\nMean Absolute Error is very stable but slow as it has linear dependency for big errors. Huber loss is a solution.\n\nThe function below can serve as simplified version of Huber error (instead of 2 inequalities), where **\ud835\udeff** can decrease from big value to 1.0 or lower in order to improve convergence speed and stability with time.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/wwc11rf1fe9b1.png?width=565&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7b4c146600a5d62ab944afc58d5b330b6f337d7f\n\n&amp;#x200B;\n\nhttps://i.redd.it/xtd3m9s8fe9b1.gif\n\n&amp;#x200B;\n\nmy name is Timur Ishuov, this is an investigation of abs(x)\\*tanh(x),  thank you!\n\n&amp;#x200B;", "author_fullname": "t2_5nqidvs7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rectified Hubber Error - ReHE (for scienctific audience)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "media_metadata": {"wwc11rf1fe9b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 39, "x": 108, "u": "https://preview.redd.it/wwc11rf1fe9b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b354365fd2f56e1383e01914985867497ee3e69"}, {"y": 78, "x": 216, "u": "https://preview.redd.it/wwc11rf1fe9b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58fa005e35758681e26bc6a8db2f28538a8544b7"}, {"y": 116, "x": 320, "u": "https://preview.redd.it/wwc11rf1fe9b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=23a36d52fdb527e3a4bf1f79dacaf973b47177ef"}], "s": {"y": 205, "x": 565, "u": "https://preview.redd.it/wwc11rf1fe9b1.png?width=565&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7b4c146600a5d62ab944afc58d5b330b6f337d7f"}, "id": "wwc11rf1fe9b1"}, "xtd3m9s8fe9b1": {"status": "valid", "e": "AnimatedImage", "m": "image/gif", "p": [{"y": 59, "x": 108, "u": "https://preview.redd.it/xtd3m9s8fe9b1.gif?width=108&amp;crop=smart&amp;format=png8&amp;v=enabled&amp;s=ad43fc77fa6e759de9f6b7683495d5787ec02e69"}, {"y": 118, "x": 216, "u": "https://preview.redd.it/xtd3m9s8fe9b1.gif?width=216&amp;crop=smart&amp;format=png8&amp;v=enabled&amp;s=d1bb452f95e7f26b2ab78e88059848fb5c60dcba"}, {"y": 176, "x": 320, "u": "https://preview.redd.it/xtd3m9s8fe9b1.gif?width=320&amp;crop=smart&amp;format=png8&amp;v=enabled&amp;s=4e1ec439a0dd9a51ee0aa709f73d346df0211603"}, {"y": 352, "x": 640, "u": "https://preview.redd.it/xtd3m9s8fe9b1.gif?width=640&amp;crop=smart&amp;format=png8&amp;v=enabled&amp;s=3ec8dbf9e71b23c792981e888a7ba837c5f1b6d1"}, {"y": 528, "x": 960, "u": "https://preview.redd.it/xtd3m9s8fe9b1.gif?width=960&amp;crop=smart&amp;format=png8&amp;v=enabled&amp;s=e56ed1f7e2796ba1a9a31e61a53488a9c0187e93"}, {"y": 594, "x": 1080, "u": "https://preview.redd.it/xtd3m9s8fe9b1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;v=enabled&amp;s=f8ea70930b6d32607e3f86525cacde201bb3b442"}], "s": {"y": 942, "gif": "https://i.redd.it/xtd3m9s8fe9b1.gif", "mp4": "https://preview.redd.it/xtd3m9s8fe9b1.gif?format=mp4&amp;v=enabled&amp;s=0ff03f7b75e4cd62b0482d178fe6a72a8a41fce7", "x": 1712}, "id": "xtd3m9s8fe9b1"}}, "name": "t3_14o2ht9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gBVSuXxbXnlLWQHvu7C-F2hE2eln4nxVdJ6CGqOTvsw.jpg", "edited": 1688240488.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688237252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mean Squarred Error has fast convergence capabilities while discarding low errors as noise, but has stability issues.&lt;/p&gt;\n\n&lt;p&gt;Mean Absolute Error is very stable but slow as it has linear dependency for big errors. Huber loss is a solution.&lt;/p&gt;\n\n&lt;p&gt;The function below can serve as simplified version of Huber error (instead of 2 inequalities), where &lt;strong&gt;\ud835\udeff&lt;/strong&gt; can decrease from big value to 1.0 or lower in order to improve convergence speed and stability with time.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wwc11rf1fe9b1.png?width=565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7b4c146600a5d62ab944afc58d5b330b6f337d7f\"&gt;https://preview.redd.it/wwc11rf1fe9b1.png?width=565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7b4c146600a5d62ab944afc58d5b330b6f337d7f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/xtd3m9s8fe9b1.gif\"&gt;https://i.redd.it/xtd3m9s8fe9b1.gif&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;my name is Timur Ishuov, this is an investigation of abs(x)*tanh(x),  thank you!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14o2ht9", "is_robot_indexable": true, "report_reasons": null, "author": "Timur_1988", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14o2ht9/rectified_hubber_error_rehe_for_scienctific/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14o2ht9/rectified_hubber_error_rehe_for_scienctific/", "subreddit_subscribers": 936279, "created_utc": 1688237252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I currently have around 50GB of product data that I\u2019m looking to store for my mobile application. Im looking into a cloud option like AWS. This data will be accessed frequently during the use of the application and I\u2019m worried this may drive up cost. Does anyone have any suggestions for reliable storage options? Thanks in advance!", "author_fullname": "t2_e0uyd6lk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on data storage options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14od931", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688266638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have around 50GB of product data that I\u2019m looking to store for my mobile application. Im looking into a cloud option like AWS. This data will be accessed frequently during the use of the application and I\u2019m worried this may drive up cost. Does anyone have any suggestions for reliable storage options? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14od931", "is_robot_indexable": true, "report_reasons": null, "author": "OccasionExisting8243", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14od931/advice_on_data_storage_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14od931/advice_on_data_storage_options/", "subreddit_subscribers": 936279, "created_utc": 1688266638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Due to incompatible library upgrades being forced upon me my IT security, I need to split apart an existing batch inference script so each step runs in a separate a Python conda environment. \n\nThis is a process that runs on my computer and crunches multiple TB of data in MB sized chunks, it takes a few days to complete. Those are \u201con disk\u201d file sizes\u2026the intermediate in-memory data is of course larger (numpy arrays mainly). Right now it\u2019s a nice tidy Python script that takes uses a loop to take each chunk through steps A -&gt; B -&gt; C, with some adjacent bookkeeping for logging and whatnot. Going forward, B will not be compatible with A and C\u2026it will require its own environment. A and C could still run in the same environment. \n\nWhat are some straightforward methods of dealing with this? I hope to maintain the \u201cin small chunks\u201d aspect of the process, and also hope to avoid pickling huge Numpy arrays to disk since that would be ungodly slow. \n\nHoping to get some advice from those of you who\u2019ve dealt with this before. Once I know how to do this it\u2019ll open up a new world in terms of building more flexible and complex pipelines, but I\u2019m not really sure where to even start! \n\nThanks!", "author_fullname": "t2_5o6jf0w4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Splitting process step to run in separate conda environments?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14obxjk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688262602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Due to incompatible library upgrades being forced upon me my IT security, I need to split apart an existing batch inference script so each step runs in a separate a Python conda environment. &lt;/p&gt;\n\n&lt;p&gt;This is a process that runs on my computer and crunches multiple TB of data in MB sized chunks, it takes a few days to complete. Those are \u201con disk\u201d file sizes\u2026the intermediate in-memory data is of course larger (numpy arrays mainly). Right now it\u2019s a nice tidy Python script that takes uses a loop to take each chunk through steps A -&amp;gt; B -&amp;gt; C, with some adjacent bookkeeping for logging and whatnot. Going forward, B will not be compatible with A and C\u2026it will require its own environment. A and C could still run in the same environment. &lt;/p&gt;\n\n&lt;p&gt;What are some straightforward methods of dealing with this? I hope to maintain the \u201cin small chunks\u201d aspect of the process, and also hope to avoid pickling huge Numpy arrays to disk since that would be ungodly slow. &lt;/p&gt;\n\n&lt;p&gt;Hoping to get some advice from those of you who\u2019ve dealt with this before. Once I know how to do this it\u2019ll open up a new world in terms of building more flexible and complex pipelines, but I\u2019m not really sure where to even start! &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14obxjk", "is_robot_indexable": true, "report_reasons": null, "author": "InternationalMany6", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14obxjk/splitting_process_step_to_run_in_separate_conda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14obxjk/splitting_process_step_to_run_in_separate_conda/", "subreddit_subscribers": 936279, "created_utc": 1688262602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all, \n\nWe are currently trying to develop an upper Ontology for a pretty big company. We are in the middle of information engineering most of the Data that is floating around. \n\nDo you have any suggestions on literature or anything? I tried doing my homework, but still feel kind of lost in the sauce. \n\nHas anyone of you ever trying to achieve something like this? If yes, what where the downsides?", "author_fullname": "t2_688zpk1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upper Ontology", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14o5mow", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688245346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, &lt;/p&gt;\n\n&lt;p&gt;We are currently trying to develop an upper Ontology for a pretty big company. We are in the middle of information engineering most of the Data that is floating around. &lt;/p&gt;\n\n&lt;p&gt;Do you have any suggestions on literature or anything? I tried doing my homework, but still feel kind of lost in the sauce. &lt;/p&gt;\n\n&lt;p&gt;Has anyone of you ever trying to achieve something like this? If yes, what where the downsides?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14o5mow", "is_robot_indexable": true, "report_reasons": null, "author": "pucki", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14o5mow/upper_ontology/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14o5mow/upper_ontology/", "subreddit_subscribers": 936279, "created_utc": 1688245346.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have this weird kind of requirement that i need for my azure ml model, not sure how to make it, so its that i will provide the model a bunch of data and in that data each row signifies a single interaction of a user with my app. Now the same user may have multiple interactions too so more than one rows is possible for same user. Also a user is uniquely determined by the customer\\_id column. Now the other columns describe the details of that user interaction, so for eg lets say my app is a shopping app, so the other columns could be like - price of the item, rating of the item, discount etc.\n\nAlso i have one special column called the level column which tells me how much further did user go with that item, like did he just click on it or added it to cart or did he buy it and so obviously buying in the level column is a stronger indicator of preference than just clicking. So i want the model to go through all these user interactions data and prepare a new table which should have this - against every customer id, a list of features/columns that are most important to him/her based on his/her interactions.\n\nFinally I want to create something like a rest api web service from this model, where i just provide it the customer id and it will give me the list of features/columns that will be important for this user. Need guidance regarding how to implement this using azure ML studio?", "author_fullname": "t2_21tk40ee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unique Feature Selection's for customers using Azure ML Studio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14o375y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688239065.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have this weird kind of requirement that i need for my azure ml model, not sure how to make it, so its that i will provide the model a bunch of data and in that data each row signifies a single interaction of a user with my app. Now the same user may have multiple interactions too so more than one rows is possible for same user. Also a user is uniquely determined by the customer_id column. Now the other columns describe the details of that user interaction, so for eg lets say my app is a shopping app, so the other columns could be like - price of the item, rating of the item, discount etc.&lt;/p&gt;\n\n&lt;p&gt;Also i have one special column called the level column which tells me how much further did user go with that item, like did he just click on it or added it to cart or did he buy it and so obviously buying in the level column is a stronger indicator of preference than just clicking. So i want the model to go through all these user interactions data and prepare a new table which should have this - against every customer id, a list of features/columns that are most important to him/her based on his/her interactions.&lt;/p&gt;\n\n&lt;p&gt;Finally I want to create something like a rest api web service from this model, where i just provide it the customer id and it will give me the list of features/columns that will be important for this user. Need guidance regarding how to implement this using azure ML studio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14o375y", "is_robot_indexable": true, "report_reasons": null, "author": "siddhantchimankar", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14o375y/unique_feature_selections_for_customers_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14o375y/unique_feature_selections_for_customers_using/", "subreddit_subscribers": 936279, "created_utc": 1688239065.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Worried I may be overpaid lol", "author_fullname": "t2_a8yt6gr6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jr mlops engineer make about 100k in a big city not nyc, am I overpaid?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14o162d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688233833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Worried I may be overpaid lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14o162d", "is_robot_indexable": true, "report_reasons": null, "author": "MembershipNice2192", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14o162d/jr_mlops_engineer_make_about_100k_in_a_big_city/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14o162d/jr_mlops_engineer_make_about_100k_in_a_big_city/", "subreddit_subscribers": 936279, "created_utc": 1688233833.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}