{"kind": "Listing", "data": {"after": "t3_14xvxak", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've got nothing to do at the moment. With all of my major projects I'm waiting for someone else's input. Usually, there's small progress, then I'm excited to get working. Then something else comes up, and there's another wait of 2-4 weeks for one thing or another. \n\nI tried padding my schedule with useful stuff like ad-hoc analyses, strategising, prep work for infrastructure changes, and networking. Infrastructure work is limited due to low data maturity and heavy silos, though. There's also a limit to how much networking one can do.\n\nI've run out of meaningful things to do. This has been going on for 6 months and I'm starting to feel burned out. So, I also don't have the energy to learn anymore, maybe because I'm exhausted, maybe because it's lonely and currently void of any practical application. \n\nI talked with my boss who is sympathetic but he can only offer me more filler work at the moment. I know it's a first world problem but I'm really losing my motivation. Would you have any advice on what to do? \n\nThanks in advance.", "author_fullname": "t2_4j7ujk5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nothing to do", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xvd0b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 73, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 73, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689183983.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689183754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got nothing to do at the moment. With all of my major projects I&amp;#39;m waiting for someone else&amp;#39;s input. Usually, there&amp;#39;s small progress, then I&amp;#39;m excited to get working. Then something else comes up, and there&amp;#39;s another wait of 2-4 weeks for one thing or another. &lt;/p&gt;\n\n&lt;p&gt;I tried padding my schedule with useful stuff like ad-hoc analyses, strategising, prep work for infrastructure changes, and networking. Infrastructure work is limited due to low data maturity and heavy silos, though. There&amp;#39;s also a limit to how much networking one can do.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve run out of meaningful things to do. This has been going on for 6 months and I&amp;#39;m starting to feel burned out. So, I also don&amp;#39;t have the energy to learn anymore, maybe because I&amp;#39;m exhausted, maybe because it&amp;#39;s lonely and currently void of any practical application. &lt;/p&gt;\n\n&lt;p&gt;I talked with my boss who is sympathetic but he can only offer me more filler work at the moment. I know it&amp;#39;s a first world problem but I&amp;#39;m really losing my motivation. Would you have any advice on what to do? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xvd0b", "is_robot_indexable": true, "report_reasons": null, "author": "norfkens2", "discussion_type": null, "num_comments": 79, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xvd0b/nothing_to_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14xvd0b/nothing_to_do/", "subreddit_subscribers": 947034, "created_utc": 1689183754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Whenever I've scrolled through Linkdin, I'm seeing heinous ratios like 60-200 applicants: 1 opening. I mean I just started my DataCamp tracks last September! Am I looking in the wrong places or am I just fucked? ", "author_fullname": "t2_icfhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is data science oversaturated now? | Job Market", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xzcdi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689192905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whenever I&amp;#39;ve scrolled through Linkdin, I&amp;#39;m seeing heinous ratios like 60-200 applicants: 1 opening. I mean I just started my DataCamp tracks last September! Am I looking in the wrong places or am I just fucked? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xzcdi", "is_robot_indexable": true, "report_reasons": null, "author": "Genedide", "discussion_type": null, "num_comments": 113, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xzcdi/is_data_science_oversaturated_now_job_market/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14xzcdi/is_data_science_oversaturated_now_job_market/", "subreddit_subscribers": 947034, "created_utc": 1689192905.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Does anyone here work somewhere that traditionally doesn't have a robust technical recruitment pipeline?\n\nAn example, and my goal, is nightclubs. A lot operate on paper and calculators and would greatly benefit from an on-site data scientist/engineer to create insightful dashboards, attendance/revenue forecasts, and just overall lending a helping hand bringing the club backend to modern standards.\n\nI just want some perspective from those who work in a non-traditional role and how you got your foot in the door.\n\nThanks", "author_fullname": "t2_p3oo7xu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting started in non-traditional roles (e.g. nightclubs)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xva5f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689183582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone here work somewhere that traditionally doesn&amp;#39;t have a robust technical recruitment pipeline?&lt;/p&gt;\n\n&lt;p&gt;An example, and my goal, is nightclubs. A lot operate on paper and calculators and would greatly benefit from an on-site data scientist/engineer to create insightful dashboards, attendance/revenue forecasts, and just overall lending a helping hand bringing the club backend to modern standards.&lt;/p&gt;\n\n&lt;p&gt;I just want some perspective from those who work in a non-traditional role and how you got your foot in the door.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xva5f", "is_robot_indexable": true, "report_reasons": null, "author": "knavishly_vibrant38", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xva5f/getting_started_in_nontraditional_roles_eg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14xva5f/getting_started_in_nontraditional_roles_eg/", "subreddit_subscribers": 947034, "created_utc": 1689183582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How often  have you actually come across over qualified candidates for a data science position ? \n\nHow have the results been when you interview them and they tick all the boxes?", "author_fullname": "t2_mloui", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Over qualified candidates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14y5os5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689208299.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How often  have you actually come across over qualified candidates for a data science position ? &lt;/p&gt;\n\n&lt;p&gt;How have the results been when you interview them and they tick all the boxes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14y5os5", "is_robot_indexable": true, "report_reasons": null, "author": "Asshaisin", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14y5os5/over_qualified_candidates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14y5os5/over_qualified_candidates/", "subreddit_subscribers": 947034, "created_utc": 1689208299.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey so I'm an intern right now, and I'm tasked with creating a model that takes in the initial order conditions (multitouch attribution, amount ordered, product, etc on first order) and predicts the LTV after 3,6,9, and 12 months. \n\nI used a decision tree and created 3 different models to predict the LTV of a month 0 customer and its of satisfactory accuracy, but I now realize I will probably need to make 4 different model for every customer cohort (when they bought), meaning I will have to make a whole lot of models.\n\nThis is very doable, the models run pretty fast, and I have around 150,000 pieces of training data, but the way I am doing it feels incredibly sloppy and suboptimal (I pretty much know it is, I just don't know an easy workaround). Any help is much appreciated for an internet in need!", "author_fullname": "t2_bmx0x96q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives to creating 20 different models to predict customer Life Time Value at different points?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xzc3e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689192887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey so I&amp;#39;m an intern right now, and I&amp;#39;m tasked with creating a model that takes in the initial order conditions (multitouch attribution, amount ordered, product, etc on first order) and predicts the LTV after 3,6,9, and 12 months. &lt;/p&gt;\n\n&lt;p&gt;I used a decision tree and created 3 different models to predict the LTV of a month 0 customer and its of satisfactory accuracy, but I now realize I will probably need to make 4 different model for every customer cohort (when they bought), meaning I will have to make a whole lot of models.&lt;/p&gt;\n\n&lt;p&gt;This is very doable, the models run pretty fast, and I have around 150,000 pieces of training data, but the way I am doing it feels incredibly sloppy and suboptimal (I pretty much know it is, I just don&amp;#39;t know an easy workaround). Any help is much appreciated for an internet in need!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xzc3e", "is_robot_indexable": true, "report_reasons": null, "author": "Opening-Education-88", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xzc3e/alternatives_to_creating_20_different_models_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14xzc3e/alternatives_to_creating_20_different_models_to/", "subreddit_subscribers": 947034, "created_utc": 1689192887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Was up my people any armature/aspiring data scientist want to show each other work and talk about our work and see what we can we improve on, talk analytics, and compare our code?", "author_fullname": "t2_s63nck7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any aspiring DS want to show each other GitHub?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14y46d5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Networking", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689204170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was up my people any armature/aspiring data scientist want to show each other work and talk about our work and see what we can we improve on, talk analytics, and compare our code?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14y46d5", "is_robot_indexable": true, "report_reasons": null, "author": "Papadude08", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14y46d5/any_aspiring_ds_want_to_show_each_other_github/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14y46d5/any_aspiring_ds_want_to_show_each_other_github/", "subreddit_subscribers": 947034, "created_utc": 1689204170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_hccf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vanna: Generate SQL using AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_14xyfav", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nCyaezNTdbXzV4H6TWK_PfbGvl_DvDoGkQHevRALOjU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689190776.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/vanna-ai/vanna-py", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YbSdxPBtgTyPbB6GGCWcl9thzHp-lUDPVdmJx6j7CBc.jpg?auto=webp&amp;s=100f4708f96b90c6da0a2aedd316991f9de1e09e", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/YbSdxPBtgTyPbB6GGCWcl9thzHp-lUDPVdmJx6j7CBc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2d372eac8bc09b5feecc623570ee0e3f256240b0", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/YbSdxPBtgTyPbB6GGCWcl9thzHp-lUDPVdmJx6j7CBc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a8c93deaaa59d7ef4dc4f93e8e59bff028a79f8d", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/YbSdxPBtgTyPbB6GGCWcl9thzHp-lUDPVdmJx6j7CBc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c49ff83eb4de1b0cad42dfe0332d9c3416b1f5c", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/YbSdxPBtgTyPbB6GGCWcl9thzHp-lUDPVdmJx6j7CBc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb08a3cbabdbb538e54e3f7b7a11a87293c1fe53", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/YbSdxPBtgTyPbB6GGCWcl9thzHp-lUDPVdmJx6j7CBc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d8d205fb460116096df1fd9f8f3b2697a5b802c", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/YbSdxPBtgTyPbB6GGCWcl9thzHp-lUDPVdmJx6j7CBc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b0fd54791bf15922a3a1bebd000b5051d7790d03", "width": 1080, "height": 607}], "variants": {}, "id": "YkvUIM_b4CNQSWNtpeCdkahtxdm_7oPRMTgo11_lAIM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xyfav", "is_robot_indexable": true, "report_reasons": null, "author": "gogolang", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xyfav/vanna_generate_sql_using_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/vanna-ai/vanna-py", "subreddit_subscribers": 947034, "created_utc": 1689190776.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is it possible to work as a data scientist in the political field? If yes, what would be an example job?", "author_fullname": "t2_8jxi3akg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "With a political science background into data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14yaqwr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689222906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to work as a data scientist in the political field? If yes, what would be an example job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14yaqwr", "is_robot_indexable": true, "report_reasons": null, "author": "Revenge_is_Coming", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14yaqwr/with_a_political_science_background_into_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14yaqwr/with_a_political_science_background_into_data/", "subreddit_subscribers": 947034, "created_utc": 1689222906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I developed a movie recommendation system using Next.js and FastAPI, but I encountered serverless function size limitations when attempting to host it on Vercel, even after experimenting with smaller models. What can I do at this point?  It would be perfect if there were a free hosting option available. ", "author_fullname": "t2_k9zi1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where can I deploy my hobby project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14yepoc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689235977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I developed a movie recommendation system using Next.js and FastAPI, but I encountered serverless function size limitations when attempting to host it on Vercel, even after experimenting with smaller models. What can I do at this point?  It would be perfect if there were a free hosting option available. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14yepoc", "is_robot_indexable": true, "report_reasons": null, "author": "bahoho", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14yepoc/where_can_i_deploy_my_hobby_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14yepoc/where_can_i_deploy_my_hobby_project/", "subreddit_subscribers": 947034, "created_utc": 1689235977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Whether hiring manager or candidate, which of the following methods do you prefer as means to vet technical experience and potential?\n\nEach of these have a flaw, but I want to get a gauge of what other methods exist for vetting candidates.\n\nRecently, my team have found that a couple of the candidates have been cheating during their technical phone screening using ChatGPT/friend to answer questions. However, when we invited them onsite and asked them the same questions, their response was vastly different and incorrect. What's sad is most of the cheaters that our recruiters vetted were ironically coming from top nearby firms (MANGA), and I cannot believe how they managed to work at those firms if they cannot answer data related questions. This includes a candidate who failed to do a simple aggregate function in SQL (e.g. getting the average number of customers that used the app in a given month).\n\nOur operation is in the SF Bay Area, and we've been receiving around 500 applications per week. Given the competition, the recruiters have been vetting based on years of experience, immediate past company experience and keyword matching. \n\nGiven this scenario, how would you vet candidates effectively given the sheer volume of candidates?\n\n[View Poll](https://www.reddit.com/poll/14xw6ql)", "author_fullname": "t2_48648", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Preferred Technical Interview Method", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xw6ql", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689185635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whether hiring manager or candidate, which of the following methods do you prefer as means to vet technical experience and potential?&lt;/p&gt;\n\n&lt;p&gt;Each of these have a flaw, but I want to get a gauge of what other methods exist for vetting candidates.&lt;/p&gt;\n\n&lt;p&gt;Recently, my team have found that a couple of the candidates have been cheating during their technical phone screening using ChatGPT/friend to answer questions. However, when we invited them onsite and asked them the same questions, their response was vastly different and incorrect. What&amp;#39;s sad is most of the cheaters that our recruiters vetted were ironically coming from top nearby firms (MANGA), and I cannot believe how they managed to work at those firms if they cannot answer data related questions. This includes a candidate who failed to do a simple aggregate function in SQL (e.g. getting the average number of customers that used the app in a given month).&lt;/p&gt;\n\n&lt;p&gt;Our operation is in the SF Bay Area, and we&amp;#39;ve been receiving around 500 applications per week. Given the competition, the recruiters have been vetting based on years of experience, immediate past company experience and keyword matching. &lt;/p&gt;\n\n&lt;p&gt;Given this scenario, how would you vet candidates effectively given the sheer volume of candidates?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/14xw6ql\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "14xw6ql", "is_robot_indexable": true, "report_reasons": null, "author": "forbiscuit", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1689531235365, "options": [{"text": "Timed Take Home Coding Test (e.g. 1-3 hour HackerRank puzzle)", "id": "23862078"}, {"text": "Live Whiteboard/Screensharing Interview", "id": "23862079"}, {"text": "Open Book Take Home Assignments", "id": "23862080"}, {"text": "Technical Phone Screening (Q&amp;A format)", "id": "23862081"}, {"text": "Other", "id": "23862082"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 181, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xw6ql/preferred_technical_interview_method/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/datascience/comments/14xw6ql/preferred_technical_interview_method/", "subreddit_subscribers": 947034, "created_utc": 1689185635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hot-Cold Data Separation: What, Why, and How?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_14xltsk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cA-TRKk2RQNysgWZvGrT-7xphAfBnKQJyJcIU1Q-Mrg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689161147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.devgenius.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://blog.devgenius.io/hot-cold-data-separation-what-why-and-how-5f7c73e7a3cf", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bW78ToRXbYSgiI_QPzu3yJPUUCxP62TNHz5pzYv1Ej4.jpg?auto=webp&amp;s=9bb433a2d2af7659bbd2aaefc86e6440fb099982", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/bW78ToRXbYSgiI_QPzu3yJPUUCxP62TNHz5pzYv1Ej4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4cebf42d866ba39ad8401ec16025d2e683cb5ec5", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/bW78ToRXbYSgiI_QPzu3yJPUUCxP62TNHz5pzYv1Ej4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0db6f5ac30249427128f2acbef98d324f22c2d1b", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/bW78ToRXbYSgiI_QPzu3yJPUUCxP62TNHz5pzYv1Ej4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=34cbad55217597ab7e3e1f50abad3c5bc069e0d7", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/bW78ToRXbYSgiI_QPzu3yJPUUCxP62TNHz5pzYv1Ej4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bc2ca5a2ba66071e1827faae996f0dc6f6e329c", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/bW78ToRXbYSgiI_QPzu3yJPUUCxP62TNHz5pzYv1Ej4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f46f655bcbbc72ddca3942cdffe318e8677b8735", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/bW78ToRXbYSgiI_QPzu3yJPUUCxP62TNHz5pzYv1Ej4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8874a5ffe10ae71db18d29c8e4b5f59d830878fe", "width": 1080, "height": 720}], "variants": {}, "id": "XHYrsxQam1UsYnZQ-ImG7NyQc780qMXf4eXuq9sAgzc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xltsk", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xltsk/hotcold_data_separation_what_why_and_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.devgenius.io/hot-cold-data-separation-what-why-and-how-5f7c73e7a3cf", "subreddit_subscribers": 947034, "created_utc": 1689161147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been out of school for 2 years and have been trying to get a job the entire time in the US. No matter how many resumes I send out, no one is even seeing me for an interview. I didn't do too badly in college (over 3.0 GPA) and am getting really frustrated with no one giving me any attention. I have also been doing coding projects online so my computer programming skills don't dull. Is there any way I can make myself more marketable?", "author_fullname": "t2_omkcn0p6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xyu3i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689191744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been out of school for 2 years and have been trying to get a job the entire time in the US. No matter how many resumes I send out, no one is even seeing me for an interview. I didn&amp;#39;t do too badly in college (over 3.0 GPA) and am getting really frustrated with no one giving me any attention. I have also been doing coding projects online so my computer programming skills don&amp;#39;t dull. Is there any way I can make myself more marketable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xyu3i", "is_robot_indexable": true, "report_reasons": null, "author": "GainAffectionate7196", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xyu3i/job_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14xyu3i/job_help/", "subreddit_subscribers": 947034, "created_utc": 1689191744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I have an important presentation in 24 hours. I really want to create a map visualisation of the data but I'm wondering the easiest/most efficient way to do it quickly?\n\nI've already cleaned the data and tried doing it in Tableau but am having trouble figuring it out quickly.\n\nMy data is the location (city), types of incidents, and number of incidents.\n\nIs this realistic to achieve in a short time frame? I'm intermediate at excel but beginner in Tableau. I don't have access to powerBI. Any other free tool you recommend I'd be happy to use.\n\nThanks!", "author_fullname": "t2_uos6k2i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to create a map visualisation of dataset?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xok3v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689168358.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have an important presentation in 24 hours. I really want to create a map visualisation of the data but I&amp;#39;m wondering the easiest/most efficient way to do it quickly?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve already cleaned the data and tried doing it in Tableau but am having trouble figuring it out quickly.&lt;/p&gt;\n\n&lt;p&gt;My data is the location (city), types of incidents, and number of incidents.&lt;/p&gt;\n\n&lt;p&gt;Is this realistic to achieve in a short time frame? I&amp;#39;m intermediate at excel but beginner in Tableau. I don&amp;#39;t have access to powerBI. Any other free tool you recommend I&amp;#39;d be happy to use.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xok3v", "is_robot_indexable": true, "report_reasons": null, "author": "Greenblueberry349", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xok3v/how_to_create_a_map_visualisation_of_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14xok3v/how_to_create_a_map_visualisation_of_dataset/", "subreddit_subscribers": 947034, "created_utc": 1689168358.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I built a python library for automating data normalisation, schema creation and loading to db. WDYT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14ygl6x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_uamr9xer", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hey folks,\n\nFor the past 2 years I've been working on a library to automate the most tedious part of my own work - data loading, normalisation, typing, schema creation, retries, ddl generation, self deployment, schema evolution... basically, as you build better and better pipelines you will want more and more.\n\nThe value proposition is to automate the tedious work you do, so you can focus on better things.\n\nSo dlt is a library where in the easiest form, you shoot response.json() json at a function and it auto manages the typing normalisation and loading.\n\nIn its most complex form, you can do almost anything you can want, from memory management, multithreading, extraction DAGs, etc.\n\nThe library is in use with early adopters, and we are now working on expanding our feature set to accommodate the larger community.\n\nFeedback is very welcome and so are requests for features or destinations.\n\nThe library is open source and will forever be open source. We will not gate any features for the sake of monetisation - instead we will take a more kafka/confluent approach where the eventual paid offering would be supportive not competing.[https://dlthub.com/](https://dlthub.com/)  \n\n\nI know lots of you are jaded and fed up with toy technologies - this is not a toy tech, it's purpose made for productivity and sanity.\n\n&amp;#x200B;", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python library for automating data normalisation, schema creation and loading to db", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14yfh6p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689238840.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689238647.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey folks,&lt;/p&gt;\n\n&lt;p&gt;For the past 2 years I&amp;#39;ve been working on a library to automate the most tedious part of my own work - data loading, normalisation, typing, schema creation, retries, ddl generation, self deployment, schema evolution... basically, as you build better and better pipelines you will want more and more.&lt;/p&gt;\n\n&lt;p&gt;The value proposition is to automate the tedious work you do, so you can focus on better things.&lt;/p&gt;\n\n&lt;p&gt;So dlt is a library where in the easiest form, you shoot response.json() json at a function and it auto manages the typing normalisation and loading.&lt;/p&gt;\n\n&lt;p&gt;In its most complex form, you can do almost anything you can want, from memory management, multithreading, extraction DAGs, etc.&lt;/p&gt;\n\n&lt;p&gt;The library is in use with early adopters, and we are now working on expanding our feature set to accommodate the larger community.&lt;/p&gt;\n\n&lt;p&gt;Feedback is very welcome and so are requests for features or destinations.&lt;/p&gt;\n\n&lt;p&gt;The library is open source and will forever be open source. We will not gate any features for the sake of monetisation - instead we will take a more kafka/confluent approach where the eventual paid offering would be supportive not competing.&lt;a href=\"https://dlthub.com/\"&gt;https://dlthub.com/&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;I know lots of you are jaded and fed up with toy technologies - this is not a toy tech, it&amp;#39;s purpose made for productivity and sanity.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?auto=webp&amp;s=cb39faa311105e22445462cdf83958c4587fedc2", "width": 1200, "height": 898}, "resolutions": [{"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2950fb9a02e9cdd0b136f3013b72924854fdca3f", "width": 108, "height": 80}, {"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f0af0e48140a561a1f7b733a96875956b6389de", "width": 216, "height": 161}, {"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25c5e30c984451ee6ea10cd4e1067e85319086d3", "width": 320, "height": 239}, {"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=784ffdd500b6cb71463c17b9d348b620c417e6b2", "width": 640, "height": 478}, {"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a966fccda316bcff3aef7e4ae872efc79cabbc1", "width": 960, "height": 718}, {"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ec5a6d47433e6864a169a05e9319caa70258c58d", "width": 1080, "height": 808}], "variants": {}, "id": "cUuEsBHL5TMhrkQNJ9leYxFMOF6VgvTJ_EKHwExrn8Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "14yfh6p", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14yfh6p/python_library_for_automating_data_normalisation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14yfh6p/python_library_for_automating_data_normalisation/", "subreddit_subscribers": 115544, "created_utc": 1689238647.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1689242433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/dataengineering/comments/14yfh6p/python_library_for_automating_data_normalisation/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?auto=webp&amp;s=cb39faa311105e22445462cdf83958c4587fedc2", "width": 1200, "height": 898}, "resolutions": [{"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2950fb9a02e9cdd0b136f3013b72924854fdca3f", "width": 108, "height": 80}, {"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f0af0e48140a561a1f7b733a96875956b6389de", "width": 216, "height": 161}, {"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25c5e30c984451ee6ea10cd4e1067e85319086d3", "width": 320, "height": 239}, {"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=784ffdd500b6cb71463c17b9d348b620c417e6b2", "width": 640, "height": 478}, {"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a966fccda316bcff3aef7e4ae872efc79cabbc1", "width": 960, "height": 718}, {"url": "https://external-preview.redd.it/xklrjiOLps--2g06mfk-O6nj8TPQriAlMdAvL1LpduA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ec5a6d47433e6864a169a05e9319caa70258c58d", "width": 1080, "height": 808}], "variants": {}, "id": "cUuEsBHL5TMhrkQNJ9leYxFMOF6VgvTJ_EKHwExrn8Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14ygl6x", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_14yfh6p", "author_flair_text_color": null, "permalink": "/r/datascience/comments/14ygl6x/i_built_a_python_library_for_automating_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/dataengineering/comments/14yfh6p/python_library_for_automating_data_normalisation/", "subreddit_subscribers": 947034, "created_utc": 1689242433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/xj29w82h6pbb1.jpg?width=2800&amp;format=pjpg&amp;auto=webp&amp;s=507fecaa336906c9bae079634d400e3c239b9f3d\n\nYou can find it interesting. In this guide from OpenCV.ai team (the consulting arm of OpenCV), you will dissect the intricate process of object position prediction in 3D space, discussing the mechanics of rotation, translation, and scale, focusing on metrics for evaluating these predictions.\n\nhttps://preview.redd.it/brv4sjzi6pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=e3689b27eed7666e91f6089fba72ed5bd74119ef\n\nIn the vast world of **3D object pose estimation**, one group of tasks demands a distinct spotlight. This is where we delve into predicting the position of rigid objects in 3D, which comes down to 6DoF (Degrees of Freedom) estimation. In this guide, we will dissect the intricate process of object position prediction in 3D space, discussing the mechanics of rotation, translation, and scale, focusing on metrics for evaluating these predictions.\n\nhttps://preview.redd.it/4knpqfnk6pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=c04b956f02f88bf5079243d96fba509224515bde\n\n**The 6DoF estimation task** encompasses predicting an object's position in 3D space (X, Y, Z coordinates), along with its rotation around these axes, called yaw, pitch, and roll.\n\nThough various approaches exist to predict these, measuring their effectiveness is no simple task. Quality metrics in 2D space tasks have reached a certain level of consensus, but in 3D, things are a bit more complicated.\n\n**Before we start reviewing metrics, let\u2019s look closer at this task.**\u200d\n\n### Task overview\n\n**3D pose estimation** begins with an **RGB** (and sometimes RGBD) image that features the target object. The aim is to predict the object's **6D position**, representing the rigid transformation from the object's coordinate system to the camera's coordinate system.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/d21rvmew6pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=9980005033f3c35c365c0f4213beec6a8cdc5709\n\nA complete 6D pose **consists of two elements** \\- the 3D rotation (3x3 matrix R) of the object and the 3D translation (3x1 vector t). For calculation convenience, they can both be padded to 4x4 matrices.\n\n### Rotation\n\nRotation involves a rotation matrix (R), which essentially breaks down into three 2D rotation matrices. The rotation matrix is defined by the yaw, pitch, and roll angles:\n\n[where \u03b1, \u03b2, and \u03b3 are yaw, pitch, and roll angles, respectively.\u200d](https://preview.redd.it/awmv6jo07pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=638579eacc6d43738dfe7341dba931e67ede32ff)\n\n### Translation\n\nIn the translation matrix, we take into account the distances in the x, y, and z coordinates (\ud835\udc97x, \ud835\udc97y, \ud835\udc97z).\u00a0Translation transformation matrix T in the 3D space is a 4D matrix with the following structure:\n\n&amp;#x200B;\n\n[where \ud835\udc97x, \ud835\udc97y, \ud835\udc97z are the translation distances in x, y, and z.](https://preview.redd.it/8e0nfma77pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=adc8cc8c0f75dad86dcdf8c039d35c28622e9813)\n\nIn other terms, the translation is a vector (t), which, when added to the original position, shifts the entire model in 3D space\u200d\n\n### Metrics\n\nEvaluation of these transformations is usually done via two groups of metrics: those measuring the whole transformation matrix and those measuring R, T, and S matrices separately. We will overview the 2 most common metrics - one from each group.\n\nThe most common overall metric is the average distance (**ADD**) or **ADD(s)** for symmetric objects. Here, the goal is to measure the distance between the Ground Truth (GT) 3D point cloud and the predicted 3D point cloud resulting from the transformation.\n\nAs a first step, predicted and GT 3D point clouds, are calculated from a base model using predicted and GT transformation matrices. Then the distance is measured for each point, and the mean distance is calculated. **The** **mean distance is calculated for each object.**\n\n&amp;#x200B;\n\nhttps://preview.redd.it/3fpkl95d7pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=e57d5ca9ad42e1c4655b7d7afe3ab3eca5d7b134\n\nAs a second step, the threshold of mean distance is picked. Then the percentage of objects with the mean distance below this threshold is calculated. This number is called ADD accuracy. ADD(s) is the same metric for symmetric objects.\n\n\u200d\n\n### R, T, and S\n\nHowever, in certain cases, evaluating rotation, translation, and scale separately can provide deeper insights into the error sources.\u00a0\n\nThe **translation error** is typically measured as the distance between the predicted and GT vectors.\u00a0\n\nThe **scale error** is calculated by dividing the GT scale by the predicted scale.\n\nCalculating the **rotation error**, on the other hand, is more challenging. Rotation matrices belong to the [**3D rotation group**](https://en.wikipedia.org/wiki/3D_rotation_group), often denoted SO(3). Therefore the difference between two rotation matrices, Rgt and Rpred, can be calculated by the metric of distance in SO(3).\u00a0\n\nThere are several approaches to defining a distance function or metric in a 3D rotation group. You can take a closer look at them in this [**paper**](https://www.cs.cmu.edu/~cga/dynopt/readings/Rmetric.pdf) in section 3. Some of them are based on quaternions, and some use the direct comparison of matrices, or for example, deviation from identity matrix. We will overview the most representable and intuitive method here.\n\n**This method calculates the solid angle between Rgt** **and Rpred** **matrices:**\n\n&amp;#x200B;\n\nhttps://preview.redd.it/acuy0gbg7pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=266805768e39067e3404c4e0a83f5885ea2a6e76\n\nFrom this equation angle of rotation can be easily calculated:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/anxijvoj7pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=b950ddc33228358f51607fc7388a7e87bc8d4c0a\n\nAs a result, we get a solid angle, which would represent the overall error angle in 3D space. The advantage of this metric is that it gives a spatial visual representation of the error.\n\n\u200d\n\n### Conclusion\n\nEvaluation is one of the critical processes in Deep Learning, and the right choice of evaluation metrics is crucial. Only with reliable and interpretable metrics can we not only make the right decisions but also explain them to our colleagues or customers.\n\n\u200d", "author_fullname": "t2_ny4qnnnz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Object position in 3D space - 6Dof (Degrees of Freedom) metrics overview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 99, "top_awarded_type": null, "hide_score": false, "media_metadata": {"3fpkl95d7pbb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/3fpkl95d7pbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2975ff8cdd7a969904213efafc8535e069ff54ab"}, {"y": 71, "x": 216, "u": "https://preview.redd.it/3fpkl95d7pbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e24bb0030ed0a21ae65da7e1211775629f3baf53"}, {"y": 106, "x": 320, "u": "https://preview.redd.it/3fpkl95d7pbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ef45daef7df2f9404043717bd58f3fe76f5002e9"}, {"y": 212, "x": 640, "u": "https://preview.redd.it/3fpkl95d7pbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c8670ba7d86f23f5e0624f931000d19cccc267b6"}, {"y": 319, "x": 960, "u": "https://preview.redd.it/3fpkl95d7pbb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7fcda22a03f4de125b8fd017038c352c5e05f9ee"}, {"y": 359, "x": 1080, "u": "https://preview.redd.it/3fpkl95d7pbb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=45c297c37bcde0b989d51382e9fe4ab48284d8bf"}], "s": {"y": 832, "x": 2500, "u": "https://preview.redd.it/3fpkl95d7pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=e57d5ca9ad42e1c4655b7d7afe3ab3eca5d7b134"}, "id": "3fpkl95d7pbb1"}, "xj29w82h6pbb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/xj29w82h6pbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=92a2a2f420b0682fd6a8ac404172c4266dc6b0a2"}, {"y": 153, "x": 216, "u": "https://preview.redd.it/xj29w82h6pbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fc120d73920484c132016d7012d01416f5e61114"}, {"y": 226, "x": 320, "u": "https://preview.redd.it/xj29w82h6pbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f0c19639c133de21d83b931099636489e4fa87a"}, {"y": 453, "x": 640, "u": "https://preview.redd.it/xj29w82h6pbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5ed82d0a3e9cffc241bed32ea92491f60f14ac79"}, {"y": 680, "x": 960, "u": "https://preview.redd.it/xj29w82h6pbb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=955c78932451d8dc7fdf29d08e20d27e04870446"}, {"y": 765, "x": 1080, "u": "https://preview.redd.it/xj29w82h6pbb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bbbe928f6256e29c4a8583fee43ef3aaed6563e1"}], "s": {"y": 1984, "x": 2800, "u": "https://preview.redd.it/xj29w82h6pbb1.jpg?width=2800&amp;format=pjpg&amp;auto=webp&amp;s=507fecaa336906c9bae079634d400e3c239b9f3d"}, "id": "xj29w82h6pbb1"}, "8e0nfma77pbb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/8e0nfma77pbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a3473d5605011c9db494e544cf41911a33467c8c"}, {"y": 71, "x": 216, "u": "https://preview.redd.it/8e0nfma77pbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=88ebba2268b12e25c218158b37eddda1cb1572fc"}, {"y": 106, "x": 320, "u": "https://preview.redd.it/8e0nfma77pbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=94f90148adb6d7032e223ed0d8afdcc0b413e661"}, {"y": 212, "x": 640, "u": "https://preview.redd.it/8e0nfma77pbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bdd7e47757911c9fe9aa178aa54995472a13ad5b"}, {"y": 319, "x": 960, "u": "https://preview.redd.it/8e0nfma77pbb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=361e12d05986bf3327873807051d05dc66898513"}, {"y": 359, "x": 1080, "u": "https://preview.redd.it/8e0nfma77pbb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1081b097f20d393e67b5bdfca2164c3e95b068ab"}], "s": {"y": 832, "x": 2500, "u": "https://preview.redd.it/8e0nfma77pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=adc8cc8c0f75dad86dcdf8c039d35c28622e9813"}, "id": "8e0nfma77pbb1"}, "anxijvoj7pbb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/anxijvoj7pbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e194848c6accdbec4ae281db4f2becd1ac902398"}, {"y": 71, "x": 216, "u": "https://preview.redd.it/anxijvoj7pbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=982e6dee78abeca73dccf88730a0ca6ba176da13"}, {"y": 106, "x": 320, "u": "https://preview.redd.it/anxijvoj7pbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c928d8a6beecfe7e1baf17b9a8681e770265201"}, {"y": 212, "x": 640, "u": "https://preview.redd.it/anxijvoj7pbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ef1dabf02a7d1c2c59e9280fc8e46e05891e294b"}, {"y": 319, "x": 960, "u": "https://preview.redd.it/anxijvoj7pbb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=98b1b9e5dc3694d00648f4d5ecf11acefc28be2f"}, {"y": 359, "x": 1080, "u": "https://preview.redd.it/anxijvoj7pbb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=482b94e7ba7602bdad0766b9e1f061530f62c826"}], "s": {"y": 832, "x": 2500, "u": "https://preview.redd.it/anxijvoj7pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=b950ddc33228358f51607fc7388a7e87bc8d4c0a"}, "id": "anxijvoj7pbb1"}, "awmv6jo07pbb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/awmv6jo07pbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f037ee8bb258590b5b9a440b9dae4fcb62c7cb7"}, {"y": 71, "x": 216, "u": "https://preview.redd.it/awmv6jo07pbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c3bb4219f4d47b34f964287f6342ee888c65b8c"}, {"y": 106, "x": 320, "u": "https://preview.redd.it/awmv6jo07pbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ccd91a387d3091b5c8ac54b2dab5cf5c501912f6"}, {"y": 212, "x": 640, "u": "https://preview.redd.it/awmv6jo07pbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=035909c378d694c572f2781ebfcd84602cbbdb83"}, {"y": 319, "x": 960, "u": "https://preview.redd.it/awmv6jo07pbb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8d83753df5b0670fcaa18d097c058cbbe0d01b0e"}, {"y": 359, "x": 1080, "u": "https://preview.redd.it/awmv6jo07pbb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=87646af791d8555ce30dd2b31c38874885135404"}], "s": {"y": 832, "x": 2500, "u": "https://preview.redd.it/awmv6jo07pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=638579eacc6d43738dfe7341dba931e67ede32ff"}, "id": "awmv6jo07pbb1"}, "4knpqfnk6pbb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 50, "x": 108, "u": "https://preview.redd.it/4knpqfnk6pbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c5da963bc09aa6f028a411587ce1fb83563cce9f"}, {"y": 100, "x": 216, "u": "https://preview.redd.it/4knpqfnk6pbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9bfd697f800c49df8fbdec9f0cd4d2f19b3fb4b7"}, {"y": 148, "x": 320, "u": "https://preview.redd.it/4knpqfnk6pbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6b618251da3902ae06042686743a97e66b4d0c5"}, {"y": 297, "x": 640, "u": "https://preview.redd.it/4knpqfnk6pbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb0499a14a7390395943cf4fa084eaba6c0e4df6"}, {"y": 446, "x": 960, "u": "https://preview.redd.it/4knpqfnk6pbb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=781bf655e29023c10c1f2f20088b6e70407ac3f1"}, {"y": 502, "x": 1080, "u": "https://preview.redd.it/4knpqfnk6pbb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=55bb2543503e3ec88dcd95b8272efa3647ad3077"}], "s": {"y": 1164, "x": 2500, "u": "https://preview.redd.it/4knpqfnk6pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=c04b956f02f88bf5079243d96fba509224515bde"}, "id": "4knpqfnk6pbb1"}, "brv4sjzi6pbb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/brv4sjzi6pbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=87c498ec51d96efd8ef439946d0f6d490e439478"}, {"y": 71, "x": 216, "u": "https://preview.redd.it/brv4sjzi6pbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=94e93779b035423bb51cd13badfdc404d69ae1ab"}, {"y": 106, "x": 320, "u": "https://preview.redd.it/brv4sjzi6pbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ce66b3c7e6e98c3c6187d286aea58bd2cd810bc"}, {"y": 212, "x": 640, "u": "https://preview.redd.it/brv4sjzi6pbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3fbf89a723e5b5c9eb055cbae93a3ec76e21922f"}, {"y": 319, "x": 960, "u": "https://preview.redd.it/brv4sjzi6pbb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f89bd037d7f7e60a4ef8b77c3f5f0bdfdea3e61d"}, {"y": 359, "x": 1080, "u": "https://preview.redd.it/brv4sjzi6pbb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ed0151b3f794d3cbcd86bfd3194a2b51467ce4e2"}], "s": {"y": 832, "x": 2500, "u": "https://preview.redd.it/brv4sjzi6pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=e3689b27eed7666e91f6089fba72ed5bd74119ef"}, "id": "brv4sjzi6pbb1"}, "d21rvmew6pbb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/d21rvmew6pbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04b045040bdbf08ef5b01f6acb6543a5a8748eea"}, {"y": 71, "x": 216, "u": "https://preview.redd.it/d21rvmew6pbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=75f1b357ad4636633a2801fae83408d50ffd6650"}, {"y": 106, "x": 320, "u": "https://preview.redd.it/d21rvmew6pbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=43bdbf6a8821b58e68f93ba3a33981d256854c99"}, {"y": 212, "x": 640, "u": "https://preview.redd.it/d21rvmew6pbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0cbf50b4a71de06ad31dfde3f95ff13cf4350194"}, {"y": 319, "x": 960, "u": "https://preview.redd.it/d21rvmew6pbb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7941b5759ca690aeb1548e8c51545f308068a81a"}, {"y": 359, "x": 1080, "u": "https://preview.redd.it/d21rvmew6pbb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=14a9ef31da217e53595012edbee45834373ea73e"}], "s": {"y": 832, "x": 2500, "u": "https://preview.redd.it/d21rvmew6pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=9980005033f3c35c365c0f4213beec6a8cdc5709"}, "id": "d21rvmew6pbb1"}, "acuy0gbg7pbb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/acuy0gbg7pbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5173dd4e494dbb740c42033955294e12ee45b58"}, {"y": 71, "x": 216, "u": "https://preview.redd.it/acuy0gbg7pbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=797618623e7e11df8731ecd333bb075f6e78cf93"}, {"y": 106, "x": 320, "u": "https://preview.redd.it/acuy0gbg7pbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e696c16aa9837bd925967ad41e191f0aa63630a"}, {"y": 212, "x": 640, "u": "https://preview.redd.it/acuy0gbg7pbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e356cc9dd3a899044401c97f638fac0ee3950582"}, {"y": 319, "x": 960, "u": "https://preview.redd.it/acuy0gbg7pbb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a7d6126c2053fa51df764fd52f89b629237004d2"}, {"y": 359, "x": 1080, "u": "https://preview.redd.it/acuy0gbg7pbb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=654cbc97dfb710d874052ec901d352685d347d01"}], "s": {"y": 832, "x": 2500, "u": "https://preview.redd.it/acuy0gbg7pbb1.jpg?width=2500&amp;format=pjpg&amp;auto=webp&amp;s=266805768e39067e3404c4e0a83f5885ea2a6e76"}, "id": "acuy0gbg7pbb1"}}, "name": "t3_14yfp3c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/TXhogetkFrFmjPRmj0Cx_eG50fAboJEiJN83nnDH9r0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689239388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/xj29w82h6pbb1.jpg?width=2800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=507fecaa336906c9bae079634d400e3c239b9f3d\"&gt;https://preview.redd.it/xj29w82h6pbb1.jpg?width=2800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=507fecaa336906c9bae079634d400e3c239b9f3d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You can find it interesting. In this guide from OpenCV.ai team (the consulting arm of OpenCV), you will dissect the intricate process of object position prediction in 3D space, discussing the mechanics of rotation, translation, and scale, focusing on metrics for evaluating these predictions.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/brv4sjzi6pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e3689b27eed7666e91f6089fba72ed5bd74119ef\"&gt;https://preview.redd.it/brv4sjzi6pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e3689b27eed7666e91f6089fba72ed5bd74119ef&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In the vast world of &lt;strong&gt;3D object pose estimation&lt;/strong&gt;, one group of tasks demands a distinct spotlight. This is where we delve into predicting the position of rigid objects in 3D, which comes down to 6DoF (Degrees of Freedom) estimation. In this guide, we will dissect the intricate process of object position prediction in 3D space, discussing the mechanics of rotation, translation, and scale, focusing on metrics for evaluating these predictions.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4knpqfnk6pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c04b956f02f88bf5079243d96fba509224515bde\"&gt;https://preview.redd.it/4knpqfnk6pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c04b956f02f88bf5079243d96fba509224515bde&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The 6DoF estimation task&lt;/strong&gt; encompasses predicting an object&amp;#39;s position in 3D space (X, Y, Z coordinates), along with its rotation around these axes, called yaw, pitch, and roll.&lt;/p&gt;\n\n&lt;p&gt;Though various approaches exist to predict these, measuring their effectiveness is no simple task. Quality metrics in 2D space tasks have reached a certain level of consensus, but in 3D, things are a bit more complicated.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Before we start reviewing metrics, let\u2019s look closer at this task.&lt;/strong&gt;\u200d&lt;/p&gt;\n\n&lt;h3&gt;Task overview&lt;/h3&gt;\n\n&lt;p&gt;&lt;strong&gt;3D pose estimation&lt;/strong&gt; begins with an &lt;strong&gt;RGB&lt;/strong&gt; (and sometimes RGBD) image that features the target object. The aim is to predict the object&amp;#39;s &lt;strong&gt;6D position&lt;/strong&gt;, representing the rigid transformation from the object&amp;#39;s coordinate system to the camera&amp;#39;s coordinate system.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/d21rvmew6pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9980005033f3c35c365c0f4213beec6a8cdc5709\"&gt;https://preview.redd.it/d21rvmew6pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9980005033f3c35c365c0f4213beec6a8cdc5709&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A complete 6D pose &lt;strong&gt;consists of two elements&lt;/strong&gt; - the 3D rotation (3x3 matrix R) of the object and the 3D translation (3x1 vector t). For calculation convenience, they can both be padded to 4x4 matrices.&lt;/p&gt;\n\n&lt;h3&gt;Rotation&lt;/h3&gt;\n\n&lt;p&gt;Rotation involves a rotation matrix (R), which essentially breaks down into three 2D rotation matrices. The rotation matrix is defined by the yaw, pitch, and roll angles:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/awmv6jo07pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=638579eacc6d43738dfe7341dba931e67ede32ff\"&gt;where \u03b1, \u03b2, and \u03b3 are yaw, pitch, and roll angles, respectively.\u200d&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;Translation&lt;/h3&gt;\n\n&lt;p&gt;In the translation matrix, we take into account the distances in the x, y, and z coordinates (\ud835\udc97x, \ud835\udc97y, \ud835\udc97z).\u00a0Translation transformation matrix T in the 3D space is a 4D matrix with the following structure:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8e0nfma77pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=adc8cc8c0f75dad86dcdf8c039d35c28622e9813\"&gt;where \ud835\udc97x, \ud835\udc97y, \ud835\udc97z are the translation distances in x, y, and z.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In other terms, the translation is a vector (t), which, when added to the original position, shifts the entire model in 3D space\u200d&lt;/p&gt;\n\n&lt;h3&gt;Metrics&lt;/h3&gt;\n\n&lt;p&gt;Evaluation of these transformations is usually done via two groups of metrics: those measuring the whole transformation matrix and those measuring R, T, and S matrices separately. We will overview the 2 most common metrics - one from each group.&lt;/p&gt;\n\n&lt;p&gt;The most common overall metric is the average distance (&lt;strong&gt;ADD&lt;/strong&gt;) or &lt;strong&gt;ADD(s)&lt;/strong&gt; for symmetric objects. Here, the goal is to measure the distance between the Ground Truth (GT) 3D point cloud and the predicted 3D point cloud resulting from the transformation.&lt;/p&gt;\n\n&lt;p&gt;As a first step, predicted and GT 3D point clouds, are calculated from a base model using predicted and GT transformation matrices. Then the distance is measured for each point, and the mean distance is calculated. &lt;strong&gt;The&lt;/strong&gt; &lt;strong&gt;mean distance is calculated for each object.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3fpkl95d7pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e57d5ca9ad42e1c4655b7d7afe3ab3eca5d7b134\"&gt;https://preview.redd.it/3fpkl95d7pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e57d5ca9ad42e1c4655b7d7afe3ab3eca5d7b134&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As a second step, the threshold of mean distance is picked. Then the percentage of objects with the mean distance below this threshold is calculated. This number is called ADD accuracy. ADD(s) is the same metric for symmetric objects.&lt;/p&gt;\n\n&lt;p&gt;\u200d&lt;/p&gt;\n\n&lt;h3&gt;R, T, and S&lt;/h3&gt;\n\n&lt;p&gt;However, in certain cases, evaluating rotation, translation, and scale separately can provide deeper insights into the error sources.\u00a0&lt;/p&gt;\n\n&lt;p&gt;The &lt;strong&gt;translation error&lt;/strong&gt; is typically measured as the distance between the predicted and GT vectors.\u00a0&lt;/p&gt;\n\n&lt;p&gt;The &lt;strong&gt;scale error&lt;/strong&gt; is calculated by dividing the GT scale by the predicted scale.&lt;/p&gt;\n\n&lt;p&gt;Calculating the &lt;strong&gt;rotation error&lt;/strong&gt;, on the other hand, is more challenging. Rotation matrices belong to the &lt;a href=\"https://en.wikipedia.org/wiki/3D_rotation_group\"&gt;&lt;strong&gt;3D rotation group&lt;/strong&gt;&lt;/a&gt;, often denoted SO(3). Therefore the difference between two rotation matrices, Rgt and Rpred, can be calculated by the metric of distance in SO(3).\u00a0&lt;/p&gt;\n\n&lt;p&gt;There are several approaches to defining a distance function or metric in a 3D rotation group. You can take a closer look at them in this &lt;a href=\"https://www.cs.cmu.edu/%7Ecga/dynopt/readings/Rmetric.pdf\"&gt;&lt;strong&gt;paper&lt;/strong&gt;&lt;/a&gt; in section 3. Some of them are based on quaternions, and some use the direct comparison of matrices, or for example, deviation from identity matrix. We will overview the most representable and intuitive method here.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;This method calculates the solid angle between Rgt&lt;/strong&gt; &lt;strong&gt;and Rpred&lt;/strong&gt; &lt;strong&gt;matrices:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/acuy0gbg7pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=266805768e39067e3404c4e0a83f5885ea2a6e76\"&gt;https://preview.redd.it/acuy0gbg7pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=266805768e39067e3404c4e0a83f5885ea2a6e76&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;From this equation angle of rotation can be easily calculated:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/anxijvoj7pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b950ddc33228358f51607fc7388a7e87bc8d4c0a\"&gt;https://preview.redd.it/anxijvoj7pbb1.jpg?width=2500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b950ddc33228358f51607fc7388a7e87bc8d4c0a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As a result, we get a solid angle, which would represent the overall error angle in 3D space. The advantage of this metric is that it gives a spatial visual representation of the error.&lt;/p&gt;\n\n&lt;p&gt;\u200d&lt;/p&gt;\n\n&lt;h3&gt;Conclusion&lt;/h3&gt;\n\n&lt;p&gt;Evaluation is one of the critical processes in Deep Learning, and the right choice of evaluation metrics is crucial. Only with reliable and interpretable metrics can we not only make the right decisions but also explain them to our colleagues or customers.&lt;/p&gt;\n\n&lt;p&gt;\u200d&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14yfp3c", "is_robot_indexable": true, "report_reasons": null, "author": "No-Independence5880", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14yfp3c/object_position_in_3d_space_6dof_degrees_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14yfp3c/object_position_in_3d_space_6dof_degrees_of/", "subreddit_subscribers": 947034, "created_utc": 1689239388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/14yfb81)", "author_fullname": "t2_2kh4l8ej", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data scientists, do you still train models locally for work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14yfb81", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689238061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/14yfb81\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "14yfb81", "is_robot_indexable": true, "report_reasons": null, "author": "supper_ham", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1689497261993, "options": [{"text": "Local machine", "id": "23870804"}, {"text": "Cloud", "id": "23870805"}, {"text": "Company servers", "id": "23870806"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 40, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14yfb81/data_scientists_do_you_still_train_models_locally/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/datascience/comments/14yfb81/data_scientists_do_you_still_train_models_locally/", "subreddit_subscribers": 947034, "created_utc": 1689238061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How to stand out in a data science home assignment?\n\nI received a task where I need to create a churn prediction model. It's for a position that involves both data science and machine learning engineering, although the task seems to have a stronger focus on modeling.\n\nThey mentioned that I should present my modeling strategy but didn't provide many details on how I should structure my code.\n\nThey asked me to explain my modeling strategy, how the model works, and how to interpret the model's output.\n\nI thought of the following strategy:\n- Perform exploratory analysis, including correlation analysis, identifying potential feature engineering opportunities, checking for missing data and noise. In the end, I can summarize the results and insights from the analysis.\n- Next, create two baselines: a heuristic baseline that relies on a user behavior feature using a simple \"if\" condition, and another baseline using a decision tree. \n- Perform feature engineering and use XGBoost. I chose the decision tree as one of the baselines to explain the workings of both models in a simpler way, as I'm using XGBoost in this step.\n- Optimize parameters using Optuna.\n- Compare the baselines with XGBoost and evaluate the metrics. I will try to explain whether it's better to focus on recall or precision and discuss the pros and cons of each.\n- Interpret the model. This is where I'm a bit uncertain. I plan to use SHAP values or feature importance to gain insights. I welcome any suggestions.\n- Refactor the code. In this part, I intend to organize everything into a well-structured Python project, using a pipeline with DVC, and if possible, include unit tests. I really want to have clean code since my background is more in software engineering than data science.\n- Finally, I will prepare the presentation. I will try to explain my decisions, assumptions, the results of the analysis, and provide an explanation of the model, among other things.\n\nThey gave me one week to complete the assignment, and it's been tiring.\n\nWhat would you do differently?", "author_fullname": "t2_lax5ak3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to stand out in data science home assignments?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14y9x43", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689220397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How to stand out in a data science home assignment?&lt;/p&gt;\n\n&lt;p&gt;I received a task where I need to create a churn prediction model. It&amp;#39;s for a position that involves both data science and machine learning engineering, although the task seems to have a stronger focus on modeling.&lt;/p&gt;\n\n&lt;p&gt;They mentioned that I should present my modeling strategy but didn&amp;#39;t provide many details on how I should structure my code.&lt;/p&gt;\n\n&lt;p&gt;They asked me to explain my modeling strategy, how the model works, and how to interpret the model&amp;#39;s output.&lt;/p&gt;\n\n&lt;p&gt;I thought of the following strategy:\n- Perform exploratory analysis, including correlation analysis, identifying potential feature engineering opportunities, checking for missing data and noise. In the end, I can summarize the results and insights from the analysis.\n- Next, create two baselines: a heuristic baseline that relies on a user behavior feature using a simple &amp;quot;if&amp;quot; condition, and another baseline using a decision tree. \n- Perform feature engineering and use XGBoost. I chose the decision tree as one of the baselines to explain the workings of both models in a simpler way, as I&amp;#39;m using XGBoost in this step.\n- Optimize parameters using Optuna.\n- Compare the baselines with XGBoost and evaluate the metrics. I will try to explain whether it&amp;#39;s better to focus on recall or precision and discuss the pros and cons of each.\n- Interpret the model. This is where I&amp;#39;m a bit uncertain. I plan to use SHAP values or feature importance to gain insights. I welcome any suggestions.\n- Refactor the code. In this part, I intend to organize everything into a well-structured Python project, using a pipeline with DVC, and if possible, include unit tests. I really want to have clean code since my background is more in software engineering than data science.\n- Finally, I will prepare the presentation. I will try to explain my decisions, assumptions, the results of the analysis, and provide an explanation of the model, among other things.&lt;/p&gt;\n\n&lt;p&gt;They gave me one week to complete the assignment, and it&amp;#39;s been tiring.&lt;/p&gt;\n\n&lt;p&gt;What would you do differently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14y9x43", "is_robot_indexable": true, "report_reasons": null, "author": "Waste_Necessary654", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14y9x43/how_to_stand_out_in_data_science_home_assignments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14y9x43/how_to_stand_out_in_data_science_home_assignments/", "subreddit_subscribers": 947034, "created_utc": 1689220397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is there an aggregated data source I can use to access social media content where I can filter and sort based on the number of likes or comments that indicate popularity/engagement?", "author_fullname": "t2_17d42esw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Aggregated data source, API, or Python libraries for social media content", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14y0yrv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689196619.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there an aggregated data source I can use to access social media content where I can filter and sort based on the number of likes or comments that indicate popularity/engagement?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14y0yrv", "is_robot_indexable": true, "report_reasons": null, "author": "Guyserbun007", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14y0yrv/aggregated_data_source_api_or_python_libraries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14y0yrv/aggregated_data_source_api_or_python_libraries/", "subreddit_subscribers": 947034, "created_utc": 1689196619.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all,\n\nI'm working on my MS in Data Science right now and we had a lecture on clustering yesterday. I really enjoyed it and saw the applications of it, so I wanted to do a personal project to throw on my resume. \n\nLet's saw I do a K-Means cluster. I remove all of the categorical data, normalize everything remaining, get my clusters and add the corresponding cluster to each row of data.\n\nI know that analysts/scientists will do analysis on those clusters specifically, but what if I were to re-add the categorical data afterwards? For example, if I removed the \"gender\" column to do my K-Means, would it be problematic if I added it back in after? \n\nMy idea is to use that data and make a dashboard that allows the user to filter by cluster. Evidently, the dashboard needs categorical data to gain valuable insight from the clusters. \n\nI'm not sure if that is problematic or not? Am I going to somehow create a false narrative if I add in the removed categorical variables after the K-Means has been run? ", "author_fullname": "t2_1x7s010", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Re-Adding Categorical Data After K-Means Clustering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xy6p4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689190218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on my MS in Data Science right now and we had a lecture on clustering yesterday. I really enjoyed it and saw the applications of it, so I wanted to do a personal project to throw on my resume. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s saw I do a K-Means cluster. I remove all of the categorical data, normalize everything remaining, get my clusters and add the corresponding cluster to each row of data.&lt;/p&gt;\n\n&lt;p&gt;I know that analysts/scientists will do analysis on those clusters specifically, but what if I were to re-add the categorical data afterwards? For example, if I removed the &amp;quot;gender&amp;quot; column to do my K-Means, would it be problematic if I added it back in after? &lt;/p&gt;\n\n&lt;p&gt;My idea is to use that data and make a dashboard that allows the user to filter by cluster. Evidently, the dashboard needs categorical data to gain valuable insight from the clusters. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if that is problematic or not? Am I going to somehow create a false narrative if I add in the removed categorical variables after the K-Means has been run? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xy6p4", "is_robot_indexable": true, "report_reasons": null, "author": "HercHuntsdirty", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xy6p4/readding_categorical_data_after_kmeans_clustering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14xy6p4/readding_categorical_data_after_kmeans_clustering/", "subreddit_subscribers": 947034, "created_utc": 1689190218.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am moving from a Data Scientist role into a Tech Lead role. Even though I worked de facto as a tech lead in the organization, it was my first promotion into a senior role. \n\nMy manager (who I highly appreciate) also told me that the change would come with changing my salary. We talked about it in general. How much (%) raise would be fair, given that I earn slightly more than the average salary in my area?\n\nThanks", "author_fullname": "t2_4udseb4x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a distinction between \"tech lead\" and \"team leader\" roles in your company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xy32c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689189980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am moving from a Data Scientist role into a Tech Lead role. Even though I worked de facto as a tech lead in the organization, it was my first promotion into a senior role. &lt;/p&gt;\n\n&lt;p&gt;My manager (who I highly appreciate) also told me that the change would come with changing my salary. We talked about it in general. How much (%) raise would be fair, given that I earn slightly more than the average salary in my area?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xy32c", "is_robot_indexable": true, "report_reasons": null, "author": "David202023", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xy32c/is_there_a_distinction_between_tech_lead_and_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14xy32c/is_there_a_distinction_between_tech_lead_and_team/", "subreddit_subscribers": 947034, "created_utc": 1689189980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nData scientist/python developer/cloud engineer here. I am working on the math side of an app, and was wondering if someone has ever worked on linear relationships within scattered 2-d data.\n\nI have attached an example image in the link below, but I wanted to basically group together data points that are &lt;x Euclidian distance apart, and segment to segment is less than a certain degree of angle. I am familiar with clustering (looked into DBSCAN, hierarchical, etc) but because the data is fairly distributed, I am having a hard time getting distinct clusters of any shape out of it. \n\nAnyone with experience with doing something like this that can point me towards some resources, my internet searches aren't yielding much.\n\nOpen to an example in pretty much any language.\n\nhttps://imgur.com/a/2G1M07E", "author_fullname": "t2_bq6l0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Like clustering, but linear paths through scattered data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xxy1q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689189660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Data scientist/python developer/cloud engineer here. I am working on the math side of an app, and was wondering if someone has ever worked on linear relationships within scattered 2-d data.&lt;/p&gt;\n\n&lt;p&gt;I have attached an example image in the link below, but I wanted to basically group together data points that are &amp;lt;x Euclidian distance apart, and segment to segment is less than a certain degree of angle. I am familiar with clustering (looked into DBSCAN, hierarchical, etc) but because the data is fairly distributed, I am having a hard time getting distinct clusters of any shape out of it. &lt;/p&gt;\n\n&lt;p&gt;Anyone with experience with doing something like this that can point me towards some resources, my internet searches aren&amp;#39;t yielding much.&lt;/p&gt;\n\n&lt;p&gt;Open to an example in pretty much any language.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/2G1M07E\"&gt;https://imgur.com/a/2G1M07E&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HjU-SkGylr94Yh5Z14KSxVUM3Qyw04ZuxZgPXu_Kd_s.jpg?auto=webp&amp;s=780675fb3b3cc5b78d12d64fe258eeebddf5e049", "width": 480, "height": 372}, "resolutions": [{"url": "https://external-preview.redd.it/HjU-SkGylr94Yh5Z14KSxVUM3Qyw04ZuxZgPXu_Kd_s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f993ef2532388dd34252ec84e50adeee997d467", "width": 108, "height": 83}, {"url": "https://external-preview.redd.it/HjU-SkGylr94Yh5Z14KSxVUM3Qyw04ZuxZgPXu_Kd_s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=775278490dd1acb782bbb20301c48dc8dd3ca058", "width": 216, "height": 167}, {"url": "https://external-preview.redd.it/HjU-SkGylr94Yh5Z14KSxVUM3Qyw04ZuxZgPXu_Kd_s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c4a5c1f2759feb8dc4192213d3eb3fa0f84d6fe", "width": 320, "height": 248}], "variants": {}, "id": "vfNU3TdrEW-RtJvOMqDyi0PC8c6LE3vGwpUJkGhcUKo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xxy1q", "is_robot_indexable": true, "report_reasons": null, "author": "laXfever34", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xxy1q/like_clustering_but_linear_paths_through/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14xxy1q/like_clustering_but_linear_paths_through/", "subreddit_subscribers": 947034, "created_utc": 1689189660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm interested in learning a bit more about formal data science methods. Curious about the proper way to approach forecasting (let's say, incoming work orders).\n\nI built a super simple model to just brainstorm a starting point. I basically used the model to produce \"data\" and am trying to use that data to reverse engineer the model.\n\nWeeks 1-12 are 1000 work orders a week, weeks 17-36 are 2000, and weeks 41-52 are 1000 (ignoring potential week 53 for simplicity). From 12-17 orders ramp up at 200/week and 36-41 ramp down by 200/week. That's the seasonality I built.\n\nI then built 130 weeks (2.5 years) of \"data\" by taking the first week = 1000, and every week I looked up the seasonal value, compounded by 1.0009 per week from the starting point (5% annual growth). That's my data set I'm trying to reverse engineer the model from.\n\nThe new data scientist who started with my company is teaching me python and we built a model together, that basically took a linear trend, and then calculated seasonality based on the deviation from the linear trend.\n\nThe problem is, the trend in the beginning model doesn't change, it's a 1.05\\^(1/52) weekly compound growth. However, the trend in the output model changes dramatically based on how many weeks are in the dataset. If it's a multiple of 52, it's fine, but at 64 weeks for instance it underestimates the trend (extra data at low seasonality) and at 88 weeks it overestimates the trend (extra data at high seasonal points).\n\nI'm not even sure a linear trend is appropriate given the expected compounding growth.\n\nWhat's the standard approach for building a simple forecast model based on this type of seasonality and compounding growth?", "author_fullname": "t2_o8c07y7p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Basic Forecasting - trend vs seasonality", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xxs56", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689189281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m interested in learning a bit more about formal data science methods. Curious about the proper way to approach forecasting (let&amp;#39;s say, incoming work orders).&lt;/p&gt;\n\n&lt;p&gt;I built a super simple model to just brainstorm a starting point. I basically used the model to produce &amp;quot;data&amp;quot; and am trying to use that data to reverse engineer the model.&lt;/p&gt;\n\n&lt;p&gt;Weeks 1-12 are 1000 work orders a week, weeks 17-36 are 2000, and weeks 41-52 are 1000 (ignoring potential week 53 for simplicity). From 12-17 orders ramp up at 200/week and 36-41 ramp down by 200/week. That&amp;#39;s the seasonality I built.&lt;/p&gt;\n\n&lt;p&gt;I then built 130 weeks (2.5 years) of &amp;quot;data&amp;quot; by taking the first week = 1000, and every week I looked up the seasonal value, compounded by 1.0009 per week from the starting point (5% annual growth). That&amp;#39;s my data set I&amp;#39;m trying to reverse engineer the model from.&lt;/p&gt;\n\n&lt;p&gt;The new data scientist who started with my company is teaching me python and we built a model together, that basically took a linear trend, and then calculated seasonality based on the deviation from the linear trend.&lt;/p&gt;\n\n&lt;p&gt;The problem is, the trend in the beginning model doesn&amp;#39;t change, it&amp;#39;s a 1.05^(1/52) weekly compound growth. However, the trend in the output model changes dramatically based on how many weeks are in the dataset. If it&amp;#39;s a multiple of 52, it&amp;#39;s fine, but at 64 weeks for instance it underestimates the trend (extra data at low seasonality) and at 88 weeks it overestimates the trend (extra data at high seasonal points).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not even sure a linear trend is appropriate given the expected compounding growth.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the standard approach for building a simple forecast model based on this type of seasonality and compounding growth?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xxs56", "is_robot_indexable": true, "report_reasons": null, "author": "Mission-Wall-3311", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xxs56/basic_forecasting_trend_vs_seasonality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14xxs56/basic_forecasting_trend_vs_seasonality/", "subreddit_subscribers": 947034, "created_utc": 1689189281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As an American FinTech company, we're looking for an Investigations Analyst to join us in making the crypto space safer.\n\nWe use the challenge program to evaluate applicants, and if you find the task intriguing, we're likely to be a good fit.\n\nTo participate, submit a pull request to the [challenge repo](https://go.inca.digital/investigation-challenge?11). To make sure your submission doesn't get lost, you can also email your PR link along with your resume and the link to this challenge to [challenge-submission@blockshop.org](mailto:challenge-submission@blockshop.org).\n\nYou can also find more about the opportunity [here](https://inca.digital/careers/#roles--data-analyst-investigations-and-research).\n\nMore about [Inca Digital](https://inca.digital/).", "author_fullname": "t2_udca2loi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Challenge announcement: generate a comprehensive report that assesses the degree of decentralization in the crypto asset project.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xx6ww", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689200658.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689187959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As an American FinTech company, we&amp;#39;re looking for an Investigations Analyst to join us in making the crypto space safer.&lt;/p&gt;\n\n&lt;p&gt;We use the challenge program to evaluate applicants, and if you find the task intriguing, we&amp;#39;re likely to be a good fit.&lt;/p&gt;\n\n&lt;p&gt;To participate, submit a pull request to the &lt;a href=\"https://go.inca.digital/investigation-challenge?11\"&gt;challenge repo&lt;/a&gt;. To make sure your submission doesn&amp;#39;t get lost, you can also email your PR link along with your resume and the link to this challenge to [&lt;a href=\"mailto:challenge-submission@blockshop.org\"&gt;challenge-submission@blockshop.org&lt;/a&gt;](mailto:&lt;a href=\"mailto:challenge-submission@blockshop.org\"&gt;challenge-submission@blockshop.org&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;You can also find more about the opportunity &lt;a href=\"https://inca.digital/careers/#roles--data-analyst-investigations-and-research\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;More about &lt;a href=\"https://inca.digital/\"&gt;Inca Digital&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JgTgOc6O3vcBWkJOm-DGPYbKp2FMGX0bK-BFwGjm0OQ.jpg?auto=webp&amp;s=29e6c98b082ae21d39fc67515135fea9259e7e60", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/JgTgOc6O3vcBWkJOm-DGPYbKp2FMGX0bK-BFwGjm0OQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c548ae572afa12b67fbf2e2d83aede4cf1ae7016", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/JgTgOc6O3vcBWkJOm-DGPYbKp2FMGX0bK-BFwGjm0OQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=62efad2016864c398862b22a8fe43de13b43ecba", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/JgTgOc6O3vcBWkJOm-DGPYbKp2FMGX0bK-BFwGjm0OQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7a9fe471347a7150dbde3f7fbf9a758766d95d1", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/JgTgOc6O3vcBWkJOm-DGPYbKp2FMGX0bK-BFwGjm0OQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8998f0fd3657fbb7852f28ec0c6b84d436bd221e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/JgTgOc6O3vcBWkJOm-DGPYbKp2FMGX0bK-BFwGjm0OQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=531cf2747a0c872bbafa606f9418a4a360573d08", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/JgTgOc6O3vcBWkJOm-DGPYbKp2FMGX0bK-BFwGjm0OQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=52ebbdc63410849b987908c7b6c4929cedcaf756", "width": 1080, "height": 540}], "variants": {}, "id": "jyZ6WfA-YqIdv509-QIu3I2Op3UgzQQwMlK0vgSRFzw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xx6ww", "is_robot_indexable": true, "report_reasons": null, "author": "IncaDigital_Inc", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xx6ww/challenge_announcement_generate_a_comprehensive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14xx6ww/challenge_announcement_generate_a_comprehensive/", "subreddit_subscribers": 947034, "created_utc": 1689187959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all, just venting I think.  I work for a manufacturing startup.  We don't have a data science dept in so much as we have two people who do data analysis.  I am currently spending my days building code to aggregated all manufacturing data into a flat file for later analysis. It feels so disappointing and sad.  I remember sitting in my labs during MA and loving playing with my datasets.  I got that data cleaning is part of the job but does it get better?\n\nI've been vague as we work with confidential stuff.  My MA is on econ I'm just doing this till I can find something better but I still feel like I'm barely treading water here.  Is this normal?  Or should I be looking to bail immediately?", "author_fullname": "t2_3d1dp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data 'Science' life", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xwj8a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689186450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, just venting I think.  I work for a manufacturing startup.  We don&amp;#39;t have a data science dept in so much as we have two people who do data analysis.  I am currently spending my days building code to aggregated all manufacturing data into a flat file for later analysis. It feels so disappointing and sad.  I remember sitting in my labs during MA and loving playing with my datasets.  I got that data cleaning is part of the job but does it get better?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been vague as we work with confidential stuff.  My MA is on econ I&amp;#39;m just doing this till I can find something better but I still feel like I&amp;#39;m barely treading water here.  Is this normal?  Or should I be looking to bail immediately?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xwj8a", "is_robot_indexable": true, "report_reasons": null, "author": "doc334ft3", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xwj8a/data_science_life/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14xwj8a/data_science_life/", "subreddit_subscribers": 947034, "created_utc": 1689186450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Howdy, I'm Tommy, and... I'm kinda mess about which are the most important metrics ( CPC, CTR, ROI, etc... ) for a healthy b2b Google Ads campaign.", "author_fullname": "t2_9x9k2l3qk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "B2B - GOOGLE ADS METRIC TIPS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xvxak", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689185029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy, I&amp;#39;m Tommy, and... I&amp;#39;m kinda mess about which are the most important metrics ( CPC, CTR, ROI, etc... ) for a healthy b2b Google Ads campaign.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14xvxak", "is_robot_indexable": true, "report_reasons": null, "author": "fcktommy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14xvxak/b2b_google_ads_metric_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14xvxak/b2b_google_ads_metric_tips/", "subreddit_subscribers": 947034, "created_utc": 1689185029.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}