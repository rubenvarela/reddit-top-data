{"kind": "Listing", "data": {"after": "t3_14yfrpt", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just in case you're feeling the woes of missing out on a WD Red Pro 18tb @ $240 (that's $13.33/tb), don't worry. [Westerndigital.com](https://Westerndigital.com) still has them available, and I'll even one-up that:  \nUsing rakuten gets you an additional 10% off, plus $10 extra if you're new, or $30 extra if you use my referral code. Won't lie, it gets me $30 too, but that drops the price all the way down to $186 (+tax, shipping is free), or $10.33/tb  \n\n\nA) It's cheaper than amazon  \nB) You're not supporting amazon  \nC) They're not sold out  \nand  \nD) You're getting it cheaper.  \n\n\nThis doesn't just apply to the 18tb WD Red Pros, it looks like the whole line of WD Red Pros is on the cheap.  \n\n\nSo if you're in need, and you want a deal (and to help me out too!) then use: https://www.rakuten.com/r/PASTAD13 to sign up, then head to the app/website version of [westerndigital.com](https://westerndigital.com), and stock up!  \n\n\nP.S. - feel free not to use my code, it's a pretty good deal anyway, but if you're feeling groovy, it saves you more and gets me money, so that's cool too. Anyway, hope it helps somebody!\n\nP.P.S. - If Rakuten gives anybody guff, DM me, I'll get it sorted out for both of us. It's a \"reward\" (not a 'redeem'), so it'll show up *after* you purchase - not before. But again, I'll make sure we all get what's ours!\n\nP.P.P.S - If you made a purchase and didn't get your $30, let me know! I can get in touch with Rakuten and get us both paid, it looks like there were a fair number of \"pending\" that haven't processed yet. It only takes about ten minutes and it's worth thirty bucks!", "author_fullname": "t2_cwjus37k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prime day sold out, but westerndigital.com + rakuten = cheaper anyway", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14y6d45", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 106, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 106, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689264043.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689210352.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just in case you&amp;#39;re feeling the woes of missing out on a WD Red Pro 18tb @ $240 (that&amp;#39;s $13.33/tb), don&amp;#39;t worry. &lt;a href=\"https://Westerndigital.com\"&gt;Westerndigital.com&lt;/a&gt; still has them available, and I&amp;#39;ll even one-up that:&lt;br/&gt;\nUsing rakuten gets you an additional 10% off, plus $10 extra if you&amp;#39;re new, or $30 extra if you use my referral code. Won&amp;#39;t lie, it gets me $30 too, but that drops the price all the way down to $186 (+tax, shipping is free), or $10.33/tb  &lt;/p&gt;\n\n&lt;p&gt;A) It&amp;#39;s cheaper than amazon&lt;br/&gt;\nB) You&amp;#39;re not supporting amazon&lt;br/&gt;\nC) They&amp;#39;re not sold out&lt;br/&gt;\nand&lt;br/&gt;\nD) You&amp;#39;re getting it cheaper.  &lt;/p&gt;\n\n&lt;p&gt;This doesn&amp;#39;t just apply to the 18tb WD Red Pros, it looks like the whole line of WD Red Pros is on the cheap.  &lt;/p&gt;\n\n&lt;p&gt;So if you&amp;#39;re in need, and you want a deal (and to help me out too!) then use: &lt;a href=\"https://www.rakuten.com/r/PASTAD13\"&gt;https://www.rakuten.com/r/PASTAD13&lt;/a&gt; to sign up, then head to the app/website version of &lt;a href=\"https://westerndigital.com\"&gt;westerndigital.com&lt;/a&gt;, and stock up!  &lt;/p&gt;\n\n&lt;p&gt;P.S. - feel free not to use my code, it&amp;#39;s a pretty good deal anyway, but if you&amp;#39;re feeling groovy, it saves you more and gets me money, so that&amp;#39;s cool too. Anyway, hope it helps somebody!&lt;/p&gt;\n\n&lt;p&gt;P.P.S. - If Rakuten gives anybody guff, DM me, I&amp;#39;ll get it sorted out for both of us. It&amp;#39;s a &amp;quot;reward&amp;quot; (not a &amp;#39;redeem&amp;#39;), so it&amp;#39;ll show up &lt;em&gt;after&lt;/em&gt; you purchase - not before. But again, I&amp;#39;ll make sure we all get what&amp;#39;s ours!&lt;/p&gt;\n\n&lt;p&gt;P.P.P.S - If you made a purchase and didn&amp;#39;t get your $30, let me know! I can get in touch with Rakuten and get us both paid, it looks like there were a fair number of &amp;quot;pending&amp;quot; that haven&amp;#39;t processed yet. It only takes about ten minutes and it&amp;#39;s worth thirty bucks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14y6d45", "is_robot_indexable": true, "report_reasons": null, "author": "Altruistic_Bat_1645", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14y6d45/prime_day_sold_out_but_westerndigitalcom_rakuten/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14y6d45/prime_day_sold_out_but_westerndigitalcom_rakuten/", "subreddit_subscribers": 692476, "created_utc": 1689210352.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1gdcqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Musk\u2019s X Corp. sues data scrapers for \u201cseverely taxing\u201d Twitter\u2019s servers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_14ypn8r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 57, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 57, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/aklKJTLmJhNU5VsWMdMj6LyEYLJ5G34VK7p4lFRHREw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689266249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arstechnica.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://arstechnica.com/tech-policy/2023/07/musk-sues-data-scrapers-blames-them-for-twitters-impaired-user-experience/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7QEUsW-Am5SD8K-1wbuuxS53WKPmHTbDD8oD2T5v4fs.jpg?auto=webp&amp;s=d24e5aadbd32d0b66ad869dba1abd3449a49ac70", "width": 760, "height": 380}, "resolutions": [{"url": "https://external-preview.redd.it/7QEUsW-Am5SD8K-1wbuuxS53WKPmHTbDD8oD2T5v4fs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ffa46c12d5610aa7f7b675bd9d959bfa2982e5d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/7QEUsW-Am5SD8K-1wbuuxS53WKPmHTbDD8oD2T5v4fs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1015a2a0b757ffbd7b9e644be7e286e0c34c6b8a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/7QEUsW-Am5SD8K-1wbuuxS53WKPmHTbDD8oD2T5v4fs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eca3daa80e88a26885103bf084ee1f50603b3d77", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/7QEUsW-Am5SD8K-1wbuuxS53WKPmHTbDD8oD2T5v4fs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=be68f67de2d740d6a16430d493de8bc3e74b0f2a", "width": 640, "height": 320}], "variants": {}, "id": "tdcbH3ICVip37Y_PY3DDqv5aG82QCqJBC0W_ugFiqbU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ypn8r", "is_robot_indexable": true, "report_reasons": null, "author": "uluqat", "discussion_type": null, "num_comments": 35, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ypn8r/musks_x_corp_sues_data_scrapers_for_severely/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://arstechnica.com/tech-policy/2023/07/musk-sues-data-scrapers-blames-them-for-twitters-impaired-user-experience/", "subreddit_subscribers": 692476, "created_utc": 1689266249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For the longest time, the 8TB Samsung 870 has been hovering in the 500-600s then, in just the last few months, it seems like the price keeps dropping.  I follow the item on CamelCamelCamel and I just got a notification that the price got down to $320 (though I missed out on getting it at that price.)\n\nI get that new tech starts off as expensive then reduces in price but this one seems faster than usual.", "author_fullname": "t2_k34ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Out Of The Loop: What's causing 8TB SSD (Specifically Samsung's) to rapidly drop in price?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14yep5l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689235920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the longest time, the 8TB Samsung 870 has been hovering in the 500-600s then, in just the last few months, it seems like the price keeps dropping.  I follow the item on CamelCamelCamel and I just got a notification that the price got down to $320 (though I missed out on getting it at that price.)&lt;/p&gt;\n\n&lt;p&gt;I get that new tech starts off as expensive then reduces in price but this one seems faster than usual.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14yep5l", "is_robot_indexable": true, "report_reasons": null, "author": "Cmdr_Nemo", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14yep5l/out_of_the_loop_whats_causing_8tb_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14yep5l/out_of_the_loop_whats_causing_8tb_ssd/", "subreddit_subscribers": 692476, "created_utc": 1689235920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I order five  Western Digital 18TB WD Red Pro NAS Internal Hard Drive  HDD (Prime Sale/Ship and Sold by Amazon),  I received them the next day and when I opened the package I  notice that the model number was wrong on 4  boxes  there was a 2nd  label place indicating the ASIN number and 18TB , but after after cutting the security  tape of one box out  , I discovered a 8 TB HD inside the box.  So 4 out of my 5 of my drives were 8TB.\n\nIn the picture you can see the model number (MFG No : WD80EFZZ) on the  box and then a 2nd label next to it with a ASIN number and description .\n\nAmazon CS said then need to open a  investigation before issuing a refund ($1840 Can) and request video and photos of the packaging and hard Drives  and to wait .\n\nQuestion has this happened to anyone else, I request a return and  shipped the HD's back to Amazon and now wait to see if they will refund  me\n\n# this is a reply from Amazon for there investigation\n\n&gt;Since the correct item shows as delivered, we require a photo of the wrong item that was delivered by the carrier received along with a piece of paper (handwritten or typed) containing the Amazon account holder's name and a date on or after July 12, 2023. The information must be visible in one photo. Also, we request you to send us the image including the shipping label on the package.  \n&gt;  \n&gt;Once you have taken the above photo, please reply to this e-mail with the Order ID : 701-000000000-0000000 and attach the photo in a .jpg or .pdf file format.  \n&gt;  \n&gt;Your photo must be submitted before July 12, 2023 + 29 Days that is by August 10, 2023 to be considered for review. Once we receive your photo, we typically respond within 3 days with the appropriate action that has to be taken on the order.  \n&gt;  \n&gt;Please send us the video evidence of opening the package so that we could add it to our investigation. If you\u2019d take the time to respond to this email and attach the requested evidence, it would be appreciated.  \n&gt;  \n&gt;If you receive a message stating that the evidence file is too large, services like Amazon Photos, iCloud, Google Drive, Google Photos, and OneDrive allow you to upload the file and create a link to share. Please make sure that the link is visible to everyone with the link, rather than creating a link that only another account can access.  \n&gt;  \n&gt;Thank you for your understanding. We look forward to your response.  \n&gt;  \n&gt;Regards,  \n&gt;  \n&gt;CCCCCCExecutive Customer RelationsAmazon.ca\n\n&amp;#x200B;\n\nhttps://preview.redd.it/3u83d4x8fqbb1.jpg?width=3000&amp;format=pjpg&amp;auto=webp&amp;s=23a3086a0a43e263701aa1b90aa0017e6925b671\n\nhttps://preview.redd.it/r2wyryw8fqbb1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=322cbd2b0f1c2462c76dba422d0d74b14e2d8092\n\nhttps://preview.redd.it/h6lt5tw8fqbb1.jpg?width=911&amp;format=pjpg&amp;auto=webp&amp;s=18b71168409411c0a7c0a93bbd32a6ff0db97d25\n\nhttps://preview.redd.it/v55gauw8fqbb1.jpg?width=1264&amp;format=pjpg&amp;auto=webp&amp;s=a5fbc0f8c9303236807e15a61a181daed6b5cce8", "author_fullname": "t2_vjc34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Order 18TB WD RED PRO Hard Drives from Amazon Canada and received 8 TB hard drives in a box with two model Number Labels.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"3u83d4x8fqbb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/3u83d4x8fqbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7218c0959c6505e48f2975dca7402afa0c5be4d4"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/3u83d4x8fqbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58ded07afe0160c1640c7a4e747f9c2deff4fd2f"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/3u83d4x8fqbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=74b4fe7971dc8eeece11b5867bc648b7fbb19a7f"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/3u83d4x8fqbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=19120e738f89b265684e0576954d17f57ae2ac0d"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/3u83d4x8fqbb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5959b54440037ab9e184a00aa5980bb88bd5f36d"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/3u83d4x8fqbb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=babff4995f1bade4ddac538a021c4911241a5392"}], "s": {"y": 4000, "x": 3000, "u": "https://preview.redd.it/3u83d4x8fqbb1.jpg?width=3000&amp;format=pjpg&amp;auto=webp&amp;s=23a3086a0a43e263701aa1b90aa0017e6925b671"}, "id": "3u83d4x8fqbb1"}, "h6lt5tw8fqbb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 74, "x": 108, "u": "https://preview.redd.it/h6lt5tw8fqbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=53a8249ec56c22b59a31393e58986ffc0ba3ca32"}, {"y": 148, "x": 216, "u": "https://preview.redd.it/h6lt5tw8fqbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=976994c905ae2f32966348c170066c1e6943a9a5"}, {"y": 220, "x": 320, "u": "https://preview.redd.it/h6lt5tw8fqbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e935ec0efc851188d8140335d6d7b995ab8059e7"}, {"y": 440, "x": 640, "u": "https://preview.redd.it/h6lt5tw8fqbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5871a8dcfd77ff6b2d6b7f76ae53ebc1abe40977"}], "s": {"y": 627, "x": 911, "u": "https://preview.redd.it/h6lt5tw8fqbb1.jpg?width=911&amp;format=pjpg&amp;auto=webp&amp;s=18b71168409411c0a7c0a93bbd32a6ff0db97d25"}, "id": "h6lt5tw8fqbb1"}, "r2wyryw8fqbb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/r2wyryw8fqbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=64a41223bf18d7260e124be36bf4535e670c9343"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/r2wyryw8fqbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8389353f7cb1a96db561e121c0a6a8b69f65a7c7"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/r2wyryw8fqbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4787c4fa69d0669427dd960bde2f4efb0e993f27"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/r2wyryw8fqbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=460fa3297f9f4faf806b99c3ea3f9ae6d79aa6ab"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/r2wyryw8fqbb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e481b9488eceaa780f531dabbaafe85ac30a1eba"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/r2wyryw8fqbb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=561ed2d732eef4223dae5187800c882a5a07d6a6"}], "s": {"y": 3000, "x": 4000, "u": "https://preview.redd.it/r2wyryw8fqbb1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=322cbd2b0f1c2462c76dba422d0d74b14e2d8092"}, "id": "r2wyryw8fqbb1"}, "v55gauw8fqbb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/v55gauw8fqbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ac22d7c3c0774cdf3b1b52054d523b433b66bcdd"}, {"y": 95, "x": 216, "u": "https://preview.redd.it/v55gauw8fqbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f93f58700c76eb81a1deb5e256e476d054d0211"}, {"y": 141, "x": 320, "u": "https://preview.redd.it/v55gauw8fqbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a630f448026dc5ccc41c374af4f2d6e4f9dd4af"}, {"y": 283, "x": 640, "u": "https://preview.redd.it/v55gauw8fqbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae398c271157bee9a738e7bb0c50a60f603a4ddf"}, {"y": 425, "x": 960, "u": "https://preview.redd.it/v55gauw8fqbb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e3884397382949b84afc36c3937405d1d7a91de"}, {"y": 478, "x": 1080, "u": "https://preview.redd.it/v55gauw8fqbb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3937a928da7f15933e323c47f25ea54024dd1951"}], "s": {"y": 560, "x": 1264, "u": "https://preview.redd.it/v55gauw8fqbb1.jpg?width=1264&amp;format=pjpg&amp;auto=webp&amp;s=a5fbc0f8c9303236807e15a61a181daed6b5cce8"}, "id": "v55gauw8fqbb1"}}, "name": "t3_14yki9k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BTlKKIqZKVl9sMK8jAOW0PceXgLCkncC2VeSaP8jCQg.jpg", "edited": 1689269317.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689253892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I order five  Western Digital 18TB WD Red Pro NAS Internal Hard Drive  HDD (Prime Sale/Ship and Sold by Amazon),  I received them the next day and when I opened the package I  notice that the model number was wrong on 4  boxes  there was a 2nd  label place indicating the ASIN number and 18TB , but after after cutting the security  tape of one box out  , I discovered a 8 TB HD inside the box.  So 4 out of my 5 of my drives were 8TB.&lt;/p&gt;\n\n&lt;p&gt;In the picture you can see the model number (MFG No : WD80EFZZ) on the  box and then a 2nd label next to it with a ASIN number and description .&lt;/p&gt;\n\n&lt;p&gt;Amazon CS said then need to open a  investigation before issuing a refund ($1840 Can) and request video and photos of the packaging and hard Drives  and to wait .&lt;/p&gt;\n\n&lt;p&gt;Question has this happened to anyone else, I request a return and  shipped the HD&amp;#39;s back to Amazon and now wait to see if they will refund  me&lt;/p&gt;\n\n&lt;h1&gt;this is a reply from Amazon for there investigation&lt;/h1&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Since the correct item shows as delivered, we require a photo of the wrong item that was delivered by the carrier received along with a piece of paper (handwritten or typed) containing the Amazon account holder&amp;#39;s name and a date on or after July 12, 2023. The information must be visible in one photo. Also, we request you to send us the image including the shipping label on the package.  &lt;/p&gt;\n\n&lt;p&gt;Once you have taken the above photo, please reply to this e-mail with the Order ID : 701-000000000-0000000 and attach the photo in a .jpg or .pdf file format.  &lt;/p&gt;\n\n&lt;p&gt;Your photo must be submitted before July 12, 2023 + 29 Days that is by August 10, 2023 to be considered for review. Once we receive your photo, we typically respond within 3 days with the appropriate action that has to be taken on the order.  &lt;/p&gt;\n\n&lt;p&gt;Please send us the video evidence of opening the package so that we could add it to our investigation. If you\u2019d take the time to respond to this email and attach the requested evidence, it would be appreciated.  &lt;/p&gt;\n\n&lt;p&gt;If you receive a message stating that the evidence file is too large, services like Amazon Photos, iCloud, Google Drive, Google Photos, and OneDrive allow you to upload the file and create a link to share. Please make sure that the link is visible to everyone with the link, rather than creating a link that only another account can access.  &lt;/p&gt;\n\n&lt;p&gt;Thank you for your understanding. We look forward to your response.  &lt;/p&gt;\n\n&lt;p&gt;Regards,  &lt;/p&gt;\n\n&lt;p&gt;CCCCCCExecutive Customer RelationsAmazon.ca&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3u83d4x8fqbb1.jpg?width=3000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23a3086a0a43e263701aa1b90aa0017e6925b671\"&gt;https://preview.redd.it/3u83d4x8fqbb1.jpg?width=3000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23a3086a0a43e263701aa1b90aa0017e6925b671&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r2wyryw8fqbb1.jpg?width=4000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=322cbd2b0f1c2462c76dba422d0d74b14e2d8092\"&gt;https://preview.redd.it/r2wyryw8fqbb1.jpg?width=4000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=322cbd2b0f1c2462c76dba422d0d74b14e2d8092&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/h6lt5tw8fqbb1.jpg?width=911&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=18b71168409411c0a7c0a93bbd32a6ff0db97d25\"&gt;https://preview.redd.it/h6lt5tw8fqbb1.jpg?width=911&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=18b71168409411c0a7c0a93bbd32a6ff0db97d25&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v55gauw8fqbb1.jpg?width=1264&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a5fbc0f8c9303236807e15a61a181daed6b5cce8\"&gt;https://preview.redd.it/v55gauw8fqbb1.jpg?width=1264&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a5fbc0f8c9303236807e15a61a181daed6b5cce8&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14yki9k", "is_robot_indexable": true, "report_reasons": null, "author": "zac2849", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14yki9k/order_18tb_wd_red_pro_hard_drives_from_amazon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14yki9k/order_18tb_wd_red_pro_hard_drives_from_amazon/", "subreddit_subscribers": 692476, "created_utc": 1689253892.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi!\n\nI will be doing a project where I scan some A4 photos with the intention of preservation and also displaying them on very large screens (4x or 5x the original photo size, hence the need for large dpi).\n\nI have access to a relatively good PC, but I know file sizes with very good quality can go insanely high.\n\nDoes anyone know how I can calculate the file size? For example A4 size, 1200 dpi or 2400 dpi, JPG or TIF. The photo is colored but relatively the same shade (think faded vintage maps, basically lots of brown and some black, no other colors).\n\nMy initial plan was 2400 dpi and TIF. But the photo store told me that would output enormous files, so I'm not sure if my PC can handle it. I do minor video/photo editing, but I'm far from having a full professional film studio setup.\n\nSo I may have to go with 1200 dpi or JPG instead of TIF. I don't need exact file size, just a general ballpark calculation (less than 10 GB? less than 100 GB? over 100 GB? etc.) so I can check if my setup will be able to handle it.\n\nI'm posting here because I know this sub has some virtuosos when dealing with scans and file sizes, hopefully it is okay!\n\nThank you all in advance!", "author_fullname": "t2_hbftt9w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to calculate file size for scanned image (A4, 2400 dpi, TIF...)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14yfvsg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689240025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I will be doing a project where I scan some A4 photos with the intention of preservation and also displaying them on very large screens (4x or 5x the original photo size, hence the need for large dpi).&lt;/p&gt;\n\n&lt;p&gt;I have access to a relatively good PC, but I know file sizes with very good quality can go insanely high.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know how I can calculate the file size? For example A4 size, 1200 dpi or 2400 dpi, JPG or TIF. The photo is colored but relatively the same shade (think faded vintage maps, basically lots of brown and some black, no other colors).&lt;/p&gt;\n\n&lt;p&gt;My initial plan was 2400 dpi and TIF. But the photo store told me that would output enormous files, so I&amp;#39;m not sure if my PC can handle it. I do minor video/photo editing, but I&amp;#39;m far from having a full professional film studio setup.&lt;/p&gt;\n\n&lt;p&gt;So I may have to go with 1200 dpi or JPG instead of TIF. I don&amp;#39;t need exact file size, just a general ballpark calculation (less than 10 GB? less than 100 GB? over 100 GB? etc.) so I can check if my setup will be able to handle it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m posting here because I know this sub has some virtuosos when dealing with scans and file sizes, hopefully it is okay!&lt;/p&gt;\n\n&lt;p&gt;Thank you all in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14yfvsg", "is_robot_indexable": true, "report_reasons": null, "author": "flyblues", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14yfvsg/how_to_calculate_file_size_for_scanned_image_a4/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14yfvsg/how_to_calculate_file_size_for_scanned_image_a4/", "subreddit_subscribers": 692476, "created_utc": 1689240025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know this is a bit of out there, but I do video production and I've updated all 8 of my 2TB T7 drives for 4TB SSD drives. So, I'm curious what I can do with these 2TB SSD T7 drives and put them to good use?! Any ideas or ways to use them, other than plugging them into a desktop via USB-C. Is there any way to RAID them or some how create a drive pool in an enclosure (instead of using all my ports, lol)?\n\nThanks!", "author_fullname": "t2_zmo7c0o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RAID/DrivePool type setup for external SSD USB-C drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xw65v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689189114.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689185595.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this is a bit of out there, but I do video production and I&amp;#39;ve updated all 8 of my 2TB T7 drives for 4TB SSD drives. So, I&amp;#39;m curious what I can do with these 2TB SSD T7 drives and put them to good use?! Any ideas or ways to use them, other than plugging them into a desktop via USB-C. Is there any way to RAID them or some how create a drive pool in an enclosure (instead of using all my ports, lol)?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14xw65v", "is_robot_indexable": true, "report_reasons": null, "author": "likelinus01", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14xw65v/raiddrivepool_type_setup_for_external_ssd_usbc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14xw65v/raiddrivepool_type_setup_for_external_ssd_usbc/", "subreddit_subscribers": 692476, "created_utc": 1689185595.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sorry if this is the wrong place to post this. Please direct me to a better suited sub as needed.\n\nI have a large DVD library and like physical media. I want to back up the media legally: By myself, making full images of discs to rewrite later. I'm not interested in extracting raw video files and hosting them on a Plex server or something, since the legality on that is iffy. \n\nBrasero seems to work well on my Linux Mint setup to make a good rip of discs, but I realize that it would take forever to rip all my discs one by one. Has anyone done something similar? \n\nIs there a multi-drive you would recommend that can serve this purpose? Ideally it would have 5 80X DVD drives and plug into a single USB A 3.1 port. Thanks!", "author_fullname": "t2_bq7vt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good external drive to rip multiple DVDs at once?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14y4k5g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689205160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is the wrong place to post this. Please direct me to a better suited sub as needed.&lt;/p&gt;\n\n&lt;p&gt;I have a large DVD library and like physical media. I want to back up the media legally: By myself, making full images of discs to rewrite later. I&amp;#39;m not interested in extracting raw video files and hosting them on a Plex server or something, since the legality on that is iffy. &lt;/p&gt;\n\n&lt;p&gt;Brasero seems to work well on my Linux Mint setup to make a good rip of discs, but I realize that it would take forever to rip all my discs one by one. Has anyone done something similar? &lt;/p&gt;\n\n&lt;p&gt;Is there a multi-drive you would recommend that can serve this purpose? Ideally it would have 5 80X DVD drives and plug into a single USB A 3.1 port. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14y4k5g", "is_robot_indexable": true, "report_reasons": null, "author": "Weboh", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14y4k5g/good_external_drive_to_rip_multiple_dvds_at_once/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14y4k5g/good_external_drive_to_rip_multiple_dvds_at_once/", "subreddit_subscribers": 692476, "created_utc": 1689205160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was interested in starting to explore archiving my stuff and learning home server things. I was looking at the cheap workstations on ebay like many recommend but the cases are typically small so cannot fit any/many drives. I'm trying to be very budget oriented so just getting a separate NAS isn't really the right solution for me.\n\nDo they make NAS-like cases that hold a bunch of hard drives but aren't computers themselves? Like something where the SATA cables can all come out the back for each drive and I cut a hole or something into the workstation to connect them all to it.\n\nI tried searching a bit for this but I kept only finding either NASes, or internal PC case hard drive caddys. Maybe I'm not using the right terminology if there's a name for it I'm unfamiliar with.\n\nOr maybe there's something else entirely that fits what I'm looking for better?", "author_fullname": "t2_eg3mitmao", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there external storage cases that are not a NAS? Or maybe there's a better solution entirely?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14y2bdo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689199942.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689199756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was interested in starting to explore archiving my stuff and learning home server things. I was looking at the cheap workstations on ebay like many recommend but the cases are typically small so cannot fit any/many drives. I&amp;#39;m trying to be very budget oriented so just getting a separate NAS isn&amp;#39;t really the right solution for me.&lt;/p&gt;\n\n&lt;p&gt;Do they make NAS-like cases that hold a bunch of hard drives but aren&amp;#39;t computers themselves? Like something where the SATA cables can all come out the back for each drive and I cut a hole or something into the workstation to connect them all to it.&lt;/p&gt;\n\n&lt;p&gt;I tried searching a bit for this but I kept only finding either NASes, or internal PC case hard drive caddys. Maybe I&amp;#39;m not using the right terminology if there&amp;#39;s a name for it I&amp;#39;m unfamiliar with.&lt;/p&gt;\n\n&lt;p&gt;Or maybe there&amp;#39;s something else entirely that fits what I&amp;#39;m looking for better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14y2bdo", "is_robot_indexable": true, "report_reasons": null, "author": "SodaPop-PodaSop", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14y2bdo/are_there_external_storage_cases_that_are_not_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14y2bdo/are_there_external_storage_cases_that_are_not_a/", "subreddit_subscribers": 692476, "created_utc": 1689199756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have four NAS varying in size from 4TB to 24TB. Total actual data capacity is 35.4TB\n\nI want to back this up using Backblaze PC back-up (the unlimited plan rather than the B2 Cloud storage which would be too expensive for me).\n\nFor this I've built a dedicated PC as a data store. Like a DIY NAS but not running NAS software. The drives are confirgured as a single Storage Pool, with the Storage Space set as a single local drive.\n\nI bought AOMEI Backupper Pro to create one-way Mirror Sync Tasks from each of the NAS and into their own folder on the local drive. I opted for Backupper as I use it on another PC for a similar task and it had always worked fine. However these tasks are failing for no apparent reason (still waiting for support to get back to me) I'm guessing the issue may be because the amount of files is so much larger than what I've used it for before?  \n\n\nSo my question is what are my alternatives? I've considered just using robocopy but that would mean that each NAS would have to be mounted as a windows share, (AOMEI Backupper can have networkIP/login credentials used as Source devices which I much prefer, and I assume is faster?)  \n\n\nAre there any other ways of doing this? Any other software I could use? Doesn't have to be free. But I really want it to be a reliable one way sync running on a schedule\n\n&amp;#x200B;", "author_fullname": "t2_13igbu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How best to one way sync multiple NAS to a local drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xwc5e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689185992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have four NAS varying in size from 4TB to 24TB. Total actual data capacity is 35.4TB&lt;/p&gt;\n\n&lt;p&gt;I want to back this up using Backblaze PC back-up (the unlimited plan rather than the B2 Cloud storage which would be too expensive for me).&lt;/p&gt;\n\n&lt;p&gt;For this I&amp;#39;ve built a dedicated PC as a data store. Like a DIY NAS but not running NAS software. The drives are confirgured as a single Storage Pool, with the Storage Space set as a single local drive.&lt;/p&gt;\n\n&lt;p&gt;I bought AOMEI Backupper Pro to create one-way Mirror Sync Tasks from each of the NAS and into their own folder on the local drive. I opted for Backupper as I use it on another PC for a similar task and it had always worked fine. However these tasks are failing for no apparent reason (still waiting for support to get back to me) I&amp;#39;m guessing the issue may be because the amount of files is so much larger than what I&amp;#39;ve used it for before?  &lt;/p&gt;\n\n&lt;p&gt;So my question is what are my alternatives? I&amp;#39;ve considered just using robocopy but that would mean that each NAS would have to be mounted as a windows share, (AOMEI Backupper can have networkIP/login credentials used as Source devices which I much prefer, and I assume is faster?)  &lt;/p&gt;\n\n&lt;p&gt;Are there any other ways of doing this? Any other software I could use? Doesn&amp;#39;t have to be free. But I really want it to be a reliable one way sync running on a schedule&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14xwc5e", "is_robot_indexable": true, "report_reasons": null, "author": "baroquedub", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14xwc5e/how_best_to_one_way_sync_multiple_nas_to_a_local/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14xwc5e/how_best_to_one_way_sync_multiple_nas_to_a_local/", "subreddit_subscribers": 692476, "created_utc": 1689185992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Paying 50/50 with my parents, my new system (Synology DS1821 and 8x22TB drives) will be arriving soon and I don't know which option to choose from SHR or Raid 5-6 I am really stuck. I have never really been that technical and really don't want to mess anything up. As well as that I am wanting to future proof my storage for years to come. \n\nUp till now, with any spare money I ever had, I kept acquiring more and more hard drives to the point that, even though, I have laboriously catalogued everything I still lose count of the exact number... At last check it is well over 15 drives - ranging from 5TB all the way to a couple of 12's, 14's 16's and 18's... As tech became cheaper.\n\nAll of them have files everywhere and expect for a few, are dedicated to only one thing (movies/tv shows, ~~adult~~  stuff etc) So my current plan/options are as follows.\n\nIdeally I am planning on using the new Synology for Plex for my movies/tv shows and audiobook library through Prologue. Since as stated above files are everywhere, it's second use would mainly be a \"man in the middle\" to help sort out everything (finally!!!) 1. Synology up and running 2. connect old drive 3. sort files and determine if it makes the new unit, or archive or delete and free up space on existing drive 4. finish, disconnect 5. rinse/lather/repeat.\n\nThis will enable me to finally get a proper grasp of everything and decide going forward, if I just free up much needed space on existing drives or turn existing drives into backups rather than them currently being my primary drives. \n\nAs stated, before this will be the most amount of space I have ever had, at least in one unit, and with today's inflation prices, probably the most I will ever have, so ideally, I want it to last as long as possible. I know I said I am not that technical what I am about to write is the extent of my tech knowledge...while I am fully aware that even though the math works out to (8x22TB) 176TB I know there is no chance I will get all of that, regardless of picking SHR or Raid 5-6 because I will be sacrificing 1 or 2 drives. I am leaning toward SHR but only because (and please correct me if I am wrong) SHR's benefit is you can put whatever size drive you want going forward, and the brand of the drives doesn't have to be the same, so going forward to that one amazing day where a 30-40TB drive becomes as affordable as a current 10 or 18TB then I could put them in there without a problem.\n\nI also got informed that the DS1821 maxes out at 108TB, is that per volume you setup? does that apply for both SHR and Raid 5-6? which setup process is easier Raid 5-6 or SHR. If I go Raid 5 I would reduce my drives to 5 instead of 8, and have 3x22TB drives spare, but if I go SHR I would use them all, and create two volumes, assuming it is pretty idiot proof to set it up out of the box. \n\nAny help would be greatly appreciated.   \n\n   ", "author_fullname": "t2_c7lcxyz2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Still stuck on deciding SHR or Raid 5-6 as well as future proof hoarding setup.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14y55gh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689206735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Paying 50/50 with my parents, my new system (Synology DS1821 and 8x22TB drives) will be arriving soon and I don&amp;#39;t know which option to choose from SHR or Raid 5-6 I am really stuck. I have never really been that technical and really don&amp;#39;t want to mess anything up. As well as that I am wanting to future proof my storage for years to come. &lt;/p&gt;\n\n&lt;p&gt;Up till now, with any spare money I ever had, I kept acquiring more and more hard drives to the point that, even though, I have laboriously catalogued everything I still lose count of the exact number... At last check it is well over 15 drives - ranging from 5TB all the way to a couple of 12&amp;#39;s, 14&amp;#39;s 16&amp;#39;s and 18&amp;#39;s... As tech became cheaper.&lt;/p&gt;\n\n&lt;p&gt;All of them have files everywhere and expect for a few, are dedicated to only one thing (movies/tv shows, &lt;del&gt;adult&lt;/del&gt;  stuff etc) So my current plan/options are as follows.&lt;/p&gt;\n\n&lt;p&gt;Ideally I am planning on using the new Synology for Plex for my movies/tv shows and audiobook library through Prologue. Since as stated above files are everywhere, it&amp;#39;s second use would mainly be a &amp;quot;man in the middle&amp;quot; to help sort out everything (finally!!!) 1. Synology up and running 2. connect old drive 3. sort files and determine if it makes the new unit, or archive or delete and free up space on existing drive 4. finish, disconnect 5. rinse/lather/repeat.&lt;/p&gt;\n\n&lt;p&gt;This will enable me to finally get a proper grasp of everything and decide going forward, if I just free up much needed space on existing drives or turn existing drives into backups rather than them currently being my primary drives. &lt;/p&gt;\n\n&lt;p&gt;As stated, before this will be the most amount of space I have ever had, at least in one unit, and with today&amp;#39;s inflation prices, probably the most I will ever have, so ideally, I want it to last as long as possible. I know I said I am not that technical what I am about to write is the extent of my tech knowledge...while I am fully aware that even though the math works out to (8x22TB) 176TB I know there is no chance I will get all of that, regardless of picking SHR or Raid 5-6 because I will be sacrificing 1 or 2 drives. I am leaning toward SHR but only because (and please correct me if I am wrong) SHR&amp;#39;s benefit is you can put whatever size drive you want going forward, and the brand of the drives doesn&amp;#39;t have to be the same, so going forward to that one amazing day where a 30-40TB drive becomes as affordable as a current 10 or 18TB then I could put them in there without a problem.&lt;/p&gt;\n\n&lt;p&gt;I also got informed that the DS1821 maxes out at 108TB, is that per volume you setup? does that apply for both SHR and Raid 5-6? which setup process is easier Raid 5-6 or SHR. If I go Raid 5 I would reduce my drives to 5 instead of 8, and have 3x22TB drives spare, but if I go SHR I would use them all, and create two volumes, assuming it is pretty idiot proof to set it up out of the box. &lt;/p&gt;\n\n&lt;p&gt;Any help would be greatly appreciated.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14y55gh", "is_robot_indexable": true, "report_reasons": null, "author": "massivlybored", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14y55gh/still_stuck_on_deciding_shr_or_raid_56_as_well_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14y55gh/still_stuck_on_deciding_shr_or_raid_56_as_well_as/", "subreddit_subscribers": 692476, "created_utc": 1689206735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys! Since the unlimited [G.Drive](https://G.Drive) \"stuff\", I started looking for disks dedicated to a third backup of important stuff.\n\nThe best deal right now are the WD Elements 18TB (295\u20ac) and the Ultrastar  DC HC550 18TB (299\u20ac), but I have some questions:\n\n\\- Some old reddit post said that the white label disk in high capacity Elements are basically ultrastar disk, is it still true? (the most recent post was from 2 years ago)\n\n\\- If they are the same, there is no reason to prefer the white label one, right?\n\n\\- Does someone have experience with their noise?\n\nI have a Red 10TB (WD100EFAX pre plus line) 5400rpm and 2 white label 10TB 5400 rpm (WD100EZAZ) and I think their noise level is perfect, does someone have both an ultrastar and these old 5400rpm disks? I need an idea on how much worse is the noise, since these will go in my bedroom.\n\n\\- Are there still Red drives (cmr) with 5400rpm? I can only find 7200 on the plus and pro line\n\n\\- I'm down to get a worse deal and buy a 10 or 12TB Elements if that means 5400rpm and less noise, does someone know if they are still used in \"lower capacity\" model?\n\n\\- Other than noise are there any other reasons to prefer the Red line over Ultrastar/white label?", "author_fullname": "t2_agg19xqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Elements vs Ultrastar vs Red - is 5400rpm totally dead?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xxf76", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689188481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys! Since the unlimited &lt;a href=\"https://G.Drive\"&gt;G.Drive&lt;/a&gt; &amp;quot;stuff&amp;quot;, I started looking for disks dedicated to a third backup of important stuff.&lt;/p&gt;\n\n&lt;p&gt;The best deal right now are the WD Elements 18TB (295\u20ac) and the Ultrastar  DC HC550 18TB (299\u20ac), but I have some questions:&lt;/p&gt;\n\n&lt;p&gt;- Some old reddit post said that the white label disk in high capacity Elements are basically ultrastar disk, is it still true? (the most recent post was from 2 years ago)&lt;/p&gt;\n\n&lt;p&gt;- If they are the same, there is no reason to prefer the white label one, right?&lt;/p&gt;\n\n&lt;p&gt;- Does someone have experience with their noise?&lt;/p&gt;\n\n&lt;p&gt;I have a Red 10TB (WD100EFAX pre plus line) 5400rpm and 2 white label 10TB 5400 rpm (WD100EZAZ) and I think their noise level is perfect, does someone have both an ultrastar and these old 5400rpm disks? I need an idea on how much worse is the noise, since these will go in my bedroom.&lt;/p&gt;\n\n&lt;p&gt;- Are there still Red drives (cmr) with 5400rpm? I can only find 7200 on the plus and pro line&lt;/p&gt;\n\n&lt;p&gt;- I&amp;#39;m down to get a worse deal and buy a 10 or 12TB Elements if that means 5400rpm and less noise, does someone know if they are still used in &amp;quot;lower capacity&amp;quot; model?&lt;/p&gt;\n\n&lt;p&gt;- Other than noise are there any other reasons to prefer the Red line over Ultrastar/white label?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14xxf76", "is_robot_indexable": true, "report_reasons": null, "author": "Aye42", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14xxf76/wd_elements_vs_ultrastar_vs_red_is_5400rpm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14xxf76/wd_elements_vs_ultrastar_vs_red_is_5400rpm/", "subreddit_subscribers": 692476, "created_utc": 1689188481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I saw new WD Ultrastar (WUH721414AL4204 0F31021) 14TB HDDs on [https://serverpartdeals.com/](https://serverpartdeals.com/) for $135. This seems almost to good to be true. I ended up buying a couple as well as a LSI SAS controller card and it seems that it really was new according to the service hours on SMART data. As a bonus it seems my existing SATA drives are plug &amp; play with the SAS controller. So far I have found Serverpartdeals far better than Amazon in terms of fast shipment &amp; well packaged drives but I am wondering if this is too good to be true.", "author_fullname": "t2_2grm50dp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good value for 14Tb HDDs or am I missing something?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14yr6xq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689269940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw new WD Ultrastar (WUH721414AL4204 0F31021) 14TB HDDs on &lt;a href=\"https://serverpartdeals.com/\"&gt;https://serverpartdeals.com/&lt;/a&gt; for $135. This seems almost to good to be true. I ended up buying a couple as well as a LSI SAS controller card and it seems that it really was new according to the service hours on SMART data. As a bonus it seems my existing SATA drives are plug &amp;amp; play with the SAS controller. So far I have found Serverpartdeals far better than Amazon in terms of fast shipment &amp;amp; well packaged drives but I am wondering if this is too good to be true.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?auto=webp&amp;s=4c1db49fea90382fef14d5213d071fac818998ff", "width": 2000, "height": 2000}, "resolutions": [{"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e7ff784b044c5a132792a51783e12546aa096933", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ebb5efa12a9e165a566f9c8d169e9861369acd2f", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=763ac82ff67bfc932ed93a299e8d40c5271a9258", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=487c4ebc47f1af49ca7b4ed1e032a4d96672a443", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=32fed97e301ba788e5b9661d6052a5e41a4a23c1", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f4c9097a5b110531b40d5950876ae2530c195a25", "width": 1080, "height": 1080}], "variants": {}, "id": "_D0H66BxLDdRjM-Vxtc776wegerPH5Vt6qcZASPwe_c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14yr6xq", "is_robot_indexable": true, "report_reasons": null, "author": "haemakatus", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14yr6xq/good_value_for_14tb_hdds_or_am_i_missing_something/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14yr6xq/good_value_for_14tb_hdds_or_am_i_missing_something/", "subreddit_subscribers": 692476, "created_utc": 1689269940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For me too the sad day came when my \"unlimited\" google enterprise account suddenly had a red banner telling me that I was way over my newly imposed 5tb limit. To stay with google drive I would either have to add more users (5tb per user at $20 a month) or pay an outrageous $300 a month for 10 extra terrabytes.\n\n  \nAfter looking around for a bit I figured the best option for unlimited data to backup my NAS (and share stuff with clients etc) was going to be Dropbox (via 3 business accounts at either $72 a month (paid yearly) or $90 a month(paid monthly)). Unfortunately my upload speeds at home leave a lot to desired and it would take months to transfer all my data over + putting a lot of strain on my home internet + having to pay for both services in the meantime...\n\n  \nThe problem is that most data transfer services out there are expensive and transferring it via a custom server on AWS or other similar server providers is also expensive as they often charge you when you go over a certain amount of network traffic (common limits are between 1TB and 5TB a month).\n\n  \nOVH Cloud to the rescue! I found out that some services, like OVH cloud don't charge you for data transfer, but instead just limit your transfer bandwidth (depending on the server you choose 250mbits, 500mbits or more which is way better than my sad 5mbits up at home). I went with their just over $10 a month plan, and using rclone on an ubuntu server I am now transferring all my data in a much faster speed then I at home, without putting any strain on my network. The whole transfer should be done within a week or two.\n\nWhile it takes a little bit of knowhow with Linux or some research I got everything running within a few hours.\n\nIf anyone would like to know more leave a comment, I'm happy to help!", "author_fullname": "t2_1lra4qqw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transferring from Google Drive to Dropbox - one way to do it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14yqp1q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689268748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For me too the sad day came when my &amp;quot;unlimited&amp;quot; google enterprise account suddenly had a red banner telling me that I was way over my newly imposed 5tb limit. To stay with google drive I would either have to add more users (5tb per user at $20 a month) or pay an outrageous $300 a month for 10 extra terrabytes.&lt;/p&gt;\n\n&lt;p&gt;After looking around for a bit I figured the best option for unlimited data to backup my NAS (and share stuff with clients etc) was going to be Dropbox (via 3 business accounts at either $72 a month (paid yearly) or $90 a month(paid monthly)). Unfortunately my upload speeds at home leave a lot to desired and it would take months to transfer all my data over + putting a lot of strain on my home internet + having to pay for both services in the meantime...&lt;/p&gt;\n\n&lt;p&gt;The problem is that most data transfer services out there are expensive and transferring it via a custom server on AWS or other similar server providers is also expensive as they often charge you when you go over a certain amount of network traffic (common limits are between 1TB and 5TB a month).&lt;/p&gt;\n\n&lt;p&gt;OVH Cloud to the rescue! I found out that some services, like OVH cloud don&amp;#39;t charge you for data transfer, but instead just limit your transfer bandwidth (depending on the server you choose 250mbits, 500mbits or more which is way better than my sad 5mbits up at home). I went with their just over $10 a month plan, and using rclone on an ubuntu server I am now transferring all my data in a much faster speed then I at home, without putting any strain on my network. The whole transfer should be done within a week or two.&lt;/p&gt;\n\n&lt;p&gt;While it takes a little bit of knowhow with Linux or some research I got everything running within a few hours.&lt;/p&gt;\n\n&lt;p&gt;If anyone would like to know more leave a comment, I&amp;#39;m happy to help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14yqp1q", "is_robot_indexable": true, "report_reasons": null, "author": "thebrokemonkey", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14yqp1q/transferring_from_google_drive_to_dropbox_one_way/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14yqp1q/transferring_from_google_drive_to_dropbox_one_way/", "subreddit_subscribers": 692476, "created_utc": 1689268748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, \n\nI was wondering if anyone knew the best and safest ways to download youtube videos to either download or convert YouTube videos to wav format? A lot of these sites ans apparently some apps and programs have some unsafe things on them and I want to download some videos. So if anyone has any advice and suggestions and recommendations I would very much appreciate it.", "author_fullname": "t2_ihklyn72", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best and Safest YouTube Downloader? YouTube to WAV downloader/Converter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ynzp8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689262371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone knew the best and safest ways to download youtube videos to either download or convert YouTube videos to wav format? A lot of these sites ans apparently some apps and programs have some unsafe things on them and I want to download some videos. So if anyone has any advice and suggestions and recommendations I would very much appreciate it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ynzp8", "is_robot_indexable": true, "report_reasons": null, "author": "Charming_Cycle8363", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ynzp8/best_and_safest_youtube_downloader_youtube_to_wav/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14ynzp8/best_and_safest_youtube_downloader_youtube_to_wav/", "subreddit_subscribers": 692476, "created_utc": 1689262371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey There,\n\nI'm looking for a solution that can be used, reliably, for cloud based file transfers in high volume. 1000's of files per day being moved around, maybe more, from multiple people.\n\nFor background - I'm QA Manager for an Energy Consulting company. This role involves QA'ing work, submitting work, managing our data storage - etc.\n\nSo far my company has used OneDrive and Sharepoint. We've got around 6 Sharepoints within our business OneDrive. Reason is there are things certain employees do/do not/cannot have access to for various reasons. I'm regularly transferring files from one Sharepoint to the other, and am constantly running into major Sync issues. \n\nI'd estimate we have close to 600,000+ files on our OneDrive / Sharepoints at any given time - from what I understand, we are constantly running into issues where an employee's OneDrive disconnects or stops syncing, and since we're moving so many files - we don't notice until there's a big mess to deal with. On top of this - it's just not functional for us to wait for each individual file to transfer before moving to the next - we'd get nothing accomplished. \n\nI can't seem to find any better \"cloud\" solutions - so I was looking at some sort of solution for myself - as I move, by far, the most amount of files. Most our other employees are simply submitting files to one location and that's it.\n\nWhat I've been considering is:\n\nAn NAS dock that syncs to our Business OneDrive, the files on our OneDrive/Sharepoints would download locally to the NAS, which I could then work / move files within the NAS without worrying about sync issues to OneDrive. At the end of each day I would sync the changes that occurred in the NAS drives to our OneDrive/Sharepoint. This would allow me to work throughout the day on local files, without worrying about OneDrive syncing all day long - it would only happen at the end of each day, and when new stuff was added to OneDrive by our employees.\n\nIssue:\n\nI'm not super familiar with this stuff. I'm not sure if my solution above actually does any good - as things ultimately still need to push to OneDrive/Sharepoint at one point or another. The only other option I can think of is that us, as a company, purchase a dedicated server and connect multiple NAS docks to it, and have all our employees connect via our own internal serves, and completely remove the OneDrive aspect.\n\nAll I know is I need a way to transfer thousands of files each day between multiple locations - that other employees can also access from their own computers; while not constantly having to solve sync issues/restore deleted and corrupted files/wait extended periods of times for files to transfer; etc. Maybe I'm asking for an impossible solution, idk.\n\nAny advice is appreciated", "author_fullname": "t2_3wvent2h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solution for High Volume File Transfers and Data Storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ylgp8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689256290.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey There,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a solution that can be used, reliably, for cloud based file transfers in high volume. 1000&amp;#39;s of files per day being moved around, maybe more, from multiple people.&lt;/p&gt;\n\n&lt;p&gt;For background - I&amp;#39;m QA Manager for an Energy Consulting company. This role involves QA&amp;#39;ing work, submitting work, managing our data storage - etc.&lt;/p&gt;\n\n&lt;p&gt;So far my company has used OneDrive and Sharepoint. We&amp;#39;ve got around 6 Sharepoints within our business OneDrive. Reason is there are things certain employees do/do not/cannot have access to for various reasons. I&amp;#39;m regularly transferring files from one Sharepoint to the other, and am constantly running into major Sync issues. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d estimate we have close to 600,000+ files on our OneDrive / Sharepoints at any given time - from what I understand, we are constantly running into issues where an employee&amp;#39;s OneDrive disconnects or stops syncing, and since we&amp;#39;re moving so many files - we don&amp;#39;t notice until there&amp;#39;s a big mess to deal with. On top of this - it&amp;#39;s just not functional for us to wait for each individual file to transfer before moving to the next - we&amp;#39;d get nothing accomplished. &lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t seem to find any better &amp;quot;cloud&amp;quot; solutions - so I was looking at some sort of solution for myself - as I move, by far, the most amount of files. Most our other employees are simply submitting files to one location and that&amp;#39;s it.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;ve been considering is:&lt;/p&gt;\n\n&lt;p&gt;An NAS dock that syncs to our Business OneDrive, the files on our OneDrive/Sharepoints would download locally to the NAS, which I could then work / move files within the NAS without worrying about sync issues to OneDrive. At the end of each day I would sync the changes that occurred in the NAS drives to our OneDrive/Sharepoint. This would allow me to work throughout the day on local files, without worrying about OneDrive syncing all day long - it would only happen at the end of each day, and when new stuff was added to OneDrive by our employees.&lt;/p&gt;\n\n&lt;p&gt;Issue:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not super familiar with this stuff. I&amp;#39;m not sure if my solution above actually does any good - as things ultimately still need to push to OneDrive/Sharepoint at one point or another. The only other option I can think of is that us, as a company, purchase a dedicated server and connect multiple NAS docks to it, and have all our employees connect via our own internal serves, and completely remove the OneDrive aspect.&lt;/p&gt;\n\n&lt;p&gt;All I know is I need a way to transfer thousands of files each day between multiple locations - that other employees can also access from their own computers; while not constantly having to solve sync issues/restore deleted and corrupted files/wait extended periods of times for files to transfer; etc. Maybe I&amp;#39;m asking for an impossible solution, idk.&lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ylgp8", "is_robot_indexable": true, "report_reasons": null, "author": "OMGCamCole", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ylgp8/solution_for_high_volume_file_transfers_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14ylgp8/solution_for_high_volume_file_transfers_and_data/", "subreddit_subscribers": 692476, "created_utc": 1689256290.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, as mentioned in the title I am looking to back my personal files up on my local NAS, what software do you recommend? (Synology NAS)", "author_fullname": "t2_51ryxoj2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What software to back up home PC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14yewdi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689236614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, as mentioned in the title I am looking to back my personal files up on my local NAS, what software do you recommend? (Synology NAS)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "20TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14yewdi", "is_robot_indexable": true, "report_reasons": null, "author": "Echo_Theta", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14yewdi/what_software_to_back_up_home_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14yewdi/what_software_to_back_up_home_pc/", "subreddit_subscribers": 692476, "created_utc": 1689236614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Have a drive that was throwing up smart errors so thought I'd do an extended test and see what it found. Started it several days ago and for the last 3 days now it's been sat at 10% remaining. How can I tell if it's still going or something has crashed. The drive is currently empty and unformatted.\n\nAlso how worried should I be about the errors it has shown?\n\n    === START OF INFORMATION SECTION ===\n    Device Model:  WDC WD101EMAZ-11G7DA0\n    Serial Number: VCH0BM3P\n    LU WWN Device Id: 5 000cca 0b0ce431c\n    Firmware Version: 81.00A81\n    User Capacity: 10,000,831,348,736 bytes [10.0 TB]\n    Sector Sizes:  512 bytes logical, 4096 bytes physical\n    Rotation Rate: 5400 rpm\n    Form Factor: 3.5 inches\n    Device is: Not in smartctl database [for details use: -P showall]\n    ATA Version is:  ACS-2, ATA8-ACS T13/1699-D revision 4\n    SATA Version is: SATA 3.2, 6.0 Gb/s (current: 6.0 Gb/s)\n    Local Time is: Thu Jul 13 06:52:36 2023 BST\n    SMART support is: Available - device has SMART capability.\n    SMART support is: Enabled\n    AAM feature is:  Unavailable\n    APM level is:  164 (intermediate level without standby)\n    Rd look-ahead is: Enabled\n    Write cache is:  Enabled\n    DSN feature is:  Unavailable\n    ATA Security is: Disabled, NOT FROZEN [SEC1]\n    Wt Cache Reorder: Enabled\n    \n    === START OF READ SMART DATA SECTION ===\n    SMART overall-health self-assessment test result: PASSED\n    General SMART Values:\n    Offline data collection status: (0x82) Offline data collection activity\n    was completed without error.\n    Auto Offline Data Collection: Enabled.\n    Self-test execution status: ( 241) Self-test routine in progress...\n    10% of test remaining.\n    Total time to complete Offline\n    data collection:  (  87) seconds.\n    Offline data collection\n    capabilities:   (0x5b) SMART execute Offline immediate.\n    Auto Offline data collection on/off support.\n    Suspend Offline collection upon new\n    command.\n    Offline surface scan supported.\n    Self-test supported.\n    No Conveyance Self-test supported.\n    Selective Self-test supported.\n    SMART capabilities: (0x0003) Saves SMART data before entering\n    power-saving mode.\n    Supports SMART auto save timer.\n    Error logging capability: (0x01) Error logging supported.\n    General Purpose Logging supported.\n    Short self-test routine\n    recommended polling time:   (  2) minutes.\n    Extended self-test routine\n    recommended polling time:   (1064) minutes.\n    SCT capabilities:   (0x003d) SCT Status supported.\n    SCT Error Recovery Control supported.\n    SCT Feature Control supported.\n    SCT Data Table supported.\n    SMART Attributes Data Structure revision number: 16\n    Vendor Specific SMART Attributes with Thresholds:\n    ID# ATTRIBUTE_NAME FLAGS VALUE WORST THRESH FAIL RAW_VALUE\n    1 Raw_Read_Error_Rate  PO-R--  100  100  016 - 0\n    2 Throughput_Performance --S---  128  128  054 - 108\n    3 Spin_Up_Time POS---  137  137  024 - 635 (Average 548)\n    4 Start_Stop_Count -O--C-  100  100  000 - 75\n    5 Reallocated_Sector_Ct  PO--CK  100  100  005 - 0\n    7 Seek_Error_Rate  -O-R--  100  100  067 - 0\n    8 Seek_Time_Performance  --S---  128  128  020 - 18\n    9 Power_On_Hours -O--C-  098  098  000 - 14235\n    10 Spin_Retry_Count -O--C-  100  100  060 - 0\n    12 Power_Cycle_Count  -O--CK  100  100  000 - 69\n    192 Power-Off_Retract_Count -O--CK  077  077  000 - 27740\n    193 Load_Cycle_Count -O--C-  077  077  000 - 27740\n    194 Temperature_Celsius  -O----  122  122  000 - 53 (Min/Max 21/57)\n    196 Reallocated_Event_Count -O--CK  100  100  000 - 0\n    197 Current_Pending_Sector -O---K  100  100  000 - 0\n    198 Offline_Uncorrectable  ---R--  100  100  000 - 0\n    199 UDMA_CRC_Error_Count -O-R--  200  200  000 - 0\n    ||||||_ K auto-keep\n    |||||__ C event count\n    ||||___ R error rate\n    |||____ S speed/performance\n    ||_____ O updated online\n    |______ P prefailure warning\n    General Purpose Log Directory Version 1\n    SMART  Log Directory Version 1 [multi-sector log support]\n    Address Access r/W  Size Description\n    0x00  GPL,SL r/O 1 Log Directory\n    0x01  SL r/O 1 Summary SMART error log\n    0x02  SL r/O 1 Comprehensive SMART error log\n    0x03  GPL  r/O 1 Ext. Comprehensive SMART error log\n    0x04  GPL  r/O 256 Device Statistics log\n    0x04  SL r/O 255 Device Statistics log\n    0x06  SL r/O 1 SMART self-test log\n    0x07  GPL  r/O 1 Extended self-test log\n    0x08  GPL  r/O 2 Power Conditions log\n    0x09  SL r/W 1 Selective self-test log\n    0x0c  GPL  r/O  5501 Pending Defects log\n    0x10  GPL  r/O 1 NCQ Command Error log\n    0x11  GPL  r/O 1 SATA Phy Event Counters log\n    0x12  GPL  r/O 1 SATA NCQ Non-Data log\n    0x13  GPL  r/O 1 SATA NCQ Send and Receive log\n    0x15  GPL  r/W 1 Rebuild Assist log\n    0x21  GPL  r/O 1 Write stream error log\n    0x22  GPL  r/O 1 Read stream error log\n    0x24  GPL  r/O 256 Current Device Internal Status Data log\n    0x25  GPL  r/O 256 Saved Device Internal Status Data log\n    0x2f  GPL  - 1 Set Sector Configuration\n    0x30  GPL,SL r/O 9 IDENTIFY DEVICE data log\n    0x80-0x9f GPL,SL r/W  16 Host vendor specific log\n    0xe0  GPL,SL r/W 1 SCT Command/Status\n    0xe1  GPL,SL r/W 1 SCT Data Transfer\n    SMART Extended Comprehensive Error Log Version: 1 (1 sectors)\n    Device Error Count: 165 (device log contains only the most recent 4 errors)\n    CR  = Command Register\n    FEATR = Features Register\n    COUNT = Count (was: Sector Count) Register\n    LBA_48 = Upper bytes of LBA High/Mid/Low Registers ] ATA-8\n    LH  = LBA High (was: Cylinder High) Register ]  LBA\n    LM  = LBA Mid (was: Cylinder Low) Register ] Register\n    LL  = LBA Low (was: Sector Number) Register  ]\n    DV  = Device (was: Device/Head) Register\n    DC  = Device Control Register\n    ER  = Error register\n    ST  = Status register\n    Powered_Up_Time is measured from power on, and printed as\n    DDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\n    SS=sec, and sss=millisec. It \"wraps\" after 49.710 days.\n    Error 165 [0] occurred at disk power-on lifetime: 13909 hours (579 days + 13 hours)\n    When the command that caused the error occurred, the device was active or idle.\n    After command completion occurred, registers were:\n    ER -- ST COUNT LBA_48 LH LM LL DV DC\n    -- -- -- == -- == == == -- -- -- -- --\n    40 -- 51 8c f0 00 00 a3 f6 73 10 40 00 Error: UNC 36080 sectors at LBA = 0xa3f67310 = 2750837520\n    Commands leading to the command that caused the error were:\n    CR FEATR COUNT LBA_48 LH LM LL DV DC Powered_Up_Time Command/Feature_Name\n    -- == -- == -- == == == -- -- -- -- -- --------------- --------------------\n    25 00 00 00 00 00 00 a3 f6 00 00 e0 00  05:29:27.421 READ DMA EXT\n    25 00 00 00 00 00 00 a3 f5 00 00 e0 00  05:28:30.118 READ DMA EXT\n    25 00 00 00 00 00 00 a3 f5 00 00 e0 00  05:28:15.777 READ DMA EXT\n    25 00 00 00 00 00 00 a3 f4 00 00 e0 00  05:28:15.639 READ DMA EXT\n    25 00 00 00 00 00 00 a3 f3 00 00 e0 00  05:28:15.494 READ DMA EXT\n    Error 164 [3] occurred at disk power-on lifetime: 13909 hours (579 days + 13 hours)\n    When the command that caused the error occurred, the device was active or idle.\n    After command completion occurred, registers were:\n    ER -- ST COUNT LBA_48 LH LM LL DV DC\n    -- -- -- == -- == == == -- -- -- -- --\n    40 -- 51 77 70 00 00 a3 f5 88 90 40 00 Error: UNC 30576 sectors at LBA = 0xa3f58890 = 2750777488\n    Commands leading to the command that caused the error were:\n    CR FEATR COUNT LBA_48 LH LM LL DV DC Powered_Up_Time Command/Feature_Name\n    -- == -- == -- == == == -- -- -- -- -- --------------- --------------------\n    25 00 00 00 00 00 00 a3 f5 00 00 e0 00  05:28:29.670 READ DMA EXT\n    25 00 00 00 00 00 00 a3 f4 00 00 e0 00  05:28:15.639 READ DMA EXT\n    25 00 00 00 00 00 00 a3 f3 00 00 e0 00  05:28:15.494 READ DMA EXT\n    25 00 00 00 00 00 00 a3 f2 00 00 e0 00  05:28:15.323 READ DMA EXT\n    25 00 00 00 00 00 00 a3 f1 00 00 e0 00  05:28:15.177 READ DMA EXT\n    Error 163 [2] occurred at disk power-on lifetime: 13909 hours (579 days + 13 hours)\n    When the command that caused the error occurred, the device was active or idle.\n    After command completion occurred, registers were:\n    ER -- ST COUNT LBA_48 LH LM LL DV DC\n    -- -- -- == -- == == == -- -- -- -- --\n    40 -- 51 a3 18 00 00 a2 bf 5c e8 40 00 Error: UNC 41752 sectors at LBA = 0xa2bf5ce8 = 2730450152\n    Commands leading to the command that caused the error were:\n    CR FEATR COUNT LBA_48 LH LM LL DV DC Powered_Up_Time Command/Feature_Name\n    -- == -- == -- == == == -- -- -- -- -- --------------- --------------------\n    25 00 00 00 00 00 00 a2 bf 00 00 e0 00  05:26:30.611 READ DMA EXT\n    25 00 00 00 00 00 00 a2 be 00 00 e0 00  05:25:25.680 READ DMA EXT\n    25 00 00 00 00 00 00 a2 be 00 00 e0 00  05:25:11.364 READ DMA EXT\n    25 00 00 00 00 00 00 a2 bd 00 00 e0 00  05:24:40.721 READ DMA EXT\n    25 00 00 00 00 00 00 a2 bd 00 00 e0 00  05:24:26.393 READ DMA EXT\n    Error 162 [1] occurred at disk power-on lifetime: 13909 hours (579 days + 13 hours)\n    When the command that caused the error occurred, the device was active or idle.\n    After command completion occurred, registers were:\n    ER -- ST COUNT LBA_48 LH LM LL DV DC\n    -- -- -- == -- == == == -- -- -- -- --\n    40 -- 51 8b c0 00 00 a2 be 74 40 40 00 Error: UNC 35776 sectors at LBA = 0xa2be7440 = 2730390592\n    Commands leading to the command that caused the error were:\n    CR FEATR COUNT LBA_48 LH LM LL DV DC Powered_Up_Time Command/Feature_Name\n    -- == -- == -- == == == -- -- -- -- -- --------------- --------------------\n    25 00 00 00 00 00 00 a2 be 00 00 e0 00  05:25:25.237 READ DMA EXT\n    25 00 00 00 00 00 00 a2 bd 00 00 e0 00  05:24:40.721 READ DMA EXT\n    25 00 00 00 00 00 00 a2 bd 00 00 e0 00  05:24:26.393 READ DMA EXT\n    25 00 00 00 00 00 00 a2 bc 00 00 e0 00  05:23:55.764 READ DMA EXT\n    25 00 00 00 00 00 00 a2 bc 00 00 e0 00  05:23:41.322 READ DMA EXT\n    SMART Extended Self-test Log Version: 1 (1 sectors)\n    Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error\n    # 1 Extended offline Aborted by host  90%  13936  -\n    # 2 Extended offline Aborted by host  70%  13926  -\n    # 3 Extended offline Aborted by host  80%  13917  -\n    # 4 Short offline  Completed without error  00%  13885  -\n    # 5 Extended offline Interrupted (host reset) 10%  13884  -\n    # 6 Extended offline Interrupted (host reset) 10%  13827  -\n    # 7 Extended offline Interrupted (host reset) 10%  13789  -\n    # 8 Short offline  Completed without error  00%  13669  -\n    # 9 Short offline  Completed without error  00%  13645  -\n    #10 Short offline  Completed without error  00%  13621  -\n    #11 Short offline  Completed without error  00%  13597  -\n    #12 Short offline  Completed without error  00%  13579  -\n    #13 Short offline  Completed without error  00%  13507  -\n    #14 Short offline  Completed without error  00%  13483  -\n    #15 Short offline  Completed without error  00%  13460  -\n    #16 Short offline  Completed without error  00%  13436  -\n    #17 Short offline  Completed without error  00%  13412  -\n    #18 Extended offline Interrupted (host reset) 10%  13394  -\n    #19 Short offline  Completed without error  00%  13340  -\n    SMART Selective self-test log data structure revision number 1\n    SPAN MIN_LBA MAX_LBA CURRENT_TEST_STATUS\n    1 0 0 Not_testing\n    2 0 0 Not_testing\n    3 0 0 Not_testing\n    4 0 0 Not_testing\n    5 0 0 Not_testing\n    Selective self-test flags (0x0):\n    After scanning selected spans, do NOT read-scan remainder of disk.\n    If Selective self-test is pending on power-up, resume after 0 minute delay.\n    SCT Status Version: 3\n    SCT Version (vendor specific):  256 (0x0100)\n    Device State: DST executing in background (3)\n    Current Temperature: 53 Celsius\n    Power Cycle Min/Max Temperature:  32/55 Celsius\n    Lifetime Min/Max Temperature:  21/57 Celsius\n    Under/Over Temperature Limit Count:  0/0\n    SCT Temperature History Version:  2\n    Temperature Sampling Period:  1 minute\n    Temperature Logging Interval: 1 minute\n    Min/Max recommended Temperature: 0/65 Celsius\n    Min/Max Temperature Limit:  -40/70 Celsius\n    Temperature History Size (Index): 128 (11)\n    Index Estimated Time  Temperature Celsius\n    12 2023-07-13 04:45 53 **********************************\n    ... ..( 3 skipped). .. **********************************\n    16 2023-07-13 04:49 53 **********************************\n    17 2023-07-13 04:50 54 ***********************************\n    18 2023-07-13 04:51 54 ***********************************\n    19 2023-07-13 04:52 53 **********************************\n    ... ..(119 skipped). .. **********************************\n    11 2023-07-13 06:52 53 **********************************\n    SCT Error Recovery Control:\n    Read:  70 (7.0 seconds)\n    Write:  70 (7.0 seconds)\n    Device Statistics (GP Log 0x04)\n    Page Offset Size Value Flags Description\n    0x01 ===== =  = === == General Statistics (rev 1) ==\n    0x01 0x008 4 69 --- Lifetime Power-On Resets\n    0x01 0x010 4  14235 --- Power-on Hours\n    0x01 0x018 6 145208591805 --- Logical Sectors Written\n    0x01 0x020 6  285865160 --- Number of Write Commands\n    0x01 0x028 6 804927873479 --- Logical Sectors Read\n    0x01 0x030 6 1557125462 --- Number of Read Commands\n    0x01 0x038 6  51249588400 --- Date and Time TimeStamp\n    0x03 ===== =  = === == Rotating Media Statistics (rev 1) ==\n    0x03 0x008 4 9618 --- Spindle Motor Power-on Hours\n    0x03 0x010 4 9618 --- Head Flying Hours\n    0x03 0x018 4  27740 --- Head Load Events\n    0x03 0x020 4  0 --- Number of Reallocated Logical Sectors\n    0x03 0x028 4  2527063 --- Read Recovery Attempts\n    0x03 0x030 4  0 --- Number of Mechanical Start Failures\n    0x04 ===== =  = === == General Errors Statistics (rev 1) ==\n    0x04 0x008 4  165 --- Number of Reported Uncorrectable Errors\n    0x04 0x010 4  9 --- Resets Between Cmd Acceptance and Completion\n    0x05 ===== =  = === == Temperature Statistics (rev 1) ==\n    0x05 0x008 1 53 --- Current Temperature\n    0x05 0x010 1 52 N-- Average Short Term Temperature\n    0x05 0x018 1 49 N-- Average Long Term Temperature\n    0x05 0x020 1 57 --- Highest Temperature\n    0x05 0x028 1 21 --- Lowest Temperature\n    0x05 0x030 1 54 N-- Highest Average Short Term Temperature\n    0x05 0x038 1 24 N-- Lowest Average Short Term Temperature\n    0x05 0x040 1 49 N-- Highest Average Long Term Temperature\n    0x05 0x048 1 25 N-- Lowest Average Long Term Temperature\n    0x05 0x050 4  0 --- Time in Over-Temperature\n    0x05 0x058 1 65 --- Specified Maximum Operating Temperature\n    0x05 0x060 4  0 --- Time in Under-Temperature\n    0x05 0x068 1  0 --- Specified Minimum Operating Temperature\n    0x06 ===== =  = === == Transport Statistics (rev 1) ==\n    0x06 0x008 4  482 --- Number of Hardware Resets\n    0x06 0x010 4  178 --- Number of ASR Events\n    0x06 0x018 4  0 --- Number of Interface CRC Errors\n    0xff ===== =  = === == Vendor Specific Statistics (rev 1) ==\n    |||_ C monitored condition met\n    ||__ D supports DSN\n    |___ N normalized value\n    Pending Defects log (GP Log 0x0c)\n    No Defects Logged\n    SATA Phy Event Counters (GP Log 0x11)\n    ID Size  Value Description\n    0x0001 2 0 Command failed due to ICRC error\n    0x0002 2 0 R_ERR response for data FIS\n    0x0003 2 0 R_ERR response for device-to-host data FIS\n    0x0004 2 0 R_ERR response for host-to-device data FIS\n    0x0005 2 0 R_ERR response for non-data FIS\n    0x0006 2 0 R_ERR response for device-to-host non-data FIS\n    0x0007 2 0 R_ERR response for host-to-device non-data FIS\n    0x0008 2 0 Device-to-host non-data FIS retries\n    0x0009 2 7 Transition from drive PhyRdy to drive PhyNRdy\n    0x000a 2 5 Device-to-host register FISes sent due to a COMRESET\n    0x000b 2 0 CRC errors within host-to-device FIS\n    0x000d 2 0 Non-CRC errors within host-to-device FIS\n\n&amp;#x200B;", "author_fullname": "t2_fwwo7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "S.M.A.R.T stuck at 10%", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ycc8p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689227934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have a drive that was throwing up smart errors so thought I&amp;#39;d do an extended test and see what it found. Started it several days ago and for the last 3 days now it&amp;#39;s been sat at 10% remaining. How can I tell if it&amp;#39;s still going or something has crashed. The drive is currently empty and unformatted.&lt;/p&gt;\n\n&lt;p&gt;Also how worried should I be about the errors it has shown?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;=== START OF INFORMATION SECTION ===\nDevice Model:  WDC WD101EMAZ-11G7DA0\nSerial Number: VCH0BM3P\nLU WWN Device Id: 5 000cca 0b0ce431c\nFirmware Version: 81.00A81\nUser Capacity: 10,000,831,348,736 bytes [10.0 TB]\nSector Sizes:  512 bytes logical, 4096 bytes physical\nRotation Rate: 5400 rpm\nForm Factor: 3.5 inches\nDevice is: Not in smartctl database [for details use: -P showall]\nATA Version is:  ACS-2, ATA8-ACS T13/1699-D revision 4\nSATA Version is: SATA 3.2, 6.0 Gb/s (current: 6.0 Gb/s)\nLocal Time is: Thu Jul 13 06:52:36 2023 BST\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\nAAM feature is:  Unavailable\nAPM level is:  164 (intermediate level without standby)\nRd look-ahead is: Enabled\nWrite cache is:  Enabled\nDSN feature is:  Unavailable\nATA Security is: Disabled, NOT FROZEN [SEC1]\nWt Cache Reorder: Enabled\n\n=== START OF READ SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\nGeneral SMART Values:\nOffline data collection status: (0x82) Offline data collection activity\nwas completed without error.\nAuto Offline Data Collection: Enabled.\nSelf-test execution status: ( 241) Self-test routine in progress...\n10% of test remaining.\nTotal time to complete Offline\ndata collection:  (  87) seconds.\nOffline data collection\ncapabilities:   (0x5b) SMART execute Offline immediate.\nAuto Offline data collection on/off support.\nSuspend Offline collection upon new\ncommand.\nOffline surface scan supported.\nSelf-test supported.\nNo Conveyance Self-test supported.\nSelective Self-test supported.\nSMART capabilities: (0x0003) Saves SMART data before entering\npower-saving mode.\nSupports SMART auto save timer.\nError logging capability: (0x01) Error logging supported.\nGeneral Purpose Logging supported.\nShort self-test routine\nrecommended polling time:   (  2) minutes.\nExtended self-test routine\nrecommended polling time:   (1064) minutes.\nSCT capabilities:   (0x003d) SCT Status supported.\nSCT Error Recovery Control supported.\nSCT Feature Control supported.\nSCT Data Table supported.\nSMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME FLAGS VALUE WORST THRESH FAIL RAW_VALUE\n1 Raw_Read_Error_Rate  PO-R--  100  100  016 - 0\n2 Throughput_Performance --S---  128  128  054 - 108\n3 Spin_Up_Time POS---  137  137  024 - 635 (Average 548)\n4 Start_Stop_Count -O--C-  100  100  000 - 75\n5 Reallocated_Sector_Ct  PO--CK  100  100  005 - 0\n7 Seek_Error_Rate  -O-R--  100  100  067 - 0\n8 Seek_Time_Performance  --S---  128  128  020 - 18\n9 Power_On_Hours -O--C-  098  098  000 - 14235\n10 Spin_Retry_Count -O--C-  100  100  060 - 0\n12 Power_Cycle_Count  -O--CK  100  100  000 - 69\n192 Power-Off_Retract_Count -O--CK  077  077  000 - 27740\n193 Load_Cycle_Count -O--C-  077  077  000 - 27740\n194 Temperature_Celsius  -O----  122  122  000 - 53 (Min/Max 21/57)\n196 Reallocated_Event_Count -O--CK  100  100  000 - 0\n197 Current_Pending_Sector -O---K  100  100  000 - 0\n198 Offline_Uncorrectable  ---R--  100  100  000 - 0\n199 UDMA_CRC_Error_Count -O-R--  200  200  000 - 0\n||||||_ K auto-keep\n|||||__ C event count\n||||___ R error rate\n|||____ S speed/performance\n||_____ O updated online\n|______ P prefailure warning\nGeneral Purpose Log Directory Version 1\nSMART  Log Directory Version 1 [multi-sector log support]\nAddress Access r/W  Size Description\n0x00  GPL,SL r/O 1 Log Directory\n0x01  SL r/O 1 Summary SMART error log\n0x02  SL r/O 1 Comprehensive SMART error log\n0x03  GPL  r/O 1 Ext. Comprehensive SMART error log\n0x04  GPL  r/O 256 Device Statistics log\n0x04  SL r/O 255 Device Statistics log\n0x06  SL r/O 1 SMART self-test log\n0x07  GPL  r/O 1 Extended self-test log\n0x08  GPL  r/O 2 Power Conditions log\n0x09  SL r/W 1 Selective self-test log\n0x0c  GPL  r/O  5501 Pending Defects log\n0x10  GPL  r/O 1 NCQ Command Error log\n0x11  GPL  r/O 1 SATA Phy Event Counters log\n0x12  GPL  r/O 1 SATA NCQ Non-Data log\n0x13  GPL  r/O 1 SATA NCQ Send and Receive log\n0x15  GPL  r/W 1 Rebuild Assist log\n0x21  GPL  r/O 1 Write stream error log\n0x22  GPL  r/O 1 Read stream error log\n0x24  GPL  r/O 256 Current Device Internal Status Data log\n0x25  GPL  r/O 256 Saved Device Internal Status Data log\n0x2f  GPL  - 1 Set Sector Configuration\n0x30  GPL,SL r/O 9 IDENTIFY DEVICE data log\n0x80-0x9f GPL,SL r/W  16 Host vendor specific log\n0xe0  GPL,SL r/W 1 SCT Command/Status\n0xe1  GPL,SL r/W 1 SCT Data Transfer\nSMART Extended Comprehensive Error Log Version: 1 (1 sectors)\nDevice Error Count: 165 (device log contains only the most recent 4 errors)\nCR  = Command Register\nFEATR = Features Register\nCOUNT = Count (was: Sector Count) Register\nLBA_48 = Upper bytes of LBA High/Mid/Low Registers ] ATA-8\nLH  = LBA High (was: Cylinder High) Register ]  LBA\nLM  = LBA Mid (was: Cylinder Low) Register ] Register\nLL  = LBA Low (was: Sector Number) Register  ]\nDV  = Device (was: Device/Head) Register\nDC  = Device Control Register\nER  = Error register\nST  = Status register\nPowered_Up_Time is measured from power on, and printed as\nDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\nSS=sec, and sss=millisec. It &amp;quot;wraps&amp;quot; after 49.710 days.\nError 165 [0] occurred at disk power-on lifetime: 13909 hours (579 days + 13 hours)\nWhen the command that caused the error occurred, the device was active or idle.\nAfter command completion occurred, registers were:\nER -- ST COUNT LBA_48 LH LM LL DV DC\n-- -- -- == -- == == == -- -- -- -- --\n40 -- 51 8c f0 00 00 a3 f6 73 10 40 00 Error: UNC 36080 sectors at LBA = 0xa3f67310 = 2750837520\nCommands leading to the command that caused the error were:\nCR FEATR COUNT LBA_48 LH LM LL DV DC Powered_Up_Time Command/Feature_Name\n-- == -- == -- == == == -- -- -- -- -- --------------- --------------------\n25 00 00 00 00 00 00 a3 f6 00 00 e0 00  05:29:27.421 READ DMA EXT\n25 00 00 00 00 00 00 a3 f5 00 00 e0 00  05:28:30.118 READ DMA EXT\n25 00 00 00 00 00 00 a3 f5 00 00 e0 00  05:28:15.777 READ DMA EXT\n25 00 00 00 00 00 00 a3 f4 00 00 e0 00  05:28:15.639 READ DMA EXT\n25 00 00 00 00 00 00 a3 f3 00 00 e0 00  05:28:15.494 READ DMA EXT\nError 164 [3] occurred at disk power-on lifetime: 13909 hours (579 days + 13 hours)\nWhen the command that caused the error occurred, the device was active or idle.\nAfter command completion occurred, registers were:\nER -- ST COUNT LBA_48 LH LM LL DV DC\n-- -- -- == -- == == == -- -- -- -- --\n40 -- 51 77 70 00 00 a3 f5 88 90 40 00 Error: UNC 30576 sectors at LBA = 0xa3f58890 = 2750777488\nCommands leading to the command that caused the error were:\nCR FEATR COUNT LBA_48 LH LM LL DV DC Powered_Up_Time Command/Feature_Name\n-- == -- == -- == == == -- -- -- -- -- --------------- --------------------\n25 00 00 00 00 00 00 a3 f5 00 00 e0 00  05:28:29.670 READ DMA EXT\n25 00 00 00 00 00 00 a3 f4 00 00 e0 00  05:28:15.639 READ DMA EXT\n25 00 00 00 00 00 00 a3 f3 00 00 e0 00  05:28:15.494 READ DMA EXT\n25 00 00 00 00 00 00 a3 f2 00 00 e0 00  05:28:15.323 READ DMA EXT\n25 00 00 00 00 00 00 a3 f1 00 00 e0 00  05:28:15.177 READ DMA EXT\nError 163 [2] occurred at disk power-on lifetime: 13909 hours (579 days + 13 hours)\nWhen the command that caused the error occurred, the device was active or idle.\nAfter command completion occurred, registers were:\nER -- ST COUNT LBA_48 LH LM LL DV DC\n-- -- -- == -- == == == -- -- -- -- --\n40 -- 51 a3 18 00 00 a2 bf 5c e8 40 00 Error: UNC 41752 sectors at LBA = 0xa2bf5ce8 = 2730450152\nCommands leading to the command that caused the error were:\nCR FEATR COUNT LBA_48 LH LM LL DV DC Powered_Up_Time Command/Feature_Name\n-- == -- == -- == == == -- -- -- -- -- --------------- --------------------\n25 00 00 00 00 00 00 a2 bf 00 00 e0 00  05:26:30.611 READ DMA EXT\n25 00 00 00 00 00 00 a2 be 00 00 e0 00  05:25:25.680 READ DMA EXT\n25 00 00 00 00 00 00 a2 be 00 00 e0 00  05:25:11.364 READ DMA EXT\n25 00 00 00 00 00 00 a2 bd 00 00 e0 00  05:24:40.721 READ DMA EXT\n25 00 00 00 00 00 00 a2 bd 00 00 e0 00  05:24:26.393 READ DMA EXT\nError 162 [1] occurred at disk power-on lifetime: 13909 hours (579 days + 13 hours)\nWhen the command that caused the error occurred, the device was active or idle.\nAfter command completion occurred, registers were:\nER -- ST COUNT LBA_48 LH LM LL DV DC\n-- -- -- == -- == == == -- -- -- -- --\n40 -- 51 8b c0 00 00 a2 be 74 40 40 00 Error: UNC 35776 sectors at LBA = 0xa2be7440 = 2730390592\nCommands leading to the command that caused the error were:\nCR FEATR COUNT LBA_48 LH LM LL DV DC Powered_Up_Time Command/Feature_Name\n-- == -- == -- == == == -- -- -- -- -- --------------- --------------------\n25 00 00 00 00 00 00 a2 be 00 00 e0 00  05:25:25.237 READ DMA EXT\n25 00 00 00 00 00 00 a2 bd 00 00 e0 00  05:24:40.721 READ DMA EXT\n25 00 00 00 00 00 00 a2 bd 00 00 e0 00  05:24:26.393 READ DMA EXT\n25 00 00 00 00 00 00 a2 bc 00 00 e0 00  05:23:55.764 READ DMA EXT\n25 00 00 00 00 00 00 a2 bc 00 00 e0 00  05:23:41.322 READ DMA EXT\nSMART Extended Self-test Log Version: 1 (1 sectors)\nNum Test_Description Status Remaining LifeTime(hours) LBA_of_first_error\n# 1 Extended offline Aborted by host  90%  13936  -\n# 2 Extended offline Aborted by host  70%  13926  -\n# 3 Extended offline Aborted by host  80%  13917  -\n# 4 Short offline  Completed without error  00%  13885  -\n# 5 Extended offline Interrupted (host reset) 10%  13884  -\n# 6 Extended offline Interrupted (host reset) 10%  13827  -\n# 7 Extended offline Interrupted (host reset) 10%  13789  -\n# 8 Short offline  Completed without error  00%  13669  -\n# 9 Short offline  Completed without error  00%  13645  -\n#10 Short offline  Completed without error  00%  13621  -\n#11 Short offline  Completed without error  00%  13597  -\n#12 Short offline  Completed without error  00%  13579  -\n#13 Short offline  Completed without error  00%  13507  -\n#14 Short offline  Completed without error  00%  13483  -\n#15 Short offline  Completed without error  00%  13460  -\n#16 Short offline  Completed without error  00%  13436  -\n#17 Short offline  Completed without error  00%  13412  -\n#18 Extended offline Interrupted (host reset) 10%  13394  -\n#19 Short offline  Completed without error  00%  13340  -\nSMART Selective self-test log data structure revision number 1\nSPAN MIN_LBA MAX_LBA CURRENT_TEST_STATUS\n1 0 0 Not_testing\n2 0 0 Not_testing\n3 0 0 Not_testing\n4 0 0 Not_testing\n5 0 0 Not_testing\nSelective self-test flags (0x0):\nAfter scanning selected spans, do NOT read-scan remainder of disk.\nIf Selective self-test is pending on power-up, resume after 0 minute delay.\nSCT Status Version: 3\nSCT Version (vendor specific):  256 (0x0100)\nDevice State: DST executing in background (3)\nCurrent Temperature: 53 Celsius\nPower Cycle Min/Max Temperature:  32/55 Celsius\nLifetime Min/Max Temperature:  21/57 Celsius\nUnder/Over Temperature Limit Count:  0/0\nSCT Temperature History Version:  2\nTemperature Sampling Period:  1 minute\nTemperature Logging Interval: 1 minute\nMin/Max recommended Temperature: 0/65 Celsius\nMin/Max Temperature Limit:  -40/70 Celsius\nTemperature History Size (Index): 128 (11)\nIndex Estimated Time  Temperature Celsius\n12 2023-07-13 04:45 53 **********************************\n... ..( 3 skipped). .. **********************************\n16 2023-07-13 04:49 53 **********************************\n17 2023-07-13 04:50 54 ***********************************\n18 2023-07-13 04:51 54 ***********************************\n19 2023-07-13 04:52 53 **********************************\n... ..(119 skipped). .. **********************************\n11 2023-07-13 06:52 53 **********************************\nSCT Error Recovery Control:\nRead:  70 (7.0 seconds)\nWrite:  70 (7.0 seconds)\nDevice Statistics (GP Log 0x04)\nPage Offset Size Value Flags Description\n0x01 ===== =  = === == General Statistics (rev 1) ==\n0x01 0x008 4 69 --- Lifetime Power-On Resets\n0x01 0x010 4  14235 --- Power-on Hours\n0x01 0x018 6 145208591805 --- Logical Sectors Written\n0x01 0x020 6  285865160 --- Number of Write Commands\n0x01 0x028 6 804927873479 --- Logical Sectors Read\n0x01 0x030 6 1557125462 --- Number of Read Commands\n0x01 0x038 6  51249588400 --- Date and Time TimeStamp\n0x03 ===== =  = === == Rotating Media Statistics (rev 1) ==\n0x03 0x008 4 9618 --- Spindle Motor Power-on Hours\n0x03 0x010 4 9618 --- Head Flying Hours\n0x03 0x018 4  27740 --- Head Load Events\n0x03 0x020 4  0 --- Number of Reallocated Logical Sectors\n0x03 0x028 4  2527063 --- Read Recovery Attempts\n0x03 0x030 4  0 --- Number of Mechanical Start Failures\n0x04 ===== =  = === == General Errors Statistics (rev 1) ==\n0x04 0x008 4  165 --- Number of Reported Uncorrectable Errors\n0x04 0x010 4  9 --- Resets Between Cmd Acceptance and Completion\n0x05 ===== =  = === == Temperature Statistics (rev 1) ==\n0x05 0x008 1 53 --- Current Temperature\n0x05 0x010 1 52 N-- Average Short Term Temperature\n0x05 0x018 1 49 N-- Average Long Term Temperature\n0x05 0x020 1 57 --- Highest Temperature\n0x05 0x028 1 21 --- Lowest Temperature\n0x05 0x030 1 54 N-- Highest Average Short Term Temperature\n0x05 0x038 1 24 N-- Lowest Average Short Term Temperature\n0x05 0x040 1 49 N-- Highest Average Long Term Temperature\n0x05 0x048 1 25 N-- Lowest Average Long Term Temperature\n0x05 0x050 4  0 --- Time in Over-Temperature\n0x05 0x058 1 65 --- Specified Maximum Operating Temperature\n0x05 0x060 4  0 --- Time in Under-Temperature\n0x05 0x068 1  0 --- Specified Minimum Operating Temperature\n0x06 ===== =  = === == Transport Statistics (rev 1) ==\n0x06 0x008 4  482 --- Number of Hardware Resets\n0x06 0x010 4  178 --- Number of ASR Events\n0x06 0x018 4  0 --- Number of Interface CRC Errors\n0xff ===== =  = === == Vendor Specific Statistics (rev 1) ==\n|||_ C monitored condition met\n||__ D supports DSN\n|___ N normalized value\nPending Defects log (GP Log 0x0c)\nNo Defects Logged\nSATA Phy Event Counters (GP Log 0x11)\nID Size  Value Description\n0x0001 2 0 Command failed due to ICRC error\n0x0002 2 0 R_ERR response for data FIS\n0x0003 2 0 R_ERR response for device-to-host data FIS\n0x0004 2 0 R_ERR response for host-to-device data FIS\n0x0005 2 0 R_ERR response for non-data FIS\n0x0006 2 0 R_ERR response for device-to-host non-data FIS\n0x0007 2 0 R_ERR response for host-to-device non-data FIS\n0x0008 2 0 Device-to-host non-data FIS retries\n0x0009 2 7 Transition from drive PhyRdy to drive PhyNRdy\n0x000a 2 5 Device-to-host register FISes sent due to a COMRESET\n0x000b 2 0 CRC errors within host-to-device FIS\n0x000d 2 0 Non-CRC errors within host-to-device FIS\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ycc8p", "is_robot_indexable": true, "report_reasons": null, "author": "Wormvortex", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ycc8p/smart_stuck_at_10/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14ycc8p/smart_stuck_at_10/", "subreddit_subscribers": 692476, "created_utc": 1689227934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there any apps that would download my liked YouTube, Instagram, and Twitter posts? I usually \"Like\" a lot of content that I personally find important and interesting, but I just want some software that would automatically crawl my Liked lists and download/screenshot them for me, is that possible?", "author_fullname": "t2_3bnahjfj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate downloading liked YouTube, Instagram, Twitter posts/videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14yrnja", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": "", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689271037.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any apps that would download my liked YouTube, Instagram, and Twitter posts? I usually &amp;quot;Like&amp;quot; a lot of content that I personally find important and interesting, but I just want some software that would automatically crawl my Liked lists and download/screenshot them for me, is that possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14yrnja", "is_robot_indexable": true, "report_reasons": null, "author": "egobamyasi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14yrnja/automate_downloading_liked_youtube_instagram/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14yrnja/automate_downloading_liked_youtube_instagram/", "subreddit_subscribers": 692476, "created_utc": 1689271037.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "External USB 3TB drive.\n\nFiles include small files (like copies of our windows profiles) text, mp3s, etc. All the way to massive 1 to 30 gigabyte single files. In searching online, the comparison between exFat and NTFS seemed unclear. As for allocation size, I understand what it does, but I want to know if there's good research or experience for what a good general size would be without creating massive churn.\n\nI don't use this drive on other systems that don't recognize NTFS. I don't access it frequently, though it will get weekly backups (waterfall method) from my backup computer.", "author_fullname": "t2_5g0ub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I like to backup all my family computers to a networked drive which waterfalls to an external drive. It's millions of files of varying sizes. What file system and allocation unit size would be best?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14yriv9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689270725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;External USB 3TB drive.&lt;/p&gt;\n\n&lt;p&gt;Files include small files (like copies of our windows profiles) text, mp3s, etc. All the way to massive 1 to 30 gigabyte single files. In searching online, the comparison between exFat and NTFS seemed unclear. As for allocation size, I understand what it does, but I want to know if there&amp;#39;s good research or experience for what a good general size would be without creating massive churn.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t use this drive on other systems that don&amp;#39;t recognize NTFS. I don&amp;#39;t access it frequently, though it will get weekly backups (waterfall method) from my backup computer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14yriv9", "is_robot_indexable": true, "report_reasons": null, "author": "suddenly_ponies", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14yriv9/i_like_to_backup_all_my_family_computers_to_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14yriv9/i_like_to_backup_all_my_family_computers_to_a/", "subreddit_subscribers": 692476, "created_utc": 1689270725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone! I would like to have my dashcam\u2019s MICRO SD card password protected once i pop it in my computer but while being able to retain the write features (save videos and overwrite them once the sd card is full) \n\nHow could i do that? Thank you all! Asking here since i have no idea where else to ask", "author_fullname": "t2_tfyu3on", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Making an sd card\u2019s reading function password protected while also retaining write function.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ygb5v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689245525.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689241480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I would like to have my dashcam\u2019s MICRO SD card password protected once i pop it in my computer but while being able to retain the write features (save videos and overwrite them once the sd card is full) &lt;/p&gt;\n\n&lt;p&gt;How could i do that? Thank you all! Asking here since i have no idea where else to ask&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ygb5v", "is_robot_indexable": true, "report_reasons": null, "author": "cannavacciuolo420", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ygb5v/making_an_sd_cards_reading_function_password/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14ygb5v/making_an_sd_cards_reading_function_password/", "subreddit_subscribers": 692476, "created_utc": 1689241480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey.  \nSo the thing is... I like to store all sort of data from my contacts. Being family, friends, past or present work colleagues...  \nOne thing that I keep is their full names, for instance.  \nMy problem is I only use Google contacts and when my phone rings it shows the full name of the person, and I may prefer to see only first and last name, or some nickname, etc. I also feel a bit embarrassed when someone sees that on my phone...  \n\n\nSo, what I thought was having all the data that I want to have, stored somewhere and only sync part of that to google contacts.  \n\n\nAny thoughts / tips on this? Any tool that you use?", "author_fullname": "t2_hhr7wxyo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Having contacts stored and synced with/to google", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14yg2m3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689240649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey.&lt;br/&gt;\nSo the thing is... I like to store all sort of data from my contacts. Being family, friends, past or present work colleagues...&lt;br/&gt;\nOne thing that I keep is their full names, for instance.&lt;br/&gt;\nMy problem is I only use Google contacts and when my phone rings it shows the full name of the person, and I may prefer to see only first and last name, or some nickname, etc. I also feel a bit embarrassed when someone sees that on my phone...  &lt;/p&gt;\n\n&lt;p&gt;So, what I thought was having all the data that I want to have, stored somewhere and only sync part of that to google contacts.  &lt;/p&gt;\n\n&lt;p&gt;Any thoughts / tips on this? Any tool that you use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14yg2m3", "is_robot_indexable": true, "report_reasons": null, "author": "ElectronicOwl01", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14yg2m3/having_contacts_stored_and_synced_withto_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14yg2m3/having_contacts_stored_and_synced_withto_google/", "subreddit_subscribers": 692476, "created_utc": 1689240649.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi y'all. I'm **really** new to this, so I wanted to ask the experts about it.\n\nI have some data on a microSD, which includes some downloaded videos, videos I edited, and maybe some images, and I wanted to continue expanding my library.\n\nBecause the microSD has 64gb of capacity, I wanted to get something bigger, and was thinking of getting either a 4TB NVMe and an enclosure (WD+Dockcase Explorer), or a WD My Passport with the same capacity.\n\nI'm open to advice. TIA.", "author_fullname": "t2_eusj9tufh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External SSD v. NVMe enlcosure. Which one to choose?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14y6rrz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689211513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi y&amp;#39;all. I&amp;#39;m &lt;strong&gt;really&lt;/strong&gt; new to this, so I wanted to ask the experts about it.&lt;/p&gt;\n\n&lt;p&gt;I have some data on a microSD, which includes some downloaded videos, videos I edited, and maybe some images, and I wanted to continue expanding my library.&lt;/p&gt;\n\n&lt;p&gt;Because the microSD has 64gb of capacity, I wanted to get something bigger, and was thinking of getting either a 4TB NVMe and an enclosure (WD+Dockcase Explorer), or a WD My Passport with the same capacity.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m open to advice. TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14y6rrz", "is_robot_indexable": true, "report_reasons": null, "author": "According-Diver-1426", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14y6rrz/external_ssd_v_nvme_enlcosure_which_one_to_choose/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14y6rrz/external_ssd_v_nvme_enlcosure_which_one_to_choose/", "subreddit_subscribers": 692476, "created_utc": 1689211513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have been trialing Bulk Rename Utility to rename files added to a common directory from several different sources that each have different file naming schemes.\n\nThe issue I am running into is with Bulk Rename Utility it isn't readily apparent how I would go about applying different rules to different naming schemes any way other than manually.\n\nFor instance, if I want to rename based on a specific top-level folder I have to set the folder depth manually and adjust it for different configurations when one may be 2 levels deep and another 5 deep.\n\nThere are a variety of different scenarios I could go through, but essentially I am trying to find something that can scan a directory with numerous subfolders and then perform renaming operations based on a variety of rules.\n\nOutside of writing something like a Python script, are there any ready-made utilities with a GUI that someone can recommend for this type of task?\n\nFWIW - Bulk Rename Utility may very well be capable of this type of functionality, but it seems like it was written in the pre-iPhone area when functionality trumped ease of use.", "author_fullname": "t2_ucsb2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rule-based file name parsing and renaming utility?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14y58a1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689206965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have been trialing Bulk Rename Utility to rename files added to a common directory from several different sources that each have different file naming schemes.&lt;/p&gt;\n\n&lt;p&gt;The issue I am running into is with Bulk Rename Utility it isn&amp;#39;t readily apparent how I would go about applying different rules to different naming schemes any way other than manually.&lt;/p&gt;\n\n&lt;p&gt;For instance, if I want to rename based on a specific top-level folder I have to set the folder depth manually and adjust it for different configurations when one may be 2 levels deep and another 5 deep.&lt;/p&gt;\n\n&lt;p&gt;There are a variety of different scenarios I could go through, but essentially I am trying to find something that can scan a directory with numerous subfolders and then perform renaming operations based on a variety of rules.&lt;/p&gt;\n\n&lt;p&gt;Outside of writing something like a Python script, are there any ready-made utilities with a GUI that someone can recommend for this type of task?&lt;/p&gt;\n\n&lt;p&gt;FWIW - Bulk Rename Utility may very well be capable of this type of functionality, but it seems like it was written in the pre-iPhone area when functionality trumped ease of use.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14y58a1", "is_robot_indexable": true, "report_reasons": null, "author": "AstonM77", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14y58a1/rulebased_file_name_parsing_and_renaming_utility/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14y58a1/rulebased_file_name_parsing_and_renaming_utility/", "subreddit_subscribers": 692476, "created_utc": 1689206965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to buy 20 or 22TB for a new 1522+", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xx3eq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_bru5x", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "synology", "selftext": "I picked up the 1522 yesterday on sale and will now upgrade my aging DS412. I\u2019m looking at hard drives and not sure between WD and Seagate. I\u2019ve always purchased WD in the past. I\u2019m looking at the WD prices $320 for 20 TB and $409 (or 2 for $700) on the WD site. Any suggestions? This will be mainly for Plex and photo storage.", "author_fullname": "t2_bru5x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to buy 20 or 22TB for a new 1522+", "link_flair_richtext": [], "subreddit_name_prefixed": "r/synology", "hidden": false, "pwls": null, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xx2yc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "NAS hardware", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689187703.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.synology", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I picked up the 1522 yesterday on sale and will now upgrade my aging DS412. I\u2019m looking at hard drives and not sure between WD and Seagate. I\u2019ve always purchased WD in the past. I\u2019m looking at the WD prices $320 for 20 TB and $409 (or 2 for $700) on the WD site. Any suggestions? This will be mainly for Plex and photo storage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1b45c7c8-4b25-11ed-a1f3-5a29a1a8c4d9", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2s4co", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#cc3600", "id": "14xx2yc", "is_robot_indexable": true, "report_reasons": null, "author": "Stryker412", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/synology/comments/14xx2yc/looking_to_buy_20_or_22tb_for_a_new_1522/", "parent_whitelist_status": null, "stickied": false, "url": "https://old.reddit.com/r/synology/comments/14xx2yc/looking_to_buy_20_or_22tb_for_a_new_1522/", "subreddit_subscribers": 125310, "created_utc": 1689187703.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1689187735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.synology", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/synology/comments/14xx2yc/looking_to_buy_20_or_22tb_for_a_new_1522/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14xx3eq", "is_robot_indexable": true, "report_reasons": null, "author": "Stryker412", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_14xx2yc", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14xx3eq/looking_to_buy_20_or_22tb_for_a_new_1522/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/synology/comments/14xx2yc/looking_to_buy_20_or_22tb_for_a_new_1522/", "subreddit_subscribers": 692476, "created_utc": 1689187735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I will be using it mostly to check out old stuff from my old laptop and PC (10 year+ old)\n\nA single bay one would be more than good enough. \n\n&amp;#x200B;", "author_fullname": "t2_k15s7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended dockinstation for hardrive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14yfrpt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689239869.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689239642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will be using it mostly to check out old stuff from my old laptop and PC (10 year+ old)&lt;/p&gt;\n\n&lt;p&gt;A single bay one would be more than good enough. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14yfrpt", "is_robot_indexable": true, "report_reasons": null, "author": "dBisha", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14yfrpt/recommended_dockinstation_for_hardrive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14yfrpt/recommended_dockinstation_for_hardrive/", "subreddit_subscribers": 692476, "created_utc": 1689239642.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}