{"kind": "Listing", "data": {"after": "t3_1532rt2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_o09cwtfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "the devs chose mongo again smh", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1531jz7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 78, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 78, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1wovt-2KDMfbJTLurQgHZgiNp-IDc2CTIZY-AeUUyLM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689694124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ux9wsli3sqcb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ux9wsli3sqcb1.png?auto=webp&amp;s=3ca5ab02a97dcefb30ae3db2136e41f324562a0e", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://preview.redd.it/ux9wsli3sqcb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d1bbf374af3b3ce4d6a5988b50fb59cf9385feb", "width": 108, "height": 108}, {"url": "https://preview.redd.it/ux9wsli3sqcb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b38fc642829334b4bb4f7cfab52bd61fa4c0d7ed", "width": 216, "height": 216}, {"url": "https://preview.redd.it/ux9wsli3sqcb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d180dd0fbc1bd6e14abc010173bc8eed17f70bd", "width": 320, "height": 320}, {"url": "https://preview.redd.it/ux9wsli3sqcb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a7db6f2ac28cac2ad10b1f476a9b4ad8df0f46f6", "width": 640, "height": 640}, {"url": "https://preview.redd.it/ux9wsli3sqcb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=582c77ccfdb1a204e0acd12139d5cddc5571d76e", "width": 960, "height": 960}, {"url": "https://preview.redd.it/ux9wsli3sqcb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9471b8ef9b34e31d89c891cc415cfb318d8ccfb", "width": 1080, "height": 1080}], "variants": {}, "id": "lmYt0kjBxhR8yCXlum05JPYX3_bpkaVBH7DuKoN5Grg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "1531jz7", "is_robot_indexable": true, "report_reasons": null, "author": "itty-bitty-birdy-tb", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1531jz7/the_devs_chose_mongo_again_smh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ux9wsli3sqcb1.png", "subreddit_subscribers": 116642, "created_utc": 1689694124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vu1osuil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free copy of \"Fundamentals of Data Engineering\" to learn DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_152yp2h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": "transparent", "ups": 51, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GJI0r0i8VjTz63FENoduduO-aG2eafmRRKm-m4dFnog.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689687366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "go.redpanda.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://go.redpanda.com/fundamentals-of-data-engineering", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8AB7VBllU8HKwvPmxMwhhPGsRfEjojCWkNhg-sOfEyA.jpg?auto=webp&amp;s=6f9d8b67339ba2e24729a4a01e4af69d6901708f", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/8AB7VBllU8HKwvPmxMwhhPGsRfEjojCWkNhg-sOfEyA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=236e8977b4b29caf518afabda6cb27e5578688e7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/8AB7VBllU8HKwvPmxMwhhPGsRfEjojCWkNhg-sOfEyA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=67afdf69c3c0cbf2a5888fd982265c8c751faf88", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/8AB7VBllU8HKwvPmxMwhhPGsRfEjojCWkNhg-sOfEyA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5fecab72d37c8cfac0c5625ef320265d80e8a454", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/8AB7VBllU8HKwvPmxMwhhPGsRfEjojCWkNhg-sOfEyA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f977129c4dd4d0d928d0ef1fd8faac749207bddf", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/8AB7VBllU8HKwvPmxMwhhPGsRfEjojCWkNhg-sOfEyA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8cb4b8b9e83333f6fb4cf1e964fc483caf512f4f", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/8AB7VBllU8HKwvPmxMwhhPGsRfEjojCWkNhg-sOfEyA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62b640989f071b0f21f1905b5666d573f4193741", "width": 1080, "height": 565}], "variants": {}, "id": "DvdgYgQFkuNvhWsi5MycPv45U50YDo5lni76VGlxImw"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 150, "id": "award_f44611f1-b89e-46dc-97fe-892280b13b82", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Thank you stranger. Shows the award.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Helpful", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "152yp2h", "is_robot_indexable": true, "report_reasons": null, "author": "TemporaryPoorFrench", "discussion_type": null, "num_comments": 11, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/152yp2h/free_copy_of_fundamentals_of_data_engineering_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://go.redpanda.com/fundamentals-of-data-engineering", "subreddit_subscribers": 116642, "created_utc": 1689687366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at a big tech company as a new grad and my title is technically data engineer, however much of my work is building distributed Python applications for ML and data replication, spark, Java, devops pipelines and infrastructure automation (ansible). If I write sql, it\u2019s always within a Python application and is nothing more than an insert, table creation or simple select query. With my experience, would I be more suited to go into software engineer roles moving forward? Scared of applying to companies that use modern data stack or no-code tools and I am stuck having to re-learn SQL or failing an interview.\n\nEdit: I\u2019d consider myself average at SQL, I\u2019ve passed all-SQL interviews, just worried about my skills atrophying as it\u2019s not something I really enjoy to use and my role doesn\u2019t require it", "author_fullname": "t2_7szv7c66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am a new grad \u201cdata engineer\u201d who barely writes SQL, how will my career be impacted?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_152own3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689657321.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689656717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a big tech company as a new grad and my title is technically data engineer, however much of my work is building distributed Python applications for ML and data replication, spark, Java, devops pipelines and infrastructure automation (ansible). If I write sql, it\u2019s always within a Python application and is nothing more than an insert, table creation or simple select query. With my experience, would I be more suited to go into software engineer roles moving forward? Scared of applying to companies that use modern data stack or no-code tools and I am stuck having to re-learn SQL or failing an interview.&lt;/p&gt;\n\n&lt;p&gt;Edit: I\u2019d consider myself average at SQL, I\u2019ve passed all-SQL interviews, just worried about my skills atrophying as it\u2019s not something I really enjoy to use and my role doesn\u2019t require it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "152own3", "is_robot_indexable": true, "report_reasons": null, "author": "aacreans", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/152own3/i_am_a_new_grad_data_engineer_who_barely_writes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/152own3/i_am_a_new_grad_data_engineer_who_barely_writes/", "subreddit_subscribers": 116642, "created_utc": 1689656717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I study 15 hours a day 7 days a week trying to make my dream a reality. ETL, ELT, coding, learning Python, SQL, reading entire chapters of data engineering books and outlining them on paper and then on the computer to make sure it sticks... Then there are people here asking and basically bragging about having a data engineer position where they do literally nothing. No actual data engineering, maybe some light leg work or job responsibilities, and some minor SQL. \n\n\nWhat I want to know is how. How are people getting these? Where do I get one?", "author_fullname": "t2_vnan4h8y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are people getting into data engineering with basically no responsibilities?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_152w0h7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689680210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I study 15 hours a day 7 days a week trying to make my dream a reality. ETL, ELT, coding, learning Python, SQL, reading entire chapters of data engineering books and outlining them on paper and then on the computer to make sure it sticks... Then there are people here asking and basically bragging about having a data engineer position where they do literally nothing. No actual data engineering, maybe some light leg work or job responsibilities, and some minor SQL. &lt;/p&gt;\n\n&lt;p&gt;What I want to know is how. How are people getting these? Where do I get one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "152w0h7", "is_robot_indexable": true, "report_reasons": null, "author": "Analyst2163", "discussion_type": null, "num_comments": 90, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/152w0h7/how_are_people_getting_into_data_engineering_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/152w0h7/how_are_people_getting_into_data_engineering_with/", "subreddit_subscribers": 116642, "created_utc": 1689680210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TLDR - you mini-interview, we donate: [VideoAsk Mini-Interview](https://www.videoask.com/fisb43vgs) \n\nI'm a founder at a small data startup, and we are looking for input from Data/Analytics/BI Engineers related to the problem our tool is solving; I am a DE myself and am too close to our solution to be objective. \n\n**We decided instead of paying a marketing research group, we would take a shot with the communities we are part of (like this one) and donate our small research budget to good causes instead.** (mini-interview Link is above, here it is again):\n\n[VideoAsk Mini-Interview](https://www.videoask.com/fisb43vgs) \n\nFor every completed (applicable) mini-interview we'll donate $5 to either [code.org](https://code.org) or [phillyPAWS](https://phillypaws.org/) (our local rescue here in Philadelphia) until we run out of $$ (we will shut the survey down before then, no free lunches on our part).   \nby \"applicable\" we mean the respondent is either a data or software professional or works directly with data/software professionals.", "author_fullname": "t2_dilsfh4j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For every Data Professional mini-interview we'll donate $5 to dogs or code.org", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1537puk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689708241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR - you mini-interview, we donate: &lt;a href=\"https://www.videoask.com/fisb43vgs\"&gt;VideoAsk Mini-Interview&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a founder at a small data startup, and we are looking for input from Data/Analytics/BI Engineers related to the problem our tool is solving; I am a DE myself and am too close to our solution to be objective. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;We decided instead of paying a marketing research group, we would take a shot with the communities we are part of (like this one) and donate our small research budget to good causes instead.&lt;/strong&gt; (mini-interview Link is above, here it is again):&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.videoask.com/fisb43vgs\"&gt;VideoAsk Mini-Interview&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;For every completed (applicable) mini-interview we&amp;#39;ll donate $5 to either &lt;a href=\"https://code.org\"&gt;code.org&lt;/a&gt; or &lt;a href=\"https://phillypaws.org/\"&gt;phillyPAWS&lt;/a&gt; (our local rescue here in Philadelphia) until we run out of $$ (we will shut the survey down before then, no free lunches on our part).&lt;br/&gt;\nby &amp;quot;applicable&amp;quot; we mean the respondent is either a data or software professional or works directly with data/software professionals.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?auto=webp&amp;s=54eb457a178d1a2f8d5b8ec0c59601357ed9c5df", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=47391c61e0302e14c4c7a003ea94a70c5327fc17", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=885c32075812ee41c63f825a070b6967f1e016a0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf00bd66185d4fc18e7211516a5619f9c01e1fd8", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac037a629514a6c2f4b6b4fde66a556edb2db7d3", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=44882261d6778024f5348d95f10764cda7f9b0bb", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aca94bf69b7fd3c42ff2d3307257d37643e08a74", "width": 1080, "height": 567}], "variants": {}, "id": "_eefME0VHeiJVwdc9ToiHwgOJ9nypQUidjH9mzYLfys"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1537puk", "is_robot_indexable": true, "report_reasons": null, "author": "sprintymcsprintface", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1537puk/for_every_data_professional_miniinterview_well/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1537puk/for_every_data_professional_miniinterview_well/", "subreddit_subscribers": 116642, "created_utc": 1689708241.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Two years ago I transitioned from Systems Analyst (app dev) to Data Engineer.  I am the only Data Engineer in our company.  When I transitioned, not only did I continue maintaining all of my legacy projects, but I have since taken over all cloud app development in Azure, designed and maintained our API Management Gateways for third-party devs and vendors (multiple environments, dozens of API products), created numerous backend APIs, implemented most file/data exchange processes between our company and third-parties, created numerous ETL processes which includes many replication jobs from third-parties apps, etc.  I did much of this as a level 1 DE (I am now level 2).  I am trying to understand how much of this is actual Data Engineering and how much of it is something else (Cloud App Development?).  I also am curious what level I should be.\n\nAdditional context:  5+ years with the company.", "author_fullname": "t2_4d6r2kn6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is my role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_152z2bq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689689718.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689688281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Two years ago I transitioned from Systems Analyst (app dev) to Data Engineer.  I am the only Data Engineer in our company.  When I transitioned, not only did I continue maintaining all of my legacy projects, but I have since taken over all cloud app development in Azure, designed and maintained our API Management Gateways for third-party devs and vendors (multiple environments, dozens of API products), created numerous backend APIs, implemented most file/data exchange processes between our company and third-parties, created numerous ETL processes which includes many replication jobs from third-parties apps, etc.  I did much of this as a level 1 DE (I am now level 2).  I am trying to understand how much of this is actual Data Engineering and how much of it is something else (Cloud App Development?).  I also am curious what level I should be.&lt;/p&gt;\n\n&lt;p&gt;Additional context:  5+ years with the company.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "152z2bq", "is_robot_indexable": true, "report_reasons": null, "author": "1_0-0_1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/152z2bq/what_is_my_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/152z2bq/what_is_my_role/", "subreddit_subscribers": 116642, "created_utc": 1689688281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do you all use to monitor your ingestion pipelines in databricks?\n\nWe currently use Azure Log Analytics/Azure monitor but the SparkListener library for Log Analytics sends Spark performance metrics. \n\nBeen using databricks notebooks that do some aggregations and push data through to Log Analytics as a custom SDK rig using log analytics and data collector API's. \n\nNot great, logs are slow to arrive in LA and the authorization headers and JSON responses get fiddly when data structures differ.", "author_fullname": "t2_r7bmwiee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Monitoring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1537ll0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689707981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you all use to monitor your ingestion pipelines in databricks?&lt;/p&gt;\n\n&lt;p&gt;We currently use Azure Log Analytics/Azure monitor but the SparkListener library for Log Analytics sends Spark performance metrics. &lt;/p&gt;\n\n&lt;p&gt;Been using databricks notebooks that do some aggregations and push data through to Log Analytics as a custom SDK rig using log analytics and data collector API&amp;#39;s. &lt;/p&gt;\n\n&lt;p&gt;Not great, logs are slow to arrive in LA and the authorization headers and JSON responses get fiddly when data structures differ.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1537ll0", "is_robot_indexable": true, "report_reasons": null, "author": "va1kyrja-kara", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1537ll0/databricks_monitoring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1537ll0/databricks_monitoring/", "subreddit_subscribers": 116642, "created_utc": 1689707981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6khnrfh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comparing Data Parallel, Task Parallel, and Agent Actor Architectures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1539nyl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TyLkTvEB3pDv-Glcl8NxhzIoTyDydo0-t0adN9VrevQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689712664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bytewax.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?auto=webp&amp;s=1ab273c790a3deab79b9a96b2a8963b849d81e2d", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=989bfe391b25edca2ffd8058f2818e412bd14ecf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ef71b43c5a4c3ed4df39f3097636a5395e92277", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebf82c1b4d733db4980b7d2a96d70bdb93348989", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=280e99d12ebe57c09258c29198d5118563158406", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e6bcdd642fb93b12d25cb210e0ead6a96438c68", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3fa4f2ed27dbc2e064f10e1af673ea9e97b8f1d0", "width": 1080, "height": 564}], "variants": {}, "id": "_GAQl_RTzqMiEFZm4ZiVzQqqGTrkugbg6LCg3kt5mlc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1539nyl", "is_robot_indexable": true, "report_reasons": null, "author": "semicausal", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1539nyl/comparing_data_parallel_task_parallel_and_agent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures", "subreddit_subscribers": 116642, "created_utc": 1689712664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm currently managing a data swamp, and working to make it clean and reliable.\n\nWe have a lot of raw data as JSONs, that we need to store and be able to manipulate on our Cloudera Data Platform (on top of HDFS).\n\nWhat would be the best way to store them on our cluster, in order to access them afterwards in our data preparation ?\n\nAtm, we are inserting them transformed in Apache Hive, but we really want to store them raw and manipulate them from a more appropriate DBMS / Document Manager", "author_fullname": "t2_7odlp6k1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing JSONs as raw data in CDP data lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15396op", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689711563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently managing a data swamp, and working to make it clean and reliable.&lt;/p&gt;\n\n&lt;p&gt;We have a lot of raw data as JSONs, that we need to store and be able to manipulate on our Cloudera Data Platform (on top of HDFS).&lt;/p&gt;\n\n&lt;p&gt;What would be the best way to store them on our cluster, in order to access them afterwards in our data preparation ?&lt;/p&gt;\n\n&lt;p&gt;Atm, we are inserting them transformed in Apache Hive, but we really want to store them raw and manipulate them from a more appropriate DBMS / Document Manager&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15396op", "is_robot_indexable": true, "report_reasons": null, "author": "TheSoulHokib", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15396op/managing_jsons_as_raw_data_in_cdp_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15396op/managing_jsons_as_raw_data_in_cdp_data_lake/", "subreddit_subscribers": 116642, "created_utc": 1689711563.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When you picked your data warehouse platform or if you would pick again, what are the most important themes for you in how you make that selection?For example:\n\n* Installation Simplicity\n* Simplcity of operations\n* Security\n* Query performance\n* Cost\n* Integrations\n* Migration effort\n* Cloud vs on-prem\n* anything else ???\n\nWhat was at the top of your list? Is there anything you regret about the data warehouse technology you ended up with? What questions would you have asked before making the decision in hindsight?", "author_fullname": "t2_plry86zh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What makes a good data warehouse platform?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1532ylc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689697307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When you picked your data warehouse platform or if you would pick again, what are the most important themes for you in how you make that selection?For example:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Installation Simplicity&lt;/li&gt;\n&lt;li&gt;Simplcity of operations&lt;/li&gt;\n&lt;li&gt;Security&lt;/li&gt;\n&lt;li&gt;Query performance&lt;/li&gt;\n&lt;li&gt;Cost&lt;/li&gt;\n&lt;li&gt;Integrations&lt;/li&gt;\n&lt;li&gt;Migration effort&lt;/li&gt;\n&lt;li&gt;Cloud vs on-prem&lt;/li&gt;\n&lt;li&gt;anything else ???&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What was at the top of your list? Is there anything you regret about the data warehouse technology you ended up with? What questions would you have asked before making the decision in hindsight?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1532ylc", "is_robot_indexable": true, "report_reasons": null, "author": "doubleblair", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1532ylc/what_makes_a_good_data_warehouse_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1532ylc/what_makes_a_good_data_warehouse_platform/", "subreddit_subscribers": 116642, "created_utc": 1689697307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company has recently gone through a merger. My team and I, an executive of the company, faced many issues that took too much time to solve. That got me thinking: how many other executives and team leads out there will have to struggle through the same issues, and how can I help them prepare? This is why I gathered some useful information that I believe could be useful for many people.  \nMergers and acquisitions (M&amp;A) bring about significant shifts in operations and culture for employees and the impact of data-related challenges on these aspects is frequently overlooked. Ensure you or your company are not making the same mistake.\n\n  \nMergers and acquisitions often trigger substantial organizational uncertainty about what lies ahead: typically, the operational model and culture undergo significant transformations for one or both of the merging entities. These alterations extend beyond just a new name and executive leadership; first and foremost, M&amp;A change the way IT teams work. Anticipating and addressing these IT challenges can pave the way for smooth and efficient integration. Conversely, failure to foresee and manage these issues can result in poor business performance, attrition of key talent, and potential data breaches. In this article, experts from AINSYS delve into the IT complications that can emerge during and post M&amp;A, and provide tools for addressing them effectively.  \nDuring and after M&amp;A, two companies\u2019 tech stacks essentially collide, and IT management has to deal with a number of challenges:\n\n# 1. Integrating disparate systems and platforms\n\nIntegrating disparate systems and platforms is one of the most significant challenges during mergers and acquisitions. When two companies come together, they often bring with them different IT systems and platforms that have been tailored to their specific business processes and needs. These systems can range from customer relationship management (CRM) and enterprise resource planning (ERP) systems, to data management platforms, financial systems, and more.\n\nHere are some of the key issues that arise during this integration process:\n\n* Compatibility: the systems used by the merging companies may not be compatible. They may be built on different architectures, use different data formats, or be based on different technologies. This can make integration a complex and time-consuming process;\n* Data Consistency: each system may have its own way of storing and structuring data. Ensuring consistency in data definitions, formats, and structures across all systems is crucial to avoid confusion and errors in data analysis and decision-making;\n* Redundancy: there may be significant overlap in the functionality of the systems used by the two companies, leading to redundancy. Identifying and eliminating these redundancies is important to avoid unnecessary costs and complexity;\n* Security and Compliance: each system will have its own security measures and compliance requirements. Ensuring that the integrated system meets all necessary security standards and regulatory requirements is crucial;\n* User Training: Employees from each company will be familiar with their own systems, but may need training to use the new integrated system effectively;\n* System Performance: Integrating two systems can put a strain on the performance of the IT infrastructure. Ensuring the integrated system performs effectively without causing downtime or delays is important.\n\n# 2. Reconciling different data formats and standards\n\nWhen two companies merge, they bring with them different data systems that have been tailored to their specific needs and processes. These systems may use different data formats and adhere to different data standards, which can create several issues during integration, including data inconsistency, data loss (if one system uses a data format that the other system does not support, some data may be lost during the conversion process, leading to incomplete or inaccurate data in the integrated system), data quality (one system might allow for missing values in certain fields, while the other does not), and compliance issues.\n\n# 3. Significant differences in the technological maturity between the two companies\n\nIf one of the companies has a significant technological advantage over the other, several issues may arise:\n\n* Integration Complexity: if one company uses modern, cloud-based systems while the other still relies on legacy systems, integrating the two can be complex and time-consuming. It may require significant resources to upgrade or replace outdated systems;\n* Operational Disruptions: if one company\u2019s systems are automated and the other\u2019s are manual, it could disrupt business processes and workflows until the less advanced systems are upgraded;\n* Security Risks: Older, less advanced systems may have more vulnerabilities, increasing the risk of data breaches or cyberattacks. It\u2019s crucial to assess and address these risks as part of the integration process.\n\n# 4. Considering the human element\n\nThis is perhaps the most complex and difficult issue to solve, seeing as no specialist is alike and supporting employees through change in M&amp;A is the most important matter you will need to solve. Why is that? Well, the reason is that the success of any merger or acquisition is not just about integrating systems and processes, but also about bringing together people from different organizational cultures. Employees are the backbone of any organization, and their acceptance and adaptation to change can significantly impact the outcome of the M&amp;A.  \nThe problem is, employees are conservative and don\u2019t want to switch from systems they have been using for years, having to learn new skills to actually use them. Additionally, companies often have to hire additional employees or IT consultants in order to support them through the process.  \nAnd it is not like executives can allow their employees to take their sweet time to figure out a whole new way of doing things \u2013 they need to start profiting from the merger or acquisition as soon as possible.\n\n  \nMerging or acquiring companies must shift the day-to-day behavior and mind-sets of their employees to protect a deal\u2019s sources of value, both financial and organizational, and to make changes sustainable. Yet when McKinsey asked 3,199 leaders if they regarded the change programs at their own companies as successful, only one-third did.\n\n  \nIn fact, comprehensive change management strategies are vital in easing transitions during mergers and acquisitions (M&amp;As), as they help increase acceptance among employees and minimize potential disruption. Here\u2019s how these strategies can be applied:\n\n1. Implement Surveys for In-depth Understanding: confidential surveys can encourage employees to express concerns they might not voice in person. These surveys should be designed to address specific issues identified during the interviews, such as opportunities for promotion. The responses can provide a more detailed understanding of the company\u2019s situation;\n2. Compile and Present a Detailed Report to Stakeholders: the findings from the interviews and surveys should be compiled into a comprehensive report. This report should highlight both the positive aspects and potential issues within the company, such as knowledge gaps or cultural problems. The report should be presented to stakeholders in a clear and concise manner;\n3. Include Personalized Action Plans in the Report: the report should also contain personalized action plans for key tasks. This assigns responsibility and accountability, ensuring that important tasks are not overlooked. A well-implemented action plan can drive value-generating changes in the company;\n4. Perform a Potential Resistance Analysis: a potential resistance analysis can identify potential obstacles within both organizations involved in the acquisition. This step should avoid creating a divisive \u2018us versus them\u2019 mentality. Instead, it should focus on understanding employee concerns, perceived negatives of the deal, and their vision of their future roles in the company. This analysis can help drive the acquisition forward in a positive and inclusive manner.\n\nOther issues can be solved via the right tool that addresses every issue mentioned.\n\n# How to effectively address data-related M&amp;A challenges?\n\nTo effectively address the challenges posed by M&amp;A, a comprehensive approach is required that addresses not only the technical aspects but also the human element. A no-code data sync and integration solution that serves as a central source of truth can be the key to this approach. This solution would transform data into a uniform format and securely store it in a single data warehouse, simplifying the integration process and ensuring data consistency and security.  \nA central source of truth (CSOT) is a concept in data management that refers to a single, authoritative source of data that everyone in the organization agrees is the real, trusted number. In the context of M&amp;A, a CSOT can help eliminate data inconsistencies and redundancies, streamline data management, and ensure all employees have access to the same, accurate data.  \nBy transforming all data into the same format, a CSOT can help to overcome the challenges of integrating disparate systems and reconciling different data formats and standards. No code aspect of the tool can significantly reduce the complexity and time required for data integration by involving business in the process. This ensures that all data is consistent and reliable.  \nMoreover, by storing all data securely in one data warehouse, a CSOT can help to maintain data privacy and security during and after the M&amp;A process. This can help to prevent data breaches and ensure compliance with all relevant regulations.\n\n# No-Code Data Solutions\n\nNo-code data solutions can play a crucial role in integrating two systems. These solutions allow users to manage and manipulate data without needing to write any code, making them accessible to both IT specialists and employees with no coding experience. This can help to speed up the integration process and enable companies to start profiting from M&amp;A right away.\n\nFurthermore, by enabling non-technical employees to work with data, no-code solutions can help to address the human element of M&amp;A. They can get more employees on board with adopting new systems and for them to adapt to new workflows. This can help to reduce resistance to change and increase employee engagement and productivity.\n\nIn addition, no-code solutions can reduce the need to hire IT consultants or spend time figuring out the best IT architecture option. This can result in significant cost savings and allow companies to focus their resources on other aspects of the M&amp;A process.\n\nThe rest of my research can be found here: [https://ainsys.com/blog/2023/06/12/data-management-challenges-in-ma/?utm\\_source=linkedin&amp;utm\\_medium=social&amp;utm\\_campaign=statistics&amp;utm\\_content=migrations\\_acquistions&amp;utm\\_term=ITarchitecture](https://ainsys.com/blog/2023/06/12/data-management-challenges-in-ma/?utm_source=linkedin&amp;utm_medium=social&amp;utm_campaign=statistics&amp;utm_content=migrations_acquistions&amp;utm_term=ITarchitecture)", "author_fullname": "t2_ct09rz3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data management challenges in M&amp;A", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1539422", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689711385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company has recently gone through a merger. My team and I, an executive of the company, faced many issues that took too much time to solve. That got me thinking: how many other executives and team leads out there will have to struggle through the same issues, and how can I help them prepare? This is why I gathered some useful information that I believe could be useful for many people.&lt;br/&gt;\nMergers and acquisitions (M&amp;amp;A) bring about significant shifts in operations and culture for employees and the impact of data-related challenges on these aspects is frequently overlooked. Ensure you or your company are not making the same mistake.&lt;/p&gt;\n\n&lt;p&gt;Mergers and acquisitions often trigger substantial organizational uncertainty about what lies ahead: typically, the operational model and culture undergo significant transformations for one or both of the merging entities. These alterations extend beyond just a new name and executive leadership; first and foremost, M&amp;amp;A change the way IT teams work. Anticipating and addressing these IT challenges can pave the way for smooth and efficient integration. Conversely, failure to foresee and manage these issues can result in poor business performance, attrition of key talent, and potential data breaches. In this article, experts from AINSYS delve into the IT complications that can emerge during and post M&amp;amp;A, and provide tools for addressing them effectively.&lt;br/&gt;\nDuring and after M&amp;amp;A, two companies\u2019 tech stacks essentially collide, and IT management has to deal with a number of challenges:&lt;/p&gt;\n\n&lt;h1&gt;1. Integrating disparate systems and platforms&lt;/h1&gt;\n\n&lt;p&gt;Integrating disparate systems and platforms is one of the most significant challenges during mergers and acquisitions. When two companies come together, they often bring with them different IT systems and platforms that have been tailored to their specific business processes and needs. These systems can range from customer relationship management (CRM) and enterprise resource planning (ERP) systems, to data management platforms, financial systems, and more.&lt;/p&gt;\n\n&lt;p&gt;Here are some of the key issues that arise during this integration process:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Compatibility: the systems used by the merging companies may not be compatible. They may be built on different architectures, use different data formats, or be based on different technologies. This can make integration a complex and time-consuming process;&lt;/li&gt;\n&lt;li&gt;Data Consistency: each system may have its own way of storing and structuring data. Ensuring consistency in data definitions, formats, and structures across all systems is crucial to avoid confusion and errors in data analysis and decision-making;&lt;/li&gt;\n&lt;li&gt;Redundancy: there may be significant overlap in the functionality of the systems used by the two companies, leading to redundancy. Identifying and eliminating these redundancies is important to avoid unnecessary costs and complexity;&lt;/li&gt;\n&lt;li&gt;Security and Compliance: each system will have its own security measures and compliance requirements. Ensuring that the integrated system meets all necessary security standards and regulatory requirements is crucial;&lt;/li&gt;\n&lt;li&gt;User Training: Employees from each company will be familiar with their own systems, but may need training to use the new integrated system effectively;&lt;/li&gt;\n&lt;li&gt;System Performance: Integrating two systems can put a strain on the performance of the IT infrastructure. Ensuring the integrated system performs effectively without causing downtime or delays is important.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;2. Reconciling different data formats and standards&lt;/h1&gt;\n\n&lt;p&gt;When two companies merge, they bring with them different data systems that have been tailored to their specific needs and processes. These systems may use different data formats and adhere to different data standards, which can create several issues during integration, including data inconsistency, data loss (if one system uses a data format that the other system does not support, some data may be lost during the conversion process, leading to incomplete or inaccurate data in the integrated system), data quality (one system might allow for missing values in certain fields, while the other does not), and compliance issues.&lt;/p&gt;\n\n&lt;h1&gt;3. Significant differences in the technological maturity between the two companies&lt;/h1&gt;\n\n&lt;p&gt;If one of the companies has a significant technological advantage over the other, several issues may arise:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Integration Complexity: if one company uses modern, cloud-based systems while the other still relies on legacy systems, integrating the two can be complex and time-consuming. It may require significant resources to upgrade or replace outdated systems;&lt;/li&gt;\n&lt;li&gt;Operational Disruptions: if one company\u2019s systems are automated and the other\u2019s are manual, it could disrupt business processes and workflows until the less advanced systems are upgraded;&lt;/li&gt;\n&lt;li&gt;Security Risks: Older, less advanced systems may have more vulnerabilities, increasing the risk of data breaches or cyberattacks. It\u2019s crucial to assess and address these risks as part of the integration process.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;4. Considering the human element&lt;/h1&gt;\n\n&lt;p&gt;This is perhaps the most complex and difficult issue to solve, seeing as no specialist is alike and supporting employees through change in M&amp;amp;A is the most important matter you will need to solve. Why is that? Well, the reason is that the success of any merger or acquisition is not just about integrating systems and processes, but also about bringing together people from different organizational cultures. Employees are the backbone of any organization, and their acceptance and adaptation to change can significantly impact the outcome of the M&amp;amp;A.&lt;br/&gt;\nThe problem is, employees are conservative and don\u2019t want to switch from systems they have been using for years, having to learn new skills to actually use them. Additionally, companies often have to hire additional employees or IT consultants in order to support them through the process.&lt;br/&gt;\nAnd it is not like executives can allow their employees to take their sweet time to figure out a whole new way of doing things \u2013 they need to start profiting from the merger or acquisition as soon as possible.&lt;/p&gt;\n\n&lt;p&gt;Merging or acquiring companies must shift the day-to-day behavior and mind-sets of their employees to protect a deal\u2019s sources of value, both financial and organizational, and to make changes sustainable. Yet when McKinsey asked 3,199 leaders if they regarded the change programs at their own companies as successful, only one-third did.&lt;/p&gt;\n\n&lt;p&gt;In fact, comprehensive change management strategies are vital in easing transitions during mergers and acquisitions (M&amp;amp;As), as they help increase acceptance among employees and minimize potential disruption. Here\u2019s how these strategies can be applied:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Implement Surveys for In-depth Understanding: confidential surveys can encourage employees to express concerns they might not voice in person. These surveys should be designed to address specific issues identified during the interviews, such as opportunities for promotion. The responses can provide a more detailed understanding of the company\u2019s situation;&lt;/li&gt;\n&lt;li&gt;Compile and Present a Detailed Report to Stakeholders: the findings from the interviews and surveys should be compiled into a comprehensive report. This report should highlight both the positive aspects and potential issues within the company, such as knowledge gaps or cultural problems. The report should be presented to stakeholders in a clear and concise manner;&lt;/li&gt;\n&lt;li&gt;Include Personalized Action Plans in the Report: the report should also contain personalized action plans for key tasks. This assigns responsibility and accountability, ensuring that important tasks are not overlooked. A well-implemented action plan can drive value-generating changes in the company;&lt;/li&gt;\n&lt;li&gt;Perform a Potential Resistance Analysis: a potential resistance analysis can identify potential obstacles within both organizations involved in the acquisition. This step should avoid creating a divisive \u2018us versus them\u2019 mentality. Instead, it should focus on understanding employee concerns, perceived negatives of the deal, and their vision of their future roles in the company. This analysis can help drive the acquisition forward in a positive and inclusive manner.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Other issues can be solved via the right tool that addresses every issue mentioned.&lt;/p&gt;\n\n&lt;h1&gt;How to effectively address data-related M&amp;amp;A challenges?&lt;/h1&gt;\n\n&lt;p&gt;To effectively address the challenges posed by M&amp;amp;A, a comprehensive approach is required that addresses not only the technical aspects but also the human element. A no-code data sync and integration solution that serves as a central source of truth can be the key to this approach. This solution would transform data into a uniform format and securely store it in a single data warehouse, simplifying the integration process and ensuring data consistency and security.&lt;br/&gt;\nA central source of truth (CSOT) is a concept in data management that refers to a single, authoritative source of data that everyone in the organization agrees is the real, trusted number. In the context of M&amp;amp;A, a CSOT can help eliminate data inconsistencies and redundancies, streamline data management, and ensure all employees have access to the same, accurate data.&lt;br/&gt;\nBy transforming all data into the same format, a CSOT can help to overcome the challenges of integrating disparate systems and reconciling different data formats and standards. No code aspect of the tool can significantly reduce the complexity and time required for data integration by involving business in the process. This ensures that all data is consistent and reliable.&lt;br/&gt;\nMoreover, by storing all data securely in one data warehouse, a CSOT can help to maintain data privacy and security during and after the M&amp;amp;A process. This can help to prevent data breaches and ensure compliance with all relevant regulations.&lt;/p&gt;\n\n&lt;h1&gt;No-Code Data Solutions&lt;/h1&gt;\n\n&lt;p&gt;No-code data solutions can play a crucial role in integrating two systems. These solutions allow users to manage and manipulate data without needing to write any code, making them accessible to both IT specialists and employees with no coding experience. This can help to speed up the integration process and enable companies to start profiting from M&amp;amp;A right away.&lt;/p&gt;\n\n&lt;p&gt;Furthermore, by enabling non-technical employees to work with data, no-code solutions can help to address the human element of M&amp;amp;A. They can get more employees on board with adopting new systems and for them to adapt to new workflows. This can help to reduce resistance to change and increase employee engagement and productivity.&lt;/p&gt;\n\n&lt;p&gt;In addition, no-code solutions can reduce the need to hire IT consultants or spend time figuring out the best IT architecture option. This can result in significant cost savings and allow companies to focus their resources on other aspects of the M&amp;amp;A process.&lt;/p&gt;\n\n&lt;p&gt;The rest of my research can be found here: &lt;a href=\"https://ainsys.com/blog/2023/06/12/data-management-challenges-in-ma/?utm_source=linkedin&amp;amp;utm_medium=social&amp;amp;utm_campaign=statistics&amp;amp;utm_content=migrations_acquistions&amp;amp;utm_term=ITarchitecture\"&gt;https://ainsys.com/blog/2023/06/12/data-management-challenges-in-ma/?utm_source=linkedin&amp;amp;utm_medium=social&amp;amp;utm_campaign=statistics&amp;amp;utm_content=migrations_acquistions&amp;amp;utm_term=ITarchitecture&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/I3xJj8ELsQOmy2H-NeHKFmGnh2tKVfHX1i7SJ1O1qwE.jpg?auto=webp&amp;s=8a29f85fd320507a70ec9694b971f494352d2655", "width": 79, "height": 86}, "resolutions": [], "variants": {}, "id": "fKkAw45B6Aa1K9gfC0CvmXNzCjb0F-J2wvKUvaKf1Vk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1539422", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive_Speech36", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1539422/data_management_challenges_in_ma/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1539422/data_management_challenges_in_ma/", "subreddit_subscribers": 116642, "created_utc": 1689711385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are just about to build the gold layer in our data lake but unsure of how to organize the directories. We aim to have a kimball model for each project, but unsure where to store the dimension tables (are in delta format). Should we create directories inside the gold container for each model and duplicate shared dimension tables? How do you guys set this up in your orgs? We have the facts and dims in their respective delta tables.", "author_fullname": "t2_56ltry44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building the gold container in the medallion architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1537lly", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689707982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are just about to build the gold layer in our data lake but unsure of how to organize the directories. We aim to have a kimball model for each project, but unsure where to store the dimension tables (are in delta format). Should we create directories inside the gold container for each model and duplicate shared dimension tables? How do you guys set this up in your orgs? We have the facts and dims in their respective delta tables.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1537lly", "is_robot_indexable": true, "report_reasons": null, "author": "Specific-Passage", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1537lly/building_the_gold_container_in_the_medallion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1537lly/building_the_gold_container_in_the_medallion/", "subreddit_subscribers": 116642, "created_utc": 1689707982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHello to all Data Engineers !\n\nI'm diving into the exciting world of Apache Spark and I'm looking for the perfect video tutorial that provides a comprehensive understanding of Spark's architecture, concepts, and practical usage. With so many options out there, both free and paid, I thought it would be great to tap into the knowledge and experiences of this community.\n\nSo, my question is: Which video tutorial would you recommend as the best resource for learning Apache Spark? It could be a free tutorial available on YouTube or other platforms, or even a paid course that you found to be incredibly valuable.\n\nI'm particularly interested in a tutorial that covers the following aspects:\n\n* Spark architecture and core concepts\n* Hands-on examples and real-world use cases\n* Detailed explanations of Spark transformations and actions\n* Spark SQL, data frames, and dataset APIs\n* Integration with popular big data tools and frameworks\n\nYour insights and recommendations would be highly appreciated. Please feel free to share any personal experiences, tutorial names, links, or even the instructors who made a significant impact on your Spark learning journey.\n\nLooking forward to your valuable suggestions and thank you in advance for your help!", "author_fullname": "t2_hvt2pp4j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking the Best Apache Spark Video Tutorial - Recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1535mkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689703458.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello to all Data Engineers !&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m diving into the exciting world of Apache Spark and I&amp;#39;m looking for the perfect video tutorial that provides a comprehensive understanding of Spark&amp;#39;s architecture, concepts, and practical usage. With so many options out there, both free and paid, I thought it would be great to tap into the knowledge and experiences of this community.&lt;/p&gt;\n\n&lt;p&gt;So, my question is: Which video tutorial would you recommend as the best resource for learning Apache Spark? It could be a free tutorial available on YouTube or other platforms, or even a paid course that you found to be incredibly valuable.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m particularly interested in a tutorial that covers the following aspects:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Spark architecture and core concepts&lt;/li&gt;\n&lt;li&gt;Hands-on examples and real-world use cases&lt;/li&gt;\n&lt;li&gt;Detailed explanations of Spark transformations and actions&lt;/li&gt;\n&lt;li&gt;Spark SQL, data frames, and dataset APIs&lt;/li&gt;\n&lt;li&gt;Integration with popular big data tools and frameworks&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Your insights and recommendations would be highly appreciated. Please feel free to share any personal experiences, tutorial names, links, or even the instructors who made a significant impact on your Spark learning journey.&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your valuable suggestions and thank you in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1535mkd", "is_robot_indexable": true, "report_reasons": null, "author": "Loki_029", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1535mkd/seeking_the_best_apache_spark_video_tutorial/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1535mkd/seeking_the_best_apache_spark_video_tutorial/", "subreddit_subscribers": 116642, "created_utc": 1689703458.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey ya'll, Airflow builds are breaking today. Yesterday Cython jumped to version 3.0, which caused \\`pymssql\\` to break. That breakage is causing Airflow to break as well.\n\nMy company doesn't use Airflow, but it bit us because we use pymssql in our application. We had to fork pymssql and pin to an earlier version of Cython.\n\nJust hoping to save someone a couple hours of investigation", "author_fullname": "t2_hoxsyt4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Builds Breaking", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1532mzq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689696579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey ya&amp;#39;ll, Airflow builds are breaking today. Yesterday Cython jumped to version 3.0, which caused `pymssql` to break. That breakage is causing Airflow to break as well.&lt;/p&gt;\n\n&lt;p&gt;My company doesn&amp;#39;t use Airflow, but it bit us because we use pymssql in our application. We had to fork pymssql and pin to an earlier version of Cython.&lt;/p&gt;\n\n&lt;p&gt;Just hoping to save someone a couple hours of investigation&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1532mzq", "is_robot_indexable": true, "report_reasons": null, "author": "AnySherbet", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1532mzq/airflow_builds_breaking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1532mzq/airflow_builds_breaking/", "subreddit_subscribers": 116642, "created_utc": 1689696579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI'm working at a company that needs to take data from health care records and put it into a data warehouse for machine learning and analysis.  I've never done anything like this before, and was wondering if there was anyone in the community who has accomplished a similar task and was willing to chat.\n\nWe're using Google Cloud for our data services right now.  I'm interested in using their Healthcare API as it looks promising, although I'm open to other solutions: [https://cloud.google.com/healthcare-api](https://cloud.google.com/healthcare-api).  Curious to hear what other peoples' thoughts are.", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice from Experts on Electronic Health Record Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1531r59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689694564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working at a company that needs to take data from health care records and put it into a data warehouse for machine learning and analysis.  I&amp;#39;ve never done anything like this before, and was wondering if there was anyone in the community who has accomplished a similar task and was willing to chat.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re using Google Cloud for our data services right now.  I&amp;#39;m interested in using their Healthcare API as it looks promising, although I&amp;#39;m open to other solutions: &lt;a href=\"https://cloud.google.com/healthcare-api\"&gt;https://cloud.google.com/healthcare-api&lt;/a&gt;.  Curious to hear what other peoples&amp;#39; thoughts are.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?auto=webp&amp;s=ada93f0d146c09556ac26cc3fa125a0aa7102150", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=05af26b5ec35c95ed95b5c40dbde3c1cc04dce06", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f36f0de65cdcbed3b705b8446710c7c83e0475e4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4067aebaadaec227b271d9c19c7af833c230fd32", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec46bff13e5c5bc616be11b375484d9d2a7fcbe8", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5db472c15d5717384ab8f8f64e9fd89efe70fa59", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fab49c1958487e16d598ede6d81140177c5c9a31", "width": 1080, "height": 567}], "variants": {}, "id": "DsiOIzUSicS_9zIKwMDQbNT2LOE1o29sSYs49HAmO_k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1531r59", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1531r59/seeking_advice_from_experts_on_electronic_health/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1531r59/seeking_advice_from_experts_on_electronic_health/", "subreddit_subscribers": 116642, "created_utc": 1689694564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, I'd like to share my current situation to understand better where to go.\n\nI started in data as a ETL developer since 2011, I did projects using DataStream (old Cognos), PowerCenter, SSIS, Talend, SAS and even VBA up to 2016 when I joined a data visualization tool company as consultant mainly focused to create conceptual DB models and reports/dashboards with this tool. However since 2018 composed by 2 periods (2016-2021 and from 2022- up-to-date)   most of my projects were related to upgrade servers to newer software versions, configuration etc. There were a couple projects here and there that were related to predictive analytics with python and/or R, NoSQL, Hadoop, Query performance, data ingestion, but summing all of this it is no more than 10% of my time.\n\nWithin 2021 I left this job for a ecommerce company which I had to work with python, Big Query, Teradata, Presto etc. But unfortunately I had to leave it to immigrate which was a bigger dream. That's why I re-joined the previously mentioned company.\n\nI feel that I have the requirements, I never stopped to study, I'm pretty good in query performance, data modeling, user specification, also know python, Scala, AWS, data bricks. a bunch of databases (SQL Server, Oracle, Teradata, MySQL, Big Query, Redshift, Snowflake, PostgreSQL) and I also know a little docker due to home projects, yet I've applied for almost 50 openings and didn't got any replies/interviews even diposed to earn a little less than I'm currently making if necessary. Sometimes I fell that the extended period working for the data viz company kinda put a stamp on me saying that I'm not capable of doing a DE job.\n\nMy question is, is it possible to go to a DE path?", "author_fullname": "t2_8c1ywuv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From data specific consultant to DE path", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_152y27j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689685752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I&amp;#39;d like to share my current situation to understand better where to go.&lt;/p&gt;\n\n&lt;p&gt;I started in data as a ETL developer since 2011, I did projects using DataStream (old Cognos), PowerCenter, SSIS, Talend, SAS and even VBA up to 2016 when I joined a data visualization tool company as consultant mainly focused to create conceptual DB models and reports/dashboards with this tool. However since 2018 composed by 2 periods (2016-2021 and from 2022- up-to-date)   most of my projects were related to upgrade servers to newer software versions, configuration etc. There were a couple projects here and there that were related to predictive analytics with python and/or R, NoSQL, Hadoop, Query performance, data ingestion, but summing all of this it is no more than 10% of my time.&lt;/p&gt;\n\n&lt;p&gt;Within 2021 I left this job for a ecommerce company which I had to work with python, Big Query, Teradata, Presto etc. But unfortunately I had to leave it to immigrate which was a bigger dream. That&amp;#39;s why I re-joined the previously mentioned company.&lt;/p&gt;\n\n&lt;p&gt;I feel that I have the requirements, I never stopped to study, I&amp;#39;m pretty good in query performance, data modeling, user specification, also know python, Scala, AWS, data bricks. a bunch of databases (SQL Server, Oracle, Teradata, MySQL, Big Query, Redshift, Snowflake, PostgreSQL) and I also know a little docker due to home projects, yet I&amp;#39;ve applied for almost 50 openings and didn&amp;#39;t got any replies/interviews even diposed to earn a little less than I&amp;#39;m currently making if necessary. Sometimes I fell that the extended period working for the data viz company kinda put a stamp on me saying that I&amp;#39;m not capable of doing a DE job.&lt;/p&gt;\n\n&lt;p&gt;My question is, is it possible to go to a DE path?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "152y27j", "is_robot_indexable": true, "report_reasons": null, "author": "jokerxbr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/152y27j/from_data_specific_consultant_to_de_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/152y27j/from_data_specific_consultant_to_de_path/", "subreddit_subscribers": 116642, "created_utc": 1689685752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are working on setting up our databricks environment and are struggling with the pattern to implement the medallion architecture. \n\nWe currently have: catalog.schema.table as project.medallion\\_layer.table and are wondering if this is the way to go? \n\nWe have our environments split into two workspaces for dev and prod, I don't know if it makes sense to have bronze, silver, gold in each workspace? But also want to give different types of users levels of control. \n\nLittle lost and databricks doesn't seem to actually have any documentation on medallion implementation.", "author_fullname": "t2_81ywblydd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to actually implement medallion layers in databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_152xzcv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689685542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are working on setting up our databricks environment and are struggling with the pattern to implement the medallion architecture. &lt;/p&gt;\n\n&lt;p&gt;We currently have: catalog.schema.table as project.medallion_layer.table and are wondering if this is the way to go? &lt;/p&gt;\n\n&lt;p&gt;We have our environments split into two workspaces for dev and prod, I don&amp;#39;t know if it makes sense to have bronze, silver, gold in each workspace? But also want to give different types of users levels of control. &lt;/p&gt;\n\n&lt;p&gt;Little lost and databricks doesn&amp;#39;t seem to actually have any documentation on medallion implementation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "152xzcv", "is_robot_indexable": true, "report_reasons": null, "author": "DataDoyle", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/152xzcv/how_to_actually_implement_medallion_layers_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/152xzcv/how_to_actually_implement_medallion_layers_in/", "subreddit_subscribers": 116642, "created_utc": 1689685542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was hired as a DS at a scale-up company, and have implemented all data engineering pipelines using the following stack: Prefect + pandas, with .parquet files on azure datalake as final storage.\n\nI am running into some performance issues, and would possibly want to separate the orchestration and compute., using some warehouse option for the compute. Additionally, the final storage should probably also be a warehouse and not a datalake, as we are hiring analysts that would probably expect some SQL-queryable storage.\n\nI am basically looking into options for warehouses that can do the whole ELT process. Would something like SnowFlake be a good option, or is it overkill for small sizes of data?   \nDoes it handle extract/normalization of fairly complex json files, or should I use some other tool for the extract part?\n\n&amp;#x200B;", "author_fullname": "t2_lixpgvfy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for warehouse options to replace pandas ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_152vv4s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689679785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was hired as a DS at a scale-up company, and have implemented all data engineering pipelines using the following stack: Prefect + pandas, with .parquet files on azure datalake as final storage.&lt;/p&gt;\n\n&lt;p&gt;I am running into some performance issues, and would possibly want to separate the orchestration and compute., using some warehouse option for the compute. Additionally, the final storage should probably also be a warehouse and not a datalake, as we are hiring analysts that would probably expect some SQL-queryable storage.&lt;/p&gt;\n\n&lt;p&gt;I am basically looking into options for warehouses that can do the whole ELT process. Would something like SnowFlake be a good option, or is it overkill for small sizes of data?&lt;br/&gt;\nDoes it handle extract/normalization of fairly complex json files, or should I use some other tool for the extract part?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "152vv4s", "is_robot_indexable": true, "report_reasons": null, "author": "Longjumping-Nail-250", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/152vv4s/looking_for_warehouse_options_to_replace_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/152vv4s/looking_for_warehouse_options_to_replace_pandas/", "subreddit_subscribers": 116642, "created_utc": 1689679785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all\n\nI'm exploring the transition into a data role as the next phase of my career. Uncovering data engineering concepts and architectures make my juices flow again after a few years of stagnation in my current infrastructure solution architect role. Data science and analysis isn't my bag, I'm focused on the engineering track.\n\nMy background: I've worked in public and private cloud (primarily Azure infrastructure and micro-services) and data management (primarily protection and disaster recovery) for c. 13 years, the last 5 at a senior architect level. \n\nI've seen a few posts on 'transitioning' roles and currently looking through the wiki to understand skillsets but would loveto hear from those who have experienced a similar transition from cloud solution architect into a data role. \n\n I'm working my way through DP-203 and I envisage the most difficult aspects will include coding. However, that's something I'll just have to work on.\n\nDid you work from a data engineering starting point up to data architect? Did you face any significant hurdles or were there any skills that you found invaluable in your journey?", "author_fullname": "t2_a0k3gzpb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transition from cloud architect into data role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_152rnc1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689665871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m exploring the transition into a data role as the next phase of my career. Uncovering data engineering concepts and architectures make my juices flow again after a few years of stagnation in my current infrastructure solution architect role. Data science and analysis isn&amp;#39;t my bag, I&amp;#39;m focused on the engineering track.&lt;/p&gt;\n\n&lt;p&gt;My background: I&amp;#39;ve worked in public and private cloud (primarily Azure infrastructure and micro-services) and data management (primarily protection and disaster recovery) for c. 13 years, the last 5 at a senior architect level. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen a few posts on &amp;#39;transitioning&amp;#39; roles and currently looking through the wiki to understand skillsets but would loveto hear from those who have experienced a similar transition from cloud solution architect into a data role. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working my way through DP-203 and I envisage the most difficult aspects will include coding. However, that&amp;#39;s something I&amp;#39;ll just have to work on.&lt;/p&gt;\n\n&lt;p&gt;Did you work from a data engineering starting point up to data architect? Did you face any significant hurdles or were there any skills that you found invaluable in your journey?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "152rnc1", "is_robot_indexable": true, "report_reasons": null, "author": "Substantial-Figure88", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/152rnc1/transition_from_cloud_architect_into_data_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/152rnc1/transition_from_cloud_architect_into_data_role/", "subreddit_subscribers": 116642, "created_utc": 1689665871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I browsed through the community posts and understood that [neetcode.io](https://neetcode.io) is by far the best and the most recommended platform for practising Python for DE interviews.\n\nI work as a Data Engineer at a small startup right now, but never in my life I have done LeetCode or even touched DSA, hence requesting for help here.\n\nIn the [neetcode.io](https://neetcode.io) platform, in the practice section, if we sort by group, the questions are grouped by topic so that you can tackle them accordingly.\n\nBased on your interview experience, can you please help me prioritise the topics? I am guessing many of these topics are for SDE interviews and not all might be relevant to DE interviews, and even if they are relevant, some must be more important than others.\n\nThe topics are:\n\n1. Arrays and Hashing\n2. Two Pointers\n3. Sliding Window\n4. Stack\n5. Binary Search\n6. Linked List\n7. Trees\n8. Tries\n9. Heap / Priority Queue\n10. Backtracking\n11. Graphs\n12. Advanced Graphs\n13. 1-D dynamic programming\n14. 2-D dynamic programming\n15. Greedy\n16. Intervals\n17. Math &amp; Geometry\n18. Bit Manipulation\n\nThe reasons I have to prioritise/filter this is because I have limited time left in the interview prep hence want the important things 1st, and also because I am not specifically targeting MAANG.\n\n&amp;#x200B;\n\nThank you for the help!!", "author_fullname": "t2_cu6910tv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview preparation help - Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_152ny8o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689653788.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I browsed through the community posts and understood that &lt;a href=\"https://neetcode.io\"&gt;neetcode.io&lt;/a&gt; is by far the best and the most recommended platform for practising Python for DE interviews.&lt;/p&gt;\n\n&lt;p&gt;I work as a Data Engineer at a small startup right now, but never in my life I have done LeetCode or even touched DSA, hence requesting for help here.&lt;/p&gt;\n\n&lt;p&gt;In the &lt;a href=\"https://neetcode.io\"&gt;neetcode.io&lt;/a&gt; platform, in the practice section, if we sort by group, the questions are grouped by topic so that you can tackle them accordingly.&lt;/p&gt;\n\n&lt;p&gt;Based on your interview experience, can you please help me prioritise the topics? I am guessing many of these topics are for SDE interviews and not all might be relevant to DE interviews, and even if they are relevant, some must be more important than others.&lt;/p&gt;\n\n&lt;p&gt;The topics are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Arrays and Hashing&lt;/li&gt;\n&lt;li&gt;Two Pointers&lt;/li&gt;\n&lt;li&gt;Sliding Window&lt;/li&gt;\n&lt;li&gt;Stack&lt;/li&gt;\n&lt;li&gt;Binary Search&lt;/li&gt;\n&lt;li&gt;Linked List&lt;/li&gt;\n&lt;li&gt;Trees&lt;/li&gt;\n&lt;li&gt;Tries&lt;/li&gt;\n&lt;li&gt;Heap / Priority Queue&lt;/li&gt;\n&lt;li&gt;Backtracking&lt;/li&gt;\n&lt;li&gt;Graphs&lt;/li&gt;\n&lt;li&gt;Advanced Graphs&lt;/li&gt;\n&lt;li&gt;1-D dynamic programming&lt;/li&gt;\n&lt;li&gt;2-D dynamic programming&lt;/li&gt;\n&lt;li&gt;Greedy&lt;/li&gt;\n&lt;li&gt;Intervals&lt;/li&gt;\n&lt;li&gt;Math &amp;amp; Geometry&lt;/li&gt;\n&lt;li&gt;Bit Manipulation&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The reasons I have to prioritise/filter this is because I have limited time left in the interview prep hence want the important things 1st, and also because I am not specifically targeting MAANG.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for the help!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "152ny8o", "is_robot_indexable": true, "report_reasons": null, "author": "table_data", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/152ny8o/interview_preparation_help_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/152ny8o/interview_preparation_help_python/", "subreddit_subscribers": 116642, "created_utc": 1689653788.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I need some output regarding this situation.  \nI have a flink batch job that needs to run based on a trigger. By trigger I meant suppose there is a FASTAPI endpoint exposed for clients to post \"submit\", as soon as that happens, the job needs to be submitted!  \nWe are using Flink-Kubernetes-Operator and its on Session Cluster mode. I can't really use cron, because the its aperiodic as you can see. What will be the flink way to achieve this? Has anyone solved this kind of use cases? If so how?  \nLove to know your thoughts!  \nThanks", "author_fullname": "t2_xqvyn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flink Batch Job needs to run based on a trigger", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_152m3hr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689648465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I need some output regarding this situation.&lt;br/&gt;\nI have a flink batch job that needs to run based on a trigger. By trigger I meant suppose there is a FASTAPI endpoint exposed for clients to post &amp;quot;submit&amp;quot;, as soon as that happens, the job needs to be submitted!&lt;br/&gt;\nWe are using Flink-Kubernetes-Operator and its on Session Cluster mode. I can&amp;#39;t really use cron, because the its aperiodic as you can see. What will be the flink way to achieve this? Has anyone solved this kind of use cases? If so how?&lt;br/&gt;\nLove to know your thoughts!&lt;br/&gt;\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "152m3hr", "is_robot_indexable": true, "report_reasons": null, "author": "Salekeen01", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/152m3hr/flink_batch_job_needs_to_run_based_on_a_trigger/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/152m3hr/flink_batch_job_needs_to_run_based_on_a_trigger/", "subreddit_subscribers": 116642, "created_utc": 1689648465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**TLDR**: I created [this package](https://github.com/Elsayed91/easy_ge) so you can use Great Expectations without knowing much about it or if you have simple use cases and would rather an option with better readability and easier implementation.\n\nIf you'd rather watch than read, you can check [this low-budget demo](https://www.youtube.com/watch?v=9v8mlDb2oRo). a mm a mmm a. \n\nThis is my first package ever. I called it \"Easy G.E\", pretty cringe, but at least it rhymes. Now I am going to present it as if I'm selling you a real enterprise-grade solution.\n\n**Features**:\n\n1. Provides a low-code approach for validating files stored on various filesystems (local, GCS, S3) or in-memory dataframes.\n2. Works with both Pandas and Spark engines.\n3. Reduces configuration complexity to just four schema-enforced fields in a YAML file.\n4. Allows dynamic variable definition/substitution at runtime, eliminating the need for any hardcoded values in the configuration YAML and facilitating the utilization as part of dynamic data pipelines.\n\n**How it works**:\n\n1. Create an expectation suite for your data that you want to validate. This is straightforward, and a guide is available in the repository.\n2. Fill out a `YAML` file with the dataframe name/file path, destination to save results, and the expectation suite file name.\n3. Place the expectation suite in a folder named \"expectations\" in the destination you've defined in the YAML file.\n4. Import the package and execute `results = easy_validation('/path/to/yaml_config.yaml')`   \nThat's it! You're done!   \nAlternatively, you can use the docker image provided to validate files on the fly. It also integrates well with  `Kubernetes` and `Airflow` when using Kubernetes Jobs for tasks.\n\n**Motivation**:\n\n* When I first tried using Great Expectations, I became so frustrated that I decided to include DBT in my project just to leverage the easier-to-implement DBT Expectations.\n* When you are not using `great_expectations.yaml` you might have to write \\~ 100 lines of cryptic code to run the validation.\n* The documentation is hard to navigate, at times difficult to understand, undergoes significant changes between major releases. Moreover, the examples are riddled with [irrelevant context ](https://github.com/great-expectations/great_expectations/blob/develop/tests/integration/docusaurus/connecting_to_your_data/cloud/gcs/pandas/inferred_and_runtime_python_example.py)for users, mainly because the examples serve as tests.\n* Another technology to add to the learning list.\n\n**Target Users**:\n\n|Who Could Benefit from the package?|Problems Addressed by the package:|\n|:-|:-|\n|Individuals who are unfamiliar with `Great Expectations`  but wish to implement data quality checks and documentation.|Overcoming Great Expectation's learning curve required to run basic use-cases.|\n|Individuals experienced with `Great Expectations`|Providing a low-code solution without the need for extensive module creation, documentation, and unit testing.|\n|Users seeking a solution that is easily readable by peers unfamiliar with the tool.|Enhancing readability for individuals who are unfamiliar with Great Expectations, ensuring ease of understanding and collaboration.|\n\n&amp;#x200B;\n\nDisclaimer #1:  *It ain't much but it is honest work.*\n\nThis package is rudimentary but will carry out the advertised functionalities. However, as I'm no Great Expectations power user, I've only implemented what I believe to be common use cases. Nevertheless, I've put in my best effort to design the package to be extensible.\n\nDisclaimer #2\n\nI have approximately 6 months of data engineering learning under my belt, and a total of 10 months in the programming ecosystem. Zero real world experience, which makes me feel insanely embarrassed posting this here (imposter syndrome intensifies), but YOLO. Receiving feedback could greatly benefit my ongoing job hunt and help me grow. Also considering how useful I'd have found this package myself some months back, it could be of some use to others, if the whole concept is not a disaster that I am unaware of, that is.", "author_fullname": "t2_jwq2blxl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created a great expectations wrapper so that even my mum could validate her data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_153bm5a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689717195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: I created &lt;a href=\"https://github.com/Elsayed91/easy_ge\"&gt;this package&lt;/a&gt; so you can use Great Expectations without knowing much about it or if you have simple use cases and would rather an option with better readability and easier implementation.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;d rather watch than read, you can check &lt;a href=\"https://www.youtube.com/watch?v=9v8mlDb2oRo\"&gt;this low-budget demo&lt;/a&gt;. a mm a mmm a. &lt;/p&gt;\n\n&lt;p&gt;This is my first package ever. I called it &amp;quot;Easy G.E&amp;quot;, pretty cringe, but at least it rhymes. Now I am going to present it as if I&amp;#39;m selling you a real enterprise-grade solution.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Provides a low-code approach for validating files stored on various filesystems (local, GCS, S3) or in-memory dataframes.&lt;/li&gt;\n&lt;li&gt;Works with both Pandas and Spark engines.&lt;/li&gt;\n&lt;li&gt;Reduces configuration complexity to just four schema-enforced fields in a YAML file.&lt;/li&gt;\n&lt;li&gt;Allows dynamic variable definition/substitution at runtime, eliminating the need for any hardcoded values in the configuration YAML and facilitating the utilization as part of dynamic data pipelines.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Create an expectation suite for your data that you want to validate. This is straightforward, and a guide is available in the repository.&lt;/li&gt;\n&lt;li&gt;Fill out a &lt;code&gt;YAML&lt;/code&gt; file with the dataframe name/file path, destination to save results, and the expectation suite file name.&lt;/li&gt;\n&lt;li&gt;Place the expectation suite in a folder named &amp;quot;expectations&amp;quot; in the destination you&amp;#39;ve defined in the YAML file.&lt;/li&gt;\n&lt;li&gt;Import the package and execute &lt;code&gt;results = easy_validation(&amp;#39;/path/to/yaml_config.yaml&amp;#39;)&lt;/code&gt;&lt;br/&gt;\nThat&amp;#39;s it! You&amp;#39;re done!&lt;br/&gt;\nAlternatively, you can use the docker image provided to validate files on the fly. It also integrates well with  &lt;code&gt;Kubernetes&lt;/code&gt; and &lt;code&gt;Airflow&lt;/code&gt; when using Kubernetes Jobs for tasks.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When I first tried using Great Expectations, I became so frustrated that I decided to include DBT in my project just to leverage the easier-to-implement DBT Expectations.&lt;/li&gt;\n&lt;li&gt;When you are not using &lt;code&gt;great_expectations.yaml&lt;/code&gt; you might have to write ~ 100 lines of cryptic code to run the validation.&lt;/li&gt;\n&lt;li&gt;The documentation is hard to navigate, at times difficult to understand, undergoes significant changes between major releases. Moreover, the examples are riddled with &lt;a href=\"https://github.com/great-expectations/great_expectations/blob/develop/tests/integration/docusaurus/connecting_to_your_data/cloud/gcs/pandas/inferred_and_runtime_python_example.py\"&gt;irrelevant context &lt;/a&gt;for users, mainly because the examples serve as tests.&lt;/li&gt;\n&lt;li&gt;Another technology to add to the learning list.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Target Users&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Who Could Benefit from the package?&lt;/th&gt;\n&lt;th align=\"left\"&gt;Problems Addressed by the package:&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Individuals who are unfamiliar with &lt;code&gt;Great Expectations&lt;/code&gt;  but wish to implement data quality checks and documentation.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Overcoming Great Expectation&amp;#39;s learning curve required to run basic use-cases.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Individuals experienced with &lt;code&gt;Great Expectations&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Providing a low-code solution without the need for extensive module creation, documentation, and unit testing.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Users seeking a solution that is easily readable by peers unfamiliar with the tool.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Enhancing readability for individuals who are unfamiliar with Great Expectations, ensuring ease of understanding and collaboration.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Disclaimer #1:  &lt;em&gt;It ain&amp;#39;t much but it is honest work.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;This package is rudimentary but will carry out the advertised functionalities. However, as I&amp;#39;m no Great Expectations power user, I&amp;#39;ve only implemented what I believe to be common use cases. Nevertheless, I&amp;#39;ve put in my best effort to design the package to be extensible.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer #2&lt;/p&gt;\n\n&lt;p&gt;I have approximately 6 months of data engineering learning under my belt, and a total of 10 months in the programming ecosystem. Zero real world experience, which makes me feel insanely embarrassed posting this here (imposter syndrome intensifies), but YOLO. Receiving feedback could greatly benefit my ongoing job hunt and help me grow. Also considering how useful I&amp;#39;d have found this package myself some months back, it could be of some use to others, if the whole concept is not a disaster that I am unaware of, that is.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?auto=webp&amp;s=7423cb1aba2e435f016e105029d3c10b47077bc5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=153d78c5238578c181f0639d713cfddc046ea1f3", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bbde46af15c6568fdb673d17fd97c6cfa17a5a81", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=986b8f948c8a4caa3c1371de460c227fb994d955", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=117ae1527a69c6d19e673493c26820f8171c19b1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8a4b89ba266d4855fdf73dfc6c4e70c7a114479b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=32849eaa6edc4791ae9dda074ede8bbaeab6a59b", "width": 1080, "height": 540}], "variants": {}, "id": "VbnN3cGyjwsaRntjS52g6SlwMjoYtHezmOYEburSEyM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "153bm5a", "is_robot_indexable": true, "report_reasons": null, "author": "whatisthisdataman", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153bm5a/i_created_a_great_expectations_wrapper_so_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153bm5a/i_created_a_great_expectations_wrapper_so_that/", "subreddit_subscribers": 116642, "created_utc": 1689717195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have several parquet files (around 100 files), all have the same format, the only difference is that each file is the historical data of an specific date. Currently I have all the files stored in AWS S3, but i need to clean, add columns, and manipulate some columns. After that, i will need to use these data for analytics and to train a ML model.  WHAT WOULD BE THE BEST APPROACH FOR ALL OF THIS? Should i merge all the files into a database (all files have the same format and columns) and that would be easier to use and would increase the performance of the data cleaning and the analytics? HOW CAN I DO IT? WHICH SERVICES DO YOU RECOMMEND (redshift, glue databrew, etc??)?\n\nBtw, Im new on this stuff. Thanks\n\n&amp;#x200B;", "author_fullname": "t2_6bq784p6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Merge multiple parquet files into a single table in database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_153arfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689715203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have several parquet files (around 100 files), all have the same format, the only difference is that each file is the historical data of an specific date. Currently I have all the files stored in AWS S3, but i need to clean, add columns, and manipulate some columns. After that, i will need to use these data for analytics and to train a ML model.  WHAT WOULD BE THE BEST APPROACH FOR ALL OF THIS? Should i merge all the files into a database (all files have the same format and columns) and that would be easier to use and would increase the performance of the data cleaning and the analytics? HOW CAN I DO IT? WHICH SERVICES DO YOU RECOMMEND (redshift, glue databrew, etc??)?&lt;/p&gt;\n\n&lt;p&gt;Btw, Im new on this stuff. Thanks&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "153arfs", "is_robot_indexable": true, "report_reasons": null, "author": "pussydestroyerSPY", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153arfs/merge_multiple_parquet_files_into_a_single_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153arfs/merge_multiple_parquet_files_into_a_single_table/", "subreddit_subscribers": 116642, "created_utc": 1689715203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2cbhndmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why not Flink?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "name": "t3_1537lsw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/DoVZotXbdsmVJKmOK8zw8LGsEWaJ8h1FqxvG72NPA50.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689707994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arroyo.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.arroyo.dev/blog/why-not-flink", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p9vYTWByyn_RWfyv76gB-cIsx13GJSKrLKVQaozRgGA.jpg?auto=webp&amp;s=dbaf44dd0884335f2568c45bac1b286641f291dd", "width": 1500, "height": 938}, "resolutions": [{"url": "https://external-preview.redd.it/p9vYTWByyn_RWfyv76gB-cIsx13GJSKrLKVQaozRgGA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5035122efce19bc2c313301bb9650f47ecf2e340", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/p9vYTWByyn_RWfyv76gB-cIsx13GJSKrLKVQaozRgGA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c43c40755f82a2fda0da737c04e485514d58aee", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/p9vYTWByyn_RWfyv76gB-cIsx13GJSKrLKVQaozRgGA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e5e44026dfdeaee1962edf49402ba47f9fb319c", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/p9vYTWByyn_RWfyv76gB-cIsx13GJSKrLKVQaozRgGA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0927a64888fa50dcfbbf100b673e9d542f96ecd", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/p9vYTWByyn_RWfyv76gB-cIsx13GJSKrLKVQaozRgGA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=864d757975e890385d83b0edd9ad2a8bf48ab632", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/p9vYTWByyn_RWfyv76gB-cIsx13GJSKrLKVQaozRgGA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8a9c5790c897e1e14771fa3417897ca4450d9142", "width": 1080, "height": 675}], "variants": {}, "id": "okhvCYUSSbhOVo3pQ6FK0konOSDUmaObUupo1lx6XNo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1537lsw", "is_robot_indexable": true, "report_reasons": null, "author": "mwylde_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1537lsw/why_not_flink/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.arroyo.dev/blog/why-not-flink", "subreddit_subscribers": 116642, "created_utc": 1689707994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a new grad and have cleared AZ-900,DP-900 and DP-203 certifications and currently working on my portfolio projects. Finding it difficult to find End-to-End Data engineering projects on Azure. But I can see there are so many such projects executed and well documented on AWS and GCP. Decided to learn AWS and GCP on side. So wondering what is the best way to learn AWS and GCP after learning Azure. And if you have any crazy data engineering projects on Azure, kindly let me know.", "author_fullname": "t2_70f26rvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cleared Azure Cloud Certifications. Wondering what is next. Need help !", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1532rt2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689696882.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a new grad and have cleared AZ-900,DP-900 and DP-203 certifications and currently working on my portfolio projects. Finding it difficult to find End-to-End Data engineering projects on Azure. But I can see there are so many such projects executed and well documented on AWS and GCP. Decided to learn AWS and GCP on side. So wondering what is the best way to learn AWS and GCP after learning Azure. And if you have any crazy data engineering projects on Azure, kindly let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1532rt2", "is_robot_indexable": true, "report_reasons": null, "author": "h_buddana", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1532rt2/cleared_azure_cloud_certifications_wondering_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1532rt2/cleared_azure_cloud_certifications_wondering_what/", "subreddit_subscribers": 116642, "created_utc": 1689696882.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}