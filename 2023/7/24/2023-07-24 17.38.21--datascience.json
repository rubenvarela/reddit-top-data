{"kind": "Listing", "data": {"after": "t3_158ehsh", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm not a Data scientist but I do have a few questions for you all. I just saw oppenheimer which, no spoilers, simply revolves around the Manhattan project and the creation of nuclear bombs. \n\nBack in the day, weren't data scientists just the physicists and chemists and mathematicians who did their own studies? Or was there a specific role for someone who handles all the data and does something else with it?", "author_fullname": "t2_f4m0dd2qo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just saw oppenheimer, did data scientists exist back then?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157m5sj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 109, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 109, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690136715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not a Data scientist but I do have a few questions for you all. I just saw oppenheimer which, no spoilers, simply revolves around the Manhattan project and the creation of nuclear bombs. &lt;/p&gt;\n\n&lt;p&gt;Back in the day, weren&amp;#39;t data scientists just the physicists and chemists and mathematicians who did their own studies? Or was there a specific role for someone who handles all the data and does something else with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157m5sj", "is_robot_indexable": true, "report_reasons": null, "author": "Sacred_Tomato", "discussion_type": null, "num_comments": 127, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157m5sj/just_saw_oppenheimer_did_data_scientists_exist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157m5sj/just_saw_oppenheimer_did_data_scientists_exist/", "subreddit_subscribers": 959674, "created_utc": 1690136715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Do you do AI/ML stuff often? Deep learning with Neural Networks?\n\nOr do you just make charts with SQL queries? Excel and .csv files?\n\nOr something in between?", "author_fullname": "t2_kytqh4tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What extent is your job \"Advanced Data Science\" vs \"low-level Data Science\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157sidi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690151635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you do AI/ML stuff often? Deep learning with Neural Networks?&lt;/p&gt;\n\n&lt;p&gt;Or do you just make charts with SQL queries? Excel and .csv files?&lt;/p&gt;\n\n&lt;p&gt;Or something in between?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157sidi", "is_robot_indexable": true, "report_reasons": null, "author": "SeriouslySally36", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157sidi/what_extent_is_your_job_advanced_data_science_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157sidi/what_extent_is_your_job_advanced_data_science_vs/", "subreddit_subscribers": 959674, "created_utc": 1690151635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I am currently using python and tableau separately. Most of the ETLs, data wrangling are done in python and some aggregations. Though I also use LODs in tableau and, of course, the viz.\n\nI just learned about this tabpy but I can't see why would you use it. Any tabpy users here? Hope you can give me some tabpy use cases or what is it's best use.", "author_fullname": "t2_ftzx68gnj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why would you use tabpy? or why not?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1583q4u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690184715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently using python and tableau separately. Most of the ETLs, data wrangling are done in python and some aggregations. Though I also use LODs in tableau and, of course, the viz.&lt;/p&gt;\n\n&lt;p&gt;I just learned about this tabpy but I can&amp;#39;t see why would you use it. Any tabpy users here? Hope you can give me some tabpy use cases or what is it&amp;#39;s best use.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1583q4u", "is_robot_indexable": true, "report_reasons": null, "author": "medyosuper", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1583q4u/why_would_you_use_tabpy_or_why_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1583q4u/why_would_you_use_tabpy_or_why_not/", "subreddit_subscribers": 959674, "created_utc": 1690184715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_h3a1v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[OC] Create your Own Artificial Neural Network in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 133, "top_awarded_type": null, "hide_score": false, "name": "t3_157p1xy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-B07hLh7SWkvm-8aZcuYKPg4CEt0v9ylaDtFqXXBLzc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690143441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/kjjvyye6wrdb1.gif", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?format=png8&amp;s=dee8b32aac95f2353722253648d23e9de68aca89", "width": 400, "height": 381}, "resolutions": [{"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=3b2fcdb4900dd3899def7e3cd7f61addb33d1fc3", "width": 108, "height": 102}, {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=becf43f69cb79e54e448725bb820d8239493286c", "width": 216, "height": 205}, {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=724a6c49a28669d5555c103b439f506de805627c", "width": 320, "height": 304}], "variants": {"gif": {"source": {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?s=cfba8de4c12e7e3c803e8be0a368021277520538", "width": 400, "height": 381}, "resolutions": [{"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=108&amp;crop=smart&amp;s=616ffeb49f0e7f535b950dc1b5fb9bb5258613a4", "width": 108, "height": 102}, {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=216&amp;crop=smart&amp;s=4e6dfda62d035dc593dffc9045451c9da5061a11", "width": 216, "height": 205}, {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=320&amp;crop=smart&amp;s=df724883ded6b38ace50533f4e5d3da7f74299b3", "width": 320, "height": 304}]}, "mp4": {"source": {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?format=mp4&amp;s=69360b7f6256282bd1eae1064ce558e48bcc9574", "width": 400, "height": 381}, "resolutions": [{"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=108&amp;format=mp4&amp;s=d56f84fec62bb582387a03c361c6aa2646e48700", "width": 108, "height": 102}, {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=216&amp;format=mp4&amp;s=9b11c2bb44a7ee57dbb111dd38a9fbd33a3e521d", "width": 216, "height": 205}, {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=320&amp;format=mp4&amp;s=3ee15f162795714a7df05658a1647376d2a4ce10", "width": 320, "height": 304}]}}, "id": "Xtc8PF38RpYdQrQI3yd99HMkHr8pKSFx0oQn4XWWwzM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "157p1xy", "is_robot_indexable": true, "report_reasons": null, "author": "pmocz", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157p1xy/oc_create_your_own_artificial_neural_network_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/kjjvyye6wrdb1.gif", "subreddit_subscribers": 959674, "created_utc": 1690143441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all, \n\n&amp;#x200B;\n\nNext Monday, I start a new a career in data science. \n\nIm moving from academia in science/engineering (PhD 2020, Postdoct 2020-2023) and have been preparing for a move into data science for the past year and a half (taking online courses/workshops in SQL, machine learning, ETL, etc... ) \n\n&amp;#x200B;\n\nA bit nervous for the change. I have used data science principles throughout my academic career, but moving into a 100% data science job is a big step. Any advice from people who made the move from academia (science/engineering) into data science?", "author_fullname": "t2_bemft", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Start My first Data Science Job Next Week...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_158b0bg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690206005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Next Monday, I start a new a career in data science. &lt;/p&gt;\n\n&lt;p&gt;Im moving from academia in science/engineering (PhD 2020, Postdoct 2020-2023) and have been preparing for a move into data science for the past year and a half (taking online courses/workshops in SQL, machine learning, ETL, etc... ) &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;A bit nervous for the change. I have used data science principles throughout my academic career, but moving into a 100% data science job is a big step. Any advice from people who made the move from academia (science/engineering) into data science?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "158b0bg", "is_robot_indexable": true, "report_reasons": null, "author": "lipring69", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/158b0bg/start_my_first_data_science_job_next_week/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/158b0bg/start_my_first_data_science_job_next_week/", "subreddit_subscribers": 959674, "created_utc": 1690206005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello r/datascience,\n\nI work at [Meilisearch](https://github.com/meilisearch/meilisearch), an open-source search engine built in Rust. \ud83e\udd80\n\nWe're exploring semantic search &amp; are [launching vector search](https://blog.meilisearch.com/vector-search-announcement/). It works like this:\n\n* Generate embeddings using third-party (like OpenAI or Hugging Face)\n* Store your vector embeddings alongside documents in Meilisearch\n* Query the database to retrieve your results\n\nWe've built a documentation chatbot prototype and seen users implementing vector search to offer \"similar videos\" recommendations.\n\nLet me know what you think!\n\nThanks for reading,", "author_fullname": "t2_hq7t9ih", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open-source search engine Meilisearch launches vector search", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1589lpv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690202546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I work at &lt;a href=\"https://github.com/meilisearch/meilisearch\"&gt;Meilisearch&lt;/a&gt;, an open-source search engine built in Rust. \ud83e\udd80&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re exploring semantic search &amp;amp; are &lt;a href=\"https://blog.meilisearch.com/vector-search-announcement/\"&gt;launching vector search&lt;/a&gt;. It works like this:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Generate embeddings using third-party (like OpenAI or Hugging Face)&lt;/li&gt;\n&lt;li&gt;Store your vector embeddings alongside documents in Meilisearch&lt;/li&gt;\n&lt;li&gt;Query the database to retrieve your results&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We&amp;#39;ve built a documentation chatbot prototype and seen users implementing vector search to offer &amp;quot;similar videos&amp;quot; recommendations.&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think!&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/e2iItn7IS03UYmRAly73xdLq38XHkqm3KvA50uVh8Rc.jpg?auto=webp&amp;s=54f4b4ba145aae5ee6dc3085a1a2689aec545aa6", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/e2iItn7IS03UYmRAly73xdLq38XHkqm3KvA50uVh8Rc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c7521f5ea1a9167e0b8692b2964050822b1cd104", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/e2iItn7IS03UYmRAly73xdLq38XHkqm3KvA50uVh8Rc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f5a35d0d37f2a6bd60744cf1b69f5133e3564945", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/e2iItn7IS03UYmRAly73xdLq38XHkqm3KvA50uVh8Rc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=842ee95bc98654a59c1262789a3753ca4e7a056d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/e2iItn7IS03UYmRAly73xdLq38XHkqm3KvA50uVh8Rc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc368af3d445e6a3f36328d6d77affa8bd1bf6e7", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/e2iItn7IS03UYmRAly73xdLq38XHkqm3KvA50uVh8Rc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6750c0c7fad8543dcda52cee9160557b5c9e979a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/e2iItn7IS03UYmRAly73xdLq38XHkqm3KvA50uVh8Rc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0bf15908226187fe44ef050af47922af26c3d3da", "width": 1080, "height": 540}], "variants": {}, "id": "TaZMAKevp1jnSx7H7fyYrBp01M2lK7YyAD8QLf5YNfM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1589lpv", "is_robot_indexable": true, "report_reasons": null, "author": "ggStrift", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1589lpv/opensource_search_engine_meilisearch_launches/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1589lpv/opensource_search_engine_meilisearch_launches/", "subreddit_subscribers": 959674, "created_utc": 1690202546.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a large dataset (3000 features are used here), and I am calculating mutual information scores between each row and every other row. Then, I print out the top X mutual information scores (and which two rows correspond to that mutual information score).\n\nWhen I print these values in Google Collab, however, the top scores seem to give different values vs. Jupyter Notebook, even though I am using the same code.\n\nWhat's also interesting is that these values remain the exact same if they are re-run (in both Collab and Jupyter), so I don't believe it's random.\n\nI will show the first 10 printed lines to demonstrate the difference:\n\nJupyter Notebook\n\n&gt;\\#1: 1397 and 1427: 1.202717856027216  \n&gt;  \n&gt;\\#2: 1400 and 1431: 1.074839333797198  \n&gt;  \n&gt;\\#3: 239 and 423: 1.068564020019758  \n&gt;  \n&gt;\\#4: 1146 and 1400: 1.06539274118781  \n&gt;  \n&gt;\\#5: 1146 and 1177: 1.0448225876789148  \n&gt;  \n&gt;\\#6: 1146 and 1431: 1.0431195289315978  \n&gt;  \n&gt;\\#7: 1411 and 1431: 1.0103705901911808  \n&gt;  \n&gt;\\#8: 1111 and 1525: 1.0037660750701747  \n&gt;  \n&gt;\\#9: 1177 and 1431: 0.9890857137951587  \n&gt;  \n&gt;\\#10: 1146 and 1411: 0.9852993714583413\n\nGoogle Collab\n\n&gt;\\#1: 1146 and 1400: 1.1822506247498457  \n&gt;  \n&gt;\\#2: 239 and 423: 1.0994706698596624  \n&gt;  \n&gt;\\#3: 1397 and 1427: 1.0838558257556066  \n&gt;  \n&gt;\\#4: 1146 and 1177: 1.0766228782259293  \n&gt;  \n&gt;\\#5: 423 and 73: 1.0258894687690598  \n&gt;  \n&gt;\\#6: 1177 and 1411: 1.021696037520684  \n&gt;  \n&gt;\\#7: 1400 and 1431: 1.0134240574963582  \n&gt;  \n&gt;\\#8: 1111 and 1525: 1.0071214141815927  \n&gt;  \n&gt;\\#9: 1146 and 1431: 0.972276347390304  \n&gt;  \n&gt;\\#10: 1146 and 1411: 0.9689222844930194\n\n**Here is the relevant code**\n\nFirst cell:\n\n&amp;#x200B;\n\n    # Note: The following is after inputting the Excel spreadsheet data into dataframe then transposing\n    df = df.T\n    \n    import pandas as pd\n    from sklearn.feature_selection import mutual_info_regression\n\n    # Load the dataset\n    #data = pd.read_excel(\"analysis_file2.xlsx\")\n\n    # Select the first 3,000 feature columns\n    features = df.columns[1:3001]\n\n    # Compute the mutual information between each pair of features\n    mi_matrix = np.zeros((len(features), len(features)))\n    for i in range(len(features)):\n        for j in range(i+1, len(features)):\n            feature_A = df[features[i]].values\n            feature_B = df[features[j]].values\n            mi = mutual_info_regression(feature_A.reshape(-1, 1), feature_B)[0]\n            mi_matrix[i, j] = mi\n            mi_matrix[j, i] = mi\n\n\nSecond cell:\n\n    # Find the top 30 mutual information values\n\n    num = 30 * 2\n\n    top = np.argsort(mi_matrix, axis=None)[-num:]\n\n    sorted_pairs = sorted(zip(top, mi_matrix[np.unravel_index(top, mi_matrix.shape)]), key=lambda x: x[1],     reverse=True)\n\n    # Sorted values and indeces\n    sorted_indices = [pair[0] for pair in sorted_pairs]\n    sorted_values = [pair[1] for pair in sorted_pairs]\n\n    top = np.unique(top)\n    top = np.unravel_index(top, mi_matrix.shape)\n\n    # Print top values Sorted (greatest to least)\n\n    feature1 = [None] * num\n    feature2 = [None] * num\n    mi_value = [None] * num\n    for i in range(num):\n        idx1, idx2 = top[0][i], top[1][i]\n        feature1[i] = features[idx1]\n        feature2[i] = features[idx2]\n        mi_value[i] = mi_matrix[idx1, idx2]\n\n    ind = 0\n    i = 0\n    while (ind != num):\n        if (mi_value[i] == sorted_values[ind]):\n          ind += 2\n          print(f\"#{int(ind/2)}: {feature1[i]} and {feature2[i]}: {mi_value[i]}\")\n          i = 0\n        xi += 1", "author_fullname": "t2_efotpocwb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about getting different results in Google Collab than Jupyter Notebook when calculating mutual information scores.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157ubmj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690156320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a large dataset (3000 features are used here), and I am calculating mutual information scores between each row and every other row. Then, I print out the top X mutual information scores (and which two rows correspond to that mutual information score).&lt;/p&gt;\n\n&lt;p&gt;When I print these values in Google Collab, however, the top scores seem to give different values vs. Jupyter Notebook, even though I am using the same code.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s also interesting is that these values remain the exact same if they are re-run (in both Collab and Jupyter), so I don&amp;#39;t believe it&amp;#39;s random.&lt;/p&gt;\n\n&lt;p&gt;I will show the first 10 printed lines to demonstrate the difference:&lt;/p&gt;\n\n&lt;p&gt;Jupyter Notebook&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;#1: 1397 and 1427: 1.202717856027216  &lt;/p&gt;\n\n&lt;p&gt;#2: 1400 and 1431: 1.074839333797198  &lt;/p&gt;\n\n&lt;p&gt;#3: 239 and 423: 1.068564020019758  &lt;/p&gt;\n\n&lt;p&gt;#4: 1146 and 1400: 1.06539274118781  &lt;/p&gt;\n\n&lt;p&gt;#5: 1146 and 1177: 1.0448225876789148  &lt;/p&gt;\n\n&lt;p&gt;#6: 1146 and 1431: 1.0431195289315978  &lt;/p&gt;\n\n&lt;p&gt;#7: 1411 and 1431: 1.0103705901911808  &lt;/p&gt;\n\n&lt;p&gt;#8: 1111 and 1525: 1.0037660750701747  &lt;/p&gt;\n\n&lt;p&gt;#9: 1177 and 1431: 0.9890857137951587  &lt;/p&gt;\n\n&lt;p&gt;#10: 1146 and 1411: 0.9852993714583413&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Google Collab&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;#1: 1146 and 1400: 1.1822506247498457  &lt;/p&gt;\n\n&lt;p&gt;#2: 239 and 423: 1.0994706698596624  &lt;/p&gt;\n\n&lt;p&gt;#3: 1397 and 1427: 1.0838558257556066  &lt;/p&gt;\n\n&lt;p&gt;#4: 1146 and 1177: 1.0766228782259293  &lt;/p&gt;\n\n&lt;p&gt;#5: 423 and 73: 1.0258894687690598  &lt;/p&gt;\n\n&lt;p&gt;#6: 1177 and 1411: 1.021696037520684  &lt;/p&gt;\n\n&lt;p&gt;#7: 1400 and 1431: 1.0134240574963582  &lt;/p&gt;\n\n&lt;p&gt;#8: 1111 and 1525: 1.0071214141815927  &lt;/p&gt;\n\n&lt;p&gt;#9: 1146 and 1431: 0.972276347390304  &lt;/p&gt;\n\n&lt;p&gt;#10: 1146 and 1411: 0.9689222844930194&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Here is the relevant code&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;First cell:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# Note: The following is after inputting the Excel spreadsheet data into dataframe then transposing\ndf = df.T\n\nimport pandas as pd\nfrom sklearn.feature_selection import mutual_info_regression\n\n# Load the dataset\n#data = pd.read_excel(&amp;quot;analysis_file2.xlsx&amp;quot;)\n\n# Select the first 3,000 feature columns\nfeatures = df.columns[1:3001]\n\n# Compute the mutual information between each pair of features\nmi_matrix = np.zeros((len(features), len(features)))\nfor i in range(len(features)):\n    for j in range(i+1, len(features)):\n        feature_A = df[features[i]].values\n        feature_B = df[features[j]].values\n        mi = mutual_info_regression(feature_A.reshape(-1, 1), feature_B)[0]\n        mi_matrix[i, j] = mi\n        mi_matrix[j, i] = mi\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Second cell:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# Find the top 30 mutual information values\n\nnum = 30 * 2\n\ntop = np.argsort(mi_matrix, axis=None)[-num:]\n\nsorted_pairs = sorted(zip(top, mi_matrix[np.unravel_index(top, mi_matrix.shape)]), key=lambda x: x[1],     reverse=True)\n\n# Sorted values and indeces\nsorted_indices = [pair[0] for pair in sorted_pairs]\nsorted_values = [pair[1] for pair in sorted_pairs]\n\ntop = np.unique(top)\ntop = np.unravel_index(top, mi_matrix.shape)\n\n# Print top values Sorted (greatest to least)\n\nfeature1 = [None] * num\nfeature2 = [None] * num\nmi_value = [None] * num\nfor i in range(num):\n    idx1, idx2 = top[0][i], top[1][i]\n    feature1[i] = features[idx1]\n    feature2[i] = features[idx2]\n    mi_value[i] = mi_matrix[idx1, idx2]\n\nind = 0\ni = 0\nwhile (ind != num):\n    if (mi_value[i] == sorted_values[ind]):\n      ind += 2\n      print(f&amp;quot;#{int(ind/2)}: {feature1[i]} and {feature2[i]}: {mi_value[i]}&amp;quot;)\n      i = 0\n    xi += 1\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157ubmj", "is_robot_indexable": true, "report_reasons": null, "author": "MLquestionAccount", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157ubmj/question_about_getting_different_results_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157ubmj/question_about_getting_different_results_in/", "subreddit_subscribers": 959674, "created_utc": 1690156320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys, just need your ideas on how to approach this. So i have this dataset to the left with numerical and categorical variables. Can I just do a one-hot encoding for the categorical and normally proceed to the usual time-series modeling using the transformed dataset to the right? Thanks!\n\nhttps://preview.redd.it/8zysqjfgktdb1.png?width=1675&amp;format=png&amp;auto=webp&amp;s=f402b245ce8622993c5422b1e0f4c9b037adea8e", "author_fullname": "t2_sh45a0v7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time-series modeling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 18, "top_awarded_type": null, "hide_score": false, "media_metadata": {"8zysqjfgktdb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 14, "x": 108, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8ec25c1d8c5260b32044a8d1d2c7965452fe93f"}, {"y": 28, "x": 216, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=76da074f6a345718dce8f97a31f4582c3d199887"}, {"y": 41, "x": 320, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=19758c3989b169e5457115c77d1a35303eccddd5"}, {"y": 83, "x": 640, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=71d4208e9d6ac68817068452826eae25f0e622b5"}, {"y": 124, "x": 960, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc332fcf9f0bcd5c4aa6171426006a331459333b"}, {"y": 140, "x": 1080, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4bbe1bd5d0b90c30f042e4ddb18db52bc22f94ad"}], "s": {"y": 218, "x": 1675, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=1675&amp;format=png&amp;auto=webp&amp;s=f402b245ce8622993c5422b1e0f4c9b037adea8e"}, "id": "8zysqjfgktdb1"}}, "name": "t3_157wzje", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/wISw3YFzaRxhr815zlGCWlQvuLttFnCJ37dol4wQXV8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690163751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, just need your ideas on how to approach this. So i have this dataset to the left with numerical and categorical variables. Can I just do a one-hot encoding for the categorical and normally proceed to the usual time-series modeling using the transformed dataset to the right? Thanks!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8zysqjfgktdb1.png?width=1675&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f402b245ce8622993c5422b1e0f4c9b037adea8e\"&gt;https://preview.redd.it/8zysqjfgktdb1.png?width=1675&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f402b245ce8622993c5422b1e0f4c9b037adea8e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157wzje", "is_robot_indexable": true, "report_reasons": null, "author": "chime_enjoyer", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157wzje/timeseries_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157wzje/timeseries_modeling/", "subreddit_subscribers": 959674, "created_utc": 1690163751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This is what I am currently stuck with. With more than a hundred sales places and, with so little data on some less successful SKU in less successful spots, like one per a few month. Or newer SKU having less than a month of data.\n\nWhat I am currently doing is \"best guessing\" using excel spreadsheet(i know, i know), and finding analogues in attempt to even remotely forecast fresh SKU, taking sales per spot, per day, per SKU, and filling blanks with the same data of tge same days of another week. Then summarizing it and applying reasonable multipliers.\n\nIs there a better way? It's a lot to do with so many products for so many places, even with careful pivoting.", "author_fullname": "t2_2ndz0you", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forecasting on SKU-level bu points of sale?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15814hq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690176111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is what I am currently stuck with. With more than a hundred sales places and, with so little data on some less successful SKU in less successful spots, like one per a few month. Or newer SKU having less than a month of data.&lt;/p&gt;\n\n&lt;p&gt;What I am currently doing is &amp;quot;best guessing&amp;quot; using excel spreadsheet(i know, i know), and finding analogues in attempt to even remotely forecast fresh SKU, taking sales per spot, per day, per SKU, and filling blanks with the same data of tge same days of another week. Then summarizing it and applying reasonable multipliers.&lt;/p&gt;\n\n&lt;p&gt;Is there a better way? It&amp;#39;s a lot to do with so many products for so many places, even with careful pivoting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15814hq", "is_robot_indexable": true, "report_reasons": null, "author": "WlrsWrwgn", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15814hq/forecasting_on_skulevel_bu_points_of_sale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15814hq/forecasting_on_skulevel_bu_points_of_sale/", "subreddit_subscribers": 959674, "created_utc": 1690176111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am recently unemployed R&amp;D scientist and was messing with statistics, data and programing (but my core background is materials eng, solid state physics, chemistry and nanotech) I have phd and was always an enthusiastic amateur working with R and so on. I last few years, it was almost exclusively my job at large corporation (pehaps because i was the only scientist remaining). \n\nBut I dont consider myself data scientist, maybe an engineer, maybe scientist, maybe chemist, with certain level of know how in R and other tools. I know quite a bit of math behind models and regresions but not consider myself pure or applied math major. PhD level, but still more engineer.  \n\nMy programming skills are VERY limited, and i feel that at least solid python and sql would HAVE to be added if i am to get another employment in data field. How hard would that be? How do i best approach it? How many months or years would it take to get to the level of being safely employable in the atlantic region of the US?\n\nI see ton of ads in my social networks, berkley, purdue global, UVA, UT austin, VCU (that's some 3-6 months certificate) MIT, Georgia Tech, ... are they any good? Would it be good use of my time? Would it help with employment? Or are those bootcamps and \"online\" schools more a scam and red flag for job applications? What is your take and why?   ", "author_fullname": "t2_zma26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "online masters? what is your take on those courses? are they any good? for employment? for knowledge?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157xgpk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690165087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am recently unemployed R&amp;amp;D scientist and was messing with statistics, data and programing (but my core background is materials eng, solid state physics, chemistry and nanotech) I have phd and was always an enthusiastic amateur working with R and so on. I last few years, it was almost exclusively my job at large corporation (pehaps because i was the only scientist remaining). &lt;/p&gt;\n\n&lt;p&gt;But I dont consider myself data scientist, maybe an engineer, maybe scientist, maybe chemist, with certain level of know how in R and other tools. I know quite a bit of math behind models and regresions but not consider myself pure or applied math major. PhD level, but still more engineer.  &lt;/p&gt;\n\n&lt;p&gt;My programming skills are VERY limited, and i feel that at least solid python and sql would HAVE to be added if i am to get another employment in data field. How hard would that be? How do i best approach it? How many months or years would it take to get to the level of being safely employable in the atlantic region of the US?&lt;/p&gt;\n\n&lt;p&gt;I see ton of ads in my social networks, berkley, purdue global, UVA, UT austin, VCU (that&amp;#39;s some 3-6 months certificate) MIT, Georgia Tech, ... are they any good? Would it be good use of my time? Would it help with employment? Or are those bootcamps and &amp;quot;online&amp;quot; schools more a scam and red flag for job applications? What is your take and why?   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157xgpk", "is_robot_indexable": true, "report_reasons": null, "author": "yik77", "discussion_type": null, "num_comments": 16, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157xgpk/online_masters_what_is_your_take_on_those_courses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157xgpk/online_masters_what_is_your_take_on_those_courses/", "subreddit_subscribers": 959674, "created_utc": 1690165087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm not a Data Scientist but I'm currently writing my master's thesis on the current state of the Data Science market.\n\nI've noticed that the market seems saturated compared with previous years, and yet it seems to me that the current challenges still require a lot of Data Scientists - GenAI and NLP challenges, for example.\n\n* What do you think are the reasons for this?\n* How is the profession becoming hyper-specialised (arrival of MLOps, vision specialists, etc.)?\n* With the arrival of 'packaged', low-code solutions from big tech, which could be suitable for 80% of projects, do you think 'home-made' DS solutions have a future? Is there a paradox here with the hyperspecialisation mentioned above?\n* What are the current strategic issues surrounding Data Science that your company is facing?\n* As a Data Scientist, how do you see your job evolving over the next few years?\n\nI look forward to reading your answers!\n\nThanks for your time!", "author_fullname": "t2_adwuq6omo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Current state of the Data Science market", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1584ht7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690189044.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690187272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not a Data Scientist but I&amp;#39;m currently writing my master&amp;#39;s thesis on the current state of the Data Science market.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed that the market seems saturated compared with previous years, and yet it seems to me that the current challenges still require a lot of Data Scientists - GenAI and NLP challenges, for example.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What do you think are the reasons for this?&lt;/li&gt;\n&lt;li&gt;How is the profession becoming hyper-specialised (arrival of MLOps, vision specialists, etc.)?&lt;/li&gt;\n&lt;li&gt;With the arrival of &amp;#39;packaged&amp;#39;, low-code solutions from big tech, which could be suitable for 80% of projects, do you think &amp;#39;home-made&amp;#39; DS solutions have a future? Is there a paradox here with the hyperspecialisation mentioned above?&lt;/li&gt;\n&lt;li&gt;What are the current strategic issues surrounding Data Science that your company is facing?&lt;/li&gt;\n&lt;li&gt;As a Data Scientist, how do you see your job evolving over the next few years?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I look forward to reading your answers!&lt;/p&gt;\n\n&lt;p&gt;Thanks for your time!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1584ht7", "is_robot_indexable": true, "report_reasons": null, "author": "Pole_l", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1584ht7/current_state_of_the_data_science_market/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1584ht7/current_state_of_the_data_science_market/", "subreddit_subscribers": 959674, "created_utc": 1690187272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a data set that boils down to Three clomuns: 1.Supplier name 2. Number of transactions with supplier 3. Total value of those transaction.\n\nI'm trying to find the best way to rank all suppliers based on these two features which are equally important in this case. \nNote: -more transaction doesn't mean more value\n\nWhat I did is I normalized each feature. Making all values scale down to 0 to 1. Then I added the 2 normalized values to each other. Then used this new values to rank all suppliers. Looking at the results and  what I expected a good ranking to be, I'm happy with the result.\n\nHowever, I feel like the existing of outliers might've made the ranking.. not as accurate as it can be. Am I right or wrong? How would you suggest fixing this? Is there an entierly better way to do this?\n\nFinal note: I tried standarizing and got the exact same ranking. I'm not sure if that was supposed to happen or if it was a special case.", "author_fullname": "t2_59mf5tbp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would normalizing be affected by outliers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157o4qn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690141303.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data set that boils down to Three clomuns: 1.Supplier name 2. Number of transactions with supplier 3. Total value of those transaction.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to find the best way to rank all suppliers based on these two features which are equally important in this case. \nNote: -more transaction doesn&amp;#39;t mean more value&lt;/p&gt;\n\n&lt;p&gt;What I did is I normalized each feature. Making all values scale down to 0 to 1. Then I added the 2 normalized values to each other. Then used this new values to rank all suppliers. Looking at the results and  what I expected a good ranking to be, I&amp;#39;m happy with the result.&lt;/p&gt;\n\n&lt;p&gt;However, I feel like the existing of outliers might&amp;#39;ve made the ranking.. not as accurate as it can be. Am I right or wrong? How would you suggest fixing this? Is there an entierly better way to do this?&lt;/p&gt;\n\n&lt;p&gt;Final note: I tried standarizing and got the exact same ranking. I&amp;#39;m not sure if that was supposed to happen or if it was a special case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157o4qn", "is_robot_indexable": true, "report_reasons": null, "author": "kit_kaat", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157o4qn/how_would_normalizing_be_affected_by_outliers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157o4qn/how_would_normalizing_be_affected_by_outliers/", "subreddit_subscribers": 959674, "created_utc": 1690141303.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Under Data Generalist they have - \n\nData Analysts ([https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Analyst.pdf](https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Analyst.pdf))\n\nData Engineers ([https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Engineer.pdf](https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Engineer.pdf)) \n\nData Managers ([https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Manager.pdf](https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Manager.pdf)) \n\nGeospatial Analysts ([https://www.apsc.gov.au/sites/default/files/2022-12/Geospatial%20Analyst.pdf](https://www.apsc.gov.au/sites/default/files/2022-12/Geospatial%20Analyst.pdf)) \n\n&amp;#x200B;\n\nUnder Data Specialist they have - \n\nData Scientists ([https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Scientist.pdf](https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Scientist.pdf)) \n\nStatistical Methodologists ([https://www.apsc.gov.au/sites/default/files/2022-12/Statistician.pdf](https://www.apsc.gov.au/sites/default/files/2022-12/Statistician.pdf)) \n\n&amp;#x200B;\n\nAll this can be found here [https://www.apsjobs.gov.au/s/graduate-portal/stream/data-stream-MCLCEFMBKO5ZH4TCQQTNQTBMUB4A](https://www.apsjobs.gov.au/s/graduate-portal/stream/data-stream-MCLCEFMBKO5ZH4TCQQTNQTBMUB4A) \n\n&amp;#x200B;\n\nInsightful? Misleading? A bit of both? \n\nInterested in what you people think. ", "author_fullname": "t2_lrr4olcp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "This is how the Australian Government classifies each data science related job for their graduate program.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1588lx6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690199890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Under Data Generalist they have - &lt;/p&gt;\n\n&lt;p&gt;Data Analysts (&lt;a href=\"https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Analyst.pdf\"&gt;https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Analyst.pdf&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Data Engineers (&lt;a href=\"https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Engineer.pdf\"&gt;https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Engineer.pdf&lt;/a&gt;) &lt;/p&gt;\n\n&lt;p&gt;Data Managers (&lt;a href=\"https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Manager.pdf\"&gt;https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Manager.pdf&lt;/a&gt;) &lt;/p&gt;\n\n&lt;p&gt;Geospatial Analysts (&lt;a href=\"https://www.apsc.gov.au/sites/default/files/2022-12/Geospatial%20Analyst.pdf\"&gt;https://www.apsc.gov.au/sites/default/files/2022-12/Geospatial%20Analyst.pdf&lt;/a&gt;) &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Under Data Specialist they have - &lt;/p&gt;\n\n&lt;p&gt;Data Scientists (&lt;a href=\"https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Scientist.pdf\"&gt;https://www.apsc.gov.au/sites/default/files/2022-12/Data%20Scientist.pdf&lt;/a&gt;) &lt;/p&gt;\n\n&lt;p&gt;Statistical Methodologists (&lt;a href=\"https://www.apsc.gov.au/sites/default/files/2022-12/Statistician.pdf\"&gt;https://www.apsc.gov.au/sites/default/files/2022-12/Statistician.pdf&lt;/a&gt;) &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;All this can be found here &lt;a href=\"https://www.apsjobs.gov.au/s/graduate-portal/stream/data-stream-MCLCEFMBKO5ZH4TCQQTNQTBMUB4A\"&gt;https://www.apsjobs.gov.au/s/graduate-portal/stream/data-stream-MCLCEFMBKO5ZH4TCQQTNQTBMUB4A&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Insightful? Misleading? A bit of both? &lt;/p&gt;\n\n&lt;p&gt;Interested in what you people think. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1588lx6", "is_robot_indexable": true, "report_reasons": null, "author": "Crazy_Cucumber_3888", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1588lx6/this_is_how_the_australian_government_classifies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1588lx6/this_is_how_the_australian_government_classifies/", "subreddit_subscribers": 959674, "created_utc": 1690199890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "According to most career advice it would be good to have some projects in the portfolio to tick the skill boxes when applying.\n\nHowever, Github can get crowded fast. Thus, posting a link to GitHub won\u2018t help much. Ain\u2018t no one got tile to browse all projects just to find the one where you used a lot of SQL or XGBoost.\n\nThe goal would be to give a sentence like \u201eSkills in SQL\u201c and directly link somehow the corresponding repositories. This will maximise the number of checked checkboxes when applying I guess.\n\nDo you have an idea how to achieve this in a decent (beautiful, simple, well structured) way?", "author_fullname": "t2_e9pqzw56", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Project Portfolio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1583uu6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690185125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;According to most career advice it would be good to have some projects in the portfolio to tick the skill boxes when applying.&lt;/p&gt;\n\n&lt;p&gt;However, Github can get crowded fast. Thus, posting a link to GitHub won\u2018t help much. Ain\u2018t no one got tile to browse all projects just to find the one where you used a lot of SQL or XGBoost.&lt;/p&gt;\n\n&lt;p&gt;The goal would be to give a sentence like \u201eSkills in SQL\u201c and directly link somehow the corresponding repositories. This will maximise the number of checked checkboxes when applying I guess.&lt;/p&gt;\n\n&lt;p&gt;Do you have an idea how to achieve this in a decent (beautiful, simple, well structured) way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1583uu6", "is_robot_indexable": true, "report_reasons": null, "author": "1DimensionIsViolence", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1583uu6/project_portfolio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1583uu6/project_portfolio/", "subreddit_subscribers": 959674, "created_utc": 1690185125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, \n\nI came up with a certain statistical model that can produce gaussian-like and \"asymmetric-gaussian\" (Maxwell-Boltxmann, chi-squared, etc) type distributions. I want to test it against a large social dataset, in the order of 1e5 and more points per curve. The content (subject, topic) does not matter, it's actually interesting to see how unexpected the use can be. But it must look like a bell-shaped curve, and be freely available. Any tips please?", "author_fullname": "t2_17gpcw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help request - need a large social dataset", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1586uhg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690194783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I came up with a certain statistical model that can produce gaussian-like and &amp;quot;asymmetric-gaussian&amp;quot; (Maxwell-Boltxmann, chi-squared, etc) type distributions. I want to test it against a large social dataset, in the order of 1e5 and more points per curve. The content (subject, topic) does not matter, it&amp;#39;s actually interesting to see how unexpected the use can be. But it must look like a bell-shaped curve, and be freely available. Any tips please?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1586uhg", "is_robot_indexable": true, "report_reasons": null, "author": "nctrd", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1586uhg/help_request_need_a_large_social_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1586uhg/help_request_need_a_large_social_dataset/", "subreddit_subscribers": 959674, "created_utc": 1690194783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_3t7r4sfm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please help with type of questions one should be ready to attend in a Python coding interview for a Data Scientist role while transitioning from a different function and any suggestions on preparation strategy.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_158fllk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690216385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "158fllk", "is_robot_indexable": true, "report_reasons": null, "author": "djch1989", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/158fllk/please_help_with_type_of_questions_one_should_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/158fllk/please_help_with_type_of_questions_one_should_be/", "subreddit_subscribers": 959674, "created_utc": 1690216385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "When we have such a simple neural network, where we have all neurons of the hidden layer, receiving exactly the same input and the same number of inputs (nth features), from the previous layer. Will it be right to say that the only reason of using multiple neurons in the hidden layer is to initialize random weights and/or biases to all neurons, just so we avoid getting trapped into some local minima? Or are there other reasons of using multiple neurons?  I do understand that there could be several reasons to use multiple neurons, but my question is specifically for the neural network shown in image above. \n\nhttps://preview.redd.it/xr0pn64dpxdb1.png?width=804&amp;format=png&amp;auto=webp&amp;s=6d19a3f6f2c9c82ef92ffcaa66a6ff6c04467202", "author_fullname": "t2_hcgjj0xo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do we use multiple neurons in hidden layer for a simple ANN?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 51, "top_awarded_type": null, "hide_score": true, "media_metadata": {"xr0pn64dpxdb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 39, "x": 108, "u": "https://preview.redd.it/xr0pn64dpxdb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d13cc751a1ff208c596a5b318f49ba504211c25"}, {"y": 78, "x": 216, "u": "https://preview.redd.it/xr0pn64dpxdb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55e4065fe89ee0914b0c077b069a0ef89e46562e"}, {"y": 116, "x": 320, "u": "https://preview.redd.it/xr0pn64dpxdb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1e99f4bdfe1a546bcfd980ccd8ccb60e79fbf78"}, {"y": 233, "x": 640, "u": "https://preview.redd.it/xr0pn64dpxdb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4dc53a7201c4ae74e14671fcb32f1f0286670d55"}], "s": {"y": 293, "x": 804, "u": "https://preview.redd.it/xr0pn64dpxdb1.png?width=804&amp;format=png&amp;auto=webp&amp;s=6d19a3f6f2c9c82ef92ffcaa66a6ff6c04467202"}, "id": "xr0pn64dpxdb1"}}, "name": "t3_158ef4u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZNyUSSr6oghZFHtyhqh-NBhePYSndnXjp-kq6WzUFmA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690213796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When we have such a simple neural network, where we have all neurons of the hidden layer, receiving exactly the same input and the same number of inputs (nth features), from the previous layer. Will it be right to say that the only reason of using multiple neurons in the hidden layer is to initialize random weights and/or biases to all neurons, just so we avoid getting trapped into some local minima? Or are there other reasons of using multiple neurons?  I do understand that there could be several reasons to use multiple neurons, but my question is specifically for the neural network shown in image above. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/xr0pn64dpxdb1.png?width=804&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d19a3f6f2c9c82ef92ffcaa66a6ff6c04467202\"&gt;https://preview.redd.it/xr0pn64dpxdb1.png?width=804&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d19a3f6f2c9c82ef92ffcaa66a6ff6c04467202&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "158ef4u", "is_robot_indexable": true, "report_reasons": null, "author": "Total-Opposite-8396", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/158ef4u/why_do_we_use_multiple_neurons_in_hidden_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/158ef4u/why_do_we_use_multiple_neurons_in_hidden_layer/", "subreddit_subscribers": 959674, "created_utc": 1690213796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a problem I'm trying to solve and I'm block so I thought about consulting here, maybe it would help  \nI have a table and one of the features can receive multiple labels , lets say a several numbers. There is no limit to how many labels can be assigned or any limit to the labels value themselves (for clarification, one user can have \\[1,2\\] - 2 labels  and other can have \\[3,22,345,999999999\\] - 4 labels and so on. Also as you can see labels can be from 1 to 999999999 with no limit)\n\nThe trivial solution would be OHE it but even as sparse matrix it can lead to insainely large matrix because there is no limit to have many labels can exist. Target encoding I don't belive to be a solution here and Binary Relevance seems too computationally heavy too. \n\nI'm out of ideas and don't know what could be the possible solution here. Maybe if someone encountered any similar issue or have any insight in general could help?\n\nThanks!", "author_fullname": "t2_3hqmko1r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have multi label features I can't OneHotEncode because it will create insainely large dataset, any other solutions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1585c7p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690190088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a problem I&amp;#39;m trying to solve and I&amp;#39;m block so I thought about consulting here, maybe it would help&lt;br/&gt;\nI have a table and one of the features can receive multiple labels , lets say a several numbers. There is no limit to how many labels can be assigned or any limit to the labels value themselves (for clarification, one user can have [1,2] - 2 labels  and other can have [3,22,345,999999999] - 4 labels and so on. Also as you can see labels can be from 1 to 999999999 with no limit)&lt;/p&gt;\n\n&lt;p&gt;The trivial solution would be OHE it but even as sparse matrix it can lead to insainely large matrix because there is no limit to have many labels can exist. Target encoding I don&amp;#39;t belive to be a solution here and Binary Relevance seems too computationally heavy too. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m out of ideas and don&amp;#39;t know what could be the possible solution here. Maybe if someone encountered any similar issue or have any insight in general could help?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1585c7p", "is_robot_indexable": true, "report_reasons": null, "author": "nuriel8833", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1585c7p/i_have_multi_label_features_i_cant_onehotencode/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1585c7p/i_have_multi_label_features_i_cant_onehotencode/", "subreddit_subscribers": 959674, "created_utc": 1690190088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have more than 5 year experience for data sciences work. I used to build ETL and abtest for regional leading enterprise. and I am seeking working aboard for better career and need to prepare the interview in English. unfortunately, I do not need to spoke or write English in my past work,   and I worried I could not express my role and experience clearly in English. how to practice and prepare for the interview for a non-native speaker.", "author_fullname": "t2_egz713h9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to prepare a job interview\uff1f", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157z1qi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690169649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have more than 5 year experience for data sciences work. I used to build ETL and abtest for regional leading enterprise. and I am seeking working aboard for better career and need to prepare the interview in English. unfortunately, I do not need to spoke or write English in my past work,   and I worried I could not express my role and experience clearly in English. how to practice and prepare for the interview for a non-native speaker.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157z1qi", "is_robot_indexable": true, "report_reasons": null, "author": "LunaSolarMilkway", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157z1qi/how_to_prepare_a_job_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157z1qi/how_to_prepare_a_job_interview/", "subreddit_subscribers": 959674, "created_utc": 1690169649.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have 3 level categorical response, kind of imbalanced (roughly 60, 25, 15, been a nightmare trying to apply SMOTE to more than 2 levels but that's down the road). Most predictors are 0, but the data are not sparse (in the strict definition of sparse). I tried LDA and decision trees, both models just predict the majority class for every obs. Do I need to rotate or scale the predictors in order to control for them being mostly 0?", "author_fullname": "t2_abhp8o9x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying classification on a dataset where most predictors are zero - do I need to scale or rotate, and if so how?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1589ow8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690202777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 3 level categorical response, kind of imbalanced (roughly 60, 25, 15, been a nightmare trying to apply SMOTE to more than 2 levels but that&amp;#39;s down the road). Most predictors are 0, but the data are not sparse (in the strict definition of sparse). I tried LDA and decision trees, both models just predict the majority class for every obs. Do I need to rotate or scale the predictors in order to control for them being mostly 0?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1589ow8", "is_robot_indexable": true, "report_reasons": null, "author": "son_of_tv_c", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1589ow8/trying_classification_on_a_dataset_where_most/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1589ow8/trying_classification_on_a_dataset_where_most/", "subreddit_subscribers": 959674, "created_utc": 1690202777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\nI am currently envolved in a project to convert calls data from my company to text, so we can derive some value from it or apply NLP techniques... right now we are using already existing Speech-to-text models that cover the Portuguese language (dont need english, only portuguese) but we are struggling with finding a model that allows us to add our own training data to the model. What we want is a pre-trained model that allows us to add extra training data, that we will be creating internally, with the script of the people from the call center, and their voices reading the script.\n\nAnyone knows of the right model or speech-to-text tool that allows for incorporating own training data?  \n\nThanks is advance", "author_fullname": "t2_g4i7s8lsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Speech to Text - Adding My Own Training Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15854m1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690189372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently envolved in a project to convert calls data from my company to text, so we can derive some value from it or apply NLP techniques... right now we are using already existing Speech-to-text models that cover the Portuguese language (dont need english, only portuguese) but we are struggling with finding a model that allows us to add our own training data to the model. What we want is a pre-trained model that allows us to add extra training data, that we will be creating internally, with the script of the people from the call center, and their voices reading the script.&lt;/p&gt;\n\n&lt;p&gt;Anyone knows of the right model or speech-to-text tool that allows for incorporating own training data?  &lt;/p&gt;\n\n&lt;p&gt;Thanks is advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15854m1", "is_robot_indexable": true, "report_reasons": null, "author": "CallMeDataSciDaddy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15854m1/speech_to_text_adding_my_own_training_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15854m1/speech_to_text_adding_my_own_training_data/", "subreddit_subscribers": 959674, "created_utc": 1690189372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm going into my second year in university and I want to break into the data science and data analytics field. Unfortunately, I'm not enrolled in a computer science or a data-related major, I'm a computational psychology major. However, I want to get some real work experience in the data industry. I've been completing a few courses online for data analytics, but I'm not sure what I should do to get a real job or internship in a related field. What are some recommended stepping stones I should take to enter this field?", "author_fullname": "t2_42bxyp3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some ways to get experience for jobs in data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1582l16", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690180948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m going into my second year in university and I want to break into the data science and data analytics field. Unfortunately, I&amp;#39;m not enrolled in a computer science or a data-related major, I&amp;#39;m a computational psychology major. However, I want to get some real work experience in the data industry. I&amp;#39;ve been completing a few courses online for data analytics, but I&amp;#39;m not sure what I should do to get a real job or internship in a related field. What are some recommended stepping stones I should take to enter this field?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1582l16", "is_robot_indexable": true, "report_reasons": null, "author": "posturegeek", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1582l16/what_are_some_ways_to_get_experience_for_jobs_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1582l16/what_are_some_ways_to_get_experience_for_jobs_in/", "subreddit_subscribers": 959674, "created_utc": 1690180948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Apologies if this is the wrong subreddit, will be happy to delete if so! \n\nI'm conducting a social science research project and am hoping to use Twitter data in my paper. Specifically, I need to pull all tweets that mention a specific term, the replies to those tweets, and replies to tweets made by a specific user from within a very specific time frame. I had been planning to just use Twitter's advanced search function to do so, but I've since learned that that tool is not comprehensive and does not reflect all tweets. Is this something I can use Twitter's free API/Python to do? I'm not opposed to learning some basic programming if necessary. \n\nApologies if this is obvious, my data science experience is limited to an introductory statistics class from undergrad (ages ago!) and try to Google the answer is making my brain melt! Thanks in advance for any help or guidance :) ", "author_fullname": "t2_138vkd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Research Methodology Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_158f4s2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690215355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if this is the wrong subreddit, will be happy to delete if so! &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m conducting a social science research project and am hoping to use Twitter data in my paper. Specifically, I need to pull all tweets that mention a specific term, the replies to those tweets, and replies to tweets made by a specific user from within a very specific time frame. I had been planning to just use Twitter&amp;#39;s advanced search function to do so, but I&amp;#39;ve since learned that that tool is not comprehensive and does not reflect all tweets. Is this something I can use Twitter&amp;#39;s free API/Python to do? I&amp;#39;m not opposed to learning some basic programming if necessary. &lt;/p&gt;\n\n&lt;p&gt;Apologies if this is obvious, my data science experience is limited to an introductory statistics class from undergrad (ages ago!) and try to Google the answer is making my brain melt! Thanks in advance for any help or guidance :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "158f4s2", "is_robot_indexable": true, "report_reasons": null, "author": "colemi1995", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/158f4s2/research_methodology_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/158f4s2/research_methodology_question/", "subreddit_subscribers": 959674, "created_utc": 1690215355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello - I\u2019m getting back in to DS after a few years doing far more operational analytics type work. I\u2019m looking to find a refresher course(s) to kick off the dust on for example - why GLM vs. OLS?\n\nHappy to do podcast, YouTube, coursera etc", "author_fullname": "t2_33rogmrn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Crash Courses on Models &amp; Why", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_158ejg9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690214066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello - I\u2019m getting back in to DS after a few years doing far more operational analytics type work. I\u2019m looking to find a refresher course(s) to kick off the dust on for example - why GLM vs. OLS?&lt;/p&gt;\n\n&lt;p&gt;Happy to do podcast, YouTube, coursera etc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "158ejg9", "is_robot_indexable": true, "report_reasons": null, "author": "FewStruggle", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/158ejg9/crash_courses_on_models_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/158ejg9/crash_courses_on_models_why/", "subreddit_subscribers": 959674, "created_utc": 1690214066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi,\n\nI have free vouchers for Azure AI Engineer AI-102 and Azure Data Scientist exams. I don't know whether I should go and spend considerable amounts of time learning these tools. Are they fun to work with? What are the pros and cons from your perspective? Do they have a bright future? \n\n&amp;#x200B;", "author_fullname": "t2_hecq3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure ML&amp;AI Pros and Cons?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_158ehsh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690213957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have free vouchers for Azure AI Engineer AI-102 and Azure Data Scientist exams. I don&amp;#39;t know whether I should go and spend considerable amounts of time learning these tools. Are they fun to work with? What are the pros and cons from your perspective? Do they have a bright future? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "158ehsh", "is_robot_indexable": true, "report_reasons": null, "author": "rayman903", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/158ehsh/azure_mlai_pros_and_cons/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/158ehsh/azure_mlai_pros_and_cons/", "subreddit_subscribers": 959674, "created_utc": 1690213957.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}