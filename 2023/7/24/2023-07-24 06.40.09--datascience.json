{"kind": "Listing", "data": {"after": "t3_157bfv5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Long story short, I run a data team at a 100 person company that includes Data Science and Data engineering. Unexpectedly, I find myself fending off a hostile takeover from a leader on the engineering team, who is declaring that data engineering needs to be moved in with the engineering org. I won\u2019t give any more details because everybody reads Reddit nowadays, but I am curious for people\u2019s opinions here.  I have my own, of course, but would like to hear from you all. \n\nWhat are the pros and cons of having the data engineering function on the same team as data science, versus having them separate?", "author_fullname": "t2_dv159drh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where does Data Engineering belong?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157j2zn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 86, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 86, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690129380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short, I run a data team at a 100 person company that includes Data Science and Data engineering. Unexpectedly, I find myself fending off a hostile takeover from a leader on the engineering team, who is declaring that data engineering needs to be moved in with the engineering org. I won\u2019t give any more details because everybody reads Reddit nowadays, but I am curious for people\u2019s opinions here.  I have my own, of course, but would like to hear from you all. &lt;/p&gt;\n\n&lt;p&gt;What are the pros and cons of having the data engineering function on the same team as data science, versus having them separate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157j2zn", "is_robot_indexable": true, "report_reasons": null, "author": "Sad-Fail-5337", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157j2zn/where_does_data_engineering_belong/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157j2zn/where_does_data_engineering_belong/", "subreddit_subscribers": 958944, "created_utc": 1690129380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm not a Data scientist but I do have a few questions for you all. I just saw oppenheimer which, no spoilers, simply revolves around the Manhattan project and the creation of nuclear bombs. \n\nBack in the day, weren't data scientists just the physicists and chemists and mathematicians who did their own studies? Or was there a specific role for someone who handles all the data and does something else with it?", "author_fullname": "t2_f4m0dd2qo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just saw oppenheimer, did data scientists exist back then?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157m5sj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690136715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not a Data scientist but I do have a few questions for you all. I just saw oppenheimer which, no spoilers, simply revolves around the Manhattan project and the creation of nuclear bombs. &lt;/p&gt;\n\n&lt;p&gt;Back in the day, weren&amp;#39;t data scientists just the physicists and chemists and mathematicians who did their own studies? Or was there a specific role for someone who handles all the data and does something else with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157m5sj", "is_robot_indexable": true, "report_reasons": null, "author": "Sacred_Tomato", "discussion_type": null, "num_comments": 82, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157m5sj/just_saw_oppenheimer_did_data_scientists_exist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157m5sj/just_saw_oppenheimer_did_data_scientists_exist/", "subreddit_subscribers": 958944, "created_utc": 1690136715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been working on a new implementation of Agglomerative clustering called Reciprocal Agglomerative Clustering (RAC) based off of this paper: [https://arxiv.org/abs/2105.11653](https://arxiv.org/abs/2105.11653). The short of it is Agglomerative clustering can be broken down into finding and merging pairs of reciprocal nearest neighbors in parallel, as long as the linkage function is one of the following:\n\n* Single\n* Average\n* Complete\n* Ward\n\nMost importantly, RAC *produces the exact same results as traditional Agglomerative clustering when the dataset is fully connected.* Even with connectivity constraints, the results are almost always the same.\n\nThe authors showed that RAC has a linear runtime when connectivity is limited to k and the distance matrix is precomputed. I have not added the ability to pass in the distance matrix yet, so the runtime is roughly quadratic, which is still a major improvement over the cubic runtime of Agglomerative clustering. In addition the entire algorithm is parallelized, and so can scale up to more and more cores.\n\nIt's very much in development - only average linkage works at the moment, however, I think it has a lot of potential. The benchmarks have blown me away so far:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/bbjxw0qsyodb1.png?width=850&amp;format=png&amp;auto=webp&amp;s=4bcdd014aea6ac14eca51d9c7403dadda2e4012e\n\nHere is the code: [https://github.com/porterehunley/RACplusplus](https://github.com/porterehunley/RACplusplus). It would be great to have some people try it out (and find the bugs)!", "author_fullname": "t2_n63m16v8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[P] I created a parallelized implementation of Agglomerative clustering that's many times faster than existing implementations and has a better runtime", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 89, "top_awarded_type": null, "hide_score": false, "media_metadata": {"bbjxw0qsyodb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 69, "x": 108, "u": "https://preview.redd.it/bbjxw0qsyodb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=46cd69b3b26d632620779a7b51d511769a4d6053"}, {"y": 138, "x": 216, "u": "https://preview.redd.it/bbjxw0qsyodb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=590cf437e74b1e487bc71875e5c73d606bca59be"}, {"y": 205, "x": 320, "u": "https://preview.redd.it/bbjxw0qsyodb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b015a76a9ea58d78f72958eda6a493884757e4c6"}, {"y": 410, "x": 640, "u": "https://preview.redd.it/bbjxw0qsyodb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=30a645af0740616051dd760751ea8c75c0a86bb2"}], "s": {"y": 545, "x": 850, "u": "https://preview.redd.it/bbjxw0qsyodb1.png?width=850&amp;format=png&amp;auto=webp&amp;s=4bcdd014aea6ac14eca51d9c7403dadda2e4012e"}, "id": "bbjxw0qsyodb1"}}, "name": "t3_157bax6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QnX1OEC5i7I5fYXeuKfzrEZp-gXBNdbzXndyyLr3lUw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1690108031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a new implementation of Agglomerative clustering called Reciprocal Agglomerative Clustering (RAC) based off of this paper: &lt;a href=\"https://arxiv.org/abs/2105.11653\"&gt;https://arxiv.org/abs/2105.11653&lt;/a&gt;. The short of it is Agglomerative clustering can be broken down into finding and merging pairs of reciprocal nearest neighbors in parallel, as long as the linkage function is one of the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Single&lt;/li&gt;\n&lt;li&gt;Average&lt;/li&gt;\n&lt;li&gt;Complete&lt;/li&gt;\n&lt;li&gt;Ward&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Most importantly, RAC &lt;em&gt;produces the exact same results as traditional Agglomerative clustering when the dataset is fully connected.&lt;/em&gt; Even with connectivity constraints, the results are almost always the same.&lt;/p&gt;\n\n&lt;p&gt;The authors showed that RAC has a linear runtime when connectivity is limited to k and the distance matrix is precomputed. I have not added the ability to pass in the distance matrix yet, so the runtime is roughly quadratic, which is still a major improvement over the cubic runtime of Agglomerative clustering. In addition the entire algorithm is parallelized, and so can scale up to more and more cores.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s very much in development - only average linkage works at the moment, however, I think it has a lot of potential. The benchmarks have blown me away so far:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bbjxw0qsyodb1.png?width=850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bcdd014aea6ac14eca51d9c7403dadda2e4012e\"&gt;https://preview.redd.it/bbjxw0qsyodb1.png?width=850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bcdd014aea6ac14eca51d9c7403dadda2e4012e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here is the code: &lt;a href=\"https://github.com/porterehunley/RACplusplus\"&gt;https://github.com/porterehunley/RACplusplus&lt;/a&gt;. It would be great to have some people try it out (and find the bugs)!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?auto=webp&amp;s=f1cd025aeb52ffa82fc9e5a4a2f157da0d919147", "width": 1200, "height": 700}, "resolutions": [{"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2711d572cfc6c713893cf24e8c4a7344d5ad8a4c", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6624f0c1eedc14997e7f1780efbe6e5cb50c1e2", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9db38144ef3065833b9ba158c764f7be47de3016", "width": 320, "height": 186}, {"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075", "width": 640, "height": 373}, {"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2637f961ee21190172b9ca6c8adf3ac9612db083", "width": 960, "height": 560}, {"url": "https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=782eead871df2939a587ee3beae442cc59282f64", "width": 1080, "height": 630}], "variants": {}, "id": "q3evP6JeDpAC2MdSQHWYxnCYTqbJkElIQsLFqVSdkss"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "157bax6", "is_robot_indexable": true, "report_reasons": null, "author": "Ridaleneas", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157bax6/p_i_created_a_parallelized_implementation_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157bax6/p_i_created_a_parallelized_implementation_of/", "subreddit_subscribers": 958944, "created_utc": 1690108031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Do you do AI/ML stuff often? Deep learning with Neural Networks?\n\nOr do you just make charts with SQL queries? Excel and .csv files?\n\nOr something in between?", "author_fullname": "t2_kytqh4tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What extent is your job \"Advanced Data Science\" vs \"low-level Data Science\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157sidi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690151635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you do AI/ML stuff often? Deep learning with Neural Networks?&lt;/p&gt;\n\n&lt;p&gt;Or do you just make charts with SQL queries? Excel and .csv files?&lt;/p&gt;\n\n&lt;p&gt;Or something in between?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157sidi", "is_robot_indexable": true, "report_reasons": null, "author": "SeriouslySally36", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157sidi/what_extent_is_your_job_advanced_data_science_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157sidi/what_extent_is_your_job_advanced_data_science_vs/", "subreddit_subscribers": 958944, "created_utc": 1690151635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_h3a1v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[OC] Create your Own Artificial Neural Network in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 133, "top_awarded_type": null, "hide_score": false, "name": "t3_157p1xy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-B07hLh7SWkvm-8aZcuYKPg4CEt0v9ylaDtFqXXBLzc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690143441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/kjjvyye6wrdb1.gif", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?format=png8&amp;s=dee8b32aac95f2353722253648d23e9de68aca89", "width": 400, "height": 381}, "resolutions": [{"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=3b2fcdb4900dd3899def7e3cd7f61addb33d1fc3", "width": 108, "height": 102}, {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=becf43f69cb79e54e448725bb820d8239493286c", "width": 216, "height": 205}, {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=724a6c49a28669d5555c103b439f506de805627c", "width": 320, "height": 304}], "variants": {"gif": {"source": {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?s=cfba8de4c12e7e3c803e8be0a368021277520538", "width": 400, "height": 381}, "resolutions": [{"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=108&amp;crop=smart&amp;s=616ffeb49f0e7f535b950dc1b5fb9bb5258613a4", "width": 108, "height": 102}, {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=216&amp;crop=smart&amp;s=4e6dfda62d035dc593dffc9045451c9da5061a11", "width": 216, "height": 205}, {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=320&amp;crop=smart&amp;s=df724883ded6b38ace50533f4e5d3da7f74299b3", "width": 320, "height": 304}]}, "mp4": {"source": {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?format=mp4&amp;s=69360b7f6256282bd1eae1064ce558e48bcc9574", "width": 400, "height": 381}, "resolutions": [{"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=108&amp;format=mp4&amp;s=d56f84fec62bb582387a03c361c6aa2646e48700", "width": 108, "height": 102}, {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=216&amp;format=mp4&amp;s=9b11c2bb44a7ee57dbb111dd38a9fbd33a3e521d", "width": 216, "height": 205}, {"url": "https://preview.redd.it/kjjvyye6wrdb1.gif?width=320&amp;format=mp4&amp;s=3ee15f162795714a7df05658a1647376d2a4ce10", "width": 320, "height": 304}]}}, "id": "Xtc8PF38RpYdQrQI3yd99HMkHr8pKSFx0oQn4XWWwzM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "157p1xy", "is_robot_indexable": true, "report_reasons": null, "author": "pmocz", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157p1xy/oc_create_your_own_artificial_neural_network_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/kjjvyye6wrdb1.gif", "subreddit_subscribers": 958944, "created_utc": 1690143441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_xj5pb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Decoding the ACL Paper: Gzip and KNN Rival BERT in Text Classification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1579qy7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ll4r9_pwUGqg72AsM1LP1Rqusl158PoU5ZvAIpQdb6A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690102828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "codeconfessions.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://codeconfessions.substack.com/p/decoding-the-acl-paper-gzip-and-knn", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BNJNjygjI26Qzq618gBHHaEs_8q8mubtji6R2f416OM.jpg?auto=webp&amp;s=8869decfb98ee3f81c132ceccdcd226bc52a1665", "width": 1080, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/BNJNjygjI26Qzq618gBHHaEs_8q8mubtji6R2f416OM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=38fdaad8619c80598f8180411e4ad66936cf670b", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/BNJNjygjI26Qzq618gBHHaEs_8q8mubtji6R2f416OM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=210dc8f4fa7627f9296df82ed8dd8c1b8ff5f118", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/BNJNjygjI26Qzq618gBHHaEs_8q8mubtji6R2f416OM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2714fd534001d06506681c49788e47f89957eb3", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/BNJNjygjI26Qzq618gBHHaEs_8q8mubtji6R2f416OM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=056302bd549a22f1471808140b3511781123e059", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/BNJNjygjI26Qzq618gBHHaEs_8q8mubtji6R2f416OM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bcf72e791d030d9291cfd071cc82bd38468f08b8", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/BNJNjygjI26Qzq618gBHHaEs_8q8mubtji6R2f416OM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e972c7057a494dc0e2ff3feb0750b72ad72196b1", "width": 1080, "height": 720}], "variants": {}, "id": "nF766ThFN6J1Dv_cSr4PSjoVHitYsK4g5cWriwI18Dg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "1579qy7", "is_robot_indexable": true, "report_reasons": null, "author": "abhi9u", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1579qy7/decoding_the_acl_paper_gzip_and_knn_rival_bert_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://codeconfessions.substack.com/p/decoding-the-acl-paper-gzip-and-knn", "subreddit_subscribers": 958944, "created_utc": 1690102828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a large dataset (3000 features are used here), and I am calculating mutual information scores between each row and every other row. Then, I print out the top X mutual information scores (and which two rows correspond to that mutual information score).\n\nWhen I print these values in Google Collab, however, the top scores seem to give different values vs. Jupyter Notebook, even though I am using the same code.\n\nWhat's also interesting is that these values remain the exact same if they are re-run (in both Collab and Jupyter), so I don't believe it's random.\n\nI will show the first 10 printed lines to demonstrate the difference:\n\nJupyter Notebook\n\n&gt;\\#1: 1397 and 1427: 1.202717856027216  \n&gt;  \n&gt;\\#2: 1400 and 1431: 1.074839333797198  \n&gt;  \n&gt;\\#3: 239 and 423: 1.068564020019758  \n&gt;  \n&gt;\\#4: 1146 and 1400: 1.06539274118781  \n&gt;  \n&gt;\\#5: 1146 and 1177: 1.0448225876789148  \n&gt;  \n&gt;\\#6: 1146 and 1431: 1.0431195289315978  \n&gt;  \n&gt;\\#7: 1411 and 1431: 1.0103705901911808  \n&gt;  \n&gt;\\#8: 1111 and 1525: 1.0037660750701747  \n&gt;  \n&gt;\\#9: 1177 and 1431: 0.9890857137951587  \n&gt;  \n&gt;\\#10: 1146 and 1411: 0.9852993714583413\n\nGoogle Collab\n\n&gt;\\#1: 1146 and 1400: 1.1822506247498457  \n&gt;  \n&gt;\\#2: 239 and 423: 1.0994706698596624  \n&gt;  \n&gt;\\#3: 1397 and 1427: 1.0838558257556066  \n&gt;  \n&gt;\\#4: 1146 and 1177: 1.0766228782259293  \n&gt;  \n&gt;\\#5: 423 and 73: 1.0258894687690598  \n&gt;  \n&gt;\\#6: 1177 and 1411: 1.021696037520684  \n&gt;  \n&gt;\\#7: 1400 and 1431: 1.0134240574963582  \n&gt;  \n&gt;\\#8: 1111 and 1525: 1.0071214141815927  \n&gt;  \n&gt;\\#9: 1146 and 1431: 0.972276347390304  \n&gt;  \n&gt;\\#10: 1146 and 1411: 0.9689222844930194\n\n**Here is the relevant code**\n\nFirst cell:\n\n&amp;#x200B;\n\n    # Note: The following is after inputting the Excel spreadsheet data into dataframe then transposing\n    df = df.T\n    \n    import pandas as pd\n    from sklearn.feature_selection import mutual_info_regression\n\n    # Load the dataset\n    #data = pd.read_excel(\"analysis_file2.xlsx\")\n\n    # Select the first 3,000 feature columns\n    features = df.columns[1:3001]\n\n    # Compute the mutual information between each pair of features\n    mi_matrix = np.zeros((len(features), len(features)))\n    for i in range(len(features)):\n        for j in range(i+1, len(features)):\n            feature_A = df[features[i]].values\n            feature_B = df[features[j]].values\n            mi = mutual_info_regression(feature_A.reshape(-1, 1), feature_B)[0]\n            mi_matrix[i, j] = mi\n            mi_matrix[j, i] = mi\n\n\nSecond cell:\n\n    # Find the top 30 mutual information values\n\n    num = 30 * 2\n\n    top = np.argsort(mi_matrix, axis=None)[-num:]\n\n    sorted_pairs = sorted(zip(top, mi_matrix[np.unravel_index(top, mi_matrix.shape)]), key=lambda x: x[1],     reverse=True)\n\n    # Sorted values and indeces\n    sorted_indices = [pair[0] for pair in sorted_pairs]\n    sorted_values = [pair[1] for pair in sorted_pairs]\n\n    top = np.unique(top)\n    top = np.unravel_index(top, mi_matrix.shape)\n\n    # Print top values Sorted (greatest to least)\n\n    feature1 = [None] * num\n    feature2 = [None] * num\n    mi_value = [None] * num\n    for i in range(num):\n        idx1, idx2 = top[0][i], top[1][i]\n        feature1[i] = features[idx1]\n        feature2[i] = features[idx2]\n        mi_value[i] = mi_matrix[idx1, idx2]\n\n    ind = 0\n    i = 0\n    while (ind != num):\n        if (mi_value[i] == sorted_values[ind]):\n          ind += 2\n          print(f\"#{int(ind/2)}: {feature1[i]} and {feature2[i]}: {mi_value[i]}\")\n          i = 0\n        xi += 1", "author_fullname": "t2_efotpocwb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about getting different results in Google Collab than Jupyter Notebook when calculating mutual information scores.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157ubmj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690156320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a large dataset (3000 features are used here), and I am calculating mutual information scores between each row and every other row. Then, I print out the top X mutual information scores (and which two rows correspond to that mutual information score).&lt;/p&gt;\n\n&lt;p&gt;When I print these values in Google Collab, however, the top scores seem to give different values vs. Jupyter Notebook, even though I am using the same code.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s also interesting is that these values remain the exact same if they are re-run (in both Collab and Jupyter), so I don&amp;#39;t believe it&amp;#39;s random.&lt;/p&gt;\n\n&lt;p&gt;I will show the first 10 printed lines to demonstrate the difference:&lt;/p&gt;\n\n&lt;p&gt;Jupyter Notebook&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;#1: 1397 and 1427: 1.202717856027216  &lt;/p&gt;\n\n&lt;p&gt;#2: 1400 and 1431: 1.074839333797198  &lt;/p&gt;\n\n&lt;p&gt;#3: 239 and 423: 1.068564020019758  &lt;/p&gt;\n\n&lt;p&gt;#4: 1146 and 1400: 1.06539274118781  &lt;/p&gt;\n\n&lt;p&gt;#5: 1146 and 1177: 1.0448225876789148  &lt;/p&gt;\n\n&lt;p&gt;#6: 1146 and 1431: 1.0431195289315978  &lt;/p&gt;\n\n&lt;p&gt;#7: 1411 and 1431: 1.0103705901911808  &lt;/p&gt;\n\n&lt;p&gt;#8: 1111 and 1525: 1.0037660750701747  &lt;/p&gt;\n\n&lt;p&gt;#9: 1177 and 1431: 0.9890857137951587  &lt;/p&gt;\n\n&lt;p&gt;#10: 1146 and 1411: 0.9852993714583413&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Google Collab&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;#1: 1146 and 1400: 1.1822506247498457  &lt;/p&gt;\n\n&lt;p&gt;#2: 239 and 423: 1.0994706698596624  &lt;/p&gt;\n\n&lt;p&gt;#3: 1397 and 1427: 1.0838558257556066  &lt;/p&gt;\n\n&lt;p&gt;#4: 1146 and 1177: 1.0766228782259293  &lt;/p&gt;\n\n&lt;p&gt;#5: 423 and 73: 1.0258894687690598  &lt;/p&gt;\n\n&lt;p&gt;#6: 1177 and 1411: 1.021696037520684  &lt;/p&gt;\n\n&lt;p&gt;#7: 1400 and 1431: 1.0134240574963582  &lt;/p&gt;\n\n&lt;p&gt;#8: 1111 and 1525: 1.0071214141815927  &lt;/p&gt;\n\n&lt;p&gt;#9: 1146 and 1431: 0.972276347390304  &lt;/p&gt;\n\n&lt;p&gt;#10: 1146 and 1411: 0.9689222844930194&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Here is the relevant code&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;First cell:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# Note: The following is after inputting the Excel spreadsheet data into dataframe then transposing\ndf = df.T\n\nimport pandas as pd\nfrom sklearn.feature_selection import mutual_info_regression\n\n# Load the dataset\n#data = pd.read_excel(&amp;quot;analysis_file2.xlsx&amp;quot;)\n\n# Select the first 3,000 feature columns\nfeatures = df.columns[1:3001]\n\n# Compute the mutual information between each pair of features\nmi_matrix = np.zeros((len(features), len(features)))\nfor i in range(len(features)):\n    for j in range(i+1, len(features)):\n        feature_A = df[features[i]].values\n        feature_B = df[features[j]].values\n        mi = mutual_info_regression(feature_A.reshape(-1, 1), feature_B)[0]\n        mi_matrix[i, j] = mi\n        mi_matrix[j, i] = mi\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Second cell:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# Find the top 30 mutual information values\n\nnum = 30 * 2\n\ntop = np.argsort(mi_matrix, axis=None)[-num:]\n\nsorted_pairs = sorted(zip(top, mi_matrix[np.unravel_index(top, mi_matrix.shape)]), key=lambda x: x[1],     reverse=True)\n\n# Sorted values and indeces\nsorted_indices = [pair[0] for pair in sorted_pairs]\nsorted_values = [pair[1] for pair in sorted_pairs]\n\ntop = np.unique(top)\ntop = np.unravel_index(top, mi_matrix.shape)\n\n# Print top values Sorted (greatest to least)\n\nfeature1 = [None] * num\nfeature2 = [None] * num\nmi_value = [None] * num\nfor i in range(num):\n    idx1, idx2 = top[0][i], top[1][i]\n    feature1[i] = features[idx1]\n    feature2[i] = features[idx2]\n    mi_value[i] = mi_matrix[idx1, idx2]\n\nind = 0\ni = 0\nwhile (ind != num):\n    if (mi_value[i] == sorted_values[ind]):\n      ind += 2\n      print(f&amp;quot;#{int(ind/2)}: {feature1[i]} and {feature2[i]}: {mi_value[i]}&amp;quot;)\n      i = 0\n    xi += 1\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157ubmj", "is_robot_indexable": true, "report_reasons": null, "author": "MLquestionAccount", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157ubmj/question_about_getting_different_results_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157ubmj/question_about_getting_different_results_in/", "subreddit_subscribers": 958944, "created_utc": 1690156320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am recently unemployed R&amp;D scientist and was messing with statistics, data and programing (but my core background is materials eng, solid state physics, chemistry and nanotech) I have phd and was always an enthusiastic amateur working with R and so on. I last few years, it was almost exclusively my job at large corporation (pehaps because i was the only scientist remaining). \n\nBut I dont consider myself data scientist, maybe an engineer, maybe scientist, maybe chemist, with certain level of know how in R and other tools. I know quite a bit of math behind models and regresions but not consider myself pure or applied math major. PhD level, but still more engineer.  \n\nMy programming skills are VERY limited, and i feel that at least solid python and sql would HAVE to be added if i am to get another employment in data field. How hard would that be? How do i best approach it? How many months or years would it take to get to the level of being safely employable in the atlantic region of the US?\n\nI see ton of ads in my social networks, berkley, purdue global, UVA, UT austin, VCU (that's some 3-6 months certificate) MIT, Georgia Tech, ... are they any good? Would it be good use of my time? Would it help with employment? Or are those bootcamps and \"online\" schools more a scam and red flag for job applications? What is your take and why?   ", "author_fullname": "t2_zma26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "online masters? what is your take on those courses? are they any good? for employment? for knowledge?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157xgpk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690165087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am recently unemployed R&amp;amp;D scientist and was messing with statistics, data and programing (but my core background is materials eng, solid state physics, chemistry and nanotech) I have phd and was always an enthusiastic amateur working with R and so on. I last few years, it was almost exclusively my job at large corporation (pehaps because i was the only scientist remaining). &lt;/p&gt;\n\n&lt;p&gt;But I dont consider myself data scientist, maybe an engineer, maybe scientist, maybe chemist, with certain level of know how in R and other tools. I know quite a bit of math behind models and regresions but not consider myself pure or applied math major. PhD level, but still more engineer.  &lt;/p&gt;\n\n&lt;p&gt;My programming skills are VERY limited, and i feel that at least solid python and sql would HAVE to be added if i am to get another employment in data field. How hard would that be? How do i best approach it? How many months or years would it take to get to the level of being safely employable in the atlantic region of the US?&lt;/p&gt;\n\n&lt;p&gt;I see ton of ads in my social networks, berkley, purdue global, UVA, UT austin, VCU (that&amp;#39;s some 3-6 months certificate) MIT, Georgia Tech, ... are they any good? Would it be good use of my time? Would it help with employment? Or are those bootcamps and &amp;quot;online&amp;quot; schools more a scam and red flag for job applications? What is your take and why?   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157xgpk", "is_robot_indexable": true, "report_reasons": null, "author": "yik77", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157xgpk/online_masters_what_is_your_take_on_those_courses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157xgpk/online_masters_what_is_your_take_on_those_courses/", "subreddit_subscribers": 958944, "created_utc": 1690165087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a data set that boils down to Three clomuns: 1.Supplier name 2. Number of transactions with supplier 3. Total value of those transaction.\n\nI'm trying to find the best way to rank all suppliers based on these two features which are equally important in this case. \nNote: -more transaction doesn't mean more value\n\nWhat I did is I normalized each feature. Making all values scale down to 0 to 1. Then I added the 2 normalized values to each other. Then used this new values to rank all suppliers. Looking at the results and  what I expected a good ranking to be, I'm happy with the result.\n\nHowever, I feel like the existing of outliers might've made the ranking.. not as accurate as it can be. Am I right or wrong? How would you suggest fixing this? Is there an entierly better way to do this?\n\nFinal note: I tried standarizing and got the exact same ranking. I'm not sure if that was supposed to happen or if it was a special case.", "author_fullname": "t2_59mf5tbp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would normalizing be affected by outliers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157o4qn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690141303.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data set that boils down to Three clomuns: 1.Supplier name 2. Number of transactions with supplier 3. Total value of those transaction.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to find the best way to rank all suppliers based on these two features which are equally important in this case. \nNote: -more transaction doesn&amp;#39;t mean more value&lt;/p&gt;\n\n&lt;p&gt;What I did is I normalized each feature. Making all values scale down to 0 to 1. Then I added the 2 normalized values to each other. Then used this new values to rank all suppliers. Looking at the results and  what I expected a good ranking to be, I&amp;#39;m happy with the result.&lt;/p&gt;\n\n&lt;p&gt;However, I feel like the existing of outliers might&amp;#39;ve made the ranking.. not as accurate as it can be. Am I right or wrong? How would you suggest fixing this? Is there an entierly better way to do this?&lt;/p&gt;\n\n&lt;p&gt;Final note: I tried standarizing and got the exact same ranking. I&amp;#39;m not sure if that was supposed to happen or if it was a special case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157o4qn", "is_robot_indexable": true, "report_reasons": null, "author": "kit_kaat", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157o4qn/how_would_normalizing_be_affected_by_outliers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157o4qn/how_would_normalizing_be_affected_by_outliers/", "subreddit_subscribers": 958944, "created_utc": 1690141303.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys, just need your ideas on how to approach this. So i have this dataset to the left with numerical and categorical variables. Can I just do a one-hot encoding for the categorical and normally proceed to the usual time-series modeling using the transformed dataset to the right? Thanks!\n\nhttps://preview.redd.it/8zysqjfgktdb1.png?width=1675&amp;format=png&amp;auto=webp&amp;s=f402b245ce8622993c5422b1e0f4c9b037adea8e", "author_fullname": "t2_sh45a0v7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time-series modeling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 18, "top_awarded_type": null, "hide_score": false, "media_metadata": {"8zysqjfgktdb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 14, "x": 108, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8ec25c1d8c5260b32044a8d1d2c7965452fe93f"}, {"y": 28, "x": 216, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=76da074f6a345718dce8f97a31f4582c3d199887"}, {"y": 41, "x": 320, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=19758c3989b169e5457115c77d1a35303eccddd5"}, {"y": 83, "x": 640, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=71d4208e9d6ac68817068452826eae25f0e622b5"}, {"y": 124, "x": 960, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc332fcf9f0bcd5c4aa6171426006a331459333b"}, {"y": 140, "x": 1080, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4bbe1bd5d0b90c30f042e4ddb18db52bc22f94ad"}], "s": {"y": 218, "x": 1675, "u": "https://preview.redd.it/8zysqjfgktdb1.png?width=1675&amp;format=png&amp;auto=webp&amp;s=f402b245ce8622993c5422b1e0f4c9b037adea8e"}, "id": "8zysqjfgktdb1"}}, "name": "t3_157wzje", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/wISw3YFzaRxhr815zlGCWlQvuLttFnCJ37dol4wQXV8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690163751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, just need your ideas on how to approach this. So i have this dataset to the left with numerical and categorical variables. Can I just do a one-hot encoding for the categorical and normally proceed to the usual time-series modeling using the transformed dataset to the right? Thanks!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8zysqjfgktdb1.png?width=1675&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f402b245ce8622993c5422b1e0f4c9b037adea8e\"&gt;https://preview.redd.it/8zysqjfgktdb1.png?width=1675&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f402b245ce8622993c5422b1e0f4c9b037adea8e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157wzje", "is_robot_indexable": true, "report_reasons": null, "author": "chime_enjoyer", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157wzje/timeseries_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157wzje/timeseries_modeling/", "subreddit_subscribers": 958944, "created_utc": 1690163751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Cost-effective too. ", "author_fullname": "t2_44tf9ypj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Ways to Put an R Shiny Dashboard into Prod for 1000+ to view daily?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157jtbr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690131147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cost-effective too. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157jtbr", "is_robot_indexable": true, "report_reasons": null, "author": "PSM182", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157jtbr/best_ways_to_put_an_r_shiny_dashboard_into_prod/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157jtbr/best_ways_to_put_an_r_shiny_dashboard_into_prod/", "subreddit_subscribers": 958944, "created_utc": 1690131147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Lots of existing datasets contain some sort of textual data which is usually overlooked because it is hard to work with. The recent advancement in AI (GPT-like models) make it possible to extract valuable data easily (examples: various specifications, names, dates, prices, etc) - converting free texts into tabular data. Example tools: the incredible [Textraction.ai](https://www.textraction.ai/), engineered ChatGPT prompts, etc.\n\n**Have you already used such tools to do that? Which datasets have the highest potential?**\n\nSome examples that come to my mind:\n\n* Medical datasets that often contain a free text describing symptoms, diagnosis, prescriptions, etc.\n* E-commerce products descriptions.\n* Real estate posts listings.\n* Automatically filling forms for the user based on free text.\n\nThese new tools open up a wide range of new applications now made possible.", "author_fullname": "t2_mqeuzn8j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Text Data Mining to Enrich Existing Datasets Using AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157fgnd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690120860.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690120509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lots of existing datasets contain some sort of textual data which is usually overlooked because it is hard to work with. The recent advancement in AI (GPT-like models) make it possible to extract valuable data easily (examples: various specifications, names, dates, prices, etc) - converting free texts into tabular data. Example tools: the incredible &lt;a href=\"https://www.textraction.ai/\"&gt;Textraction.ai&lt;/a&gt;, engineered ChatGPT prompts, etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Have you already used such tools to do that? Which datasets have the highest potential?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Some examples that come to my mind:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Medical datasets that often contain a free text describing symptoms, diagnosis, prescriptions, etc.&lt;/li&gt;\n&lt;li&gt;E-commerce products descriptions.&lt;/li&gt;\n&lt;li&gt;Real estate posts listings.&lt;/li&gt;\n&lt;li&gt;Automatically filling forms for the user based on free text.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;These new tools open up a wide range of new applications now made possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157fgnd", "is_robot_indexable": true, "report_reasons": null, "author": "DoorDesigner7589", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157fgnd/text_data_mining_to_enrich_existing_datasets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157fgnd/text_data_mining_to_enrich_existing_datasets/", "subreddit_subscribers": 958944, "created_utc": 1690120509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This is what I am currently stuck with. With more than a hundred sales places and, with so little data on some less successful SKU in less successful spots, like one per a few month. Or newer SKU having less than a month of data.\n\nWhat I am currently doing is \"best guessing\" using excel spreadsheet(i know, i know), and finding analogues in attempt to even remotely forecast fresh SKU, taking sales per spot, per day, per SKU, and filling blanks with the same data of tge same days of another week. Then summarizing it and applying reasonable multipliers.\n\nIs there a better way? It's a lot to do with so many products for so many places, even with careful pivoting.", "author_fullname": "t2_2ndz0you", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forecasting on SKU-level bu points of sale?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15814hq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690176111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is what I am currently stuck with. With more than a hundred sales places and, with so little data on some less successful SKU in less successful spots, like one per a few month. Or newer SKU having less than a month of data.&lt;/p&gt;\n\n&lt;p&gt;What I am currently doing is &amp;quot;best guessing&amp;quot; using excel spreadsheet(i know, i know), and finding analogues in attempt to even remotely forecast fresh SKU, taking sales per spot, per day, per SKU, and filling blanks with the same data of tge same days of another week. Then summarizing it and applying reasonable multipliers.&lt;/p&gt;\n\n&lt;p&gt;Is there a better way? It&amp;#39;s a lot to do with so many products for so many places, even with careful pivoting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15814hq", "is_robot_indexable": true, "report_reasons": null, "author": "WlrsWrwgn", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15814hq/forecasting_on_skulevel_bu_points_of_sale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15814hq/forecasting_on_skulevel_bu_points_of_sale/", "subreddit_subscribers": 958944, "created_utc": 1690176111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Wanting to get into data science, but not sure if the best way. I have no tech background, except for a bit of stats. Bootcamps seem very touch and go and leaning on my own might be a waste of time if I focus on the wrong stuff. Didn\u2019t know if this degree and doing some personal/volunteer projects to build up a portfolio would be the way to go. Any input appreciated. Thanks!", "author_fullname": "t2_bf6kfyxc0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WGU for data science degree worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157he0v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690125335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wanting to get into data science, but not sure if the best way. I have no tech background, except for a bit of stats. Bootcamps seem very touch and go and leaning on my own might be a waste of time if I focus on the wrong stuff. Didn\u2019t know if this degree and doing some personal/volunteer projects to build up a portfolio would be the way to go. Any input appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157he0v", "is_robot_indexable": true, "report_reasons": null, "author": "vacanthorizon1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157he0v/wgu_for_data_science_degree_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157he0v/wgu_for_data_science_degree_worth_it/", "subreddit_subscribers": 958944, "created_utc": 1690125335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Weekly Entering &amp; Transitioning - Thread 24 Jul, 2023 - 31 Jul, 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157zlci", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690171285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;\n&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;\n&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;\n&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;\n&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=\"https://www.reddit.com/r/datascience/wiki/frequently-asked-questions\"&gt;FAQ&lt;/a&gt; and Resources pages on our wiki. You can also search for answers in &lt;a href=\"https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new\"&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "new", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157zlci", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157zlci/weekly_entering_transitioning_thread_24_jul_2023/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/datascience/comments/157zlci/weekly_entering_transitioning_thread_24_jul_2023/", "subreddit_subscribers": 958944, "created_utc": 1690171285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have more than 5 year experience for data sciences work. I used to build ETL and abtest for regional leading enterprise. and I am seeking working aboard for better career and need to prepare the interview in English. unfortunately, I do not need to spoke or write English in my past work,   and I worried I could not express my role and experience clearly in English. how to practice and prepare for the interview for a non-native speaker.", "author_fullname": "t2_egz713h9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to prepare a job interview\uff1f", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157z1qi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690169649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have more than 5 year experience for data sciences work. I used to build ETL and abtest for regional leading enterprise. and I am seeking working aboard for better career and need to prepare the interview in English. unfortunately, I do not need to spoke or write English in my past work,   and I worried I could not express my role and experience clearly in English. how to practice and prepare for the interview for a non-native speaker.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157z1qi", "is_robot_indexable": true, "report_reasons": null, "author": "LunaSolarMilkway", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157z1qi/how_to_prepare_a_job_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157z1qi/how_to_prepare_a_job_interview/", "subreddit_subscribers": 958944, "created_utc": 1690169649.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi All,\n\nI will be leaving my Job at a startup in about a month to go to an organization and start a data science team. I currently run a analytics and data science team, and I like to think that I do a good job understanding the business problems and data and coming up with solutions. I have built this team up from basically scratch (my boss hasn't been very involved in analytics as he's focused on other things) and have hired all 15 people under me. Our company has been relatively successful through the pandemic and its largely because of our analytics solutions. However we are in the healthcare space which means that most of what we have done is revolving around SAS.  \n\nAs mentioned, I am going to be moving to a new organization which (seems to) have realistic expectations about applying data science.  Ill be building a team from scratch, which I have done before, but they mentioned they are primarily an R shop, which I haven't used since college. I was wondering what your best resources for learning R's applications to data analytics and Data Science were. I'm a very hands on person and as discussed in the interview 50-75% of my time will be hands on over the next year or two as I learn company and their goals so I want to be able to hit the ground running \n\nAdditionally, if anyone has any good resources on building teams (data science or non data science) I would love to read those as well. I am always looking to get better and since my last job started as analytics and grew into data science, I assume starting with data science might be different. What have you done in the past (or what did your boss do in the past) that worked really well in growing your team.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI have found some old threads but not much activity in them: [https://www.reddit.com/r/rstats/comments/mxo6k8/what\\_are\\_the\\_best\\_resources\\_to\\_learn\\_r\\_for\\_data/](https://www.reddit.com/r/rstats/comments/mxo6k8/what_are_the_best_resources_to_learn_r_for_data/)\n\n[https://www.reddit.com/r/datascience/comments/414pek/list\\_of\\_data\\_science\\_resources/](https://www.reddit.com/r/datascience/comments/414pek/list_of_data_science_resources/)\n\n[https://www.reddit.com/r/RStudio/comments/zowii1/learning\\_r\\_resources/](https://www.reddit.com/r/RStudio/comments/zowii1/learning_r_resources/)\n\n&amp;#x200B;\n\nFrom these resources it seems like [https://r4ds.had.co.nz/](https://r4ds.had.co.nz/) is the way to go -- do people here agree?", "author_fullname": "t2_dhh8p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Resources/Opinions around R and Team Building", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157pgxl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690144386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I will be leaving my Job at a startup in about a month to go to an organization and start a data science team. I currently run a analytics and data science team, and I like to think that I do a good job understanding the business problems and data and coming up with solutions. I have built this team up from basically scratch (my boss hasn&amp;#39;t been very involved in analytics as he&amp;#39;s focused on other things) and have hired all 15 people under me. Our company has been relatively successful through the pandemic and its largely because of our analytics solutions. However we are in the healthcare space which means that most of what we have done is revolving around SAS.  &lt;/p&gt;\n\n&lt;p&gt;As mentioned, I am going to be moving to a new organization which (seems to) have realistic expectations about applying data science.  Ill be building a team from scratch, which I have done before, but they mentioned they are primarily an R shop, which I haven&amp;#39;t used since college. I was wondering what your best resources for learning R&amp;#39;s applications to data analytics and Data Science were. I&amp;#39;m a very hands on person and as discussed in the interview 50-75% of my time will be hands on over the next year or two as I learn company and their goals so I want to be able to hit the ground running &lt;/p&gt;\n\n&lt;p&gt;Additionally, if anyone has any good resources on building teams (data science or non data science) I would love to read those as well. I am always looking to get better and since my last job started as analytics and grew into data science, I assume starting with data science might be different. What have you done in the past (or what did your boss do in the past) that worked really well in growing your team.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have found some old threads but not much activity in them: &lt;a href=\"https://www.reddit.com/r/rstats/comments/mxo6k8/what_are_the_best_resources_to_learn_r_for_data/\"&gt;https://www.reddit.com/r/rstats/comments/mxo6k8/what_are_the_best_resources_to_learn_r_for_data/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/datascience/comments/414pek/list_of_data_science_resources/\"&gt;https://www.reddit.com/r/datascience/comments/414pek/list_of_data_science_resources/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/RStudio/comments/zowii1/learning_r_resources/\"&gt;https://www.reddit.com/r/RStudio/comments/zowii1/learning_r_resources/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;From these resources it seems like &lt;a href=\"https://r4ds.had.co.nz/\"&gt;https://r4ds.had.co.nz/&lt;/a&gt; is the way to go -- do people here agree?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dX3T8UC_GylpVlrUMrYvH-2nM0oxTSosN0Q-7yoOac4.jpg?auto=webp&amp;s=3a730442108ffa573736d1d5f5ecd3a5a928e027", "width": 500, "height": 750}, "resolutions": [{"url": "https://external-preview.redd.it/dX3T8UC_GylpVlrUMrYvH-2nM0oxTSosN0Q-7yoOac4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e439ee362507d14f6aa9fdf48e3230c863924897", "width": 108, "height": 162}, {"url": "https://external-preview.redd.it/dX3T8UC_GylpVlrUMrYvH-2nM0oxTSosN0Q-7yoOac4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c67e132d9b81a45007aeddec0e52d26c531476fc", "width": 216, "height": 324}, {"url": "https://external-preview.redd.it/dX3T8UC_GylpVlrUMrYvH-2nM0oxTSosN0Q-7yoOac4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=560eb06cb50e433925324672ed1e5aecb14dadad", "width": 320, "height": 480}], "variants": {}, "id": "N_ZYgQxGMscE67OV4DhF9HugRbJUyzPy6u5WBBYiWc8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157pgxl", "is_robot_indexable": true, "report_reasons": null, "author": "mathblaster69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157pgxl/looking_for_resourcesopinions_around_r_and_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157pgxl/looking_for_resourcesopinions_around_r_and_team/", "subreddit_subscribers": 958944, "created_utc": 1690144386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am not a data science professional.\n\n&amp;#x200B;\n\nI work at a local government/Council department maintaining the local land and property register which is a map/database record of all the land and property with status e.g. occupied/under construction etc.\n\n&amp;#x200B;\n\nMostly i get the info for new developments from street naming and numbering department when they apply for an address. \n\n&amp;#x200B;\n\nHowever, it is the changes to status in existing properties which is hard - e.g. a business park with 50 units, when one of them becomes unnocupied or a new tenant moves in or a unit is split into two. Same with residential properties.\n\n&amp;#x200B;\n\nThe local commercial tax and residential tax departments plus the waste departmentsadminister all this but there is not much synchronisation. They don't have a common identifier for the property for and sometimes the address format might not match exactly so hard to match like for like. Talking about 100k properties.\n\n&amp;#x200B;\n\nWhat's the best way to get triggers of real world change from their data sets? Is their a smart solution here that is going to be easyish? I was thinking thingsan extract of all data sets and something like fuzzy look ups or similar to set up a common identifier so that we can baseline. I guess FME would be doable but I have only the most basic knowledge of that. I am more of a GIS person. Once we have baselined going forward what would be a good system on a weekly basis to synchronise these changes. We have different databases, formats, tools. Don't want to reinvent the wheel here, just want a solution that is practical and workable with limited technical knowledge.\n\n&amp;#x200B;\n\nMany Thanks \n\n&amp;#x200B;", "author_fullname": "t2_dzypkgpv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Local Government data sets full of duplication - how to streamline/synchronise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157nvjh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690140720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am not a data science professional.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I work at a local government/Council department maintaining the local land and property register which is a map/database record of all the land and property with status e.g. occupied/under construction etc.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Mostly i get the info for new developments from street naming and numbering department when they apply for an address. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;However, it is the changes to status in existing properties which is hard - e.g. a business park with 50 units, when one of them becomes unnocupied or a new tenant moves in or a unit is split into two. Same with residential properties.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The local commercial tax and residential tax departments plus the waste departmentsadminister all this but there is not much synchronisation. They don&amp;#39;t have a common identifier for the property for and sometimes the address format might not match exactly so hard to match like for like. Talking about 100k properties.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best way to get triggers of real world change from their data sets? Is their a smart solution here that is going to be easyish? I was thinking thingsan extract of all data sets and something like fuzzy look ups or similar to set up a common identifier so that we can baseline. I guess FME would be doable but I have only the most basic knowledge of that. I am more of a GIS person. Once we have baselined going forward what would be a good system on a weekly basis to synchronise these changes. We have different databases, formats, tools. Don&amp;#39;t want to reinvent the wheel here, just want a solution that is practical and workable with limited technical knowledge.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Many Thanks &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157nvjh", "is_robot_indexable": true, "report_reasons": null, "author": "omura777", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157nvjh/local_government_data_sets_full_of_duplication/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157nvjh/local_government_data_sets_full_of_duplication/", "subreddit_subscribers": 958944, "created_utc": 1690140720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello! I'm working on an DataScience and ML University project, and I'd like to work  with Google Ads campaigns data. Does anyone know where I can find some  recent data? I'd like to address questions related to campaign costs,  returns, and expectations. For example, what could be my income if I  spend 100 dollars on a specific campaign? Thank you and i hope you can help me  \n", "author_fullname": "t2_7kn7dtuj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datasets de Google ads campaigns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157lxdb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690136166.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I&amp;#39;m working on an DataScience and ML University project, and I&amp;#39;d like to work  with Google Ads campaigns data. Does anyone know where I can find some  recent data? I&amp;#39;d like to address questions related to campaign costs,  returns, and expectations. For example, what could be my income if I  spend 100 dollars on a specific campaign? Thank you and i hope you can help me  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157lxdb", "is_robot_indexable": true, "report_reasons": null, "author": "Ornery_Influence8875", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157lxdb/datasets_de_google_ads_campaigns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157lxdb/datasets_de_google_ads_campaigns/", "subreddit_subscribers": 958944, "created_utc": 1690136166.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone, I\u2019ve been trying to parse through Reddit and many other sources as to get a feel for how hard getting a job in Data Science will be. This Reddit is very negative in terms of expectations and chances to get a job based on scrolling through posts in here but I\u2019m sure that\u2019s due in large part to response bias, so I wanted to possibly hear other perspectives more tailored towards my specific situation.\n\nI have completed my BS in Stats and am completing my MS in Stats this upcoming school year and have had one data analyst internship. Skills wise I\u2019m fairly advanced in R, proficient in Python SQL and Tableau. Portfolio wise I have one complete end to end project and a couple other less intensive projects. Having a more heavy stats background I\u2019m definitely more solid on the theoretical side rather than the coding side.\n\nIm located in California and will look for jobs here first.", "author_fullname": "t2_55og4tpi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Realistic Job Search Expectations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157le7v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690134949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I\u2019ve been trying to parse through Reddit and many other sources as to get a feel for how hard getting a job in Data Science will be. This Reddit is very negative in terms of expectations and chances to get a job based on scrolling through posts in here but I\u2019m sure that\u2019s due in large part to response bias, so I wanted to possibly hear other perspectives more tailored towards my specific situation.&lt;/p&gt;\n\n&lt;p&gt;I have completed my BS in Stats and am completing my MS in Stats this upcoming school year and have had one data analyst internship. Skills wise I\u2019m fairly advanced in R, proficient in Python SQL and Tableau. Portfolio wise I have one complete end to end project and a couple other less intensive projects. Having a more heavy stats background I\u2019m definitely more solid on the theoretical side rather than the coding side.&lt;/p&gt;\n\n&lt;p&gt;Im located in California and will look for jobs here first.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157le7v", "is_robot_indexable": true, "report_reasons": null, "author": "Monku5427", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157le7v/realistic_job_search_expectations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157le7v/realistic_job_search_expectations/", "subreddit_subscribers": 958944, "created_utc": 1690134949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Keeping it short!\n\nI have a computer engineering background, 3 years as a developer, and 2 DevOps, currently working as a Project Manager for 2 years - aged 29.   \n\n\nSince I'm feeling great now as a Project Manager, I want to start specializing in a niche, and I am looking mostly into Data and/or Cloud Technologies, hence I've got 3 questions!  \n\n\n1. Can someone draw me a starting line (e.g. if it's a course send me the link)? Consider me a beginner even though I am familiar with databases, AWS Cloud mostly, etc.\n2. Can I find some project(s) or platform(s) I can even work for around 2h per day for free just to practice?\n3. Which is the cheapest cloud provider to start working with data (AWS vs. GCP vs. Azure, or something else)?\n\nYour help will be appreciated!", "author_fullname": "t2_5439wehf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Project Manager aiming on finding his specialty niche, can you help me with these 3 questions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157ifs9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690127863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Keeping it short!&lt;/p&gt;\n\n&lt;p&gt;I have a computer engineering background, 3 years as a developer, and 2 DevOps, currently working as a Project Manager for 2 years - aged 29.   &lt;/p&gt;\n\n&lt;p&gt;Since I&amp;#39;m feeling great now as a Project Manager, I want to start specializing in a niche, and I am looking mostly into Data and/or Cloud Technologies, hence I&amp;#39;ve got 3 questions!  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Can someone draw me a starting line (e.g. if it&amp;#39;s a course send me the link)? Consider me a beginner even though I am familiar with databases, AWS Cloud mostly, etc.&lt;/li&gt;\n&lt;li&gt;Can I find some project(s) or platform(s) I can even work for around 2h per day for free just to practice?&lt;/li&gt;\n&lt;li&gt;Which is the cheapest cloud provider to start working with data (AWS vs. GCP vs. Azure, or something else)?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Your help will be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157ifs9", "is_robot_indexable": true, "report_reasons": null, "author": "albion_shala", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157ifs9/a_project_manager_aiming_on_finding_his_specialty/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157ifs9/a_project_manager_aiming_on_finding_his_specialty/", "subreddit_subscribers": 958944, "created_utc": 1690127863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello fellow redditors!\n\nCan someone tell me how to deploy a streamlit app to railway?", "author_fullname": "t2_ue0qb0lb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help deploying a Streamlit app to Railway.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157b9dj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690107894.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow redditors!&lt;/p&gt;\n\n&lt;p&gt;Can someone tell me how to deploy a streamlit app to railway?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157b9dj", "is_robot_indexable": true, "report_reasons": null, "author": "Soft-Wish9738", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157b9dj/need_help_deploying_a_streamlit_app_to_railway/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157b9dj/need_help_deploying_a_streamlit_app_to_railway/", "subreddit_subscribers": 958944, "created_utc": 1690107894.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a CS freshman from South America and i have been getting into Data and some ML stuff recently.\n\nA big goal of mine is to be able to find remote work with US and EU companies and get paid in USD while working in SA.\n\nI heard that in this area DS, Data analytics and DE are harder to find opportunities if compared to regular development, is that true? Will it be really hard to find such opportunities and maybe data isnt for me if thats my main goal?", "author_fullname": "t2_7xe340s7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Opportunities for remote offshoring in DS and DE positions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157m39g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690136545.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a CS freshman from South America and i have been getting into Data and some ML stuff recently.&lt;/p&gt;\n\n&lt;p&gt;A big goal of mine is to be able to find remote work with US and EU companies and get paid in USD while working in SA.&lt;/p&gt;\n\n&lt;p&gt;I heard that in this area DS, Data analytics and DE are harder to find opportunities if compared to regular development, is that true? Will it be really hard to find such opportunities and maybe data isnt for me if thats my main goal?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157m39g", "is_robot_indexable": true, "report_reasons": null, "author": "SnooPineapples7791", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157m39g/opportunities_for_remote_offshoring_in_ds_and_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157m39g/opportunities_for_remote_offshoring_in_ds_and_de/", "subreddit_subscribers": 958944, "created_utc": 1690136545.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Do i need to learn Scala as Data Scientists?", "author_fullname": "t2_qposd3eu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do i need to learn Scala as Data Scientists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157emen", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.52, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690122363.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690118164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do i need to learn Scala as Data Scientists?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157emen", "is_robot_indexable": true, "report_reasons": null, "author": "zzzzzzyyyyyyy", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157emen/do_i_need_to_learn_scala_as_data_scientists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157emen/do_i_need_to_learn_scala_as_data_scientists/", "subreddit_subscribers": 958944, "created_utc": 1690118164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been analyzing a specific dataset in Tableau for work (highly retained users vs churned out). Ive reached the limits of what tableau can do, and it's time to go to linear regression to see if there's any signal here about what causes retention/churn.\nMy dataset is 3.5 million rows and 20 columns. When I try to export it to a pandas dataframe (straight from Big query) it's takes a long time.\nWhat should I do? Work with all 3.5M in pandas? Take a proportional sample from each population? 50-50 split (highly retained is only 10% of the overall population)?", "author_fullname": "t2_13b1f3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Select dataset for pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_157bfv5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690108467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been analyzing a specific dataset in Tableau for work (highly retained users vs churned out). Ive reached the limits of what tableau can do, and it&amp;#39;s time to go to linear regression to see if there&amp;#39;s any signal here about what causes retention/churn.\nMy dataset is 3.5 million rows and 20 columns. When I try to export it to a pandas dataframe (straight from Big query) it&amp;#39;s takes a long time.\nWhat should I do? Work with all 3.5M in pandas? Take a proportional sample from each population? 50-50 split (highly retained is only 10% of the overall population)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "157bfv5", "is_robot_indexable": true, "report_reasons": null, "author": "aaquad", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/157bfv5/select_dataset_for_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/157bfv5/select_dataset_for_pandas/", "subreddit_subscribers": 958944, "created_utc": 1690108467.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}