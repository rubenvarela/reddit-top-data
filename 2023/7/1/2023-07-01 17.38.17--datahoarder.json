{"kind": "Listing", "data": {"after": "t3_14nvoru", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I made the hard choice. I deleted my 11 TB porn Jellyfin server. Had been collecting for years. But I felt it was time to let go and focus on real life in this situation. Felt like a hard choice to make, it was a big step. But I also feel somehow clean. A new start.\n\n(Ps a small part of the decision MAY have also been that I have in on Google Drive and slowly preparing for the inevitable, lol)", "author_fullname": "t2_10fiz2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I deleted stuff. I feel dirty, LOL but clean", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nqoaz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 159, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 159, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688203545.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made the hard choice. I deleted my 11 TB porn Jellyfin server. Had been collecting for years. But I felt it was time to let go and focus on real life in this situation. Felt like a hard choice to make, it was a big step. But I also feel somehow clean. A new start.&lt;/p&gt;\n\n&lt;p&gt;(Ps a small part of the decision MAY have also been that I have in on Google Drive and slowly preparing for the inevitable, lol)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14nqoaz", "is_robot_indexable": true, "report_reasons": null, "author": "Boogertwilliams", "discussion_type": null, "num_comments": 86, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nqoaz/i_deleted_stuff_i_feel_dirty_lol_but_clean/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nqoaz/i_deleted_stuff_i_feel_dirty_lol_but_clean/", "subreddit_subscribers": 690531, "created_utc": 1688203545.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I know that 30TB HDDs are gonna happen and would probably cost nothing compared to 30TB SSDs, but HDDs won't hit 60TB anytime soon!", "author_fullname": "t2_9igqvq1b4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just found out that the company where I work have many 60TB SSD units. Couldn't get more info from the IT guy but I'm curious how we can get those and their pricing.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14n76g6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 101, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 101, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688147435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that 30TB HDDs are gonna happen and would probably cost nothing compared to 30TB SSDs, but HDDs won&amp;#39;t hit 60TB anytime soon!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "150TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14n76g6", "is_robot_indexable": true, "report_reasons": null, "author": "500xp1", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14n76g6/just_found_out_that_the_company_where_i_work_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14n76g6/just_found_out_that_the_company_where_i_work_have/", "subreddit_subscribers": 690531, "created_utc": 1688147435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, everybody. One month ago I suddenly lost a dear friend from Twitter, who was only 40 years old when she died in an accident. Now, I would like to download her entire feed to preserve her memory and give all her tweets to her family (whose members do not have Twitter), as a way to preserve her memory. I already searched on the Internet and in this very subreddit for a way to do this and found some suggestions involving Github and Python scripts. However, I have an additional problem: her account is PRIVATE, so I didn't manage to get her tweets using these methods. Is there some way of doing this directly from her page, since at least I follow her and can read what she wrote? Or is it the only way to screenshot everything? Hope someone can help me with this!", "author_fullname": "t2_91g8a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download the entire Twitter feed from a deceased friend: is it possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14njo5z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 74, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 74, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688179811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, everybody. One month ago I suddenly lost a dear friend from Twitter, who was only 40 years old when she died in an accident. Now, I would like to download her entire feed to preserve her memory and give all her tweets to her family (whose members do not have Twitter), as a way to preserve her memory. I already searched on the Internet and in this very subreddit for a way to do this and found some suggestions involving Github and Python scripts. However, I have an additional problem: her account is PRIVATE, so I didn&amp;#39;t manage to get her tweets using these methods. Is there some way of doing this directly from her page, since at least I follow her and can read what she wrote? Or is it the only way to screenshot everything? Hope someone can help me with this!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14njo5z", "is_robot_indexable": true, "report_reasons": null, "author": "vitordornelles", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14njo5z/download_the_entire_twitter_feed_from_a_deceased/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14njo5z/download_the_entire_twitter_feed_from_a_deceased/", "subreddit_subscribers": 690531, "created_utc": 1688179811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With all of the concern over Reddit tightening their API, the obvious solution is for software to use the human-facing html interface (https://reddit.com/r/*) in lieu of the formal API (https://reddit.com/api/v1/*).\n\nThis has already been discussed on a handful of subreddits that I have seen, but I haven't seen anyone say that they are writing a client library like that, or that one already exists.  I've poked around some and haven't found one in any of the obvious places, yet (github, pypi, npm, etc).\n\nAre any of you aware of such a project, before I start one of my own?  I need another project like I need a hole in my head, but I want a future-proof Reddit client library, too.", "author_fullname": "t2_cpegz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone working on a human-interface Reddit crawler?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nih44", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688176129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With all of the concern over Reddit tightening their API, the obvious solution is for software to use the human-facing html interface (&lt;a href=\"https://reddit.com/r/*\"&gt;https://reddit.com/r/*&lt;/a&gt;) in lieu of the formal API (&lt;a href=\"https://reddit.com/api/v1/*\"&gt;https://reddit.com/api/v1/*&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;This has already been discussed on a handful of subreddits that I have seen, but I haven&amp;#39;t seen anyone say that they are writing a client library like that, or that one already exists.  I&amp;#39;ve poked around some and haven&amp;#39;t found one in any of the obvious places, yet (github, pypi, npm, etc).&lt;/p&gt;\n\n&lt;p&gt;Are any of you aware of such a project, before I start one of my own?  I need another project like I need a hole in my head, but I want a future-proof Reddit client library, too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nih44", "is_robot_indexable": true, "report_reasons": null, "author": "ttkciar", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nih44/is_anyone_working_on_a_humaninterface_reddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nih44/is_anyone_working_on_a_humaninterface_reddit/", "subreddit_subscribers": 690531, "created_utc": 1688176129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As in, searching for certain text in tens of thousands of images. Not trying to OCR individual images.", "author_fullname": "t2_13mwqlw5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I search for text in a large number of images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nomwg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688196189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As in, searching for certain text in tens of thousands of images. Not trying to OCR individual images.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nomwg", "is_robot_indexable": true, "report_reasons": null, "author": "catinterpreter", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nomwg/how_can_i_search_for_text_in_a_large_number_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nomwg/how_can_i_search_for_text_in_a_large_number_of/", "subreddit_subscribers": 690531, "created_utc": 1688196189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I procrastinated in going back to download content from a banned NSFW sub and now have lost the ability to use PushShift to do so. \n\nI know it's possible to download all of reddit up to March of 2023, but would I be able to access the imgur/gif links of the posts that hold the content? \n\nAny information or advice would be helpful. ", "author_fullname": "t2_238c78rb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to use the PushShift data dump info to pull content from old banned sub", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nke6r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688182045.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I procrastinated in going back to download content from a banned NSFW sub and now have lost the ability to use PushShift to do so. &lt;/p&gt;\n\n&lt;p&gt;I know it&amp;#39;s possible to download all of reddit up to March of 2023, but would I be able to access the imgur/gif links of the posts that hold the content? &lt;/p&gt;\n\n&lt;p&gt;Any information or advice would be helpful. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nke6r", "is_robot_indexable": true, "report_reasons": null, "author": "notapornalt9669", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nke6r/how_to_use_the_pushshift_data_dump_info_to_pull/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nke6r/how_to_use_the_pushshift_data_dump_info_to_pull/", "subreddit_subscribers": 690531, "created_utc": 1688182045.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am archiving some data to blu-ray m-discs. I ran a checksum comparison on my burned disc and I get a single mismatch one one video file on the disc. But the video file from the disc plays just fine. Does that mean the data is ok? I have tried running the checksum multiple times, same result. I tried md5 and Sha1.", "author_fullname": "t2_b9gcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Checksum mismatch, but file still plays", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ngpkd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688170848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am archiving some data to blu-ray m-discs. I ran a checksum comparison on my burned disc and I get a single mismatch one one video file on the disc. But the video file from the disc plays just fine. Does that mean the data is ok? I have tried running the checksum multiple times, same result. I tried md5 and Sha1.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ngpkd", "is_robot_indexable": true, "report_reasons": null, "author": "Frolikewoah", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ngpkd/checksum_mismatch_but_file_still_plays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14ngpkd/checksum_mismatch_but_file_still_plays/", "subreddit_subscribers": 690531, "created_utc": 1688170848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Enormous data breach reveals 360M records from popular free VPN service", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": true, "name": "t3_14nyddd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_d2xe0c33", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XOXFGRla5a-gkoOr-ZU3NOFi433PjY1ZApTRbVapxfc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "vpnnetwork", "selftext": "", "author_fullname": "t2_d2xe0c33", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Enormous data breach reveals 360M records from popular free VPN service", "link_flair_richtext": [], "subreddit_name_prefixed": "r/vpnnetwork", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": true, "name": "t3_14nycky", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XOXFGRla5a-gkoOr-ZU3NOFi433PjY1ZApTRbVapxfc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688226574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "foxnews.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.foxnews.com/tech/massive-free-vpn-data-breach-exposes-360-million-records", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?auto=webp&amp;v=enabled&amp;s=90f64d921391fd22675a2ad03e2ea56b97b32f9d", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d259061d15e7adf2df73a0b519439d70ffe6f6a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9c91323aeaad3277948c13cdb331b176214a326", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56c98bf3a71ade7633e193ca8722c135d6674f3e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cea0d43fe8e4be8862cb76163deb7ebffb5708b3", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=575b4226651f539f78ea8b09ddc200cd9b61d928", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=00e19c6ff4eba51968e14dc4412c8eed2e9c433d", "width": 1080, "height": 607}], "variants": {}, "id": "8HzFKKG1Og5LTkg7SwDuXi2wkqMW2i00oPa89_yAlGY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ead29b9a-58f1-11ed-9b9a-86d24560bd17", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_740hil", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "14nycky", "is_robot_indexable": true, "report_reasons": null, "author": "More_Breakfast2084", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/vpnnetwork/comments/14nycky/enormous_data_breach_reveals_360m_records_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.foxnews.com/tech/massive-free-vpn-data-breach-exposes-360-million-records", "subreddit_subscribers": 15088, "created_utc": 1688226574.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1688226629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "foxnews.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.foxnews.com/tech/massive-free-vpn-data-breach-exposes-360-million-records", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?auto=webp&amp;v=enabled&amp;s=90f64d921391fd22675a2ad03e2ea56b97b32f9d", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d259061d15e7adf2df73a0b519439d70ffe6f6a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9c91323aeaad3277948c13cdb331b176214a326", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56c98bf3a71ade7633e193ca8722c135d6674f3e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cea0d43fe8e4be8862cb76163deb7ebffb5708b3", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=575b4226651f539f78ea8b09ddc200cd9b61d928", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=00e19c6ff4eba51968e14dc4412c8eed2e9c433d", "width": 1080, "height": 607}], "variants": {}, "id": "8HzFKKG1Og5LTkg7SwDuXi2wkqMW2i00oPa89_yAlGY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14nyddd", "is_robot_indexable": true, "report_reasons": null, "author": "More_Breakfast2084", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_14nycky", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nyddd/enormous_data_breach_reveals_360m_records_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.foxnews.com/tech/massive-free-vpn-data-breach-exposes-360-million-records", "subreddit_subscribers": 690531, "created_utc": 1688226629.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "as per title..  \n\n\nI'm vaguely contemplating buying a unit.. I want to attach it to a LSI HBA.  \n\n\nJust wanting to confirm it will be play happily with non-Dell hardware. I only have Supermicro/Intel Xeon/ LSI gear etc ('whitebox')  \n\n\ncheers", "author_fullname": "t2_as0s5xdl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone running the Dell Power Vault MD3460 as a DAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nn3ox", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688190934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as per title..  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m vaguely contemplating buying a unit.. I want to attach it to a LSI HBA.  &lt;/p&gt;\n\n&lt;p&gt;Just wanting to confirm it will be play happily with non-Dell hardware. I only have Supermicro/Intel Xeon/ LSI gear etc (&amp;#39;whitebox&amp;#39;)  &lt;/p&gt;\n\n&lt;p&gt;cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "0.145PB, ZFS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nn3ox", "is_robot_indexable": true, "report_reasons": null, "author": "kaheksajalg7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14nn3ox/anyone_running_the_dell_power_vault_md3460_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nn3ox/anyone_running_the_dell_power_vault_md3460_as_a/", "subreddit_subscribers": 690531, "created_utc": 1688190934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My H310 makes an electrical noise. I'm looking for something to boost the audio, haven't had any success yet.\n\nAny tips or advice would be appreciated.\n\nThis sound occurs when the LED, which is lit up in green on the card, turns off.  \nAm I right that this is a diagnostic LED? Dell says it's rebuilding RAID when it's flashing, which can't be because I'm using Unraid.  \nIt's the sound that sounds like a metronome, unfortunately I've gotta crank up the volume on my phone to hear it.  \n\n\nIt could be a relay, but this is new.I've moved the server about a month ago by car.\nThe only recent change is a new corsair SF600 PSU about a month ago, which was around the same time this noise started. This PC is offsite so I don't hear it often.\n\nhttps://preview.redd.it/xbpuwrevu79b1.png?width=2544&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5f2a3aad46425a6e16b568d85e35a390ae338d8d&amp;height=1422\n\n&amp;#x200B;\n\n[Imgur video](https://imgur.com/a/ENkDr5f)\n\nEdit: formatting and additional info", "author_fullname": "t2_dg3rb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Electrical noise from LSI 9240-8i Dell H310", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"xbpuwrevu79b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/xbpuwrevu79b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d6ed05451773627327aefda6757c31fe93b89ea"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/xbpuwrevu79b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33b6c50a11acb6a93766295c1bd116a01f6cd029"}, {"y": 178, "x": 320, "u": "https://preview.redd.it/xbpuwrevu79b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2eb545df9d5b4c2bc46525dbf01516ed3e5b071e"}, {"y": 357, "x": 640, "u": "https://preview.redd.it/xbpuwrevu79b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0cb87210512ff9a0804add2441731b1c573796f3"}, {"y": 536, "x": 960, "u": "https://preview.redd.it/xbpuwrevu79b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19bae76621119184274c8543dd7a1a510a67d7fc"}, {"y": 603, "x": 1080, "u": "https://preview.redd.it/xbpuwrevu79b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b8331c9054675174758b2a52d5a5c5567d084d4"}], "s": {"y": 1422, "x": 2544, "u": "https://preview.redd.it/xbpuwrevu79b1.png?width=2544&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5f2a3aad46425a6e16b568d85e35a390ae338d8d"}, "id": "xbpuwrevu79b1"}}, "name": "t3_14nbhkk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/VuqrMFj0v16PA2C4nV9nuZjW2dG1i36l8d2B7mSH3hU.jpg", "edited": 1688158258.0, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1688157628.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My H310 makes an electrical noise. I&amp;#39;m looking for something to boost the audio, haven&amp;#39;t had any success yet.&lt;/p&gt;\n\n&lt;p&gt;Any tips or advice would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;This sound occurs when the LED, which is lit up in green on the card, turns off.&lt;br/&gt;\nAm I right that this is a diagnostic LED? Dell says it&amp;#39;s rebuilding RAID when it&amp;#39;s flashing, which can&amp;#39;t be because I&amp;#39;m using Unraid.&lt;br/&gt;\nIt&amp;#39;s the sound that sounds like a metronome, unfortunately I&amp;#39;ve gotta crank up the volume on my phone to hear it.  &lt;/p&gt;\n\n&lt;p&gt;It could be a relay, but this is new.I&amp;#39;ve moved the server about a month ago by car.\nThe only recent change is a new corsair SF600 PSU about a month ago, which was around the same time this noise started. This PC is offsite so I don&amp;#39;t hear it often.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/xbpuwrevu79b1.png?width=2544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5f2a3aad46425a6e16b568d85e35a390ae338d8d&amp;amp;height=1422\"&gt;https://preview.redd.it/xbpuwrevu79b1.png?width=2544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5f2a3aad46425a6e16b568d85e35a390ae338d8d&amp;amp;height=1422&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/ENkDr5f\"&gt;Imgur video&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Edit: formatting and additional info&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Z24eIjCtdCH6jWGPLXe2-5eG-lp840-UHqtoPnajZXY.jpg?auto=webp&amp;v=enabled&amp;s=5887b59ca4411790c986e94e4d23b9a6c88268ab", "width": 600, "height": 315}, "resolutions": [{"url": "https://external-preview.redd.it/Z24eIjCtdCH6jWGPLXe2-5eG-lp840-UHqtoPnajZXY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de8d1da02e597a210e79a0695378bcc29828d4aa", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Z24eIjCtdCH6jWGPLXe2-5eG-lp840-UHqtoPnajZXY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7a63b7dabc55d2762693d6a597985f24902f78c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Z24eIjCtdCH6jWGPLXe2-5eG-lp840-UHqtoPnajZXY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76cd03474dd749a759fe3e6e83ca07be81025a36", "width": 320, "height": 168}], "variants": {}, "id": "dxR8HjOIMmC0GVrnNe2-KfzIa7ycqFjSET5VtlIBEfE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "...", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nbhkk", "is_robot_indexable": true, "report_reasons": null, "author": "THEMighter", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14nbhkk/electrical_noise_from_lsi_92408i_dell_h310/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nbhkk/electrical_noise_from_lsi_92408i_dell_h310/", "subreddit_subscribers": 690531, "created_utc": 1688157628.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought a WD 6TB Red plus drive (model WD60EFPX).  WD's documentation says this disk is CMR.   This is a 5400 rpm, 256MB cache drive (the older model is WD60EFZX-5640rpm/128MB).    \n\nThe original drive gave me IDNF errors on writes multiple times under raid.  I RMA'ed that drive and got a replacement red plus.     I tested the replacement by creating a filesystems and writing data to it and then erasing data and writing new data.  After filling the disk about 2x I started getting IDNF errors.  The given machine has other 6TB disks in it and they are all working just fine.\n\nI have seen a lot of disks errors but the whole IDNF errors seems to make no sense on a real CMR device and should technically be impossible on a CMR disk, unless the firmware is has a serious bug.\n\nIs this a SMR disk or is this screwed up firmware, or is something else going on?  I have not yet tried it in another machine to see if the new disks are somehow more sensitive to power supply issues.\n\nI am out of ideas outside of trying it in another machine.  It has been RMA'ed and the replacement has the same issue.  This is on linux.  Ideas?", "author_fullname": "t2_dymi0cog", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New 6TB WD Red Plus drives given IDNF (SMR like) errors when written to, reads always work. RME'ed and new drive gets the same errors.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14na9av", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688154677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a WD 6TB Red plus drive (model WD60EFPX).  WD&amp;#39;s documentation says this disk is CMR.   This is a 5400 rpm, 256MB cache drive (the older model is WD60EFZX-5640rpm/128MB).    &lt;/p&gt;\n\n&lt;p&gt;The original drive gave me IDNF errors on writes multiple times under raid.  I RMA&amp;#39;ed that drive and got a replacement red plus.     I tested the replacement by creating a filesystems and writing data to it and then erasing data and writing new data.  After filling the disk about 2x I started getting IDNF errors.  The given machine has other 6TB disks in it and they are all working just fine.&lt;/p&gt;\n\n&lt;p&gt;I have seen a lot of disks errors but the whole IDNF errors seems to make no sense on a real CMR device and should technically be impossible on a CMR disk, unless the firmware is has a serious bug.&lt;/p&gt;\n\n&lt;p&gt;Is this a SMR disk or is this screwed up firmware, or is something else going on?  I have not yet tried it in another machine to see if the new disks are somehow more sensitive to power supply issues.&lt;/p&gt;\n\n&lt;p&gt;I am out of ideas outside of trying it in another machine.  It has been RMA&amp;#39;ed and the replacement has the same issue.  This is on linux.  Ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14na9av", "is_robot_indexable": true, "report_reasons": null, "author": "RandomUser3777", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14na9av/new_6tb_wd_red_plus_drives_given_idnf_smr_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14na9av/new_6tb_wd_red_plus_drives_given_idnf_smr_like/", "subreddit_subscribers": 690531, "created_utc": 1688154677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is an article I wrote about some challenges our archive project faces, which I believe may resonate with many of you. Its the first article I have written, so feedback is appreciated!", "author_fullname": "t2_few9393b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Digital Paradox: How the Internet Age is Erasing our Past", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_14ny22h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/vPsxmNBkYXmAunsVedf8A75tl78GaSZYLtgkeUZUxk4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688225804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is an article I wrote about some challenges our archive project faces, which I believe may resonate with many of you. Its the first article I have written, so feedback is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@avenstewart/the-digital-paradox-how-the-internet-age-is-erasing-our-past-5a367ecf42f0", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?auto=webp&amp;v=enabled&amp;s=305e568c784b704078d9e4d7714755382bc2ac51", "width": 1200, "height": 847}, "resolutions": [{"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff5a00630be5b2d65486c6ab5990461f93fcf87b", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ddcd9713fdbcd6edf184405f303aed0a9529b48e", "width": 216, "height": 152}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=538c88dba59703a953a9afb57171ed4043a0b050", "width": 320, "height": 225}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f29633e0477d93433d360d014bc32f05a0a56f0b", "width": 640, "height": 451}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d017b1f3e34d8d7bd2f95821d1c8b93e0d62ccd", "width": 960, "height": 677}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=951f3eb45588a5881eb69d13818cb9306e1f8e87", "width": 1080, "height": 762}], "variants": {}, "id": "mWw6L_GlywXuGnjjl0mRiUxsvMKC7FTgCCQ4lXCfims"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14ny22h", "is_robot_indexable": true, "report_reasons": null, "author": "SciencePlaceArchives", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ny22h/the_digital_paradox_how_the_internet_age_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@avenstewart/the-digital-paradox-how-the-internet-age-is-erasing-our-past-5a367ecf42f0", "subreddit_subscribers": 690531, "created_utc": 1688225804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I tried doing it with MakeMKV but it isn't able to read it most of the time and can't export it. I don't care about which format they're saved in as long as they can be played or at least converted. Does anyone have experience with this or have any ideas? I have a few hundred to digitize, maybe more, so I'm open to suggestions. Thanks!", "author_fullname": "t2_3lurx74i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digitizing Video CDs, how can I do it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nkcfr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688181888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried doing it with MakeMKV but it isn&amp;#39;t able to read it most of the time and can&amp;#39;t export it. I don&amp;#39;t care about which format they&amp;#39;re saved in as long as they can be played or at least converted. Does anyone have experience with this or have any ideas? I have a few hundred to digitize, maybe more, so I&amp;#39;m open to suggestions. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.5TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nkcfr", "is_robot_indexable": true, "report_reasons": null, "author": "LavaCreeperBOSSB", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14nkcfr/digitizing_video_cds_how_can_i_do_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nkcfr/digitizing_video_cds_how_can_i_do_it/", "subreddit_subscribers": 690531, "created_utc": 1688181888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nHi I have gained access to drive for a few minutes and I was able to download all files using IDM then after 2 hours it says that it needs to refrech the download link but opennig the browser leads to message acess denied\n\nis there any way I could continue downloading", "author_fullname": "t2_7st7gs73", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Refreching download link of google drive file without permission ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nj4q8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688178099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I have gained access to drive for a few minutes and I was able to download all files using IDM then after 2 hours it says that it needs to refrech the download link but opennig the browser leads to message acess denied&lt;/p&gt;\n\n&lt;p&gt;is there any way I could continue downloading&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14nj4q8", "is_robot_indexable": true, "report_reasons": null, "author": "No-Park7347", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nj4q8/refreching_download_link_of_google_drive_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nj4q8/refreching_download_link_of_google_drive_file/", "subreddit_subscribers": 690531, "created_utc": 1688178099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I remember announcements earlier this year about 24TB drives ... are they out yet?", "author_fullname": "t2_mgmtm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did 24TB Drives Come Out Yet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nivcy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688177307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I remember announcements earlier this year about 24TB drives ... are they out yet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nivcy", "is_robot_indexable": true, "report_reasons": null, "author": "HarryMuscle", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nivcy/did_24tb_drives_come_out_yet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nivcy/did_24tb_drives_come_out_yet/", "subreddit_subscribers": 690531, "created_utc": 1688177307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My OS (Win11) boot drive which is a newer 1TB Seagate FireCuda 530 NVMe is dropping about 2% per week in health. Now u/I 89% health with just 1,200 hours. This is compared to my SK Hynix P31 which has 30k hours at 99% health. \n\nI'd like to move my data to a new NVMe of equal size. I have a spare 1TB NVMe SSD of equal size installed in another m.2 slot and have two more available m.2 slots.\n\nI'd like to use the motherboard's onboard RAID (MSI Z790-A) to create a mirror of the new NVMe to an existing (formatted) NVMe of equal size. \n\nThen I would like to then clone the existing partitions (including Windows) from (failing) NVMe SSD to a new NVMe SSD of equal size.\n\nI have Macrium reflect, but am unclear if I clone the drive to one of the pair of mirrored disks if it will then copy to both automatically to create a mirror? (never used RAID) \n\nIs it advisable to have a mirror of the OS boot drive? What is the best way to accomplish the clone of a NVMe that has the operating system on it? I have several software programs (namely the Arr's and would like to keep the configuration and settings as they are.\n\nAny help is appreciated.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended way to clone data from NVMe SSD to NVMe SSD rapidly dropping in Health", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nhqld", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688173866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My OS (Win11) boot drive which is a newer 1TB Seagate FireCuda 530 NVMe is dropping about 2% per week in health. Now &lt;a href=\"/u/I\"&gt;u/I&lt;/a&gt; 89% health with just 1,200 hours. This is compared to my SK Hynix P31 which has 30k hours at 99% health. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to move my data to a new NVMe of equal size. I have a spare 1TB NVMe SSD of equal size installed in another m.2 slot and have two more available m.2 slots.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to use the motherboard&amp;#39;s onboard RAID (MSI Z790-A) to create a mirror of the new NVMe to an existing (formatted) NVMe of equal size. &lt;/p&gt;\n\n&lt;p&gt;Then I would like to then clone the existing partitions (including Windows) from (failing) NVMe SSD to a new NVMe SSD of equal size.&lt;/p&gt;\n\n&lt;p&gt;I have Macrium reflect, but am unclear if I clone the drive to one of the pair of mirrored disks if it will then copy to both automatically to create a mirror? (never used RAID) &lt;/p&gt;\n\n&lt;p&gt;Is it advisable to have a mirror of the OS boot drive? What is the best way to accomplish the clone of a NVMe that has the operating system on it? I have several software programs (namely the Arr&amp;#39;s and would like to keep the configuration and settings as they are.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nhqld", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nhqld/recommended_way_to_clone_data_from_nvme_ssd_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nhqld/recommended_way_to_clone_data_from_nvme_ssd_to/", "subreddit_subscribers": 690531, "created_utc": 1688173866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "in particular the open source ones?, and if there is no archive; what is the best way to make one?;\n\nhowever if there are archives, where are they?\n\nso i can see about aiding/solo-completing any of them, if its possible.", "author_fullname": "t2_2dbwmuha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "way for complete archival of the reddit bots?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14n770e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688147471.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;in particular the open source ones?, and if there is no archive; what is the best way to make one?;&lt;/p&gt;\n\n&lt;p&gt;however if there are archives, where are they?&lt;/p&gt;\n\n&lt;p&gt;so i can see about aiding/solo-completing any of them, if its possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14n770e", "is_robot_indexable": true, "report_reasons": null, "author": "565gta", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14n770e/way_for_complete_archival_of_the_reddit_bots/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14n770e/way_for_complete_archival_of_the_reddit_bots/", "subreddit_subscribers": 690531, "created_utc": 1688147471.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am using FDM, does it automatically assign a contiguous block of storage? Or does it keep on writing stuff on the fly to whatever small blocks it finds and basically results in a highly fragmented disc? All my movies range from 50 to 80 GB. I've got 3 of them downloading simultaneously for about an hour now. Any info on this topic is really appreciated, thanks!", "author_fullname": "t2_5b1mdb8x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am downloading multiple UHD movies simultaneously on my external HDD, will it result in disc fragmentation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14nzqjj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688230131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using FDM, does it automatically assign a contiguous block of storage? Or does it keep on writing stuff on the fly to whatever small blocks it finds and basically results in a highly fragmented disc? All my movies range from 50 to 80 GB. I&amp;#39;ve got 3 of them downloading simultaneously for about an hour now. Any info on this topic is really appreciated, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14nzqjj", "is_robot_indexable": true, "report_reasons": null, "author": "theADDMIN", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nzqjj/i_am_downloading_multiple_uhd_movies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nzqjj/i_am_downloading_multiple_uhd_movies/", "subreddit_subscribers": 690531, "created_utc": 1688230131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**Results are shown in separate posts below because this exceeds the 40000 Reddit Character Limit. Replies only accept 10000 character limit. Sorry about that.**\n\nI am posting this here but you can also view video showing the results (https://youtu.be/y0-VhMfUWwI), or on my blog (https://htwingnut.com/2023/07/01/smr-hard-drive-perforamnce-rebuild-with-parity-raid/) which is basically the exact same info presented here. The YouTube video is chaptered so you can skip to sections you want.\n\nIt is essentially the same content whether in this post or video, except I go into more detail explaining what SMR is in the video and has graphical charts instead of text results. There is a lot of data here and did my best to keep it concise and organized. There's so many ways to slice it, but feel free to digest it however you see fit. I don't blame you if you don't want to hear my nasally voice ramble on for an hour.\n\nThis started as a simple curiosity experiment but ballooned into a much larger test set and took much longer than expected. Through multiple iterations, troubleshooting, learning new things, tests run as time permitted, took me well over a year to complete.\n\nBasically testing that nobody asked for and probably don't care to know, but I did it anyhow, LOL.\n\nWhile most people here probably know what SMR is, I'll just give my two cent summary.\n\nShingled Magnetic Recording technology, abbreviated as SMR, allows for disk drives to store more capacity per platter than a traditional hard drive. The technology is intended to reduce costs, because more data per platter means fewer platters and read/write heads. But because of how they store data the tradeoff is that write performance may degrade over time after enough data has been written to, deleted and then overwritten on the disk.\n\nSMR drives are really intended for archival data, or data that is not frequently deleted or changed. However, since Western Digital decided to change their bottom tier WD RED NAS line of disk from CMR to SMR a few years back without full disclosure, many users were not happy with this change, as it had adverse effects greatly increasing rebuild times primarily of ZFS arrays. This got me wondering how much of an effect it really has in a parity RAID situation in other configurations.\n\n**DRIVES and TEST CONFIGS:**\n\nThree different SMR disks were tested to see how they would fare in popular RAID configurations. \n\nThe SMR test disks were all 2TB 3.5\" SATA drives:\n\n* Seagate Barracuda Compute ST2000DM008\n* WD Red WD20EFAX\n* WD Blue WD20EZAZ\n\nSeveral control CMR test disks were also utilized. The control set of disks were Seagate ST2000DM001 2TB 7200 RPM hard drives. The following 2TB CMR disks were also tested for comparison:\n\n* Seagate Barracuda ST2000DM001\n* WD Red Plus WD20EFZX\n* Seagate Skyhawk ST2000VX008\n\nWhy were 2TB disks used? Bottom line, time and money. Some disks were already on hand, others were purchased solely for this test. Ultimately a Seagate Exos drive would have been preferable instead of Skyhawk, but at time of purchase, the Skyhawk was appreciably less expensive ($30 vs $80). 2TB also is the smallest capacity in 3.5\" form factor that comes in SMR as well as CMR. I also didn't want to have to fill up more data than needed because writing multiple TB's of data through dozens of tests is time consuming enough. Anything  more than 2TB would have extended the test time considerably.\n\nThe following configurations were tested for single disk REBUILD times:\n\n* OMV mdadm 4 disk RAID 5\n* OMV ZFS 4 disk RAID Z1\n* OMV SnapRAID Data &amp; Parity disk\n* Synology 4 disk SHR-1\n* QNAP TR-004 4 disk \"hardware\" RAID 5\n* UnRAID Data &amp; Parity Disk\n* Linux EXT4 Single Disk with and without TRIM\n* Windows NTFS Single Disk with and without TRIM\n* Controller: LSI 9211-8i vs Marvell 9215 vs Intel H77 MDADM RAID 5\n* Controller: LSI 9211-8i vs Marvell 9215 vs Intel H77 ZFS RAID Z1\n\nThe configurations that were tested both WRITE and READ:\n\n* NTFS Windows 10 Single Disk - tested over local SATA\n* EXT4 OMV Single Disk - tested over 1GbE SMB\n* XFS UnRAID single parity - tested over 1GbE SMB\n* MDADM 4x 2TB RAID 5 OMV, formatted EXT4, tested over 1GbE SMB\n* ZFS 4x 2TB RAID Z1 OMV, formatted ZFS (of course), tested over 1GbE SMB\n* Synology BTRFS/MDADM 4x 2TB RAID 5 with DS920+, formatted BTRFS, tested over 1GbE SMB\n* QNAP TR-004 DAS \"Hardware\" 4x 2TB RAID 5, formatted NTFS, tested over USB\n\n**HARDWARE SETUP**\n\nI made use of older hardware I had on hand. The PC config that was used for software NAS setup (i.e. OMV, UnRAID):\n\n* CPU: Intel Core i5-3570 3.4GHz 4 core / 4 thread\n* OS SSD: Sandisk Extreme 240GB\n* Motherboard: Asus P8H77-I\n* RAM: 2x8GB DDR3 1600\n* PSU for PC: Seasonic S12II 430W 80plus Bronze\n* PSU for HDD's: FSP Group FSP270-60LE 270W\n* SATA Controller: LSI 9211-8i\n* LAN: Onboard Realtek RTL8111F Gigabit\n* Disk Rack: Sans Digital HDD Rack 5 (https://www.sansdigital.com/hddrack5.html)\n\nNote: the onboard SATA controller (Intel H77 Express) and also PCIe SATA controller (Marvell 9215) were utilized to compare performance between controllers in a few scenarios\n\nThe PC that ran the test programs to perform send and receive of file over ethernet and USB was configured as:\n\n* OS: Windows 10 Pro\n* Test Files SSD: Muskin Reactor 1TB 2.5\" SATA (READ FROM SSD WRITE TO TEST ARRAY)\n* Test Files SSD: Crucial MX500 1TB 2.5\" SATA (WRITE TO SSD FROM TEST ARRAY)\n* CPU: Intel Xeon E5-2630 v3 2.4GHz 8 core / 16 thread\n* Motherboard: ASRock X99 Extreme4/3.1\n* RAM: 2x16GB DDR4 1866 ECC\n* SATA controller: onboard Intel X99\n* LAN: onboard Intel 1218V 1GbE\n* USB: onboard USB 3.1 5Gbps\n\n**TEST METHODOLOGY**\n\nDisks were not formatted or wiped between tests. All data from previous tests were left on the disk. Prior to each test, the disk was \"cleared\" or \"formatted\" using the minimal recommend process. For example in OMV, just running a  \"quick wipe\" before adding the disk to a RAID 5 array.\n\nAll default settings for setting up an array were used. No TRIM or DISCARD settings were manually implemented except for the few specific TRIM tests.\n\nFor RAID array tests, 4x ST2000DM001 hard drives were used as control level to start, build array, add filler data, and then complete WRITE/READ test. Then one ST2000DM001 disk was then replaced with a test disk, rebuild the array, and then initiate the next read/write test for that disk. Subsequent tests would just swap out the test disk with another test disk, rebuild, and perform write/read testing until all disks were tested for that particular array. So 3x ST2000DM001 disks would always remain in the array with the fourth disk being a test disk.\n\nFor each initial array setup, the array was filled with random size and content data leaving approximately 1200 to 1500 GB free on a RAID array (out of ~ 6GB in a four disk single parity setup), approximately 60-70% filled capacity. This is considered the filler data and is not touched after the initial write to the disk or array.\n\nFor single disk tests, the filler data was filled to about 60-70% capacity, leaving about 800GB free for testing.\n\nSample random file distribution:\n\n        Total Files: 9979\n      File Size Min: 3KB\n      File Size Max: 2098998KB (~ 2.1GB)\n      File Size Avg: 154379 (~ 154MB)\n    \n            % Files &lt; 1MB:  1.74%\n      % Files 1MB to 10MB:  9.59%\n    % Files 10MB to 100MB: 45.59%\n     % Files 100MB to 1GB: 41.48%\n            % Files &gt; 1GB:  1.60%\n\n\nAll test files were written from an SSD, and an actual file copy was executed from the Windows PC SSD over 1GbE or USB (depending on the test) using a Powershell script.\n\nFor RAID tests, gigabit ethernet was used because it is a typical use case to copy from a PC over SMB to a NAS. Any performance issues due to SMR should result in performance well below 1GbE speeds of about 112 MB/sec (realistic speeds).\n\nA Powershell script was written and utilized to automate the tasks of file fill and measuring the per file transfer performance of the test data.\n\nRandom data was generated using RNGCryptoServiceProvider:\n\n    $rnd10 = (Get-Random 10) + 1\n    $rndmax = [int64]((Get-Random (2GB - 1))/$rnd10)\n    $bytes = (Get-Random $rndmax)\n    [System.Security.Cryptography.RNGCryptoServiceProvider] $rng = New-Object System.Security.Cryptography.RNGCryptoServiceProvider\n    $rndbytes = New-Object byte[] $bytes\n    $rng.GetBytes($rndbytes)\n\n\nFile transfer performance was measured using the `StopWatch` command:\n\n    $StopWatch=[system.diagnostics.stopwatch]::startnew()\n    Copy-Item \"$spath\\$f\\$file\" \"$dpath\\TEST\\$f\"\n    $SecondsElapsed=$StopWatch.Elapsed.TotalSeconds\n    $StopWatch.Stop()\n\nThe average transfer speeds were computed using total size of files copied divided by summation of all time elapsed from above script.\n\nSetting up a proper test to mitigate skewed results due to regular fragmentation was of high consideration. The following two test scenarios were devised which should highlight any effects of SMR degradation while minimizing any effect of fragmentation:\n\n**Test 1:** Mixed Size Write / Read\n\n1. Write random size/content data to remaining full capacity of disk/array from Windows test PC.\n2. Use \"delete\" command to delete all random data just written.\n3. Write a test set of 620GB: 20x 10GB, 200x 1GB, 2000x 100MB, 20000x 1MB files from SSD and measure results per file.\n4. Read back half of files and measure results.\n5. Use \"delete\" command to delete test set of 620GB from test array.\n6. Immediately start next Scenario.\n\n**Test 2:** Alternating 10MB / 1GB files\n\n1. Write 10MB size files to remaining full capacity of disk/array from Windows test PC.\n2. Use \"delete\" command to delete EVERY OTHER 10MB file, so half the 10MB files just written.\n3. Write 800x 1GB sized files to disk/array from SSD and measure results per file.\n4. Read back half of files and measure results per file.\n5. Use \"delete\" command to delete all 10MB and 1GB test files.\n6. Remove disk and start next disk test.\n7. Shut down, remove test disk and replace with next test disk.\n\nBoth scenarios take the disk to its fullest capacity and then immediately delete and then write data back to the disk with no idle time.\n\nThe second scenario should really bring forward any issues pertaining to SMR. Alternating 10MB file deletion should remove big chunks of data from each SMR zone that would require a rewrite of data to fill each SMR zone back.\n\n**BASELINE PERFORMANCE**\n\nEvery disk was subjected to several baseline performance tests to ensure they were performing as intended:\n\n* CrystalDiskMark 5x 1GB test set\n* ATTO 512b to 64MB I/O with 1GB file size\n* Hard Disk Sentinel Full Disk WRITE and READ\n\nThe CrystalDiskMark and ATTO benchmarks are omitted from this post, only because compiling the results will be quite tedious and honestly not very value added. However, I may provide them as raw image data in a future update to this blog.\n\nThat being said, here are baseline results from the Hard Disk Sentinel Full Disk WRITE and READ tests for reference, which are probably more relevant anyhow: https://imgur.com/a/SdW5B9h\n\nThe two SSD's used to send and receive data were tested with HD Sentinel with results shown below:\n\n* Test SSD read from to the test array Mushkin Reactor 1TB 2.5\" SATA (475MB/sec read speed)\n* Test SSD write to from the test array Crucial MX500 1TB 2.5\" SATA (350MB/sec write speed)\n\n**FAILURES**\n\nThroughout testing, three of five WD Red WD20EFAX (SMR) had failures, all at different times in the testing. One became completely non responsive, it would power up but was never detected. Another started having increased failing sectors. The third one would fail out of a ZFS RAID Z1 rebuild despite having clean SMART and no other apparent issues. It was exchanged in hopes that the replacement would fare better. Each disk was successfully RMA'd promptly with a replacement within two weeks.\n\nAn SG Barracuda Compute ST2000DM008 (SMR) had to be RMA'd for being non-responsive and would hang the system periodically.\n\nEach problematic disk went through a thorough troubleshooting process, changing PC's, changing power supplies, changing cables, validating PSU voltages, etc before submitting for RMA. In each case, the replacement disk solved the issue completely (with the exception of the one instance with ZFS rebuild errors). Is this a result of SMR? Bad luck? Maybe some unknown other issue? I don't know, but each of the replacement disks soldiered on through the rest of the testing without a hitch.\n\nThe SG Barracuda Compute ST2000DM008 (SMR) was not recognized as a device supporting TRIM despite in Linux desite it running TRIM just fine with NTFS in Windows. This is why there was an N/A TRIM in the TRIM results for the Seagate Barracuda Compute ST2000DM008 drive.\n\nThere were four WD RED Disks utilized. Three were WD20EFAX-68B2RN1 and one was WD20EFAX-68FB5N0. The 68FB5N0 would not TRIM in Linux despite it running TRIM just fine in NTFS Windows. 68B2RN1 drives would accept TRIM commands fine in Linux and Windows. This is why the TRIM test only included 3x WD RED instead of 4x WD RED.\n\n**Results are shown in separate posts below because this exceeds the 40000 Reddit Character Limit. Replies only accept 10000 character limit. Sorry about that.**\n\n**Permalinks for Test Results:**\n\n**REBUILD TIMES:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xlb2/\n\n**TEST 1 RESULTS: WRITE 620GB MIXED FILES SIZES AFTER DISK FILL &amp; DELETE:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xwwd/\n\n**TEST 1 RESULTS: READ 620GB MIXED FILES SIZES AFTER DISK FILL &amp; DELETE:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9y96s/\n\n**TEST 2 RESULTS: WRITE &amp; READ 1GB FILES AFTER 10MB DISK FILL AND DELETE EVERY OTHER 10MB FILE:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yeqo/\n\n**TEST 3 RESULTS: WRITE PERFORMANCE AFTER TRIM:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9ymhv/\n\n**TEST 3 RESULTS: READ PERFORMANCE AFTER TRIM:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yq1u/", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extensive Testing - SMR Results with RAID Rebuild and File Transfers Compared with CMR", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14nz7ow", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1688232104.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688228768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Results are shown in separate posts below because this exceeds the 40000 Reddit Character Limit. Replies only accept 10000 character limit. Sorry about that.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I am posting this here but you can also view video showing the results (&lt;a href=\"https://youtu.be/y0-VhMfUWwI\"&gt;https://youtu.be/y0-VhMfUWwI&lt;/a&gt;), or on my blog (&lt;a href=\"https://htwingnut.com/2023/07/01/smr-hard-drive-perforamnce-rebuild-with-parity-raid/\"&gt;https://htwingnut.com/2023/07/01/smr-hard-drive-perforamnce-rebuild-with-parity-raid/&lt;/a&gt;) which is basically the exact same info presented here. The YouTube video is chaptered so you can skip to sections you want.&lt;/p&gt;\n\n&lt;p&gt;It is essentially the same content whether in this post or video, except I go into more detail explaining what SMR is in the video and has graphical charts instead of text results. There is a lot of data here and did my best to keep it concise and organized. There&amp;#39;s so many ways to slice it, but feel free to digest it however you see fit. I don&amp;#39;t blame you if you don&amp;#39;t want to hear my nasally voice ramble on for an hour.&lt;/p&gt;\n\n&lt;p&gt;This started as a simple curiosity experiment but ballooned into a much larger test set and took much longer than expected. Through multiple iterations, troubleshooting, learning new things, tests run as time permitted, took me well over a year to complete.&lt;/p&gt;\n\n&lt;p&gt;Basically testing that nobody asked for and probably don&amp;#39;t care to know, but I did it anyhow, LOL.&lt;/p&gt;\n\n&lt;p&gt;While most people here probably know what SMR is, I&amp;#39;ll just give my two cent summary.&lt;/p&gt;\n\n&lt;p&gt;Shingled Magnetic Recording technology, abbreviated as SMR, allows for disk drives to store more capacity per platter than a traditional hard drive. The technology is intended to reduce costs, because more data per platter means fewer platters and read/write heads. But because of how they store data the tradeoff is that write performance may degrade over time after enough data has been written to, deleted and then overwritten on the disk.&lt;/p&gt;\n\n&lt;p&gt;SMR drives are really intended for archival data, or data that is not frequently deleted or changed. However, since Western Digital decided to change their bottom tier WD RED NAS line of disk from CMR to SMR a few years back without full disclosure, many users were not happy with this change, as it had adverse effects greatly increasing rebuild times primarily of ZFS arrays. This got me wondering how much of an effect it really has in a parity RAID situation in other configurations.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;DRIVES and TEST CONFIGS:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Three different SMR disks were tested to see how they would fare in popular RAID configurations. &lt;/p&gt;\n\n&lt;p&gt;The SMR test disks were all 2TB 3.5&amp;quot; SATA drives:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Seagate Barracuda Compute ST2000DM008&lt;/li&gt;\n&lt;li&gt;WD Red WD20EFAX&lt;/li&gt;\n&lt;li&gt;WD Blue WD20EZAZ&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Several control CMR test disks were also utilized. The control set of disks were Seagate ST2000DM001 2TB 7200 RPM hard drives. The following 2TB CMR disks were also tested for comparison:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Seagate Barracuda ST2000DM001&lt;/li&gt;\n&lt;li&gt;WD Red Plus WD20EFZX&lt;/li&gt;\n&lt;li&gt;Seagate Skyhawk ST2000VX008&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Why were 2TB disks used? Bottom line, time and money. Some disks were already on hand, others were purchased solely for this test. Ultimately a Seagate Exos drive would have been preferable instead of Skyhawk, but at time of purchase, the Skyhawk was appreciably less expensive ($30 vs $80). 2TB also is the smallest capacity in 3.5&amp;quot; form factor that comes in SMR as well as CMR. I also didn&amp;#39;t want to have to fill up more data than needed because writing multiple TB&amp;#39;s of data through dozens of tests is time consuming enough. Anything  more than 2TB would have extended the test time considerably.&lt;/p&gt;\n\n&lt;p&gt;The following configurations were tested for single disk REBUILD times:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OMV mdadm 4 disk RAID 5&lt;/li&gt;\n&lt;li&gt;OMV ZFS 4 disk RAID Z1&lt;/li&gt;\n&lt;li&gt;OMV SnapRAID Data &amp;amp; Parity disk&lt;/li&gt;\n&lt;li&gt;Synology 4 disk SHR-1&lt;/li&gt;\n&lt;li&gt;QNAP TR-004 4 disk &amp;quot;hardware&amp;quot; RAID 5&lt;/li&gt;\n&lt;li&gt;UnRAID Data &amp;amp; Parity Disk&lt;/li&gt;\n&lt;li&gt;Linux EXT4 Single Disk with and without TRIM&lt;/li&gt;\n&lt;li&gt;Windows NTFS Single Disk with and without TRIM&lt;/li&gt;\n&lt;li&gt;Controller: LSI 9211-8i vs Marvell 9215 vs Intel H77 MDADM RAID 5&lt;/li&gt;\n&lt;li&gt;Controller: LSI 9211-8i vs Marvell 9215 vs Intel H77 ZFS RAID Z1&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The configurations that were tested both WRITE and READ:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;NTFS Windows 10 Single Disk - tested over local SATA&lt;/li&gt;\n&lt;li&gt;EXT4 OMV Single Disk - tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;XFS UnRAID single parity - tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;MDADM 4x 2TB RAID 5 OMV, formatted EXT4, tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;ZFS 4x 2TB RAID Z1 OMV, formatted ZFS (of course), tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;Synology BTRFS/MDADM 4x 2TB RAID 5 with DS920+, formatted BTRFS, tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;QNAP TR-004 DAS &amp;quot;Hardware&amp;quot; 4x 2TB RAID 5, formatted NTFS, tested over USB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;HARDWARE SETUP&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I made use of older hardware I had on hand. The PC config that was used for software NAS setup (i.e. OMV, UnRAID):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CPU: Intel Core i5-3570 3.4GHz 4 core / 4 thread&lt;/li&gt;\n&lt;li&gt;OS SSD: Sandisk Extreme 240GB&lt;/li&gt;\n&lt;li&gt;Motherboard: Asus P8H77-I&lt;/li&gt;\n&lt;li&gt;RAM: 2x8GB DDR3 1600&lt;/li&gt;\n&lt;li&gt;PSU for PC: Seasonic S12II 430W 80plus Bronze&lt;/li&gt;\n&lt;li&gt;PSU for HDD&amp;#39;s: FSP Group FSP270-60LE 270W&lt;/li&gt;\n&lt;li&gt;SATA Controller: LSI 9211-8i&lt;/li&gt;\n&lt;li&gt;LAN: Onboard Realtek RTL8111F Gigabit&lt;/li&gt;\n&lt;li&gt;Disk Rack: Sans Digital HDD Rack 5 (&lt;a href=\"https://www.sansdigital.com/hddrack5.html\"&gt;https://www.sansdigital.com/hddrack5.html&lt;/a&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Note: the onboard SATA controller (Intel H77 Express) and also PCIe SATA controller (Marvell 9215) were utilized to compare performance between controllers in a few scenarios&lt;/p&gt;\n\n&lt;p&gt;The PC that ran the test programs to perform send and receive of file over ethernet and USB was configured as:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OS: Windows 10 Pro&lt;/li&gt;\n&lt;li&gt;Test Files SSD: Muskin Reactor 1TB 2.5&amp;quot; SATA (READ FROM SSD WRITE TO TEST ARRAY)&lt;/li&gt;\n&lt;li&gt;Test Files SSD: Crucial MX500 1TB 2.5&amp;quot; SATA (WRITE TO SSD FROM TEST ARRAY)&lt;/li&gt;\n&lt;li&gt;CPU: Intel Xeon E5-2630 v3 2.4GHz 8 core / 16 thread&lt;/li&gt;\n&lt;li&gt;Motherboard: ASRock X99 Extreme4/3.1&lt;/li&gt;\n&lt;li&gt;RAM: 2x16GB DDR4 1866 ECC&lt;/li&gt;\n&lt;li&gt;SATA controller: onboard Intel X99&lt;/li&gt;\n&lt;li&gt;LAN: onboard Intel 1218V 1GbE&lt;/li&gt;\n&lt;li&gt;USB: onboard USB 3.1 5Gbps&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST METHODOLOGY&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Disks were not formatted or wiped between tests. All data from previous tests were left on the disk. Prior to each test, the disk was &amp;quot;cleared&amp;quot; or &amp;quot;formatted&amp;quot; using the minimal recommend process. For example in OMV, just running a  &amp;quot;quick wipe&amp;quot; before adding the disk to a RAID 5 array.&lt;/p&gt;\n\n&lt;p&gt;All default settings for setting up an array were used. No TRIM or DISCARD settings were manually implemented except for the few specific TRIM tests.&lt;/p&gt;\n\n&lt;p&gt;For RAID array tests, 4x ST2000DM001 hard drives were used as control level to start, build array, add filler data, and then complete WRITE/READ test. Then one ST2000DM001 disk was then replaced with a test disk, rebuild the array, and then initiate the next read/write test for that disk. Subsequent tests would just swap out the test disk with another test disk, rebuild, and perform write/read testing until all disks were tested for that particular array. So 3x ST2000DM001 disks would always remain in the array with the fourth disk being a test disk.&lt;/p&gt;\n\n&lt;p&gt;For each initial array setup, the array was filled with random size and content data leaving approximately 1200 to 1500 GB free on a RAID array (out of ~ 6GB in a four disk single parity setup), approximately 60-70% filled capacity. This is considered the filler data and is not touched after the initial write to the disk or array.&lt;/p&gt;\n\n&lt;p&gt;For single disk tests, the filler data was filled to about 60-70% capacity, leaving about 800GB free for testing.&lt;/p&gt;\n\n&lt;p&gt;Sample random file distribution:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    Total Files: 9979\n  File Size Min: 3KB\n  File Size Max: 2098998KB (~ 2.1GB)\n  File Size Avg: 154379 (~ 154MB)\n\n        % Files &amp;lt; 1MB:  1.74%\n  % Files 1MB to 10MB:  9.59%\n% Files 10MB to 100MB: 45.59%\n % Files 100MB to 1GB: 41.48%\n        % Files &amp;gt; 1GB:  1.60%\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;All test files were written from an SSD, and an actual file copy was executed from the Windows PC SSD over 1GbE or USB (depending on the test) using a Powershell script.&lt;/p&gt;\n\n&lt;p&gt;For RAID tests, gigabit ethernet was used because it is a typical use case to copy from a PC over SMB to a NAS. Any performance issues due to SMR should result in performance well below 1GbE speeds of about 112 MB/sec (realistic speeds).&lt;/p&gt;\n\n&lt;p&gt;A Powershell script was written and utilized to automate the tasks of file fill and measuring the per file transfer performance of the test data.&lt;/p&gt;\n\n&lt;p&gt;Random data was generated using RNGCryptoServiceProvider:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$rnd10 = (Get-Random 10) + 1\n$rndmax = [int64]((Get-Random (2GB - 1))/$rnd10)\n$bytes = (Get-Random $rndmax)\n[System.Security.Cryptography.RNGCryptoServiceProvider] $rng = New-Object System.Security.Cryptography.RNGCryptoServiceProvider\n$rndbytes = New-Object byte[] $bytes\n$rng.GetBytes($rndbytes)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;File transfer performance was measured using the &lt;code&gt;StopWatch&lt;/code&gt; command:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$StopWatch=[system.diagnostics.stopwatch]::startnew()\nCopy-Item &amp;quot;$spath\\$f\\$file&amp;quot; &amp;quot;$dpath\\TEST\\$f&amp;quot;\n$SecondsElapsed=$StopWatch.Elapsed.TotalSeconds\n$StopWatch.Stop()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The average transfer speeds were computed using total size of files copied divided by summation of all time elapsed from above script.&lt;/p&gt;\n\n&lt;p&gt;Setting up a proper test to mitigate skewed results due to regular fragmentation was of high consideration. The following two test scenarios were devised which should highlight any effects of SMR degradation while minimizing any effect of fragmentation:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Test 1:&lt;/strong&gt; Mixed Size Write / Read&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Write random size/content data to remaining full capacity of disk/array from Windows test PC.&lt;/li&gt;\n&lt;li&gt;Use &amp;quot;delete&amp;quot; command to delete all random data just written.&lt;/li&gt;\n&lt;li&gt;Write a test set of 620GB: 20x 10GB, 200x 1GB, 2000x 100MB, 20000x 1MB files from SSD and measure results per file.&lt;/li&gt;\n&lt;li&gt;Read back half of files and measure results.&lt;/li&gt;\n&lt;li&gt;Use &amp;quot;delete&amp;quot; command to delete test set of 620GB from test array.&lt;/li&gt;\n&lt;li&gt;Immediately start next Scenario.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Test 2:&lt;/strong&gt; Alternating 10MB / 1GB files&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Write 10MB size files to remaining full capacity of disk/array from Windows test PC.&lt;/li&gt;\n&lt;li&gt;Use &amp;quot;delete&amp;quot; command to delete EVERY OTHER 10MB file, so half the 10MB files just written.&lt;/li&gt;\n&lt;li&gt;Write 800x 1GB sized files to disk/array from SSD and measure results per file.&lt;/li&gt;\n&lt;li&gt;Read back half of files and measure results per file.&lt;/li&gt;\n&lt;li&gt;Use &amp;quot;delete&amp;quot; command to delete all 10MB and 1GB test files.&lt;/li&gt;\n&lt;li&gt;Remove disk and start next disk test.&lt;/li&gt;\n&lt;li&gt;Shut down, remove test disk and replace with next test disk.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Both scenarios take the disk to its fullest capacity and then immediately delete and then write data back to the disk with no idle time.&lt;/p&gt;\n\n&lt;p&gt;The second scenario should really bring forward any issues pertaining to SMR. Alternating 10MB file deletion should remove big chunks of data from each SMR zone that would require a rewrite of data to fill each SMR zone back.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;BASELINE PERFORMANCE&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Every disk was subjected to several baseline performance tests to ensure they were performing as intended:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CrystalDiskMark 5x 1GB test set&lt;/li&gt;\n&lt;li&gt;ATTO 512b to 64MB I/O with 1GB file size&lt;/li&gt;\n&lt;li&gt;Hard Disk Sentinel Full Disk WRITE and READ&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The CrystalDiskMark and ATTO benchmarks are omitted from this post, only because compiling the results will be quite tedious and honestly not very value added. However, I may provide them as raw image data in a future update to this blog.&lt;/p&gt;\n\n&lt;p&gt;That being said, here are baseline results from the Hard Disk Sentinel Full Disk WRITE and READ tests for reference, which are probably more relevant anyhow: &lt;a href=\"https://imgur.com/a/SdW5B9h\"&gt;https://imgur.com/a/SdW5B9h&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The two SSD&amp;#39;s used to send and receive data were tested with HD Sentinel with results shown below:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Test SSD read from to the test array Mushkin Reactor 1TB 2.5&amp;quot; SATA (475MB/sec read speed)&lt;/li&gt;\n&lt;li&gt;Test SSD write to from the test array Crucial MX500 1TB 2.5&amp;quot; SATA (350MB/sec write speed)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;FAILURES&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Throughout testing, three of five WD Red WD20EFAX (SMR) had failures, all at different times in the testing. One became completely non responsive, it would power up but was never detected. Another started having increased failing sectors. The third one would fail out of a ZFS RAID Z1 rebuild despite having clean SMART and no other apparent issues. It was exchanged in hopes that the replacement would fare better. Each disk was successfully RMA&amp;#39;d promptly with a replacement within two weeks.&lt;/p&gt;\n\n&lt;p&gt;An SG Barracuda Compute ST2000DM008 (SMR) had to be RMA&amp;#39;d for being non-responsive and would hang the system periodically.&lt;/p&gt;\n\n&lt;p&gt;Each problematic disk went through a thorough troubleshooting process, changing PC&amp;#39;s, changing power supplies, changing cables, validating PSU voltages, etc before submitting for RMA. In each case, the replacement disk solved the issue completely (with the exception of the one instance with ZFS rebuild errors). Is this a result of SMR? Bad luck? Maybe some unknown other issue? I don&amp;#39;t know, but each of the replacement disks soldiered on through the rest of the testing without a hitch.&lt;/p&gt;\n\n&lt;p&gt;The SG Barracuda Compute ST2000DM008 (SMR) was not recognized as a device supporting TRIM despite in Linux desite it running TRIM just fine with NTFS in Windows. This is why there was an N/A TRIM in the TRIM results for the Seagate Barracuda Compute ST2000DM008 drive.&lt;/p&gt;\n\n&lt;p&gt;There were four WD RED Disks utilized. Three were WD20EFAX-68B2RN1 and one was WD20EFAX-68FB5N0. The 68FB5N0 would not TRIM in Linux despite it running TRIM just fine in NTFS Windows. 68B2RN1 drives would accept TRIM commands fine in Linux and Windows. This is why the TRIM test only included 3x WD RED instead of 4x WD RED.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Results are shown in separate posts below because this exceeds the 40000 Reddit Character Limit. Replies only accept 10000 character limit. Sorry about that.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Permalinks for Test Results:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;REBUILD TIMES:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xlb2/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xlb2/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 1 RESULTS: WRITE 620GB MIXED FILES SIZES AFTER DISK FILL &amp;amp; DELETE:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xwwd/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xwwd/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 1 RESULTS: READ 620GB MIXED FILES SIZES AFTER DISK FILL &amp;amp; DELETE:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9y96s/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9y96s/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 2 RESULTS: WRITE &amp;amp; READ 1GB FILES AFTER 10MB DISK FILL AND DELETE EVERY OTHER 10MB FILE:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yeqo/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yeqo/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 3 RESULTS: WRITE PERFORMANCE AFTER TRIM:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9ymhv/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9ymhv/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 3 RESULTS: READ PERFORMANCE AFTER TRIM:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yq1u/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yq1u/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?auto=webp&amp;v=enabled&amp;s=a93ffd455f3489bc77ec3d13e7c75810e8ebad7a", "width": 3963, "height": 2230}, "resolutions": [{"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63194afa5000a7878ea279f0c201eb331bec8712", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f637559669ff57373e38461489bbc2676d8ea35e", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b64c32ad58a594e9094304d9b29977b2a2be870", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=805b211d783e8bb5ef6011192400c522a70c6558", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e2a476bb967ad6fa6dd3ebf145c54ee12077651", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=642beb2a64abcde2c0d264317cd3994570bb0676", "width": 1080, "height": 607}], "variants": {}, "id": "Uz_t8KVSq-Mkp85blH-_19UMIuSwthpYy-MMFKHvHgM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14nz7ow", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/", "subreddit_subscribers": 690531, "created_utc": 1688228768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First one is 8 bay, the other is 6. Can't find much online, especially about the terramaster.\n\nI'm planning to use 5 Seagate 18TB drives since they seems to offer the best value for money, still not sure about one or two drives for parity (two migh be overkill tbh).\n\nQNAP is more futureproof with 2 more bays, but I don't think i'll need those in the near future. It also has 2 120mm fans, which means they're generally quieter than 80mm (i think i'll be swapping those out for Noctua anyway).\n\nTerraMaster cost less and should be good enough for my needs since i'll be getting the same amount of storage early on with 5 drives. Has the same connectivity with 10gbps USB and looks like is well built. I don't like 80mm fans as I said, in case i'll be swapping those too but from experience, 120mm fans are quieter so, that's a minor point to it. The main issue is that there is nothing online beside announcements. No review, no opinions, nothing.\n\n&amp;#x200B;\n\nIf you have suggestions, or even other models to suggest, please feel free to do so.", "author_fullname": "t2_k99xt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "USB HDD enclosure: QNAP TL-D800C vs TerraMaster D6-320", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14nz3xf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688228498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First one is 8 bay, the other is 6. Can&amp;#39;t find much online, especially about the terramaster.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to use 5 Seagate 18TB drives since they seems to offer the best value for money, still not sure about one or two drives for parity (two migh be overkill tbh).&lt;/p&gt;\n\n&lt;p&gt;QNAP is more futureproof with 2 more bays, but I don&amp;#39;t think i&amp;#39;ll need those in the near future. It also has 2 120mm fans, which means they&amp;#39;re generally quieter than 80mm (i think i&amp;#39;ll be swapping those out for Noctua anyway).&lt;/p&gt;\n\n&lt;p&gt;TerraMaster cost less and should be good enough for my needs since i&amp;#39;ll be getting the same amount of storage early on with 5 drives. Has the same connectivity with 10gbps USB and looks like is well built. I don&amp;#39;t like 80mm fans as I said, in case i&amp;#39;ll be swapping those too but from experience, 120mm fans are quieter so, that&amp;#39;s a minor point to it. The main issue is that there is nothing online beside announcements. No review, no opinions, nothing.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you have suggestions, or even other models to suggest, please feel free to do so.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nz3xf", "is_robot_indexable": true, "report_reasons": null, "author": "NaXter24R", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nz3xf/usb_hdd_enclosure_qnap_tld800c_vs_terramaster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nz3xf/usb_hdd_enclosure_qnap_tld800c_vs_terramaster/", "subreddit_subscribers": 690531, "created_utc": 1688228498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've seen similar questions many times looking through this sub but need advice for my specific situation so any advice would help.\n\nExternal ssd or hdd?\n\nI have an old (8-10 yr) seagate external hard drive that has served me well, but it has been whirring for the past year and occasionally loses connection mid transfers. I use it mostly for storing video and access it maybe 5-6 times in a month, sometimes with many months of gaps in between.\n\nI've read keeping an ssd without power for too long can lead to data loss, but would a couple months also be a problem?\n\nI want something that'll last me another decade.\nCurrently considering Samsung T7 for ssd and either Seagate or WD for hdd.\n\nAny advice for my usage?\n\nEdit: Need a minimum 2TB", "author_fullname": "t2_5t4nbhds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to buy for an average user?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14nz277", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688231498.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688228377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen similar questions many times looking through this sub but need advice for my specific situation so any advice would help.&lt;/p&gt;\n\n&lt;p&gt;External ssd or hdd?&lt;/p&gt;\n\n&lt;p&gt;I have an old (8-10 yr) seagate external hard drive that has served me well, but it has been whirring for the past year and occasionally loses connection mid transfers. I use it mostly for storing video and access it maybe 5-6 times in a month, sometimes with many months of gaps in between.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read keeping an ssd without power for too long can lead to data loss, but would a couple months also be a problem?&lt;/p&gt;\n\n&lt;p&gt;I want something that&amp;#39;ll last me another decade.\nCurrently considering Samsung T7 for ssd and either Seagate or WD for hdd.&lt;/p&gt;\n\n&lt;p&gt;Any advice for my usage?&lt;/p&gt;\n\n&lt;p&gt;Edit: Need a minimum 2TB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nz277", "is_robot_indexable": true, "report_reasons": null, "author": "IncorrectCoffee", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nz277/what_to_buy_for_an_average_user/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nz277/what_to_buy_for_an_average_user/", "subreddit_subscribers": 690531, "created_utc": 1688228377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I deleted a file and I don't see them in the Recycle Bin. Can I recover the files without that USB Stick in case they are somewhere in my PC?", "author_fullname": "t2_6jgi2097", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do the files deleted from a USB Stick go?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nxct8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688223983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I deleted a file and I don&amp;#39;t see them in the Recycle Bin. Can I recover the files without that USB Stick in case they are somewhere in my PC?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nxct8", "is_robot_indexable": true, "report_reasons": null, "author": "Feistee", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nxct8/where_do_the_files_deleted_from_a_usb_stick_go/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nxct8/where_do_the_files_deleted_from_a_usb_stick_go/", "subreddit_subscribers": 690531, "created_utc": 1688223983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm thinking of taking the plunge into ZFS coming from Windows 11. I routinely use the Arr apps and would like help/suggestions on whether to go with Ubuntu on ZFS or to go with Unraid or similar with BTRFS?\n\nMy present setup is a z790 chipset 13th gen Intel with onboard graphics.\n\n Is there any suggested reading or guides available for someone like me who has zero experience with Linux and wants to switch to utilize the benefits of ZFS? \n\nI currently have a 16TB, 18TB, 18TB enterprise hard drives installed internally which contain data that I would like to have on my new ZFS system. I am booting from NVMe SSD. \n\nProblem is, all of my data is on NTFS partitions. I do have external HDD's (also NTFS) which contain the same data so I can format the above three HDD's to create the raid group.\n\nI am considering buying a 3rd 18tb HDD so I have equal size drives to create my RAIDZ2 group starting with just 3 drives and not including my 16tb to start with. \n\nWould it be best for me to have equal sized drives in creating this group? Can I expand the group when I buy more drives or need more space? How will I handle copying my files from a drive with NTFS to a ZFS pool?\n\nIs it wise to create more than one pool or manage just one pool? I've read that others do daily short SMART tests, weekly long tests, and bi-weekly/monthly scrubs. Would this be sufficient? \n\nThe purpose of this build will be to do one thing. To operate the Arr apps. (Sonarr, Prowlarr, Overseerr, etc.) I will be installing Plex and using it as a HTPC - and little to nothing else. \n\nThe reason I am looking to switch to ZFS is due to it detecting/repairing bitrot. I have thought about Unraid but became confused on whether to use Ubuntu or Unraid. The idea of parity drives kind of confused me when it comes to Unraid.\n\n I am unclear on what would be best for my usage scenario. What would you recommend for ease of implementation - taking into consideration that my primary usage scenario is the Arr apps and Plex? ", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guides or tips for setting up first ZFS file/plex server?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nwq7w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688222356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of taking the plunge into ZFS coming from Windows 11. I routinely use the Arr apps and would like help/suggestions on whether to go with Ubuntu on ZFS or to go with Unraid or similar with BTRFS?&lt;/p&gt;\n\n&lt;p&gt;My present setup is a z790 chipset 13th gen Intel with onboard graphics.&lt;/p&gt;\n\n&lt;p&gt;Is there any suggested reading or guides available for someone like me who has zero experience with Linux and wants to switch to utilize the benefits of ZFS? &lt;/p&gt;\n\n&lt;p&gt;I currently have a 16TB, 18TB, 18TB enterprise hard drives installed internally which contain data that I would like to have on my new ZFS system. I am booting from NVMe SSD. &lt;/p&gt;\n\n&lt;p&gt;Problem is, all of my data is on NTFS partitions. I do have external HDD&amp;#39;s (also NTFS) which contain the same data so I can format the above three HDD&amp;#39;s to create the raid group.&lt;/p&gt;\n\n&lt;p&gt;I am considering buying a 3rd 18tb HDD so I have equal size drives to create my RAIDZ2 group starting with just 3 drives and not including my 16tb to start with. &lt;/p&gt;\n\n&lt;p&gt;Would it be best for me to have equal sized drives in creating this group? Can I expand the group when I buy more drives or need more space? How will I handle copying my files from a drive with NTFS to a ZFS pool?&lt;/p&gt;\n\n&lt;p&gt;Is it wise to create more than one pool or manage just one pool? I&amp;#39;ve read that others do daily short SMART tests, weekly long tests, and bi-weekly/monthly scrubs. Would this be sufficient? &lt;/p&gt;\n\n&lt;p&gt;The purpose of this build will be to do one thing. To operate the Arr apps. (Sonarr, Prowlarr, Overseerr, etc.) I will be installing Plex and using it as a HTPC - and little to nothing else. &lt;/p&gt;\n\n&lt;p&gt;The reason I am looking to switch to ZFS is due to it detecting/repairing bitrot. I have thought about Unraid but became confused on whether to use Ubuntu or Unraid. The idea of parity drives kind of confused me when it comes to Unraid.&lt;/p&gt;\n\n&lt;p&gt;I am unclear on what would be best for my usage scenario. What would you recommend for ease of implementation - taking into consideration that my primary usage scenario is the Arr apps and Plex? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nwq7w", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nwq7w/guides_or_tips_for_setting_up_first_zfs_fileplex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nwq7w/guides_or_tips_for_setting_up_first_zfs_fileplex/", "subreddit_subscribers": 690531, "created_utc": 1688222356.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there ways to reuse random hard drive you got for free without much work? I'd image a small computer, you find a free hard drive, attach it to the computer and it will be automatically added to the Nas as a raid or so?", "author_fullname": "t2_jp3bv4xk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ghetto-NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nwaqu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688221205.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there ways to reuse random hard drive you got for free without much work? I&amp;#39;d image a small computer, you find a free hard drive, attach it to the computer and it will be automatically added to the Nas as a raid or so?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nwaqu", "is_robot_indexable": true, "report_reasons": null, "author": "CryptographerOdd299", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nwaqu/ghettonas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nwaqu/ghettonas/", "subreddit_subscribers": 690531, "created_utc": 1688221205.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\n&amp;#x200B;\n\nI have a Google workspace enterprise account with 10tb of used storage.  Paying \u00a315 a month and over my limit by 5tb.\n\n&amp;#x200B;\n\nHave data backed up on a HD at home.\n\n&amp;#x200B;\n\nData in cloud is for my Plex server using netdrive on a Windows server hosted in the cloud.\n\n&amp;#x200B;\n\nMy options are.\n\n&amp;#x200B;\n\nAdd another user and pay a total of \u00a330 a month for Google storage.\n\n&amp;#x200B;\n\nOr\n\n&amp;#x200B;\n\nUse my existing office 365 account, set up 5 users with 1 tb of OneDrive\n\n&amp;#x200B;\n\nMake several drives on my server from the OneDrive accounts using netdrive.\n\n&amp;#x200B;\n\nCopy over 1tb of data per account using multcloud.\n\n&amp;#x200B;\n\nThen create a Plex library combing the drives together.\n\n&amp;#x200B;\n\nGet another office 365 account and set up another 5 users to handle another 5TB of data.\n\n&amp;#x200B;\n\nThat way I'll spend \u00a3100 a year on 2x office 365 accounts and 12tb of OneDrive storage instead over \u00a3300 with Google for 10tb storage.\n\n&amp;#x200B;\n\nI've done a test using the OneDrive scenario and it seems to work ok with Plex.\n\n&amp;#x200B;\n\nHas anyone done this and any disadvantages apart from having to manage several OneDrive accounts?", "author_fullname": "t2_rco24", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just want to check before I implement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nvoru", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688219542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have a Google workspace enterprise account with 10tb of used storage.  Paying \u00a315 a month and over my limit by 5tb.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Have data backed up on a HD at home.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Data in cloud is for my Plex server using netdrive on a Windows server hosted in the cloud.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My options are.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Add another user and pay a total of \u00a330 a month for Google storage.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Or&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Use my existing office 365 account, set up 5 users with 1 tb of OneDrive&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Make several drives on my server from the OneDrive accounts using netdrive.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Copy over 1tb of data per account using multcloud.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Then create a Plex library combing the drives together.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Get another office 365 account and set up another 5 users to handle another 5TB of data.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;That way I&amp;#39;ll spend \u00a3100 a year on 2x office 365 accounts and 12tb of OneDrive storage instead over \u00a3300 with Google for 10tb storage.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done a test using the OneDrive scenario and it seems to work ok with Plex.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anyone done this and any disadvantages apart from having to manage several OneDrive accounts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nvoru", "is_robot_indexable": true, "report_reasons": null, "author": "richarduklon", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nvoru/just_want_to_check_before_i_implement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nvoru/just_want_to_check_before_i_implement/", "subreddit_subscribers": 690531, "created_utc": 1688219542.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}