{"kind": "Listing", "data": {"after": "t3_14nwq7w", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I made the hard choice. I deleted my 11 TB porn Jellyfin server. Had been collecting for years. But I felt it was time to let go and focus on real life in this situation. Felt like a hard choice to make, it was a big step. But I also feel somehow clean. A new start.\n\n(Ps a small part of the decision MAY have also been that I have in on Google Drive and slowly preparing for the inevitable, lol)", "author_fullname": "t2_10fiz2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I deleted stuff. I feel dirty, LOL but clean", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nqoaz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 275, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 275, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688203545.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made the hard choice. I deleted my 11 TB porn Jellyfin server. Had been collecting for years. But I felt it was time to let go and focus on real life in this situation. Felt like a hard choice to make, it was a big step. But I also feel somehow clean. A new start.&lt;/p&gt;\n\n&lt;p&gt;(Ps a small part of the decision MAY have also been that I have in on Google Drive and slowly preparing for the inevitable, lol)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14nqoaz", "is_robot_indexable": true, "report_reasons": null, "author": "Boogertwilliams", "discussion_type": null, "num_comments": 144, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nqoaz/i_deleted_stuff_i_feel_dirty_lol_but_clean/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nqoaz/i_deleted_stuff_i_feel_dirty_lol_but_clean/", "subreddit_subscribers": 690577, "created_utc": 1688203545.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, everybody. One month ago I suddenly lost a dear friend from Twitter, who was only 40 years old when she died in an accident. Now, I would like to download her entire feed to preserve her memory and give all her tweets to her family (whose members do not have Twitter), as a way to preserve her memory. I already searched on the Internet and in this very subreddit for a way to do this and found some suggestions involving Github and Python scripts. However, I have an additional problem: her account is PRIVATE, so I didn't manage to get her tweets using these methods. Is there some way of doing this directly from her page, since at least I follow her and can read what she wrote? Or is it the only way to screenshot everything? Hope someone can help me with this!", "author_fullname": "t2_91g8a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download the entire Twitter feed from a deceased friend: is it possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14njo5z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 79, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 79, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688179811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, everybody. One month ago I suddenly lost a dear friend from Twitter, who was only 40 years old when she died in an accident. Now, I would like to download her entire feed to preserve her memory and give all her tweets to her family (whose members do not have Twitter), as a way to preserve her memory. I already searched on the Internet and in this very subreddit for a way to do this and found some suggestions involving Github and Python scripts. However, I have an additional problem: her account is PRIVATE, so I didn&amp;#39;t manage to get her tweets using these methods. Is there some way of doing this directly from her page, since at least I follow her and can read what she wrote? Or is it the only way to screenshot everything? Hope someone can help me with this!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14njo5z", "is_robot_indexable": true, "report_reasons": null, "author": "vitordornelles", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14njo5z/download_the_entire_twitter_feed_from_a_deceased/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14njo5z/download_the_entire_twitter_feed_from_a_deceased/", "subreddit_subscribers": 690577, "created_utc": 1688179811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Enormous data breach reveals 360M records from popular free VPN service", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_14nyddd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_d2xe0c33", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XOXFGRla5a-gkoOr-ZU3NOFi433PjY1ZApTRbVapxfc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "vpnnetwork", "selftext": "", "author_fullname": "t2_d2xe0c33", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Enormous data breach reveals 360M records from popular free VPN service", "link_flair_richtext": [], "subreddit_name_prefixed": "r/vpnnetwork", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_14nycky", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XOXFGRla5a-gkoOr-ZU3NOFi433PjY1ZApTRbVapxfc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688226574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "foxnews.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.foxnews.com/tech/massive-free-vpn-data-breach-exposes-360-million-records", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?auto=webp&amp;v=enabled&amp;s=90f64d921391fd22675a2ad03e2ea56b97b32f9d", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d259061d15e7adf2df73a0b519439d70ffe6f6a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9c91323aeaad3277948c13cdb331b176214a326", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56c98bf3a71ade7633e193ca8722c135d6674f3e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cea0d43fe8e4be8862cb76163deb7ebffb5708b3", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=575b4226651f539f78ea8b09ddc200cd9b61d928", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=00e19c6ff4eba51968e14dc4412c8eed2e9c433d", "width": 1080, "height": 607}], "variants": {}, "id": "8HzFKKG1Og5LTkg7SwDuXi2wkqMW2i00oPa89_yAlGY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ead29b9a-58f1-11ed-9b9a-86d24560bd17", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_740hil", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "14nycky", "is_robot_indexable": true, "report_reasons": null, "author": "More_Breakfast2084", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/vpnnetwork/comments/14nycky/enormous_data_breach_reveals_360m_records_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.foxnews.com/tech/massive-free-vpn-data-breach-exposes-360-million-records", "subreddit_subscribers": 15092, "created_utc": 1688226574.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1688226629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "foxnews.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.foxnews.com/tech/massive-free-vpn-data-breach-exposes-360-million-records", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?auto=webp&amp;v=enabled&amp;s=90f64d921391fd22675a2ad03e2ea56b97b32f9d", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d259061d15e7adf2df73a0b519439d70ffe6f6a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9c91323aeaad3277948c13cdb331b176214a326", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56c98bf3a71ade7633e193ca8722c135d6674f3e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cea0d43fe8e4be8862cb76163deb7ebffb5708b3", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=575b4226651f539f78ea8b09ddc200cd9b61d928", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/KvVXmEIGdM2AcmK4zOK4PjAV3uuWoZRmb2I4pxDHk5c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=00e19c6ff4eba51968e14dc4412c8eed2e9c433d", "width": 1080, "height": 607}], "variants": {}, "id": "8HzFKKG1Og5LTkg7SwDuXi2wkqMW2i00oPa89_yAlGY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14nyddd", "is_robot_indexable": true, "report_reasons": null, "author": "More_Breakfast2084", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_14nycky", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nyddd/enormous_data_breach_reveals_360m_records_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.foxnews.com/tech/massive-free-vpn-data-breach-exposes-360-million-records", "subreddit_subscribers": 690577, "created_utc": 1688226629.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With all of the concern over Reddit tightening their API, the obvious solution is for software to use the human-facing html interface (https://reddit.com/r/*) in lieu of the formal API (https://reddit.com/api/v1/*).\n\nThis has already been discussed on a handful of subreddits that I have seen, but I haven't seen anyone say that they are writing a client library like that, or that one already exists.  I've poked around some and haven't found one in any of the obvious places, yet (github, pypi, npm, etc).\n\nAre any of you aware of such a project, before I start one of my own?  I need another project like I need a hole in my head, but I want a future-proof Reddit client library, too.", "author_fullname": "t2_cpegz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone working on a human-interface Reddit crawler?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nih44", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688176129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With all of the concern over Reddit tightening their API, the obvious solution is for software to use the human-facing html interface (&lt;a href=\"https://reddit.com/r/*\"&gt;https://reddit.com/r/*&lt;/a&gt;) in lieu of the formal API (&lt;a href=\"https://reddit.com/api/v1/*\"&gt;https://reddit.com/api/v1/*&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;This has already been discussed on a handful of subreddits that I have seen, but I haven&amp;#39;t seen anyone say that they are writing a client library like that, or that one already exists.  I&amp;#39;ve poked around some and haven&amp;#39;t found one in any of the obvious places, yet (github, pypi, npm, etc).&lt;/p&gt;\n\n&lt;p&gt;Are any of you aware of such a project, before I start one of my own?  I need another project like I need a hole in my head, but I want a future-proof Reddit client library, too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nih44", "is_robot_indexable": true, "report_reasons": null, "author": "ttkciar", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nih44/is_anyone_working_on_a_humaninterface_reddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nih44/is_anyone_working_on_a_humaninterface_reddit/", "subreddit_subscribers": 690577, "created_utc": 1688176129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Gfycat](https://gfycat.com/) is apparently being permanently discontinued, according to its front page:\n\n&gt;The Gfycat service is being discontinued. Please save or delete your Gfycat content by visiting [https://www.gfycat.com](https://www.gfycat.com) and logging in to your account. After September 1, 2023, all Gfycat content and data will be deleted from gfycat.com\n\nI haven't seen much news about this, granted most of the current surface content is findable elsewhere but it's still a shame. Its days were clearly numbered. The owners already announced the deletion of a lot of content in 2019, which [Archive Team acted on](https://twitter.com/textfiles/status/1192518085997137920), with Gfycat [threatening to sue](https://www.reddit.com/r/DataHoarder/comments/13mjaw5/comment/jkw0h01/?utm_source=share&amp;utm_medium=web2x&amp;context=3) before they decided to [work with the team](https://wiki.archiveteam.org/index.php/Gfycat). Last month the [HTTPS certificate expired](https://www.reddit.com/r/DataHoarder/comments/13mjaw5/rip_gfycat_site_does_not_look_to_be_maintained/) briefly, but its provider [renewed](https://www.vice.com/en/article/dy37d7/gfycat-down-tls-security-certificate-expired) it after a few days, guess that was just foreshadowing its real death.\n\nAn r/gfycat post from yesterday: [https://www.reddit.com/r/gfycat/comments/14nf5r2/gfycat\\_is\\_shutting\\_down\\_on\\_the\\_1st\\_of\\_september/](https://www.reddit.com/r/gfycat/comments/14nf5r2/gfycat_is_shutting_down_on_the_1st_of_september/)", "author_fullname": "t2_yz4rz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gfycat is shutting down in September", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14o5u5d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688245843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://gfycat.com/\"&gt;Gfycat&lt;/a&gt; is apparently being permanently discontinued, according to its front page:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The Gfycat service is being discontinued. Please save or delete your Gfycat content by visiting &lt;a href=\"https://www.gfycat.com\"&gt;https://www.gfycat.com&lt;/a&gt; and logging in to your account. After September 1, 2023, all Gfycat content and data will be deleted from gfycat.com&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I haven&amp;#39;t seen much news about this, granted most of the current surface content is findable elsewhere but it&amp;#39;s still a shame. Its days were clearly numbered. The owners already announced the deletion of a lot of content in 2019, which &lt;a href=\"https://twitter.com/textfiles/status/1192518085997137920\"&gt;Archive Team acted on&lt;/a&gt;, with Gfycat &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13mjaw5/comment/jkw0h01/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;threatening to sue&lt;/a&gt; before they decided to &lt;a href=\"https://wiki.archiveteam.org/index.php/Gfycat\"&gt;work with the team&lt;/a&gt;. Last month the &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13mjaw5/rip_gfycat_site_does_not_look_to_be_maintained/\"&gt;HTTPS certificate expired&lt;/a&gt; briefly, but its provider &lt;a href=\"https://www.vice.com/en/article/dy37d7/gfycat-down-tls-security-certificate-expired\"&gt;renewed&lt;/a&gt; it after a few days, guess that was just foreshadowing its real death.&lt;/p&gt;\n\n&lt;p&gt;An &lt;a href=\"/r/gfycat\"&gt;r/gfycat&lt;/a&gt; post from yesterday: &lt;a href=\"https://www.reddit.com/r/gfycat/comments/14nf5r2/gfycat_is_shutting_down_on_the_1st_of_september/\"&gt;https://www.reddit.com/r/gfycat/comments/14nf5r2/gfycat_is_shutting_down_on_the_1st_of_september/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?auto=webp&amp;v=enabled&amp;s=197ee160d951682c6c0923bd3452e34a367cba2f", "width": 1112, "height": 1112}, "resolutions": [{"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b53a728ee6a9aa5fc5e63bff233e9d4bba7d5c57", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34626711083be36d145e367ae860a804462d995a", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46940425f27bf2e5573bd249d4df1818ae11e1a8", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e01a0d7743d0100648fed90e77ab6e3a6837d94b", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b054cf3c5f782f2b1c6d45eb6fcecf5ea2467f3", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/MTTROQ9irUHu8Ku3ZiKwKavk-gRV_9d_sUVA6J-E2yo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d1664d345739dd6b5ff669deef461b863f3b710d", "width": 1080, "height": 1080}], "variants": {}, "id": "BqPyhyADuPOou7QJXixclbY9Kh6b4lY0i-ZqXVZOrLo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14o5u5d", "is_robot_indexable": true, "report_reasons": null, "author": "minindo", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14o5u5d/gfycat_is_shutting_down_in_september/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14o5u5d/gfycat_is_shutting_down_in_september/", "subreddit_subscribers": 690577, "created_utc": 1688245843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As in, searching for certain text in tens of thousands of images. Not trying to OCR individual images.", "author_fullname": "t2_13mwqlw5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I search for text in a large number of images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nomwg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688196189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As in, searching for certain text in tens of thousands of images. Not trying to OCR individual images.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nomwg", "is_robot_indexable": true, "report_reasons": null, "author": "catinterpreter", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nomwg/how_can_i_search_for_text_in_a_large_number_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nomwg/how_can_i_search_for_text_in_a_large_number_of/", "subreddit_subscribers": 690577, "created_utc": 1688196189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I mainly just want to rip my anime Blurays to watch on PC/Oculus and watch 3D movies (not really 4K as i dont even have a 4k tv) via said Oculus. I would like to just insert disk and watch too but I hear you need a paid software to do that. I would rather save that as a last resort, but if its too complicated then feel free to recommend a good pc player and I'll just stick with that. So I've been looking for a bluray reader for my pc to rip disks. I already know about Makemkv. I just need the right hardware\n\nProblem is I can only seem to find stuff related to 4k/UHD and I am not interested in that. Aside from that i'll see stuff like \"get this model its great for what you need!\" only to find out that i cant get JUST that. I have to buy that, an enclosure for the drive, and some cables to hook it up. And there were a few times where I would see listings for an external bluray player only for the description to say it supports only DVDs.\n\nIs there something out there that already has the necessary parts that I can hook up and rip my blurays/ 3D blurays without getting an unnecessary UHD/4K ripper?", "author_fullname": "t2_txn5u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to understand how to get to where i can rip Bluray disks for backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14o1jya", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688235197.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688234806.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I mainly just want to rip my anime Blurays to watch on PC/Oculus and watch 3D movies (not really 4K as i dont even have a 4k tv) via said Oculus. I would like to just insert disk and watch too but I hear you need a paid software to do that. I would rather save that as a last resort, but if its too complicated then feel free to recommend a good pc player and I&amp;#39;ll just stick with that. So I&amp;#39;ve been looking for a bluray reader for my pc to rip disks. I already know about Makemkv. I just need the right hardware&lt;/p&gt;\n\n&lt;p&gt;Problem is I can only seem to find stuff related to 4k/UHD and I am not interested in that. Aside from that i&amp;#39;ll see stuff like &amp;quot;get this model its great for what you need!&amp;quot; only to find out that i cant get JUST that. I have to buy that, an enclosure for the drive, and some cables to hook it up. And there were a few times where I would see listings for an external bluray player only for the description to say it supports only DVDs.&lt;/p&gt;\n\n&lt;p&gt;Is there something out there that already has the necessary parts that I can hook up and rip my blurays/ 3D blurays without getting an unnecessary UHD/4K ripper?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14o1jya", "is_robot_indexable": true, "report_reasons": null, "author": "Dullapan", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14o1jya/trying_to_understand_how_to_get_to_where_i_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14o1jya/trying_to_understand_how_to_get_to_where_i_can/", "subreddit_subscribers": 690577, "created_utc": 1688234806.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**Results are shown in separate posts below because this exceeds the 40000 Reddit Character Limit. Replies only accept 10000 character limit. Sorry about that.**\n\n**EDIT: tl;dr at the bottom of this post:** \n\nI am posting this here but you can also view video showing the results (https://youtu.be/y0-VhMfUWwI), or on my blog (https://htwingnut.com/2023/07/01/smr-hard-drive-perforamnce-rebuild-with-parity-raid/) which is basically the exact same info presented here. The YouTube video is chaptered so you can skip to sections you want.\n\nIt is essentially the same content whether in this post or video, except I go into more detail explaining what SMR is in the video and has graphical charts instead of text results. There is a lot of data here and did my best to keep it concise and organized. There's so many ways to slice it, but feel free to digest it however you see fit. I don't blame you if you don't want to hear my nasally voice ramble on for an hour.\n\nThis started as a simple curiosity experiment but ballooned into a much larger test set and took much longer than expected. Through multiple iterations, troubleshooting, learning new things, tests run as time permitted, took me well over a year to complete.\n\nBasically testing that nobody asked for and probably don't care to know, but I did it anyhow, LOL.\n\nWhile most people here probably know what SMR is, I'll just give my two cent summary.\n\nShingled Magnetic Recording technology, abbreviated as SMR, allows for disk drives to store more capacity per platter than a traditional hard drive. The technology is intended to reduce costs, because more data per platter means fewer platters and read/write heads. But because of how they store data the tradeoff is that write performance may degrade over time after enough data has been written to, deleted and then overwritten on the disk.\n\nSMR drives are really intended for archival data, or data that is not frequently deleted or changed. However, since Western Digital decided to change their bottom tier WD RED NAS line of disk from CMR to SMR a few years back without full disclosure, many users were not happy with this change, as it had adverse effects greatly increasing rebuild times primarily of ZFS arrays. This got me wondering how much of an effect it really has in a parity RAID situation in other configurations.\n\n**DRIVES and TEST CONFIGS:**\n\nThree different SMR disks were tested to see how they would fare in popular RAID configurations. \n\nThe SMR test disks were all 2TB 3.5\" SATA drives:\n\n* Seagate Barracuda Compute ST2000DM008\n* WD Red WD20EFAX\n* WD Blue WD20EZAZ\n\nSeveral control CMR test disks were also utilized. The control set of disks were Seagate ST2000DM001 2TB 7200 RPM hard drives. The following 2TB CMR disks were also tested for comparison:\n\n* Seagate Barracuda ST2000DM001\n* WD Red Plus WD20EFZX\n* Seagate Skyhawk ST2000VX008\n\nWhy were 2TB disks used? Bottom line, time and money. Some disks were already on hand, others were purchased solely for this test. Ultimately a Seagate Exos drive would have been preferable instead of Skyhawk, but at time of purchase, the Skyhawk was appreciably less expensive ($30 vs $80). 2TB also is the smallest capacity in 3.5\" form factor that comes in SMR as well as CMR. I also didn't want to have to fill up more data than needed because writing multiple TB's of data through dozens of tests is time consuming enough. Anything  more than 2TB would have extended the test time considerably.\n\nThe following configurations were tested for single disk REBUILD times:\n\n* OMV mdadm 4 disk RAID 5\n* OMV ZFS 4 disk RAID Z1\n* OMV SnapRAID Data &amp; Parity disk\n* Synology 4 disk SHR-1\n* QNAP TR-004 4 disk \"hardware\" RAID 5\n* UnRAID Data &amp; Parity Disk\n* Linux EXT4 Single Disk with and without TRIM\n* Windows NTFS Single Disk with and without TRIM\n* Controller: LSI 9211-8i vs Marvell 9215 vs Intel H77 MDADM RAID 5\n* Controller: LSI 9211-8i vs Marvell 9215 vs Intel H77 ZFS RAID Z1\n\nThe configurations that were tested both WRITE and READ:\n\n* NTFS Windows 10 Single Disk - tested over local SATA\n* EXT4 OMV Single Disk - tested over 1GbE SMB\n* XFS UnRAID single parity - tested over 1GbE SMB\n* MDADM 4x 2TB RAID 5 OMV, formatted EXT4, tested over 1GbE SMB\n* ZFS 4x 2TB RAID Z1 OMV, formatted ZFS (of course), tested over 1GbE SMB\n* Synology BTRFS/MDADM 4x 2TB RAID 5 with DS920+, formatted BTRFS, tested over 1GbE SMB\n* QNAP TR-004 DAS \"Hardware\" 4x 2TB RAID 5, formatted NTFS, tested over USB\n\n**HARDWARE SETUP**\n\nI made use of older hardware I had on hand. The PC config that was used for software NAS setup (i.e. OMV, UnRAID):\n\n* CPU: Intel Core i5-3570 3.4GHz 4 core / 4 thread\n* OS SSD: Sandisk Extreme 240GB\n* Motherboard: Asus P8H77-I\n* RAM: 2x8GB DDR3 1600\n* PSU for PC: Seasonic S12II 430W 80plus Bronze\n* PSU for HDD's: FSP Group FSP270-60LE 270W\n* SATA Controller: LSI 9211-8i\n* LAN: Onboard Realtek RTL8111F Gigabit\n* Disk Rack: Sans Digital HDD Rack 5 (https://www.sansdigital.com/hddrack5.html)\n\nNote: the onboard SATA controller (Intel H77 Express) and also PCIe SATA controller (Marvell 9215) were utilized to compare performance between controllers in a few scenarios\n\nThe PC that ran the test programs to perform send and receive of file over ethernet and USB was configured as:\n\n* OS: Windows 10 Pro\n* Test Files SSD: Muskin Reactor 1TB 2.5\" SATA (READ FROM SSD WRITE TO TEST ARRAY)\n* Test Files SSD: Crucial MX500 1TB 2.5\" SATA (WRITE TO SSD FROM TEST ARRAY)\n* CPU: Intel Xeon E5-2630 v3 2.4GHz 8 core / 16 thread\n* Motherboard: ASRock X99 Extreme4/3.1\n* RAM: 2x16GB DDR4 1866 ECC\n* SATA controller: onboard Intel X99\n* LAN: onboard Intel 1218V 1GbE\n* USB: onboard USB 3.1 5Gbps\n\n**TEST METHODOLOGY**\n\nDisks were not formatted or wiped between tests. All data from previous tests were left on the disk. Prior to each test, the disk was \"cleared\" or \"formatted\" using the minimal recommend process. For example in OMV, just running a  \"quick wipe\" before adding the disk to a RAID 5 array.\n\nAll default settings for setting up an array were used. No TRIM or DISCARD settings were manually implemented except for the few specific TRIM tests.\n\nFor RAID array tests, 4x ST2000DM001 hard drives were used as control level to start, build array, add filler data, and then complete WRITE/READ test. Then one ST2000DM001 disk was then replaced with a test disk, rebuild the array, and then initiate the next read/write test for that disk. Subsequent tests would just swap out the test disk with another test disk, rebuild, and perform write/read testing until all disks were tested for that particular array. So 3x ST2000DM001 disks would always remain in the array with the fourth disk being a test disk.\n\nFor each initial array setup, the array was filled with random size and content data leaving approximately 1200 to 1500 GB free on a RAID array (out of ~ 6GB in a four disk single parity setup), approximately 60-70% filled capacity. This is considered the filler data and is not touched after the initial write to the disk or array.\n\nFor single disk tests, the filler data was filled to about 60-70% capacity, leaving about 800GB free for testing.\n\nSample random file distribution:\n\n        Total Files: 9979\n      File Size Min: 3KB\n      File Size Max: 2098998KB (~ 2.1GB)\n      File Size Avg: 154379 (~ 154MB)\n    \n            % Files &lt; 1MB:  1.74%\n      % Files 1MB to 10MB:  9.59%\n    % Files 10MB to 100MB: 45.59%\n     % Files 100MB to 1GB: 41.48%\n            % Files &gt; 1GB:  1.60%\n\n\nAll test files were written from an SSD, and an actual file copy was executed from the Windows PC SSD over 1GbE or USB (depending on the test) using a Powershell script.\n\nFor RAID tests, gigabit ethernet was used because it is a typical use case to copy from a PC over SMB to a NAS. Any performance issues due to SMR should result in performance well below 1GbE speeds of about 112 MB/sec (realistic speeds).\n\nA Powershell script was written and utilized to automate the tasks of file fill and measuring the per file transfer performance of the test data.\n\nRandom data was generated using RNGCryptoServiceProvider:\n\n    $rnd10 = (Get-Random 10) + 1\n    $rndmax = [int64]((Get-Random (2GB - 1))/$rnd10)\n    $bytes = (Get-Random $rndmax)\n    [System.Security.Cryptography.RNGCryptoServiceProvider] $rng = New-Object System.Security.Cryptography.RNGCryptoServiceProvider\n    $rndbytes = New-Object byte[] $bytes\n    $rng.GetBytes($rndbytes)\n\n\nFile transfer performance was measured using the `StopWatch` command:\n\n    $StopWatch=[system.diagnostics.stopwatch]::startnew()\n    Copy-Item \"$spath\\$f\\$file\" \"$dpath\\TEST\\$f\"\n    $SecondsElapsed=$StopWatch.Elapsed.TotalSeconds\n    $StopWatch.Stop()\n\nThe average transfer speeds were computed using total size of files copied divided by summation of all time elapsed from above script.\n\nSetting up a proper test to mitigate skewed results due to regular fragmentation was of high consideration. The following two test scenarios were devised which should highlight any effects of SMR degradation while minimizing any effect of fragmentation:\n\n**Test 1:** Mixed Size Write / Read\n\n1. Write random size/content data to remaining full capacity of disk/array from Windows test PC.\n2. Use \"delete\" command to delete all random data just written.\n3. Write a test set of 620GB: 20x 10GB, 200x 1GB, 2000x 100MB, 20000x 1MB files from SSD and measure results per file.\n4. Read back half of files and measure results.\n5. Use \"delete\" command to delete test set of 620GB from test array.\n6. Immediately start next Scenario.\n\n**Test 2:** Alternating 10MB / 1GB files\n\n1. Write 10MB size files to remaining full capacity of disk/array from Windows test PC.\n2. Use \"delete\" command to delete EVERY OTHER 10MB file, so half the 10MB files just written.\n3. Write 800x 1GB sized files to disk/array from SSD and measure results per file.\n4. Read back half of files and measure results per file.\n5. Use \"delete\" command to delete all 10MB and 1GB test files.\n6. Remove disk and start next disk test.\n7. Shut down, remove test disk and replace with next test disk.\n\nBoth scenarios take the disk to its fullest capacity and then immediately delete and then write data back to the disk with no idle time.\n\nThe second scenario should really bring forward any issues pertaining to SMR. Alternating 10MB file deletion should remove big chunks of data from each SMR zone that would require a rewrite of data to fill each SMR zone back.\n\n**BASELINE PERFORMANCE**\n\nEvery disk was subjected to several baseline performance tests to ensure they were performing as intended:\n\n* CrystalDiskMark 5x 1GB test set\n* ATTO 512b to 64MB I/O with 1GB file size\n* Hard Disk Sentinel Full Disk WRITE and READ\n\nThe CrystalDiskMark and ATTO benchmarks are omitted from this post, only because compiling the results will be quite tedious and honestly not very value added. However, I may provide them as raw image data in a future update to this blog.\n\nThat being said, here are baseline results from the Hard Disk Sentinel Full Disk WRITE and READ tests for reference, which are probably more relevant anyhow: https://imgur.com/a/SdW5B9h\n\nThe two SSD's used to send and receive data were tested with HD Sentinel with results shown below:\n\n* Test SSD read from to the test array Mushkin Reactor 1TB 2.5\" SATA (475MB/sec read speed)\n* Test SSD write to from the test array Crucial MX500 1TB 2.5\" SATA (350MB/sec write speed)\n\n**FAILURES**\n\nThroughout testing, three of five WD Red WD20EFAX (SMR) had failures, all at different times in the testing. One became completely non responsive, it would power up but was never detected. Another started having increased failing sectors. The third one would fail out of a ZFS RAID Z1 rebuild despite having clean SMART and no other apparent issues. It was exchanged in hopes that the replacement would fare better. Each disk was successfully RMA'd promptly with a replacement within two weeks.\n\nAn SG Barracuda Compute ST2000DM008 (SMR) had to be RMA'd for being non-responsive and would hang the system periodically.\n\nEach problematic disk went through a thorough troubleshooting process, changing PC's, changing power supplies, changing cables, validating PSU voltages, etc before submitting for RMA. In each case, the replacement disk solved the issue completely (with the exception of the one instance with ZFS rebuild errors). Is this a result of SMR? Bad luck? Maybe some unknown other issue? I don't know, but each of the replacement disks soldiered on through the rest of the testing without a hitch.\n\nThe SG Barracuda Compute ST2000DM008 (SMR) was not recognized as a device supporting TRIM despite in Linux desite it running TRIM just fine with NTFS in Windows. This is why there was an N/A TRIM in the TRIM results for the Seagate Barracuda Compute ST2000DM008 drive.\n\nThere were four WD RED Disks utilized. Three were WD20EFAX-68B2RN1 and one was WD20EFAX-68FB5N0. The 68FB5N0 would not TRIM in Linux despite it running TRIM just fine in NTFS Windows. 68B2RN1 drives would accept TRIM commands fine in Linux and Windows. This is why the TRIM test only included 3x WD RED instead of 4x WD RED.\n\n**Results are shown in separate posts below because this exceeds the 40000 Reddit Character Limit. Replies only accept 10000 character limit. Sorry about that.**\n\n**Permalinks for Test Results:**\n\n**REBUILD TIMES:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xlb2/\n\n**TEST 1 RESULTS: WRITE 620GB MIXED FILES SIZES AFTER DISK FILL &amp; DELETE:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xwwd/\n\n**TEST 1 RESULTS: READ 620GB MIXED FILES SIZES AFTER DISK FILL &amp; DELETE:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9y96s/\n\n**TEST 2 RESULTS: WRITE &amp; READ 1GB FILES AFTER 10MB DISK FILL AND DELETE EVERY OTHER 10MB FILE:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yeqo/\n\n**TEST 3 RESULTS: WRITE PERFORMANCE AFTER TRIM:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9ymhv/\n\n**TEST 3 RESULTS: READ PERFORMANCE AFTER TRIM:** https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yq1u/\n\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n\n**tl;dr** - This is hard to summarize and make generalities, but I'll do my best.\n\nCheck out these percentage difference charts here which may help: https://imgur.com/a/FdDITyR\n\nOn the linked charts, 1GB alternating are results from \"Test 2\" (write 10MB files, delete every other 10MB file, write 1GB files), the other file sizes are from \"Test 1\" (fill with random data, delete, then write 1MB, 100MB, 1GB, 10GB files / 620GB total).\n\nTest 1, SMR disks did perform worse than CMR across the board with file writes. The Seagate Barracuda Compute ST2000DM008 performing the worse, by about 50%. The rest pretty much had less than a 30% performance penalty. TRIM had minimal impact on improving performance for Test 1. My thoughts on this are because it was mass deletion of files and the disks possibly have some level of intelligence to realize freed ups SMR zones it just overwrites them.\n\nTest 2, all SMR disks had a write performance penalty of 30-90%, but it was mitigated with a TRIM command and 4 hrs idle time. It seems TRIM does its job given enough time. Whether 4 hours is needed, that amount of time was not tested, just used 4 hrs to give it ample time to do its job.\n\nTest 3, TRIM results - as noted above, TRIM had minimal impact on Test 1, then again most test the performance wasn't degraded significantly compared with CMR disks. TRIM after file deletion during Test 2 had a significant performance improvement, nearing CMR level of performance.\n\nTest 4 REBUILD - There was no significant performance penalty during rebuild times except for the Seagate Barracuda Compute with ZFS RAID Z1 where it took over 500 minutes vs about 200 minutes for the other disks. It also faltered a bit with the SnapRAID EXT4 test Where it took 160 minutes vs about 90 minutes with the other disks. WD Red also faltered a bit with SnapRAID EXT4 rebuild running 123 minutes vs about 90 minutes.\n\nCONTROLLER REBUILD Difference - LSI 9211-8i PCIe vs Intel H77 SATA onboard vs Marvell 9215 SATA PCIe : LSI &amp; Intel were within 10% of each other, Marvell was about 25% slower than the LSI.\n\nREBUILD Swap all CMR with all SMR: Swapping 4x ST2000DM001 CMR with 4x WD20EFAX SMR single parity RAID (RAID 5) one at a time with QNAP TR-004 Hardware RAID, Synology DS920+ SHR-1, Linux MDADM RAID 5. There was no significant difference in performance.\n\nSurprisingly the WD Blue seemed to fare best out of the three SMR disks.", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extensive Testing - SMR Results with RAID Rebuild and File Transfers Compared with CMR", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nz7ow", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1688254558.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688228768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Results are shown in separate posts below because this exceeds the 40000 Reddit Character Limit. Replies only accept 10000 character limit. Sorry about that.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT: tl;dr at the bottom of this post:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;I am posting this here but you can also view video showing the results (&lt;a href=\"https://youtu.be/y0-VhMfUWwI\"&gt;https://youtu.be/y0-VhMfUWwI&lt;/a&gt;), or on my blog (&lt;a href=\"https://htwingnut.com/2023/07/01/smr-hard-drive-perforamnce-rebuild-with-parity-raid/\"&gt;https://htwingnut.com/2023/07/01/smr-hard-drive-perforamnce-rebuild-with-parity-raid/&lt;/a&gt;) which is basically the exact same info presented here. The YouTube video is chaptered so you can skip to sections you want.&lt;/p&gt;\n\n&lt;p&gt;It is essentially the same content whether in this post or video, except I go into more detail explaining what SMR is in the video and has graphical charts instead of text results. There is a lot of data here and did my best to keep it concise and organized. There&amp;#39;s so many ways to slice it, but feel free to digest it however you see fit. I don&amp;#39;t blame you if you don&amp;#39;t want to hear my nasally voice ramble on for an hour.&lt;/p&gt;\n\n&lt;p&gt;This started as a simple curiosity experiment but ballooned into a much larger test set and took much longer than expected. Through multiple iterations, troubleshooting, learning new things, tests run as time permitted, took me well over a year to complete.&lt;/p&gt;\n\n&lt;p&gt;Basically testing that nobody asked for and probably don&amp;#39;t care to know, but I did it anyhow, LOL.&lt;/p&gt;\n\n&lt;p&gt;While most people here probably know what SMR is, I&amp;#39;ll just give my two cent summary.&lt;/p&gt;\n\n&lt;p&gt;Shingled Magnetic Recording technology, abbreviated as SMR, allows for disk drives to store more capacity per platter than a traditional hard drive. The technology is intended to reduce costs, because more data per platter means fewer platters and read/write heads. But because of how they store data the tradeoff is that write performance may degrade over time after enough data has been written to, deleted and then overwritten on the disk.&lt;/p&gt;\n\n&lt;p&gt;SMR drives are really intended for archival data, or data that is not frequently deleted or changed. However, since Western Digital decided to change their bottom tier WD RED NAS line of disk from CMR to SMR a few years back without full disclosure, many users were not happy with this change, as it had adverse effects greatly increasing rebuild times primarily of ZFS arrays. This got me wondering how much of an effect it really has in a parity RAID situation in other configurations.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;DRIVES and TEST CONFIGS:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Three different SMR disks were tested to see how they would fare in popular RAID configurations. &lt;/p&gt;\n\n&lt;p&gt;The SMR test disks were all 2TB 3.5&amp;quot; SATA drives:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Seagate Barracuda Compute ST2000DM008&lt;/li&gt;\n&lt;li&gt;WD Red WD20EFAX&lt;/li&gt;\n&lt;li&gt;WD Blue WD20EZAZ&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Several control CMR test disks were also utilized. The control set of disks were Seagate ST2000DM001 2TB 7200 RPM hard drives. The following 2TB CMR disks were also tested for comparison:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Seagate Barracuda ST2000DM001&lt;/li&gt;\n&lt;li&gt;WD Red Plus WD20EFZX&lt;/li&gt;\n&lt;li&gt;Seagate Skyhawk ST2000VX008&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Why were 2TB disks used? Bottom line, time and money. Some disks were already on hand, others were purchased solely for this test. Ultimately a Seagate Exos drive would have been preferable instead of Skyhawk, but at time of purchase, the Skyhawk was appreciably less expensive ($30 vs $80). 2TB also is the smallest capacity in 3.5&amp;quot; form factor that comes in SMR as well as CMR. I also didn&amp;#39;t want to have to fill up more data than needed because writing multiple TB&amp;#39;s of data through dozens of tests is time consuming enough. Anything  more than 2TB would have extended the test time considerably.&lt;/p&gt;\n\n&lt;p&gt;The following configurations were tested for single disk REBUILD times:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OMV mdadm 4 disk RAID 5&lt;/li&gt;\n&lt;li&gt;OMV ZFS 4 disk RAID Z1&lt;/li&gt;\n&lt;li&gt;OMV SnapRAID Data &amp;amp; Parity disk&lt;/li&gt;\n&lt;li&gt;Synology 4 disk SHR-1&lt;/li&gt;\n&lt;li&gt;QNAP TR-004 4 disk &amp;quot;hardware&amp;quot; RAID 5&lt;/li&gt;\n&lt;li&gt;UnRAID Data &amp;amp; Parity Disk&lt;/li&gt;\n&lt;li&gt;Linux EXT4 Single Disk with and without TRIM&lt;/li&gt;\n&lt;li&gt;Windows NTFS Single Disk with and without TRIM&lt;/li&gt;\n&lt;li&gt;Controller: LSI 9211-8i vs Marvell 9215 vs Intel H77 MDADM RAID 5&lt;/li&gt;\n&lt;li&gt;Controller: LSI 9211-8i vs Marvell 9215 vs Intel H77 ZFS RAID Z1&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The configurations that were tested both WRITE and READ:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;NTFS Windows 10 Single Disk - tested over local SATA&lt;/li&gt;\n&lt;li&gt;EXT4 OMV Single Disk - tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;XFS UnRAID single parity - tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;MDADM 4x 2TB RAID 5 OMV, formatted EXT4, tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;ZFS 4x 2TB RAID Z1 OMV, formatted ZFS (of course), tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;Synology BTRFS/MDADM 4x 2TB RAID 5 with DS920+, formatted BTRFS, tested over 1GbE SMB&lt;/li&gt;\n&lt;li&gt;QNAP TR-004 DAS &amp;quot;Hardware&amp;quot; 4x 2TB RAID 5, formatted NTFS, tested over USB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;HARDWARE SETUP&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I made use of older hardware I had on hand. The PC config that was used for software NAS setup (i.e. OMV, UnRAID):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CPU: Intel Core i5-3570 3.4GHz 4 core / 4 thread&lt;/li&gt;\n&lt;li&gt;OS SSD: Sandisk Extreme 240GB&lt;/li&gt;\n&lt;li&gt;Motherboard: Asus P8H77-I&lt;/li&gt;\n&lt;li&gt;RAM: 2x8GB DDR3 1600&lt;/li&gt;\n&lt;li&gt;PSU for PC: Seasonic S12II 430W 80plus Bronze&lt;/li&gt;\n&lt;li&gt;PSU for HDD&amp;#39;s: FSP Group FSP270-60LE 270W&lt;/li&gt;\n&lt;li&gt;SATA Controller: LSI 9211-8i&lt;/li&gt;\n&lt;li&gt;LAN: Onboard Realtek RTL8111F Gigabit&lt;/li&gt;\n&lt;li&gt;Disk Rack: Sans Digital HDD Rack 5 (&lt;a href=\"https://www.sansdigital.com/hddrack5.html\"&gt;https://www.sansdigital.com/hddrack5.html&lt;/a&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Note: the onboard SATA controller (Intel H77 Express) and also PCIe SATA controller (Marvell 9215) were utilized to compare performance between controllers in a few scenarios&lt;/p&gt;\n\n&lt;p&gt;The PC that ran the test programs to perform send and receive of file over ethernet and USB was configured as:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OS: Windows 10 Pro&lt;/li&gt;\n&lt;li&gt;Test Files SSD: Muskin Reactor 1TB 2.5&amp;quot; SATA (READ FROM SSD WRITE TO TEST ARRAY)&lt;/li&gt;\n&lt;li&gt;Test Files SSD: Crucial MX500 1TB 2.5&amp;quot; SATA (WRITE TO SSD FROM TEST ARRAY)&lt;/li&gt;\n&lt;li&gt;CPU: Intel Xeon E5-2630 v3 2.4GHz 8 core / 16 thread&lt;/li&gt;\n&lt;li&gt;Motherboard: ASRock X99 Extreme4/3.1&lt;/li&gt;\n&lt;li&gt;RAM: 2x16GB DDR4 1866 ECC&lt;/li&gt;\n&lt;li&gt;SATA controller: onboard Intel X99&lt;/li&gt;\n&lt;li&gt;LAN: onboard Intel 1218V 1GbE&lt;/li&gt;\n&lt;li&gt;USB: onboard USB 3.1 5Gbps&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST METHODOLOGY&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Disks were not formatted or wiped between tests. All data from previous tests were left on the disk. Prior to each test, the disk was &amp;quot;cleared&amp;quot; or &amp;quot;formatted&amp;quot; using the minimal recommend process. For example in OMV, just running a  &amp;quot;quick wipe&amp;quot; before adding the disk to a RAID 5 array.&lt;/p&gt;\n\n&lt;p&gt;All default settings for setting up an array were used. No TRIM or DISCARD settings were manually implemented except for the few specific TRIM tests.&lt;/p&gt;\n\n&lt;p&gt;For RAID array tests, 4x ST2000DM001 hard drives were used as control level to start, build array, add filler data, and then complete WRITE/READ test. Then one ST2000DM001 disk was then replaced with a test disk, rebuild the array, and then initiate the next read/write test for that disk. Subsequent tests would just swap out the test disk with another test disk, rebuild, and perform write/read testing until all disks were tested for that particular array. So 3x ST2000DM001 disks would always remain in the array with the fourth disk being a test disk.&lt;/p&gt;\n\n&lt;p&gt;For each initial array setup, the array was filled with random size and content data leaving approximately 1200 to 1500 GB free on a RAID array (out of ~ 6GB in a four disk single parity setup), approximately 60-70% filled capacity. This is considered the filler data and is not touched after the initial write to the disk or array.&lt;/p&gt;\n\n&lt;p&gt;For single disk tests, the filler data was filled to about 60-70% capacity, leaving about 800GB free for testing.&lt;/p&gt;\n\n&lt;p&gt;Sample random file distribution:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    Total Files: 9979\n  File Size Min: 3KB\n  File Size Max: 2098998KB (~ 2.1GB)\n  File Size Avg: 154379 (~ 154MB)\n\n        % Files &amp;lt; 1MB:  1.74%\n  % Files 1MB to 10MB:  9.59%\n% Files 10MB to 100MB: 45.59%\n % Files 100MB to 1GB: 41.48%\n        % Files &amp;gt; 1GB:  1.60%\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;All test files were written from an SSD, and an actual file copy was executed from the Windows PC SSD over 1GbE or USB (depending on the test) using a Powershell script.&lt;/p&gt;\n\n&lt;p&gt;For RAID tests, gigabit ethernet was used because it is a typical use case to copy from a PC over SMB to a NAS. Any performance issues due to SMR should result in performance well below 1GbE speeds of about 112 MB/sec (realistic speeds).&lt;/p&gt;\n\n&lt;p&gt;A Powershell script was written and utilized to automate the tasks of file fill and measuring the per file transfer performance of the test data.&lt;/p&gt;\n\n&lt;p&gt;Random data was generated using RNGCryptoServiceProvider:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$rnd10 = (Get-Random 10) + 1\n$rndmax = [int64]((Get-Random (2GB - 1))/$rnd10)\n$bytes = (Get-Random $rndmax)\n[System.Security.Cryptography.RNGCryptoServiceProvider] $rng = New-Object System.Security.Cryptography.RNGCryptoServiceProvider\n$rndbytes = New-Object byte[] $bytes\n$rng.GetBytes($rndbytes)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;File transfer performance was measured using the &lt;code&gt;StopWatch&lt;/code&gt; command:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$StopWatch=[system.diagnostics.stopwatch]::startnew()\nCopy-Item &amp;quot;$spath\\$f\\$file&amp;quot; &amp;quot;$dpath\\TEST\\$f&amp;quot;\n$SecondsElapsed=$StopWatch.Elapsed.TotalSeconds\n$StopWatch.Stop()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The average transfer speeds were computed using total size of files copied divided by summation of all time elapsed from above script.&lt;/p&gt;\n\n&lt;p&gt;Setting up a proper test to mitigate skewed results due to regular fragmentation was of high consideration. The following two test scenarios were devised which should highlight any effects of SMR degradation while minimizing any effect of fragmentation:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Test 1:&lt;/strong&gt; Mixed Size Write / Read&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Write random size/content data to remaining full capacity of disk/array from Windows test PC.&lt;/li&gt;\n&lt;li&gt;Use &amp;quot;delete&amp;quot; command to delete all random data just written.&lt;/li&gt;\n&lt;li&gt;Write a test set of 620GB: 20x 10GB, 200x 1GB, 2000x 100MB, 20000x 1MB files from SSD and measure results per file.&lt;/li&gt;\n&lt;li&gt;Read back half of files and measure results.&lt;/li&gt;\n&lt;li&gt;Use &amp;quot;delete&amp;quot; command to delete test set of 620GB from test array.&lt;/li&gt;\n&lt;li&gt;Immediately start next Scenario.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Test 2:&lt;/strong&gt; Alternating 10MB / 1GB files&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Write 10MB size files to remaining full capacity of disk/array from Windows test PC.&lt;/li&gt;\n&lt;li&gt;Use &amp;quot;delete&amp;quot; command to delete EVERY OTHER 10MB file, so half the 10MB files just written.&lt;/li&gt;\n&lt;li&gt;Write 800x 1GB sized files to disk/array from SSD and measure results per file.&lt;/li&gt;\n&lt;li&gt;Read back half of files and measure results per file.&lt;/li&gt;\n&lt;li&gt;Use &amp;quot;delete&amp;quot; command to delete all 10MB and 1GB test files.&lt;/li&gt;\n&lt;li&gt;Remove disk and start next disk test.&lt;/li&gt;\n&lt;li&gt;Shut down, remove test disk and replace with next test disk.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Both scenarios take the disk to its fullest capacity and then immediately delete and then write data back to the disk with no idle time.&lt;/p&gt;\n\n&lt;p&gt;The second scenario should really bring forward any issues pertaining to SMR. Alternating 10MB file deletion should remove big chunks of data from each SMR zone that would require a rewrite of data to fill each SMR zone back.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;BASELINE PERFORMANCE&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Every disk was subjected to several baseline performance tests to ensure they were performing as intended:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CrystalDiskMark 5x 1GB test set&lt;/li&gt;\n&lt;li&gt;ATTO 512b to 64MB I/O with 1GB file size&lt;/li&gt;\n&lt;li&gt;Hard Disk Sentinel Full Disk WRITE and READ&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The CrystalDiskMark and ATTO benchmarks are omitted from this post, only because compiling the results will be quite tedious and honestly not very value added. However, I may provide them as raw image data in a future update to this blog.&lt;/p&gt;\n\n&lt;p&gt;That being said, here are baseline results from the Hard Disk Sentinel Full Disk WRITE and READ tests for reference, which are probably more relevant anyhow: &lt;a href=\"https://imgur.com/a/SdW5B9h\"&gt;https://imgur.com/a/SdW5B9h&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The two SSD&amp;#39;s used to send and receive data were tested with HD Sentinel with results shown below:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Test SSD read from to the test array Mushkin Reactor 1TB 2.5&amp;quot; SATA (475MB/sec read speed)&lt;/li&gt;\n&lt;li&gt;Test SSD write to from the test array Crucial MX500 1TB 2.5&amp;quot; SATA (350MB/sec write speed)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;FAILURES&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Throughout testing, three of five WD Red WD20EFAX (SMR) had failures, all at different times in the testing. One became completely non responsive, it would power up but was never detected. Another started having increased failing sectors. The third one would fail out of a ZFS RAID Z1 rebuild despite having clean SMART and no other apparent issues. It was exchanged in hopes that the replacement would fare better. Each disk was successfully RMA&amp;#39;d promptly with a replacement within two weeks.&lt;/p&gt;\n\n&lt;p&gt;An SG Barracuda Compute ST2000DM008 (SMR) had to be RMA&amp;#39;d for being non-responsive and would hang the system periodically.&lt;/p&gt;\n\n&lt;p&gt;Each problematic disk went through a thorough troubleshooting process, changing PC&amp;#39;s, changing power supplies, changing cables, validating PSU voltages, etc before submitting for RMA. In each case, the replacement disk solved the issue completely (with the exception of the one instance with ZFS rebuild errors). Is this a result of SMR? Bad luck? Maybe some unknown other issue? I don&amp;#39;t know, but each of the replacement disks soldiered on through the rest of the testing without a hitch.&lt;/p&gt;\n\n&lt;p&gt;The SG Barracuda Compute ST2000DM008 (SMR) was not recognized as a device supporting TRIM despite in Linux desite it running TRIM just fine with NTFS in Windows. This is why there was an N/A TRIM in the TRIM results for the Seagate Barracuda Compute ST2000DM008 drive.&lt;/p&gt;\n\n&lt;p&gt;There were four WD RED Disks utilized. Three were WD20EFAX-68B2RN1 and one was WD20EFAX-68FB5N0. The 68FB5N0 would not TRIM in Linux despite it running TRIM just fine in NTFS Windows. 68B2RN1 drives would accept TRIM commands fine in Linux and Windows. This is why the TRIM test only included 3x WD RED instead of 4x WD RED.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Results are shown in separate posts below because this exceeds the 40000 Reddit Character Limit. Replies only accept 10000 character limit. Sorry about that.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Permalinks for Test Results:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;REBUILD TIMES:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xlb2/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xlb2/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 1 RESULTS: WRITE 620GB MIXED FILES SIZES AFTER DISK FILL &amp;amp; DELETE:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xwwd/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9xwwd/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 1 RESULTS: READ 620GB MIXED FILES SIZES AFTER DISK FILL &amp;amp; DELETE:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9y96s/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9y96s/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 2 RESULTS: WRITE &amp;amp; READ 1GB FILES AFTER 10MB DISK FILL AND DELETE EVERY OTHER 10MB FILE:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yeqo/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yeqo/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 3 RESULTS: WRITE PERFORMANCE AFTER TRIM:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9ymhv/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9ymhv/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TEST 3 RESULTS: READ PERFORMANCE AFTER TRIM:&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yq1u/\"&gt;https://www.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/jq9yq1u/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; - This is hard to summarize and make generalities, but I&amp;#39;ll do my best.&lt;/p&gt;\n\n&lt;p&gt;Check out these percentage difference charts here which may help: &lt;a href=\"https://imgur.com/a/FdDITyR\"&gt;https://imgur.com/a/FdDITyR&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;On the linked charts, 1GB alternating are results from &amp;quot;Test 2&amp;quot; (write 10MB files, delete every other 10MB file, write 1GB files), the other file sizes are from &amp;quot;Test 1&amp;quot; (fill with random data, delete, then write 1MB, 100MB, 1GB, 10GB files / 620GB total).&lt;/p&gt;\n\n&lt;p&gt;Test 1, SMR disks did perform worse than CMR across the board with file writes. The Seagate Barracuda Compute ST2000DM008 performing the worse, by about 50%. The rest pretty much had less than a 30% performance penalty. TRIM had minimal impact on improving performance for Test 1. My thoughts on this are because it was mass deletion of files and the disks possibly have some level of intelligence to realize freed ups SMR zones it just overwrites them.&lt;/p&gt;\n\n&lt;p&gt;Test 2, all SMR disks had a write performance penalty of 30-90%, but it was mitigated with a TRIM command and 4 hrs idle time. It seems TRIM does its job given enough time. Whether 4 hours is needed, that amount of time was not tested, just used 4 hrs to give it ample time to do its job.&lt;/p&gt;\n\n&lt;p&gt;Test 3, TRIM results - as noted above, TRIM had minimal impact on Test 1, then again most test the performance wasn&amp;#39;t degraded significantly compared with CMR disks. TRIM after file deletion during Test 2 had a significant performance improvement, nearing CMR level of performance.&lt;/p&gt;\n\n&lt;p&gt;Test 4 REBUILD - There was no significant performance penalty during rebuild times except for the Seagate Barracuda Compute with ZFS RAID Z1 where it took over 500 minutes vs about 200 minutes for the other disks. It also faltered a bit with the SnapRAID EXT4 test Where it took 160 minutes vs about 90 minutes with the other disks. WD Red also faltered a bit with SnapRAID EXT4 rebuild running 123 minutes vs about 90 minutes.&lt;/p&gt;\n\n&lt;p&gt;CONTROLLER REBUILD Difference - LSI 9211-8i PCIe vs Intel H77 SATA onboard vs Marvell 9215 SATA PCIe : LSI &amp;amp; Intel were within 10% of each other, Marvell was about 25% slower than the LSI.&lt;/p&gt;\n\n&lt;p&gt;REBUILD Swap all CMR with all SMR: Swapping 4x ST2000DM001 CMR with 4x WD20EFAX SMR single parity RAID (RAID 5) one at a time with QNAP TR-004 Hardware RAID, Synology DS920+ SHR-1, Linux MDADM RAID 5. There was no significant difference in performance.&lt;/p&gt;\n\n&lt;p&gt;Surprisingly the WD Blue seemed to fare best out of the three SMR disks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?auto=webp&amp;v=enabled&amp;s=a93ffd455f3489bc77ec3d13e7c75810e8ebad7a", "width": 3963, "height": 2230}, "resolutions": [{"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63194afa5000a7878ea279f0c201eb331bec8712", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f637559669ff57373e38461489bbc2676d8ea35e", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b64c32ad58a594e9094304d9b29977b2a2be870", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=805b211d783e8bb5ef6011192400c522a70c6558", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e2a476bb967ad6fa6dd3ebf145c54ee12077651", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/OK6y8HRpENcy1eLfSMEJy2pCRDsuPX768t7RuCDW7pQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=642beb2a64abcde2c0d264317cd3994570bb0676", "width": 1080, "height": 607}], "variants": {}, "id": "Uz_t8KVSq-Mkp85blH-_19UMIuSwthpYy-MMFKHvHgM"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 250, "id": "award_c8503d66-6450-40c5-963f-35ced99bd361", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/Respect_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/Respect_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/Respect_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/Respect_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/Respect_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/Respect_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Tip of my hat to you", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Respect", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/anog86pfyh471_Respect.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=e56ad76b9e711337f46ae69af291e984bf6dd7b1", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/anog86pfyh471_Respect.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=b5adaa02054f8972a11162db7880f717ab4532e0", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/anog86pfyh471_Respect.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=74eeefc55c481d40a60d83c5fa047299890e3d8d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/anog86pfyh471_Respect.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=d20ca628a887d55f1e7ad05aaee33a405737b1c6", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/anog86pfyh471_Respect.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=b1bde27d7935331234a88caf1cd1fbb76c48372b", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 512, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/anog86pfyh471_Respect.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14nz7ow", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/14nz7ow/extensive_testing_smr_results_with_raid_rebuild/", "subreddit_subscribers": 690577, "created_utc": 1688228768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is an article I wrote about some challenges our archive project faces, which I believe may resonate with many of you. Its the first article I have written, so feedback is appreciated!", "author_fullname": "t2_few9393b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Digital Paradox: How the Internet Age is Erasing our Past", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_14ny22h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/vPsxmNBkYXmAunsVedf8A75tl78GaSZYLtgkeUZUxk4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688225804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is an article I wrote about some challenges our archive project faces, which I believe may resonate with many of you. Its the first article I have written, so feedback is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@avenstewart/the-digital-paradox-how-the-internet-age-is-erasing-our-past-5a367ecf42f0", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?auto=webp&amp;v=enabled&amp;s=305e568c784b704078d9e4d7714755382bc2ac51", "width": 1200, "height": 847}, "resolutions": [{"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff5a00630be5b2d65486c6ab5990461f93fcf87b", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ddcd9713fdbcd6edf184405f303aed0a9529b48e", "width": 216, "height": 152}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=538c88dba59703a953a9afb57171ed4043a0b050", "width": 320, "height": 225}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f29633e0477d93433d360d014bc32f05a0a56f0b", "width": 640, "height": 451}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d017b1f3e34d8d7bd2f95821d1c8b93e0d62ccd", "width": 960, "height": 677}, {"url": "https://external-preview.redd.it/p5N6CtdtL7tvmIdhVAjXhfxehFN3Pz5WkV8LdCno6wQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=951f3eb45588a5881eb69d13818cb9306e1f8e87", "width": 1080, "height": 762}], "variants": {}, "id": "mWw6L_GlywXuGnjjl0mRiUxsvMKC7FTgCCQ4lXCfims"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14ny22h", "is_robot_indexable": true, "report_reasons": null, "author": "SciencePlaceArchives", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ny22h/the_digital_paradox_how_the_internet_age_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@avenstewart/the-digital-paradox-how-the-internet-age-is-erasing-our-past-5a367ecf42f0", "subreddit_subscribers": 690577, "created_utc": 1688225804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I remember announcements earlier this year about 24TB drives ... are they out yet?", "author_fullname": "t2_mgmtm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did 24TB Drives Come Out Yet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nivcy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688177307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I remember announcements earlier this year about 24TB drives ... are they out yet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nivcy", "is_robot_indexable": true, "report_reasons": null, "author": "HarryMuscle", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nivcy/did_24tb_drives_come_out_yet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nivcy/did_24tb_drives_come_out_yet/", "subreddit_subscribers": 690577, "created_utc": 1688177307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am archiving some data to blu-ray m-discs. I ran a checksum comparison on my burned disc and I get a single mismatch one one video file on the disc. But the video file from the disc plays just fine. Does that mean the data is ok? I have tried running the checksum multiple times, same result. I tried md5 and Sha1.", "author_fullname": "t2_b9gcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Checksum mismatch, but file still plays", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ngpkd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688170848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am archiving some data to blu-ray m-discs. I ran a checksum comparison on my burned disc and I get a single mismatch one one video file on the disc. But the video file from the disc plays just fine. Does that mean the data is ok? I have tried running the checksum multiple times, same result. I tried md5 and Sha1.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ngpkd", "is_robot_indexable": true, "report_reasons": null, "author": "Frolikewoah", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ngpkd/checksum_mismatch_but_file_still_plays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14ngpkd/checksum_mismatch_but_file_still_plays/", "subreddit_subscribers": 690577, "created_utc": 1688170848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "as per title..  \n\n\nI'm vaguely contemplating buying a unit.. I want to attach it to a LSI HBA.  \n\n\nJust wanting to confirm it will be play happily with non-Dell hardware. I only have Supermicro/Intel Xeon/ LSI gear etc ('whitebox')  \n\n\ncheers", "author_fullname": "t2_as0s5xdl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone running the Dell Power Vault MD3460 as a DAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nn3ox", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688190934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as per title..  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m vaguely contemplating buying a unit.. I want to attach it to a LSI HBA.  &lt;/p&gt;\n\n&lt;p&gt;Just wanting to confirm it will be play happily with non-Dell hardware. I only have Supermicro/Intel Xeon/ LSI gear etc (&amp;#39;whitebox&amp;#39;)  &lt;/p&gt;\n\n&lt;p&gt;cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "0.145PB, ZFS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nn3ox", "is_robot_indexable": true, "report_reasons": null, "author": "kaheksajalg7", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14nn3ox/anyone_running_the_dell_power_vault_md3460_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nn3ox/anyone_running_the_dell_power_vault_md3460_as_a/", "subreddit_subscribers": 690577, "created_utc": 1688190934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I tried doing it with MakeMKV but it isn't able to read it most of the time and can't export it. I don't care about which format they're saved in as long as they can be played or at least converted. Does anyone have experience with this or have any ideas? I have a few hundred to digitize, maybe more, so I'm open to suggestions. Thanks!", "author_fullname": "t2_3lurx74i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digitizing Video CDs, how can I do it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nkcfr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688181888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried doing it with MakeMKV but it isn&amp;#39;t able to read it most of the time and can&amp;#39;t export it. I don&amp;#39;t care about which format they&amp;#39;re saved in as long as they can be played or at least converted. Does anyone have experience with this or have any ideas? I have a few hundred to digitize, maybe more, so I&amp;#39;m open to suggestions. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.5TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nkcfr", "is_robot_indexable": true, "report_reasons": null, "author": "LavaCreeperBOSSB", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14nkcfr/digitizing_video_cds_how_can_i_do_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nkcfr/digitizing_video_cds_how_can_i_do_it/", "subreddit_subscribers": 690577, "created_utc": 1688181888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My OS (Win11) boot drive which is a newer 1TB Seagate FireCuda 530 NVMe is dropping about 2% per week in health. Now u/I 89% health with just 1,200 hours. This is compared to my SK Hynix P31 which has 30k hours at 99% health. \n\nI'd like to move my data to a new NVMe of equal size. I have a spare 1TB NVMe SSD of equal size installed in another m.2 slot and have two more available m.2 slots.\n\nI'd like to use the motherboard's onboard RAID (MSI Z790-A) to create a mirror of the new NVMe to an existing (formatted) NVMe of equal size. \n\nThen I would like to then clone the existing partitions (including Windows) from (failing) NVMe SSD to a new NVMe SSD of equal size.\n\nI have Macrium reflect, but am unclear if I clone the drive to one of the pair of mirrored disks if it will then copy to both automatically to create a mirror? (never used RAID) \n\nIs it advisable to have a mirror of the OS boot drive? What is the best way to accomplish the clone of a NVMe that has the operating system on it? I have several software programs (namely the Arr's and would like to keep the configuration and settings as they are.\n\nAny help is appreciated.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended way to clone data from NVMe SSD to NVMe SSD rapidly dropping in Health", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nhqld", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688173866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My OS (Win11) boot drive which is a newer 1TB Seagate FireCuda 530 NVMe is dropping about 2% per week in health. Now &lt;a href=\"/u/I\"&gt;u/I&lt;/a&gt; 89% health with just 1,200 hours. This is compared to my SK Hynix P31 which has 30k hours at 99% health. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to move my data to a new NVMe of equal size. I have a spare 1TB NVMe SSD of equal size installed in another m.2 slot and have two more available m.2 slots.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to use the motherboard&amp;#39;s onboard RAID (MSI Z790-A) to create a mirror of the new NVMe to an existing (formatted) NVMe of equal size. &lt;/p&gt;\n\n&lt;p&gt;Then I would like to then clone the existing partitions (including Windows) from (failing) NVMe SSD to a new NVMe SSD of equal size.&lt;/p&gt;\n\n&lt;p&gt;I have Macrium reflect, but am unclear if I clone the drive to one of the pair of mirrored disks if it will then copy to both automatically to create a mirror? (never used RAID) &lt;/p&gt;\n\n&lt;p&gt;Is it advisable to have a mirror of the OS boot drive? What is the best way to accomplish the clone of a NVMe that has the operating system on it? I have several software programs (namely the Arr&amp;#39;s and would like to keep the configuration and settings as they are.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nhqld", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nhqld/recommended_way_to_clone_data_from_nvme_ssd_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nhqld/recommended_way_to_clone_data_from_nvme_ssd_to/", "subreddit_subscribers": 690577, "created_utc": 1688173866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First one is 8 bay, the other is 6. Can't find much online, especially about the terramaster.\n\nI'm planning to use 5 Seagate 18TB drives since they seems to offer the best value for money, still not sure about one or two drives for parity (two migh be overkill tbh).\n\nQNAP is more futureproof with 2 more bays, but I don't think i'll need those in the near future. It also has 2 120mm fans, which means they're generally quieter than 80mm (i think i'll be swapping those out for Noctua anyway).\n\nTerraMaster cost less and should be good enough for my needs since i'll be getting the same amount of storage early on with 5 drives. Has the same connectivity with 10gbps USB and looks like is well built. I don't like 80mm fans as I said, in case i'll be swapping those too but from experience, 120mm fans are quieter so, that's a minor point to it. The main issue is that there is nothing online beside announcements. No review, no opinions, nothing.\n\n&amp;#x200B;\n\nIf you have suggestions, or even other models to suggest, please feel free to do so.", "author_fullname": "t2_k99xt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "USB HDD enclosure: QNAP TL-D800C vs TerraMaster D6-320", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nz3xf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688228498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First one is 8 bay, the other is 6. Can&amp;#39;t find much online, especially about the terramaster.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to use 5 Seagate 18TB drives since they seems to offer the best value for money, still not sure about one or two drives for parity (two migh be overkill tbh).&lt;/p&gt;\n\n&lt;p&gt;QNAP is more futureproof with 2 more bays, but I don&amp;#39;t think i&amp;#39;ll need those in the near future. It also has 2 120mm fans, which means they&amp;#39;re generally quieter than 80mm (i think i&amp;#39;ll be swapping those out for Noctua anyway).&lt;/p&gt;\n\n&lt;p&gt;TerraMaster cost less and should be good enough for my needs since i&amp;#39;ll be getting the same amount of storage early on with 5 drives. Has the same connectivity with 10gbps USB and looks like is well built. I don&amp;#39;t like 80mm fans as I said, in case i&amp;#39;ll be swapping those too but from experience, 120mm fans are quieter so, that&amp;#39;s a minor point to it. The main issue is that there is nothing online beside announcements. No review, no opinions, nothing.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you have suggestions, or even other models to suggest, please feel free to do so.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nz3xf", "is_robot_indexable": true, "report_reasons": null, "author": "NaXter24R", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nz3xf/usb_hdd_enclosure_qnap_tld800c_vs_terramaster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nz3xf/usb_hdd_enclosure_qnap_tld800c_vs_terramaster/", "subreddit_subscribers": 690577, "created_utc": 1688228498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've seen similar questions many times looking through this sub but need advice for my specific situation so any advice would help.\n\nExternal ssd or hdd?\n\nI have an old (8-10 yr) seagate external hard drive that has served me well, but it has been whirring for the past year and occasionally loses connection mid transfers. I use it mostly for storing video and access it maybe 5-6 times in a month, sometimes with many months of gaps in between.\n\nI've read keeping an ssd without power for too long can lead to data loss, but would a couple months also be a problem?\n\nI want something that'll last me another decade.\nCurrently considering Samsung T7 for ssd and either Seagate or WD for hdd.\n\nAny advice for my usage?\n\nEdit: Need a minimum 2TB", "author_fullname": "t2_5t4nbhds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to buy for an average user?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nz277", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688231498.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688228377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen similar questions many times looking through this sub but need advice for my specific situation so any advice would help.&lt;/p&gt;\n\n&lt;p&gt;External ssd or hdd?&lt;/p&gt;\n\n&lt;p&gt;I have an old (8-10 yr) seagate external hard drive that has served me well, but it has been whirring for the past year and occasionally loses connection mid transfers. I use it mostly for storing video and access it maybe 5-6 times in a month, sometimes with many months of gaps in between.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read keeping an ssd without power for too long can lead to data loss, but would a couple months also be a problem?&lt;/p&gt;\n\n&lt;p&gt;I want something that&amp;#39;ll last me another decade.\nCurrently considering Samsung T7 for ssd and either Seagate or WD for hdd.&lt;/p&gt;\n\n&lt;p&gt;Any advice for my usage?&lt;/p&gt;\n\n&lt;p&gt;Edit: Need a minimum 2TB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nz277", "is_robot_indexable": true, "report_reasons": null, "author": "IncorrectCoffee", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nz277/what_to_buy_for_an_average_user/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nz277/what_to_buy_for_an_average_user/", "subreddit_subscribers": 690577, "created_utc": 1688228377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there ways to reuse random hard drive you got for free without much work? I'd image a small computer, you find a free hard drive, attach it to the computer and it will be automatically added to the Nas as a raid or so?", "author_fullname": "t2_jp3bv4xk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ghetto-NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nwaqu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688221205.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there ways to reuse random hard drive you got for free without much work? I&amp;#39;d image a small computer, you find a free hard drive, attach it to the computer and it will be automatically added to the Nas as a raid or so?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nwaqu", "is_robot_indexable": true, "report_reasons": null, "author": "CryptographerOdd299", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nwaqu/ghettonas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nwaqu/ghettonas/", "subreddit_subscribers": 690577, "created_utc": 1688221205.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there, I recently upgraded from a M5015 to a Adaptec 71605. I have the new card connected to a IBM X3400 SAS Expander. The problem I'm having is that the new card does not detect the drives connected to the expander. 2 drives connected directly to the card via backplane are detected. The old card with the expander did and still does show all drives when reconnected. I have attached an image of the cards boot screen showing it detects the expander but finds no logical drives. There also appears to be a no bios warning but I'm not sure if that's relevant. Any help that you can provide would be appreciated, Thanks.\n\nhttps://preview.redd.it/yx9bf9s0tc9b1.jpg?width=1851&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=2443fa2e91da638387a2aeeb9a0903f9c6940c19", "author_fullname": "t2_zm7dz8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adaptec 71605 + ServeRAID X3400 HDD Detection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "media_metadata": {"yx9bf9s0tc9b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 75, "x": 108, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5dd973a07f3d3404327497dcf8df18df791604e9"}, {"y": 151, "x": 216, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdd64a9184cd125321cae0109d41c0f7bb807d1a"}, {"y": 224, "x": 320, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=375773e9db8f6e60828e377e563b2d63697be333"}, {"y": 449, "x": 640, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=173a9de5d16c07498c6183c509b3826a64a20c34"}, {"y": 673, "x": 960, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a7946c3a27212e91b9e4f2589e5eaf5ca574f21"}, {"y": 757, "x": 1080, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4aaefe93f24cb16e6239efe08e6876dbb999fab9"}], "s": {"y": 1299, "x": 1851, "u": "https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=1851&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=2443fa2e91da638387a2aeeb9a0903f9c6940c19"}, "id": "yx9bf9s0tc9b1"}}, "name": "t3_14nuwfj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fD3BXNmxibK2pJO_WwL5hBfguRsIiOTUPcZVOU_orpo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688217333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I recently upgraded from a M5015 to a Adaptec 71605. I have the new card connected to a IBM X3400 SAS Expander. The problem I&amp;#39;m having is that the new card does not detect the drives connected to the expander. 2 drives connected directly to the card via backplane are detected. The old card with the expander did and still does show all drives when reconnected. I have attached an image of the cards boot screen showing it detects the expander but finds no logical drives. There also appears to be a no bios warning but I&amp;#39;m not sure if that&amp;#39;s relevant. Any help that you can provide would be appreciated, Thanks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=1851&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2443fa2e91da638387a2aeeb9a0903f9c6940c19\"&gt;https://preview.redd.it/yx9bf9s0tc9b1.jpg?width=1851&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2443fa2e91da638387a2aeeb9a0903f9c6940c19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nuwfj", "is_robot_indexable": true, "report_reasons": null, "author": "smorgisborg1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nuwfj/adaptec_71605_serveraid_x3400_hdd_detection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nuwfj/adaptec_71605_serveraid_x3400_hdd_detection/", "subreddit_subscribers": 690577, "created_utc": 1688217333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I have found several old CD-R, DVD-R and DVD+R discs. Since they provide only little storage by modern standards, I wonder what I can for example record to these CD-R/DVD-R discs.\n\nIf they are from a brand that doesn't exist anymore or are they a 650 MB variety (which are practically not made anymore), are they worth more? What about slightly scratched disks?", "author_fullname": "t2_rej6q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to do with several old CD-R/DVD-R blank discs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ntca7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688212792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have found several old CD-R, DVD-R and DVD+R discs. Since they provide only little storage by modern standards, I wonder what I can for example record to these CD-R/DVD-R discs.&lt;/p&gt;\n\n&lt;p&gt;If they are from a brand that doesn&amp;#39;t exist anymore or are they a 650 MB variety (which are practically not made anymore), are they worth more? What about slightly scratched disks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ntca7", "is_robot_indexable": true, "report_reasons": null, "author": "smsaczek", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ntca7/what_to_do_with_several_old_cdrdvdr_blank_discs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14ntca7/what_to_do_with_several_old_cdrdvdr_blank_discs/", "subreddit_subscribers": 690577, "created_utc": 1688212792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently added a large hdd to my Raspberry pi setup in order to download media on it. I don't know if this counts as data hoarding but this seems like a sub who knows an answer to my problem.\n\nMy old 2tb drive listens to hdparm and hd-idle. However this drive I can definitely spin down manually however it seems to go into standby after only 3 minutes. Which is just too little. It let's me set the time and APM but it just changes back after a bit. I know I can probably get a cronjob to touch a file but I don't want to lose sleep, just set it to something more reasonable.\n\nSo so you guys know some larger current hdd's that let you set/disable sleep in their firmware. Or at least have a 20+ minute sleep period. The one I'm having trouble with is an Intenso drive which got good reviews and seems fine otherwise. Thanks!", "author_fullname": "t2_p59vu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recent external hdd that obeys sleep settings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nrmra", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688206990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently added a large hdd to my Raspberry pi setup in order to download media on it. I don&amp;#39;t know if this counts as data hoarding but this seems like a sub who knows an answer to my problem.&lt;/p&gt;\n\n&lt;p&gt;My old 2tb drive listens to hdparm and hd-idle. However this drive I can definitely spin down manually however it seems to go into standby after only 3 minutes. Which is just too little. It let&amp;#39;s me set the time and APM but it just changes back after a bit. I know I can probably get a cronjob to touch a file but I don&amp;#39;t want to lose sleep, just set it to something more reasonable.&lt;/p&gt;\n\n&lt;p&gt;So so you guys know some larger current hdd&amp;#39;s that let you set/disable sleep in their firmware. Or at least have a 20+ minute sleep period. The one I&amp;#39;m having trouble with is an Intenso drive which got good reviews and seems fine otherwise. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nrmra", "is_robot_indexable": true, "report_reasons": null, "author": "MrRenegado", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nrmra/recent_external_hdd_that_obeys_sleep_settings/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nrmra/recent_external_hdd_that_obeys_sleep_settings/", "subreddit_subscribers": 690577, "created_utc": 1688206990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not sure where else to ask this, I've been trying to find an external enclosure for my very old Seagate Freeplay drive. The old enclosure broke so I've been kinda lugging it around without one. The problem is it has significantly more height than most 2.5-inch enclosures and so I can't find one that fits it. I mainly use a laptop so can't stick it in a PC. Is my only option to get a 3.5-inch enclosure?", "author_fullname": "t2_v6gyu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there enclosures for a Seagate Freeplay HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14noemt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688195380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure where else to ask this, I&amp;#39;ve been trying to find an external enclosure for my very old Seagate Freeplay drive. The old enclosure broke so I&amp;#39;ve been kinda lugging it around without one. The problem is it has significantly more height than most 2.5-inch enclosures and so I can&amp;#39;t find one that fits it. I mainly use a laptop so can&amp;#39;t stick it in a PC. Is my only option to get a 3.5-inch enclosure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14noemt", "is_robot_indexable": true, "report_reasons": null, "author": "philinsaniachen", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14noemt/are_there_enclosures_for_a_seagate_freeplay_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14noemt/are_there_enclosures_for_a_seagate_freeplay_hdd/", "subreddit_subscribers": 690577, "created_utc": 1688195380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI have a web app that accepts uploads from users and serves files to users. I set it up with backblaze via the API and the upload speeds were 300Kbps. So i switched to Wasabi, but they no longer allow public access for buckets as Wasabi is intended for backup and archiving.\n\nWhat are my best options for file hosting services with API access?\n\nThanks,\n\nAlbert", "author_fullname": "t2_npfxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "File hosting for web app", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nfyol", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688168802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have a web app that accepts uploads from users and serves files to users. I set it up with backblaze via the API and the upload speeds were 300Kbps. So i switched to Wasabi, but they no longer allow public access for buckets as Wasabi is intended for backup and archiving.&lt;/p&gt;\n\n&lt;p&gt;What are my best options for file hosting services with API access?&lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n\n&lt;p&gt;Albert&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nfyol", "is_robot_indexable": true, "report_reasons": null, "author": "u2nyr", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nfyol/file_hosting_for_web_app/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nfyol/file_hosting_for_web_app/", "subreddit_subscribers": 690577, "created_utc": 1688168802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a network drive mapped in windows and I logged in with an account with full permissions, I'm wondering if this is a wise choice though. I'm careful but I worry that maybe if my computer becomes compromised it could affect my nas in some way through this network mapped drive. Should I log in with an account with read only perms? or would that not do anything in the event of malware or similar. Thanks", "author_fullname": "t2_gjxw27zv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "question about mapping network drives from nas in windows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nfqhc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688168188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a network drive mapped in windows and I logged in with an account with full permissions, I&amp;#39;m wondering if this is a wise choice though. I&amp;#39;m careful but I worry that maybe if my computer becomes compromised it could affect my nas in some way through this network mapped drive. Should I log in with an account with read only perms? or would that not do anything in the event of malware or similar. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nfqhc", "is_robot_indexable": true, "report_reasons": null, "author": "KarinAppreciator", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nfqhc/question_about_mapping_network_drives_from_nas_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nfqhc/question_about_mapping_network_drives_from_nas_in/", "subreddit_subscribers": 690577, "created_utc": 1688168188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, \n\nI have a Panasonic NV-FJ610 which has [these connections on the back](https://imgur.com/CaZWtZN) and I also have a BlackMagic Intensity Shuffle USB 3.0. \n\nCan you please help me understand what wires/equipment I need in order to capture my VHS tapes and make them digital? \n\nI was following [this tutorial on Youtube](https://www.youtube.com/watch?v=_3QH_-Tzzxk&amp;t=613s) but it's unclear how he had it set up. I know there's SCART to HDMI converters but I would want the best possible quality when capturing and from what I've read online, they can decrease quality as well as change the resolution.\n\nThere's an S video input on the BlackMagic shuffle so would it be worth getting a SCART to S video converter/cable and then capturing that way or that a similar situation with the HDMI converters?\n\nI have home movies that are over 30 years old that I want to capture and digitise and share with family as I know the shelf live on these tapes doesn't last forever so I don't want to lose memories from when I was a kid.\n\n&amp;#x200B;", "author_fullname": "t2_ce61udt7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to capture VHS footage with the setup I have? - Help me!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14o10o6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688233454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I have a Panasonic NV-FJ610 which has &lt;a href=\"https://imgur.com/CaZWtZN\"&gt;these connections on the back&lt;/a&gt; and I also have a BlackMagic Intensity Shuffle USB 3.0. &lt;/p&gt;\n\n&lt;p&gt;Can you please help me understand what wires/equipment I need in order to capture my VHS tapes and make them digital? &lt;/p&gt;\n\n&lt;p&gt;I was following &lt;a href=\"https://www.youtube.com/watch?v=_3QH_-Tzzxk&amp;amp;t=613s\"&gt;this tutorial on Youtube&lt;/a&gt; but it&amp;#39;s unclear how he had it set up. I know there&amp;#39;s SCART to HDMI converters but I would want the best possible quality when capturing and from what I&amp;#39;ve read online, they can decrease quality as well as change the resolution.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s an S video input on the BlackMagic shuffle so would it be worth getting a SCART to S video converter/cable and then capturing that way or that a similar situation with the HDMI converters?&lt;/p&gt;\n\n&lt;p&gt;I have home movies that are over 30 years old that I want to capture and digitise and share with family as I know the shelf live on these tapes doesn&amp;#39;t last forever so I don&amp;#39;t want to lose memories from when I was a kid.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nrSvQmkh_7gNq1mIySpnqf4OP4JxP6iB2DljfNuW0lo.jpg?auto=webp&amp;v=enabled&amp;s=04702e486a120242a94c1ab5292e5d5e98d2a838", "width": 600, "height": 315}, "resolutions": [{"url": "https://external-preview.redd.it/nrSvQmkh_7gNq1mIySpnqf4OP4JxP6iB2DljfNuW0lo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ad86628185d9c06006d9ba34c9b0c0b1e7dee17", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/nrSvQmkh_7gNq1mIySpnqf4OP4JxP6iB2DljfNuW0lo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a2e6e375154e7d3b162580242c241bd61940e40", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/nrSvQmkh_7gNq1mIySpnqf4OP4JxP6iB2DljfNuW0lo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=960aa84ff93ccf912f66f3e701dc953038e13142", "width": 320, "height": 168}], "variants": {}, "id": "eipK41Wyx27nBiHp4iV66587rz3Sc7KhL8MK072Dumc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14o10o6", "is_robot_indexable": true, "report_reasons": null, "author": "Uncle_Beanpole", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14o10o6/whats_the_best_way_to_capture_vhs_footage_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14o10o6/whats_the_best_way_to_capture_vhs_footage_with/", "subreddit_subscribers": 690577, "created_utc": 1688233454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm thinking of taking the plunge into ZFS coming from Windows 11. I routinely use the Arr apps and would like help/suggestions on whether to go with Ubuntu on ZFS or to go with Unraid or similar with BTRFS?\n\nMy present setup is a z790 chipset 13th gen Intel with onboard graphics.\n\n Is there any suggested reading or guides available for someone like me who has zero experience with Linux and wants to switch to utilize the benefits of ZFS? \n\nI currently have a 16TB, 18TB, 18TB enterprise hard drives installed internally which contain data that I would like to have on my new ZFS system. I am booting from NVMe SSD. \n\nProblem is, all of my data is on NTFS partitions. I do have external HDD's (also NTFS) which contain the same data so I can format the above three HDD's to create the raid group.\n\nI am considering buying a 3rd 18tb HDD so I have equal size drives to create my RAIDZ2 group starting with just 3 drives and not including my 16tb to start with. \n\nWould it be best for me to have equal sized drives in creating this group? Can I expand the group when I buy more drives or need more space? How will I handle copying my files from a drive with NTFS to a ZFS pool?\n\nIs it wise to create more than one pool or manage just one pool? I've read that others do daily short SMART tests, weekly long tests, and bi-weekly/monthly scrubs. Would this be sufficient? \n\nThe purpose of this build will be to do one thing. To operate the Arr apps. (Sonarr, Prowlarr, Overseerr, etc.) I will be installing Plex and using it as a HTPC - and little to nothing else. \n\nThe reason I am looking to switch to ZFS is due to it detecting/repairing bitrot. I have thought about Unraid but became confused on whether to use Ubuntu or Unraid. The idea of parity drives kind of confused me when it comes to Unraid.\n\n I am unclear on what would be best for my usage scenario. What would you recommend for ease of implementation - taking into consideration that my primary usage scenario is the Arr apps and Plex? ", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guides or tips for setting up first ZFS file/plex server?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14nwq7w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688222356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of taking the plunge into ZFS coming from Windows 11. I routinely use the Arr apps and would like help/suggestions on whether to go with Ubuntu on ZFS or to go with Unraid or similar with BTRFS?&lt;/p&gt;\n\n&lt;p&gt;My present setup is a z790 chipset 13th gen Intel with onboard graphics.&lt;/p&gt;\n\n&lt;p&gt;Is there any suggested reading or guides available for someone like me who has zero experience with Linux and wants to switch to utilize the benefits of ZFS? &lt;/p&gt;\n\n&lt;p&gt;I currently have a 16TB, 18TB, 18TB enterprise hard drives installed internally which contain data that I would like to have on my new ZFS system. I am booting from NVMe SSD. &lt;/p&gt;\n\n&lt;p&gt;Problem is, all of my data is on NTFS partitions. I do have external HDD&amp;#39;s (also NTFS) which contain the same data so I can format the above three HDD&amp;#39;s to create the raid group.&lt;/p&gt;\n\n&lt;p&gt;I am considering buying a 3rd 18tb HDD so I have equal size drives to create my RAIDZ2 group starting with just 3 drives and not including my 16tb to start with. &lt;/p&gt;\n\n&lt;p&gt;Would it be best for me to have equal sized drives in creating this group? Can I expand the group when I buy more drives or need more space? How will I handle copying my files from a drive with NTFS to a ZFS pool?&lt;/p&gt;\n\n&lt;p&gt;Is it wise to create more than one pool or manage just one pool? I&amp;#39;ve read that others do daily short SMART tests, weekly long tests, and bi-weekly/monthly scrubs. Would this be sufficient? &lt;/p&gt;\n\n&lt;p&gt;The purpose of this build will be to do one thing. To operate the Arr apps. (Sonarr, Prowlarr, Overseerr, etc.) I will be installing Plex and using it as a HTPC - and little to nothing else. &lt;/p&gt;\n\n&lt;p&gt;The reason I am looking to switch to ZFS is due to it detecting/repairing bitrot. I have thought about Unraid but became confused on whether to use Ubuntu or Unraid. The idea of parity drives kind of confused me when it comes to Unraid.&lt;/p&gt;\n\n&lt;p&gt;I am unclear on what would be best for my usage scenario. What would you recommend for ease of implementation - taking into consideration that my primary usage scenario is the Arr apps and Plex? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14nwq7w", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14nwq7w/guides_or_tips_for_setting_up_first_zfs_fileplex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14nwq7w/guides_or_tips_for_setting_up_first_zfs_fileplex/", "subreddit_subscribers": 690577, "created_utc": 1688222356.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}