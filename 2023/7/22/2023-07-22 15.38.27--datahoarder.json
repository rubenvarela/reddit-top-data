{"kind": "Listing", "data": {"after": "t3_156i1jc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Dropbox just yesterday limited it's \"unlimited\" subscriptions to 10TB/week. This seems to be a reaction to people migrating too much data from Google Drive, due to the limit Google is rolling out at the moment as well.\n\nIt is extremely upsetting for a lot of new customers right now, as we asked specifically for the exact amount of data we were planning to migrate and got a green light from Dropbox support and from sales beforehand. We have invested a lot of money on the migration, that is now completely lost.\n\nThey had to see this coming - they knew exactly, why we and many others are moving from Google to Dropbox and they obviously held back the information, that they will not be able to handle that much data.\n\nDropbox support and sales are telling us, that this new policy came completely out of the blue for them as well and that this is upsetting for them as well, as it was them who promised all of us a smooth migration. The German sales team had to find out about this limit through their customers and only got the official briefing afterwards.\n\nI'm not even talking about petabytes here. We are a film production and simply need some storage to keep our business running. We had been communicating the amount we needed very openly.\n\nAnyone here, that is not affected by this? Can anyone give legal advice on this matter? Also thank you for sharing this to try to reach someone at Dropbox for help.", "author_fullname": "t2_milg6ems", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dropbox Business Advanced not unlimited anymore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155so84", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 144, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 144, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689956480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dropbox just yesterday limited it&amp;#39;s &amp;quot;unlimited&amp;quot; subscriptions to 10TB/week. This seems to be a reaction to people migrating too much data from Google Drive, due to the limit Google is rolling out at the moment as well.&lt;/p&gt;\n\n&lt;p&gt;It is extremely upsetting for a lot of new customers right now, as we asked specifically for the exact amount of data we were planning to migrate and got a green light from Dropbox support and from sales beforehand. We have invested a lot of money on the migration, that is now completely lost.&lt;/p&gt;\n\n&lt;p&gt;They had to see this coming - they knew exactly, why we and many others are moving from Google to Dropbox and they obviously held back the information, that they will not be able to handle that much data.&lt;/p&gt;\n\n&lt;p&gt;Dropbox support and sales are telling us, that this new policy came completely out of the blue for them as well and that this is upsetting for them as well, as it was them who promised all of us a smooth migration. The German sales team had to find out about this limit through their customers and only got the official briefing afterwards.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not even talking about petabytes here. We are a film production and simply need some storage to keep our business running. We had been communicating the amount we needed very openly.&lt;/p&gt;\n\n&lt;p&gt;Anyone here, that is not affected by this? Can anyone give legal advice on this matter? Also thank you for sharing this to try to reach someone at Dropbox for help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155so84", "is_robot_indexable": true, "report_reasons": null, "author": "joshuanicolaspark", "discussion_type": null, "num_comments": 189, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155so84/dropbox_business_advanced_not_unlimited_anymore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155so84/dropbox_business_advanced_not_unlimited_anymore/", "subreddit_subscribers": 693639, "created_utc": 1689956480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just went to reconnect my home cloud duo on my Mac, got to the website that mounts the drive to my computer, and said no longer supported. This blows. Their web-based access still works, and so does the iPhone OS app.", "author_fullname": "t2_e0g8qmrsg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Western Digital stopped app support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15635n3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689980573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just went to reconnect my home cloud duo on my Mac, got to the website that mounts the drive to my computer, and said no longer supported. This blows. Their web-based access still works, and so does the iPhone OS app.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15635n3", "is_robot_indexable": true, "report_reasons": null, "author": "Professional-Elk3586", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15635n3/western_digital_stopped_app_support/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15635n3/western_digital_stopped_app_support/", "subreddit_subscribers": 693639, "created_utc": 1689980573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Resizing boot drives seems to break windows in the efi era.  Apparently with macrium you drag each partition manually and in sequential order, and resize c: when you get to it?  IIRC cloning ssd's of same size is smaller.  Please don't tell me to use gparted.  I've wrecked several installs with that (APFS included)", "author_fullname": "t2_d5sfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloning internal ssd to external in dock, 256gb - 1tb. I want to expand c: to 300gb and make a k: drive with the last 631. Use macrium?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156hro2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690025742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Resizing boot drives seems to break windows in the efi era.  Apparently with macrium you drag each partition manually and in sequential order, and resize c: when you get to it?  IIRC cloning ssd&amp;#39;s of same size is smaller.  Please don&amp;#39;t tell me to use gparted.  I&amp;#39;ve wrecked several installs with that (APFS included)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "156hro2", "is_robot_indexable": true, "report_reasons": null, "author": "bluejeans90210", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/156hro2/cloning_internal_ssd_to_external_in_dock_256gb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/156hro2/cloning_internal_ssd_to_external_in_dock_256gb/", "subreddit_subscribers": 693639, "created_utc": 1690025742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi again! I am looking to migrate away from my existing 60TB NAS of 7 years to a new and intentionally overkill NAS setup. I [originally posted about this a few days ago](https://www.reddit.com/r/DataHoarder/comments/1542qm6/new_300tb_data_hoarding_build_advice/) and based on some very valuable feedback I received (thanks!), I\u2019ve decided to pursue server hardware instead of consumer. I also took the advice of doing ECC RAM, higher rated TBW NVMe drives, more HBA adapters, a more space-efficient chassis, 10 GbE, and so on. Appreciate y\u2019all giving my build another look before I pull the trigger.\n\n### Requirements\n\n* For personal use\n* At least 300TB usable space, with a scalability path towards 1PB in the future\n* Hard drive redundancy\n* Rack-mounted in enclosed home basement (noise won\u2019t be an issue)\n* Support running containers, 10-12 concurrent transcodes of 1080p and 4K (Plex), CPU-intensive operations (tdarr)\n* Be able to run Ubuntu + HashiCorp Nomad\n\n### Build\n\n* Case: SuperMicro 72x Drive 4U SuperStorage 6048R-E1CR72L w/ SuperMicro Rails\n* Processor: 2x Xeon E5-2697 v4 2.3GHz 18-Core Processors\n* Motherboard: X10DRH-IT\n* Memory: 256GB (4x 64GB) DDR4 Registered Memory\n* Storage Controller: 3x Supermicro HBA AOC-S3008L-L8e 12Gbps Eight-Port SAS PCIe Internal Host Bus Adapter\n* NVMe Drives: 2x Firecuda 530\n* NVMe Adapter: 2x M.2 NVME to PCIe 3.0 x4 Adapter\n* SATA Drives: 24x 22 TB IronWolf Pro\n* Power Supply: 3x 1200W 80 PLUS Gold Power Supplies\n* Video Card: Quadro P2200\n\nThe NVMe drives will be mirrored for redundancy and will contain the OS, system files, and act as a scratch disk (downloads and extraction for better performance). The SATA HDDs will be configured as 6x 22TB drives RAIDZ2 per vdev @ 4x vdevs, bringing the total usable space to about 352TB.\n\n### Concerns\n\n* Will 256GB be enough RAM for the ZFS or should I go higher? I only need about 64GB RAM for my containers\n* I think I will need to upgrade to a better video card (e.g. P4000) to be able to do more transcodes but not sure if my case can fit something like that. It\u2019s 2U for the server components and the vendor is saying only low-profile GPUs can fit. Are there any hacks to accommodate something like a P4000?\n\nThanks, feel free to poke holes and let me know your thoughts!", "author_fullname": "t2_c7g08877", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New 300TB+ Data Hoarding Build v2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15643ds", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689983044.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi again! I am looking to migrate away from my existing 60TB NAS of 7 years to a new and intentionally overkill NAS setup. I &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/1542qm6/new_300tb_data_hoarding_build_advice/\"&gt;originally posted about this a few days ago&lt;/a&gt; and based on some very valuable feedback I received (thanks!), I\u2019ve decided to pursue server hardware instead of consumer. I also took the advice of doing ECC RAM, higher rated TBW NVMe drives, more HBA adapters, a more space-efficient chassis, 10 GbE, and so on. Appreciate y\u2019all giving my build another look before I pull the trigger.&lt;/p&gt;\n\n&lt;h3&gt;Requirements&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;For personal use&lt;/li&gt;\n&lt;li&gt;At least 300TB usable space, with a scalability path towards 1PB in the future&lt;/li&gt;\n&lt;li&gt;Hard drive redundancy&lt;/li&gt;\n&lt;li&gt;Rack-mounted in enclosed home basement (noise won\u2019t be an issue)&lt;/li&gt;\n&lt;li&gt;Support running containers, 10-12 concurrent transcodes of 1080p and 4K (Plex), CPU-intensive operations (tdarr)&lt;/li&gt;\n&lt;li&gt;Be able to run Ubuntu + HashiCorp Nomad&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;Build&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Case: SuperMicro 72x Drive 4U SuperStorage 6048R-E1CR72L w/ SuperMicro Rails&lt;/li&gt;\n&lt;li&gt;Processor: 2x Xeon E5-2697 v4 2.3GHz 18-Core Processors&lt;/li&gt;\n&lt;li&gt;Motherboard: X10DRH-IT&lt;/li&gt;\n&lt;li&gt;Memory: 256GB (4x 64GB) DDR4 Registered Memory&lt;/li&gt;\n&lt;li&gt;Storage Controller: 3x Supermicro HBA AOC-S3008L-L8e 12Gbps Eight-Port SAS PCIe Internal Host Bus Adapter&lt;/li&gt;\n&lt;li&gt;NVMe Drives: 2x Firecuda 530&lt;/li&gt;\n&lt;li&gt;NVMe Adapter: 2x M.2 NVME to PCIe 3.0 x4 Adapter&lt;/li&gt;\n&lt;li&gt;SATA Drives: 24x 22 TB IronWolf Pro&lt;/li&gt;\n&lt;li&gt;Power Supply: 3x 1200W 80 PLUS Gold Power Supplies&lt;/li&gt;\n&lt;li&gt;Video Card: Quadro P2200&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The NVMe drives will be mirrored for redundancy and will contain the OS, system files, and act as a scratch disk (downloads and extraction for better performance). The SATA HDDs will be configured as 6x 22TB drives RAIDZ2 per vdev @ 4x vdevs, bringing the total usable space to about 352TB.&lt;/p&gt;\n\n&lt;h3&gt;Concerns&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Will 256GB be enough RAM for the ZFS or should I go higher? I only need about 64GB RAM for my containers&lt;/li&gt;\n&lt;li&gt;I think I will need to upgrade to a better video card (e.g. P4000) to be able to do more transcodes but not sure if my case can fit something like that. It\u2019s 2U for the server components and the vendor is saying only low-profile GPUs can fit. Are there any hacks to accommodate something like a P4000?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks, feel free to poke holes and let me know your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15643ds", "is_robot_indexable": true, "report_reasons": null, "author": "fat_keepsake", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15643ds/new_300tb_data_hoarding_build_v2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15643ds/new_300tb_data_hoarding_build_v2/", "subreddit_subscribers": 693639, "created_utc": 1689983044.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All,\n\nMy DS380 case is full and looking for expansion options. I understand that USB is a bit temperamental and generally not advised in MergerFS / SnapRaid Configs.\n\nWhat about eSATA? Is it considered any better or worse in this regard? My NAS is older with an eSATA port and I wondering about a used ICY BOX IB-3640SU3 which has eSATA and USB3 connectivity.", "author_fullname": "t2_16z1he", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "eSATA Enclosure for MergerFS / SnapRaid?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156hf2m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690024634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;My DS380 case is full and looking for expansion options. I understand that USB is a bit temperamental and generally not advised in MergerFS / SnapRaid Configs.&lt;/p&gt;\n\n&lt;p&gt;What about eSATA? Is it considered any better or worse in this regard? My NAS is older with an eSATA port and I wondering about a used ICY BOX IB-3640SU3 which has eSATA and USB3 connectivity.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "156hf2m", "is_robot_indexable": true, "report_reasons": null, "author": "Nicoloks", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/156hf2m/esata_enclosure_for_mergerfs_snapraid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/156hf2m/esata_enclosure_for_mergerfs_snapraid/", "subreddit_subscribers": 693639, "created_utc": 1690024634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have hundreds of hours of mini-DV tapes that I need to develop for a documentary I'm making. I did a ton of research and got the necessary firewire cables / adapters, as well as a Canon XL-2 camera and the Lifelix software. Unfortunately, the Lifelix software stops my tapes constantly while importing, thinking that any lag or glitch is the end of the tape. This is unsustainable with the amount of tapes I'm developing, especially because it has a bug when using the Canon camera where you have to click play then start the import in order to read the audio. \n\nQuicktime refuses to save the footage when uploaded and doesn't actually support the native DV format, so that's not an option. iMovie hasn't recorded the audio at all, and Premiere doesn't even detect my camera. I've been working on this for weeks and had initial success with the Lifelix software but it seems to have gotten buggier with use. I'm considering trying with a different camera but was wondering if you guys have any experience or similar issues. \n\n&amp;#x200B;\n\nAppreciate it in advance, thank you ", "author_fullname": "t2_chvo1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling to develop mini-DV tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155z60v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689971223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have hundreds of hours of mini-DV tapes that I need to develop for a documentary I&amp;#39;m making. I did a ton of research and got the necessary firewire cables / adapters, as well as a Canon XL-2 camera and the Lifelix software. Unfortunately, the Lifelix software stops my tapes constantly while importing, thinking that any lag or glitch is the end of the tape. This is unsustainable with the amount of tapes I&amp;#39;m developing, especially because it has a bug when using the Canon camera where you have to click play then start the import in order to read the audio. &lt;/p&gt;\n\n&lt;p&gt;Quicktime refuses to save the footage when uploaded and doesn&amp;#39;t actually support the native DV format, so that&amp;#39;s not an option. iMovie hasn&amp;#39;t recorded the audio at all, and Premiere doesn&amp;#39;t even detect my camera. I&amp;#39;ve been working on this for weeks and had initial success with the Lifelix software but it seems to have gotten buggier with use. I&amp;#39;m considering trying with a different camera but was wondering if you guys have any experience or similar issues. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Appreciate it in advance, thank you &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155z60v", "is_robot_indexable": true, "report_reasons": null, "author": "Frankieba", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155z60v/struggling_to_develop_minidv_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155z60v/struggling_to_develop_minidv_tapes/", "subreddit_subscribers": 693639, "created_utc": 1689971223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I currently have a paid version of Macrium Reflect for backups but I hardly even use it because the backups take so long. I've got about 8 TB to back up but it takes 10-12 hours.\n\nYou're probably thinking \"just do it overnight\". But the issue is that I usually only sleep a few hours or maybe 6 hours or so at the most then when I get up I'm using the computer which I imagine can potentially lead to issues with the backup down the line which I would rather not risk.\n\nSo I'm just wondering if anyone knows of any backup software and/or backup setup for backing up to physical drives and anything at all I can do to get this 8 TB of data backed up a lot faster than the current 10-12 hours, 4-6 hours would be perfect.", "author_fullname": "t2_pnnd32g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for faster but still reliable backup software suggestions for 8 TB of data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156afid", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690001342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I currently have a paid version of Macrium Reflect for backups but I hardly even use it because the backups take so long. I&amp;#39;ve got about 8 TB to back up but it takes 10-12 hours.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re probably thinking &amp;quot;just do it overnight&amp;quot;. But the issue is that I usually only sleep a few hours or maybe 6 hours or so at the most then when I get up I&amp;#39;m using the computer which I imagine can potentially lead to issues with the backup down the line which I would rather not risk.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m just wondering if anyone knows of any backup software and/or backup setup for backing up to physical drives and anything at all I can do to get this 8 TB of data backed up a lot faster than the current 10-12 hours, 4-6 hours would be perfect.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "156afid", "is_robot_indexable": true, "report_reasons": null, "author": "Argaldus", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/156afid/looking_for_faster_but_still_reliable_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/156afid/looking_for_faster_but_still_reliable_backup/", "subreddit_subscribers": 693639, "created_utc": 1690001342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had a Linux Nas, then it filled up, so I added a drive, then an external for backup, and now I'm outgrowing again.\n\nHas anyone looked at setting up ceph? It looks like a solution that would be easy to scale up, just add drives and nodes, and also easy to retire old hw. \nI was thinking a dual system, nvme for active data and HDD for archives. \n\nSeems a lot easier going forward than a Nas.", "author_fullname": "t2_2sr8ya35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ceph?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155uajf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689960160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had a Linux Nas, then it filled up, so I added a drive, then an external for backup, and now I&amp;#39;m outgrowing again.&lt;/p&gt;\n\n&lt;p&gt;Has anyone looked at setting up ceph? It looks like a solution that would be easy to scale up, just add drives and nodes, and also easy to retire old hw. \nI was thinking a dual system, nvme for active data and HDD for archives. &lt;/p&gt;\n\n&lt;p&gt;Seems a lot easier going forward than a Nas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155uajf", "is_robot_indexable": true, "report_reasons": null, "author": "Frewtti", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155uajf/ceph/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155uajf/ceph/", "subreddit_subscribers": 693639, "created_utc": 1689960160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently run a Synology DS916+ with a DX517 expansion unit. Two separate arrays, 4x6TB and 5x10TB respectively with both in SHR. Mostly used for network accessible video hoarding but there is some personal files that I also backup offsite. I recently replaced older 4TB drives with the 10TB drives.\n\nI've now ran out of space on my expansion volume and am considering options. Costs aren't really a problem but I am conscious of them.\n\nI am probably going to buy some more drives (16TB or cost effective options), but Should I:\n\n* Just buy bigger drives and sell older drives. The cycle continues.\n* Buy bigger Synology unit (12bay DS2422+) and transfer SHR over to it. Can add more misc drives to SHR is my understanding. 12 expansion unit will give further space if required.\n* Build a DIY NAS using Define 7 XL. No personal experience here or what software to use, could be fun, possibly cheaper, possibly a headache to manage.\n\nHappy to consider other options if I am missing something here.", "author_fullname": "t2_4bqmvvpb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upgrade Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1567967", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690024842.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689991740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently run a Synology DS916+ with a DX517 expansion unit. Two separate arrays, 4x6TB and 5x10TB respectively with both in SHR. Mostly used for network accessible video hoarding but there is some personal files that I also backup offsite. I recently replaced older 4TB drives with the 10TB drives.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve now ran out of space on my expansion volume and am considering options. Costs aren&amp;#39;t really a problem but I am conscious of them.&lt;/p&gt;\n\n&lt;p&gt;I am probably going to buy some more drives (16TB or cost effective options), but Should I:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Just buy bigger drives and sell older drives. The cycle continues.&lt;/li&gt;\n&lt;li&gt;Buy bigger Synology unit (12bay DS2422+) and transfer SHR over to it. Can add more misc drives to SHR is my understanding. 12 expansion unit will give further space if required.&lt;/li&gt;\n&lt;li&gt;Build a DIY NAS using Define 7 XL. No personal experience here or what software to use, could be fun, possibly cheaper, possibly a headache to manage.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Happy to consider other options if I am missing something here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1567967", "is_robot_indexable": true, "report_reasons": null, "author": "Capt_Booyah", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1567967/upgrade_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1567967/upgrade_advice/", "subreddit_subscribers": 693639, "created_utc": 1689991740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey!\n\nBasically I have a home lab and have been using some external and internal drives\n\nThe problem is that each 3.5 drive requires a 12v power connector. So I have been connecting a lot of those.\n\nI've been investigating and learned about drives, bays and enclosures but I cannot understand them correctly\n\nWhich one is good for my use case? I want something that I can connect my drives to, like centralized\n\nI don't think I want synology or anything like that since that has a server which I don't need (have a mini pc)\n\nIf this is the wrong sub, please point me towards a more suitable one\n\nThanks!", "author_fullname": "t2_3f7jceck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about drives and how to scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155xznl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689968560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;Basically I have a home lab and have been using some external and internal drives&lt;/p&gt;\n\n&lt;p&gt;The problem is that each 3.5 drive requires a 12v power connector. So I have been connecting a lot of those.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been investigating and learned about drives, bays and enclosures but I cannot understand them correctly&lt;/p&gt;\n\n&lt;p&gt;Which one is good for my use case? I want something that I can connect my drives to, like centralized&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think I want synology or anything like that since that has a server which I don&amp;#39;t need (have a mini pc)&lt;/p&gt;\n\n&lt;p&gt;If this is the wrong sub, please point me towards a more suitable one&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155xznl", "is_robot_indexable": true, "report_reasons": null, "author": "Rafa130397", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155xznl/question_about_drives_and_how_to_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155xznl/question_about_drives_and_how_to_scale/", "subreddit_subscribers": 693639, "created_utc": 1689968560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, a question here, if I would like to scrape the entire themarginalian.org site, store it locally,  and built a chat bot out of it, what would be a good way to get started?\nI'm using Windows, and I'm open to any scraping suggestions \nright now I'm using the traditional HT Track to do the scraping\nWould be great if the web scraper can filter out all the garbage, and only return articles\nAny good ways to do it in wget?\nAny help appreciated", "author_fullname": "t2_35hhoksa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good ways/tricks to scrape themarginalian.org, and extract the articles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156hp14", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690025505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, a question here, if I would like to scrape the entire themarginalian.org site, store it locally,  and built a chat bot out of it, what would be a good way to get started?\nI&amp;#39;m using Windows, and I&amp;#39;m open to any scraping suggestions \nright now I&amp;#39;m using the traditional HT Track to do the scraping\nWould be great if the web scraper can filter out all the garbage, and only return articles\nAny good ways to do it in wget?\nAny help appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "156hp14", "is_robot_indexable": true, "report_reasons": null, "author": "kaveinthran", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/156hp14/any_good_waystricks_to_scrape_themarginalianorg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/156hp14/any_good_waystricks_to_scrape_themarginalianorg/", "subreddit_subscribers": 693639, "created_utc": 1690025505.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to backup my backups. I can't seem to drag and drop my time machine backups to another external drive or NAS.", "author_fullname": "t2_e0g8qmrsg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time Machine back ups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156298q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689978319.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to backup my backups. I can&amp;#39;t seem to drag and drop my time machine backups to another external drive or NAS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "156298q", "is_robot_indexable": true, "report_reasons": null, "author": "Professional-Elk3586", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/156298q/time_machine_back_ups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/156298q/time_machine_back_ups/", "subreddit_subscribers": 693639, "created_utc": 1689978319.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All,\n\nUnsure if this is the correct place to ask but I've got some bits laying about wondering if I can make them work in whipping up a quick sever.\n\nI've got a windows server with an IBM M1015 in IT mode and a couple of Ultrastar SN200 SAS NVME ssds\n\nGrabbed a general cable and can see the drives powered up but not recognised on the controller.\n\nWill these work on this card or do I need to buy a different one?\n\nWould these even work as pass through storage or have to be used in RAID system? \n\nTIA", "author_fullname": "t2_837h2l2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156gu4b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690022783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;Unsure if this is the correct place to ask but I&amp;#39;ve got some bits laying about wondering if I can make them work in whipping up a quick sever.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a windows server with an IBM M1015 in IT mode and a couple of Ultrastar SN200 SAS NVME ssds&lt;/p&gt;\n\n&lt;p&gt;Grabbed a general cable and can see the drives powered up but not recognised on the controller.&lt;/p&gt;\n\n&lt;p&gt;Will these work on this card or do I need to buy a different one?&lt;/p&gt;\n\n&lt;p&gt;Would these even work as pass through storage or have to be used in RAID system? &lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "156gu4b", "is_robot_indexable": true, "report_reasons": null, "author": "thiccypickle", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/156gu4b/storage_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/156gu4b/storage_question/", "subreddit_subscribers": 693639, "created_utc": 1690022783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I got a new Hard Drive after my old one started failing. I was supposed to get a refurbished one so I installed WSL to ran badblocks, but it turns out that the refurbished drive already had 56k hours of use, and I can't risk it like that so I ended up returning it and got a smaller hard drive and was wondering if I should still run badblocks or if just running a long smartctl test would suffice. I'm fairly new at this and as I searched, I came across a lot of people saying that you should run badblocks, while others think it's overkill.\n\nAnd if it's advisable to run badblocks, is there an specific command type I should run? Preferably one that won't take too long since my old HDD can fail at any second.\n\nI was thinking of running badblocks -sw -b 4096 -c 512 /dev/sdd.\n\nAlso, sorry if this isn't the place to ask such questions. I still don't own a server but I'm planning to build one as soon as I got the money to, since it's one long time wish of mine, but HDD prices are at an all time high where I live.\n\nThanks in advance.", "author_fullname": "t2_ko9v8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running badblocks on a new drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156b2hd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690003631.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690003372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I got a new Hard Drive after my old one started failing. I was supposed to get a refurbished one so I installed WSL to ran badblocks, but it turns out that the refurbished drive already had 56k hours of use, and I can&amp;#39;t risk it like that so I ended up returning it and got a smaller hard drive and was wondering if I should still run badblocks or if just running a long smartctl test would suffice. I&amp;#39;m fairly new at this and as I searched, I came across a lot of people saying that you should run badblocks, while others think it&amp;#39;s overkill.&lt;/p&gt;\n\n&lt;p&gt;And if it&amp;#39;s advisable to run badblocks, is there an specific command type I should run? Preferably one that won&amp;#39;t take too long since my old HDD can fail at any second.&lt;/p&gt;\n\n&lt;p&gt;I was thinking of running badblocks -sw -b 4096 -c 512 /dev/sdd.&lt;/p&gt;\n\n&lt;p&gt;Also, sorry if this isn&amp;#39;t the place to ask such questions. I still don&amp;#39;t own a server but I&amp;#39;m planning to build one as soon as I got the money to, since it&amp;#39;s one long time wish of mine, but HDD prices are at an all time high where I live.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "156b2hd", "is_robot_indexable": true, "report_reasons": null, "author": "Ze_Alfredo_77", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/156b2hd/running_badblocks_on_a_new_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/156b2hd/running_badblocks_on_a_new_drive/", "subreddit_subscribers": 693639, "created_utc": 1690003372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am in the middle of building a new NAS and ordered a LSI LOGIC 9201-16E. I did not notice when I ordered it that the SAS connectors faced outside the case. If I get a riser cable that you would typically use when mounting a GPU can I use that to mount the SAS controller I got inside the case without losing too much bandwidth?", "author_fullname": "t2_7c8di59i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mount PCIE SAS controller inside case", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1566p1v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689990170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in the middle of building a new NAS and ordered a LSI LOGIC 9201-16E. I did not notice when I ordered it that the SAS connectors faced outside the case. If I get a riser cable that you would typically use when mounting a GPU can I use that to mount the SAS controller I got inside the case without losing too much bandwidth?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1566p1v", "is_robot_indexable": true, "report_reasons": null, "author": "CodMost7072", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1566p1v/mount_pcie_sas_controller_inside_case/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1566p1v/mount_pcie_sas_controller_inside_case/", "subreddit_subscribers": 693639, "created_utc": 1689990170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to connect MY EX2 Ultra to https://www.multcloud.com\n\nHi I just got a EX2 ultra and wanted to connected to https://www.multcloud.com. The three options that I think could work is connecting it via WebDAV, NAS or FTP. Only problem is I have very little knowledge in anything really server based. I'm not sure what server URL I'm supposed to use or what 3 would be the best option. If some one can help me out it would really mean a lot. have been trying to connect it every other day for about a week or two and I can\u2019t figure it out.", "author_fullname": "t2_8y4rqu6y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to connect MY EX2 Ultra to https://www.multcloud.com", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1562cv8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689978569.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to connect MY EX2 Ultra to &lt;a href=\"https://www.multcloud.com\"&gt;https://www.multcloud.com&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hi I just got a EX2 ultra and wanted to connected to &lt;a href=\"https://www.multcloud.com\"&gt;https://www.multcloud.com&lt;/a&gt;. The three options that I think could work is connecting it via WebDAV, NAS or FTP. Only problem is I have very little knowledge in anything really server based. I&amp;#39;m not sure what server URL I&amp;#39;m supposed to use or what 3 would be the best option. If some one can help me out it would really mean a lot. have been trying to connect it every other day for about a week or two and I can\u2019t figure it out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1562cv8", "is_robot_indexable": true, "report_reasons": null, "author": "Equivalent_Hawk_8718", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1562cv8/trying_to_connect_my_ex2_ultra_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1562cv8/trying_to_connect_my_ex2_ultra_to/", "subreddit_subscribers": 693639, "created_utc": 1689978569.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve 5x4Tb and 5x12 Tb disks available and a machine where I can plug all of those. The total SATA ports available are 12, with 2 nvme slots.\n\nThe biggest chunk of my data are photos, videos and office documents - It\u2019s 20 years of stuff - many files still scattered in a bunch of USB disks and burned DVDs. :) \n\nI\u2019d like also to backup some of my kids data on it.\n\nWhat would be a good layout solution to have some redundancy of the disks? I would like to survive to 2 disk failures, if possible.\n\nThe OS can be any flavor of Linux, This machine is not to be a real NAS - it will also be used eventually as a desktop. OS will run on a separate ssd or nvme. I\u2019d avoid Unraid/TrueNAS/Openmediavault installations.\n \nOne solution proposed was to make a ZFS layout like this: 2x4TB+2x12TB in pool, mirrored to another 2x4TB+12TB set. That would mean I will keep the remaining 4TB and 12TB as spares. Does it seem a reasonable solution? Any suggestions? I do not need to use ZFS, it was the first thing to come to mind.\n\nI am open to ideas!\n\nEDITED: not 10tb, but 12tb disks.", "author_fullname": "t2_5jccxw9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help laying out hard drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155wd6z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690013710.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689964927.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve 5x4Tb and 5x12 Tb disks available and a machine where I can plug all of those. The total SATA ports available are 12, with 2 nvme slots.&lt;/p&gt;\n\n&lt;p&gt;The biggest chunk of my data are photos, videos and office documents - It\u2019s 20 years of stuff - many files still scattered in a bunch of USB disks and burned DVDs. :) &lt;/p&gt;\n\n&lt;p&gt;I\u2019d like also to backup some of my kids data on it.&lt;/p&gt;\n\n&lt;p&gt;What would be a good layout solution to have some redundancy of the disks? I would like to survive to 2 disk failures, if possible.&lt;/p&gt;\n\n&lt;p&gt;The OS can be any flavor of Linux, This machine is not to be a real NAS - it will also be used eventually as a desktop. OS will run on a separate ssd or nvme. I\u2019d avoid Unraid/TrueNAS/Openmediavault installations.&lt;/p&gt;\n\n&lt;p&gt;One solution proposed was to make a ZFS layout like this: 2x4TB+2x12TB in pool, mirrored to another 2x4TB+12TB set. That would mean I will keep the remaining 4TB and 12TB as spares. Does it seem a reasonable solution? Any suggestions? I do not need to use ZFS, it was the first thing to come to mind.&lt;/p&gt;\n\n&lt;p&gt;I am open to ideas!&lt;/p&gt;\n\n&lt;p&gt;EDITED: not 10tb, but 12tb disks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155wd6z", "is_robot_indexable": true, "report_reasons": null, "author": "no-dupe", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155wd6z/help_laying_out_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155wd6z/help_laying_out_hard_drives/", "subreddit_subscribers": 693639, "created_utc": 1689964927.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "via live usb, did:\n\n$ sudo dd if=/dev/sda of=/run/media/usb3ext/w7a.img\n\nTime having passed, looking for recommends to:\n\n$ sudo dd if=/dev/sda of=/run/media/usb3ext/w7b.img\n\n... compare w7[a-b].img &amp; detect which if any files were deleted, added, changed; preferably as discrete operations.", "author_fullname": "t2_dfujb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Compare Subsequent Full Images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155vc6z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689962536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;via live usb, did:&lt;/p&gt;\n\n&lt;p&gt;$ sudo dd if=/dev/sda of=/run/media/usb3ext/w7a.img&lt;/p&gt;\n\n&lt;p&gt;Time having passed, looking for recommends to:&lt;/p&gt;\n\n&lt;p&gt;$ sudo dd if=/dev/sda of=/run/media/usb3ext/w7b.img&lt;/p&gt;\n\n&lt;p&gt;... compare w7[a-b].img &amp;amp; detect which if any files were deleted, added, changed; preferably as discrete operations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155vc6z", "is_robot_indexable": true, "report_reasons": null, "author": "zombi-roboto", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155vc6z/compare_subsequent_full_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155vc6z/compare_subsequent_full_images/", "subreddit_subscribers": 693639, "created_utc": 1689962536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After about 6 months of run time, one of my [WD HC530 14 TB HDDs](https://serverpartdeals.com/products/western-digital-ultrastar-dc-hc530-wuh721414ale6l4-0f31284-14tb-7-2k-rpm-sata-6gb-s-512e-512mb-3-5-se-manufacturer-recertified-hdd) is/was reporting `Current_Pending_Sector` errors. Since then, I have run a short SMART test successfully, and have been running an extended SMART test for the past ~5 days (unsure if this is a normal timeframe or not).\n\nThis drive is still under serverpartdeal's warranty. I have already opened an RMA for it. However, I am afraid to  return it because I am not certain that it is indeed failing. Wondering if anyone here has any input or advice on how to move forward? If I can run something else besides an extended SMART test to be certain that its failing (that hopefully doesnt take a ~week), that would be great. I have no issue with replacing the drive at the moment. I just dont want to return it, only to find out serverpartdeals found no issue with it. I want to be certain it is indeed on its way out before returning it. The reason for that is because of the following from server part deal's RMA process:\n\n&gt; Please note that all returned drives undergo in-house testing and inspection. Based on the results, we will send the replacement to the address on file.\n\nThank you!\n\nHere is the latest SMART report for the drive in question:\n\n\tsmartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.1.36-Unraid] (local build)\n\tCopyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n\n\t=== START OF INFORMATION SECTION ===\n\tModel Family:     Western Digital Ultrastar DC HC530\n\tDevice Model:     WDC  WUH721414ALE6L4\n\tSerial Number:    9JHDH26T\n\tLU WWN Device Id: 5 000cca 258d3c488\n\tFirmware Version: LDGNW2L0\n\tUser Capacity:    14,000,519,643,136 bytes [14.0 TB]\n\tSector Sizes:     512 bytes logical, 4096 bytes physical\n\tRotation Rate:    7200 rpm\n\tForm Factor:      3.5 inches\n\tDevice is:        In smartctl database 7.3/5440\n\tATA Version is:   ACS-2, ATA8-ACS T13/1699-D revision 4\n\tSATA Version is:  SATA 3.2, 6.0 Gb/s (current: 6.0 Gb/s)\n\tLocal Time is:    Fri Jul 21 13:23:51 2023 EDT\n\tSMART support is: Available - device has SMART capability.\n\tSMART support is: Enabled\n\tAAM feature is:   Unavailable\n\tAPM feature is:   Disabled\n\tRd look-ahead is: Enabled\n\tWrite cache is:   Enabled\n\tDSN feature is:   Unavailable\n\tATA Security is:  Disabled, frozen [SEC2]\n\tWt Cache Reorder: Enabled\n\n\t=== START OF READ SMART DATA SECTION ===\n\tSMART overall-health self-assessment test result: PASSED\n\n\tGeneral SMART Values:\n\tOffline data collection status:  (0x84)\tOffline data collection activity\n\t\t\t\t\t\twas suspended by an interrupting command from host.\n\t\t\t\t\t\tAuto Offline Data Collection: Enabled.\n\tSelf-test execution status:      ( 241)\tSelf-test routine in progress...\n\t\t\t\t\t\t10% of test remaining.\n\tTotal time to complete Offline \n\tdata collection: \t\t(  101) seconds.\n\tOffline data collection\n\tcapabilities: \t\t\t (0x5b) SMART execute Offline immediate.\n\t\t\t\t\t\tAuto Offline data collection on/off support.\n\t\t\t\t\t\tSuspend Offline collection upon new\n\t\t\t\t\t\tcommand.\n\t\t\t\t\t\tOffline surface scan supported.\n\t\t\t\t\t\tSelf-test supported.\n\t\t\t\t\t\tNo Conveyance Self-test supported.\n\t\t\t\t\t\tSelective Self-test supported.\n\tSMART capabilities:            (0x0003)\tSaves SMART data before entering\n\t\t\t\t\t\tpower-saving mode.\n\t\t\t\t\t\tSupports SMART auto save timer.\n\tError logging capability:        (0x01)\tError logging supported.\n\t\t\t\t\t\tGeneral Purpose Logging supported.\n\tShort self-test routine \n\trecommended polling time: \t (   2) minutes.\n\tExtended self-test routine\n\trecommended polling time: \t (1434) minutes.\n\tSCT capabilities: \t       (0x003d)\tSCT Status supported.\n\t\t\t\t\t\tSCT Error Recovery Control supported.\n\t\t\t\t\t\tSCT Feature Control supported.\n\t\t\t\t\t\tSCT Data Table supported.\n\n\tSMART Attributes Data Structure revision number: 16\n\tVendor Specific SMART Attributes with Thresholds:\n\tID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE\n\t  1 Raw_Read_Error_Rate     PO-R--   100   100   001    -    0\n\t  2 Throughput_Performance  P-S---   137   137   054    -    92\n\t  3 Spin_Up_Time            POS---   084   084   001    -    332 (Average 330)\n\t  4 Start_Stop_Count        -O--C-   100   100   000    -    46\n\t  5 Reallocated_Sector_Ct   PO--CK   100   100   001    -    0\n\t  7 Seek_Error_Rate         PO-R--   100   100   001    -    0\n\t  8 Seek_Time_Performance   P-S---   128   128   020    -    18\n\t  9 Power_On_Hours          -O--C-   100   100   000    -    4490\n\t 10 Spin_Retry_Count        PO--C-   100   100   001    -    0\n\t 12 Power_Cycle_Count       -O--CK   100   100   000    -    43\n\t 22 Helium_Level            PO---K   100   100   025    -    100\n\t192 Power-Off_Retract_Count -O--CK   100   100   000    -    195\n\t193 Load_Cycle_Count        -O--C-   100   100   000    -    195\n\t194 Temperature_Celsius     -O----   058   058   000    -    36 (Min/Max 14/49)\n\t196 Reallocated_Event_Count -O--CK   100   100   000    -    0\n\t197 Current_Pending_Sector  -O---K   100   100   000    -    0\n\t198 Offline_Uncorrectable   ---R--   100   100   000    -    0\n\t199 UDMA_CRC_Error_Count    -O-R--   100   100   000    -    0\n\t\t\t\t\t\t\t\t||||||_ K auto-keep\n\t\t\t\t\t\t\t\t|||||__ C event count\n\t\t\t\t\t\t\t\t||||___ R error rate\n\t\t\t\t\t\t\t\t|||____ S speed/performance\n\t\t\t\t\t\t\t\t||_____ O updated online\n\t\t\t\t\t\t\t\t|______ P prefailure warning\n\n\tGeneral Purpose Log Directory Version 1\n\tSMART           Log Directory Version 1 [multi-sector log support]\n\tAddress    Access  R/W   Size  Description\n\t0x00       GPL,SL  R/O      1  Log Directory\n\t0x01           SL  R/O      1  Summary SMART error log\n\t0x02           SL  R/O      1  Comprehensive SMART error log\n\t0x03       GPL     R/O      1  Ext. Comprehensive SMART error log\n\t0x04       GPL     R/O    256  Device Statistics log\n\t0x04       SL      R/O    255  Device Statistics log\n\t0x06           SL  R/O      1  SMART self-test log\n\t0x07       GPL     R/O      1  Extended self-test log\n\t0x08       GPL     R/O      2  Power Conditions log\n\t0x09           SL  R/W      1  Selective self-test log\n\t0x0c       GPL     R/O   5501  Pending Defects log\n\t0x10       GPL     R/O      1  NCQ Command Error log\n\t0x11       GPL     R/O      1  SATA Phy Event Counters log\n\t0x12       GPL     R/O      1  SATA NCQ Non-Data log\n\t0x13       GPL     R/O      1  SATA NCQ Send and Receive log\n\t0x15       GPL     R/W      1  Rebuild Assist log\n\t0x21       GPL     R/O      1  Write stream error log\n\t0x22       GPL     R/O      1  Read stream error log\n\t0x24       GPL     R/O    256  Current Device Internal Status Data log\n\t0x25       GPL     R/O    256  Saved Device Internal Status Data log\n\t0x2f       GPL     -        1  Set Sector Configuration\n\t0x30       GPL,SL  R/O      9  IDENTIFY DEVICE data log\n\t0x80-0x9f  GPL,SL  R/W     16  Host vendor specific log\n\t0xe0       GPL,SL  R/W      1  SCT Command/Status\n\t0xe1       GPL,SL  R/W      1  SCT Data Transfer\n\n\tSMART Extended Comprehensive Error Log Version: 1 (1 sectors)\n\tDevice Error Count: 6 (device log contains only the most recent 4 errors)\n\t\tCR     = Command Register\n\t\tFEATR  = Features Register\n\t\tCOUNT  = Count (was: Sector Count) Register\n\t\tLBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n\t\tLH     = LBA High (was: Cylinder High) Register    ]   LBA\n\t\tLM     = LBA Mid (was: Cylinder Low) Register      ] Register\n\t\tLL     = LBA Low (was: Sector Number) Register     ]\n\t\tDV     = Device (was: Device/Head) Register\n\t\tDC     = Device Control Register\n\t\tER     = Error register\n\t\tST     = Status register\n\tPowered_Up_Time is measured from power on, and printed as\n\tDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\n\tSS=sec, and sss=millisec. It \"wraps\" after 49.710 days.\n\n\tError 6 [1] occurred at disk power-on lifetime: 4391 hours (182 days + 23 hours)\n\t  When the command that caused the error occurred, the device was doing SMART Offline or Self-test.\n\n\t  After command completion occurred, registers were:\n\t  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n\t  -- -- -- == -- == == == -- -- -- -- --\n\t  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n\t  Commands leading to the command that caused the error were:\n\t  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n\t  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n\t  60 05 40 00 00 00 06 4a 43 9f 30 40 08  4d+00:40:26.202  READ FPDMA QUEUED\n\t  61 00 08 00 48 00 00 81 29 50 70 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\t  61 00 20 00 40 00 00 81 1f 06 d8 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\t  61 00 20 00 38 00 00 81 1e ac 38 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\t  61 00 20 00 30 00 00 81 17 06 d8 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\n\tError 5 [0] occurred at disk power-on lifetime: 4391 hours (182 days + 23 hours)\n\t  When the command that caused the error occurred, the device was doing SMART Offline or Self-test.\n\n\t  After command completion occurred, registers were:\n\t  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n\t  -- -- -- == -- == == == -- -- -- -- --\n\t  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n\t  Commands leading to the command that caused the error were:\n\t  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n\t  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n\t  60 05 40 00 b8 00 06 4a 43 9f 30 40 08  4d+00:40:17.853  READ FPDMA QUEUED\n\t  60 05 40 00 f8 00 06 4a 43 b4 b0 40 08  4d+00:40:17.853  READ FPDMA QUEUED\n\t  60 01 60 00 e0 00 06 4a 43 b3 50 40 08  4d+00:40:17.852  READ FPDMA QUEUED\n\t  60 01 00 00 d8 00 06 4a 43 b2 50 40 08  4d+00:40:14.958  READ FPDMA QUEUED\n\t  60 03 60 00 d0 00 06 4a 43 ae f0 40 08  4d+00:40:14.948  READ FPDMA QUEUED\n\n\tError 4 [3] occurred at disk power-on lifetime: 4364 hours (181 days + 20 hours)\n\t  When the command that caused the error occurred, the device was active or idle.\n\n\t  After command completion occurred, registers were:\n\t  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n\t  -- -- -- == -- == == == -- -- -- -- --\n\t  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n\t  Commands leading to the command that caused the error were:\n\t  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n\t  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n\t  60 05 40 00 a8 00 06 4a 43 a0 f0 40 08  2d+21:40:35.149  READ FPDMA QUEUED\n\t  60 02 40 00 a0 00 06 4a 43 a6 30 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\t  60 05 40 00 78 00 06 4a 43 a8 70 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\t  60 03 f8 00 70 00 06 4a 43 ad b0 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\t  60 03 98 00 68 00 06 4a 43 b1 a8 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\n\tError 3 [2] occurred at disk power-on lifetime: 4364 hours (181 days + 20 hours)\n\t  When the command that caused the error occurred, the device was active or idle.\n\n\t  After command completion occurred, registers were:\n\t  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n\t  -- -- -- == -- == == == -- -- -- -- --\n\t  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n\t  Commands leading to the command that caused the error were:\n\t  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n\t  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n\t  60 05 40 00 60 00 06 4a 43 a0 f0 40 08  2d+21:40:16.224  READ FPDMA QUEUED\n\t  60 04 b0 00 88 00 06 4a 43 b5 40 40 08  2d+21:40:16.224  READ FPDMA QUEUED\n\t  60 03 98 00 80 00 06 4a 43 b1 a8 40 08  2d+21:40:13.337  READ FPDMA QUEUED\n\t  60 03 f8 00 78 00 06 4a 43 ad b0 40 08  2d+21:40:13.336  READ FPDMA QUEUED\n\t  60 05 40 00 70 00 06 4a 43 a8 70 40 08  2d+21:40:13.331  READ FPDMA QUEUED\n\n\tSMART Extended Self-test Log Version: 1 (1 sectors)\n\tNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n\t# 1  Short offline       Completed without error       00%      4393         -\n\t# 2  Short offline       Completed: read failure       90%      4364         27015749936\n\t# 3  Short offline       Completed: read failure       10%      4344         27015749936\n\t# 4  Short offline       Completed: read failure       90%      4342         27015749936\n\t# 5  Extended offline    Completed: read failure       90%      4342         27015749936\n\t# 6  Extended offline    Completed: read failure       90%      4342         27015749936\n\t# 7  Short offline       Completed without error       00%         0         -\n\n\tSMART Selective self-test log data structure revision number 1\n\t SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n\t\t1        0        0  Not_testing\n\t\t2        0        0  Not_testing\n\t\t3        0        0  Not_testing\n\t\t4        0        0  Not_testing\n\t\t5        0        0  Not_testing\n\tSelective self-test flags (0x0):\n\t  After scanning selected spans, do NOT read-scan remainder of disk.\n\tIf Selective self-test is pending on power-up, resume after 0 minute delay.\n\n\tSCT Status Version:                  3\n\tSCT Version (vendor specific):       256 (0x0100)\n\tDevice State:                        DST executing in background (3)\n\tCurrent Temperature:                    36 Celsius\n\tPower Cycle Min/Max Temperature:     33/39 Celsius\n\tLifetime    Min/Max Temperature:     14/49 Celsius\n\tUnder/Over Temperature Limit Count:   0/0\n\tSMART Status:                        0xc24f (PASSED)\n\tMinimum supported ERC Time Limit:    65 (6.5 seconds)\n\n\tSCT Temperature History Version:     2\n\tTemperature Sampling Period:         1 minute\n\tTemperature Logging Interval:        1 minute\n\tMin/Max recommended Temperature:      0/60 Celsius\n\tMin/Max Temperature Limit:           -40/70 Celsius\n\tTemperature History Size (Index):    128 (74)\n\n\tIndex    Estimated Time   Temperature Celsius\n\t  75    2023-07-21 11:16    36  *****************\n\t ...    ..(126 skipped).    ..  *****************\n\t  74    2023-07-21 13:23    36  *****************\n\n\tSCT Error Recovery Control:\n\t\t\t   Read: Disabled\n\t\t\t  Write: Disabled\n\n\tDevice Statistics (GP Log 0x04)\n\tPage  Offset Size        Value Flags Description\n\t0x01  =====  =               =  ===  == General Statistics (rev 1) ==\n\t0x01  0x008  4              43  ---  Lifetime Power-On Resets\n\t0x01  0x010  4            4490  ---  Power-on Hours\n\t0x01  0x018  6     19732537033  ---  Logical Sectors Written\n\t0x01  0x020  6        67404062  ---  Number of Write Commands\n\t0x01  0x028  6    486533164956  ---  Logical Sectors Read\n\t0x01  0x030  6      2489352044  ---  Number of Read Commands\n\t0x01  0x038  6     16167059850  ---  Date and Time TimeStamp\n\t0x03  =====  =               =  ===  == Rotating Media Statistics (rev 1) ==\n\t0x03  0x008  4            4470  ---  Spindle Motor Power-on Hours\n\t0x03  0x010  4            4470  ---  Head Flying Hours\n\t0x03  0x018  4             195  ---  Head Load Events\n\t0x03  0x020  4               0  ---  Number of Reallocated Logical Sectors\n\t0x03  0x028  4              75  ---  Read Recovery Attempts\n\t0x03  0x030  4               1  ---  Number of Mechanical Start Failures\n\t0x04  =====  =               =  ===  == General Errors Statistics (rev 1) ==\n\t0x04  0x008  4               6  ---  Number of Reported Uncorrectable Errors\n\t0x04  0x010  4               0  ---  Resets Between Cmd Acceptance and Completion\n\t0x04  0x018  4               0  ---  Physical Element Status Changed\n\t0x05  =====  =               =  ===  == Temperature Statistics (rev 1) ==\n\t0x05  0x008  1              36  ---  Current Temperature\n\t0x05  0x010  1              36  N--  Average Short Term Temperature\n\t0x05  0x018  1              35  N--  Average Long Term Temperature\n\t0x05  0x020  1              49  ---  Highest Temperature\n\t0x05  0x028  1              14  ---  Lowest Temperature\n\t0x05  0x030  1              47  N--  Highest Average Short Term Temperature\n\t0x05  0x038  1              21  N--  Lowest Average Short Term Temperature\n\t0x05  0x040  1              46  N--  Highest Average Long Term Temperature\n\t0x05  0x048  1              23  N--  Lowest Average Long Term Temperature\n\t0x05  0x050  4               0  ---  Time in Over-Temperature\n\t0x05  0x058  1              60  ---  Specified Maximum Operating Temperature\n\t0x05  0x060  4               0  ---  Time in Under-Temperature\n\t0x05  0x068  1               0  ---  Specified Minimum Operating Temperature\n\t0x06  =====  =               =  ===  == Transport Statistics (rev 1) ==\n\t0x06  0x008  4             205  ---  Number of Hardware Resets\n\t0x06  0x010  4              44  ---  Number of ASR Events\n\t0x06  0x018  4               0  ---  Number of Interface CRC Errors\n\t0xff  =====  =               =  ===  == Vendor Specific Statistics (rev 1) ==\n\t\t\t\t\t\t\t\t\t|||_ C monitored condition met\n\t\t\t\t\t\t\t\t\t||__ D supports DSN\n\t\t\t\t\t\t\t\t\t|___ N normalized value\n\n\tPending Defects log (GP Log 0x0c)\n\tNo Defects Logged\n\n\tSATA Phy Event Counters (GP Log 0x11)\n\tID      Size     Value  Description\n\t0x0001  2            0  Command failed due to ICRC error\n\t0x0002  2            1  R_ERR response for data FIS\n\t0x0003  2            1  R_ERR response for device-to-host data FIS\n\t0x0004  2            0  R_ERR response for host-to-device data FIS\n\t0x0005  2            0  R_ERR response for non-data FIS\n\t0x0006  2            0  R_ERR response for device-to-host non-data FIS\n\t0x0007  2            0  R_ERR response for host-to-device non-data FIS\n\t0x0008  2            0  Device-to-host non-data FIS retries\n\t0x0009  2           16  Transition from drive PhyRdy to drive PhyNRdy\n\t0x000a  2           13  Device-to-host register FISes sent due to a COMRESET\n\t0x000b  2            0  CRC errors within host-to-device FIS\n\t0x000d  2            0  Non-CRC errors within host-to-device FIS", "author_fullname": "t2_4t3a1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Failing Drive? ServerPartDeals Return Process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155ugtf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689960767.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689960560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After about 6 months of run time, one of my &lt;a href=\"https://serverpartdeals.com/products/western-digital-ultrastar-dc-hc530-wuh721414ale6l4-0f31284-14tb-7-2k-rpm-sata-6gb-s-512e-512mb-3-5-se-manufacturer-recertified-hdd\"&gt;WD HC530 14 TB HDDs&lt;/a&gt; is/was reporting &lt;code&gt;Current_Pending_Sector&lt;/code&gt; errors. Since then, I have run a short SMART test successfully, and have been running an extended SMART test for the past ~5 days (unsure if this is a normal timeframe or not).&lt;/p&gt;\n\n&lt;p&gt;This drive is still under serverpartdeal&amp;#39;s warranty. I have already opened an RMA for it. However, I am afraid to  return it because I am not certain that it is indeed failing. Wondering if anyone here has any input or advice on how to move forward? If I can run something else besides an extended SMART test to be certain that its failing (that hopefully doesnt take a ~week), that would be great. I have no issue with replacing the drive at the moment. I just dont want to return it, only to find out serverpartdeals found no issue with it. I want to be certain it is indeed on its way out before returning it. The reason for that is because of the following from server part deal&amp;#39;s RMA process:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Please note that all returned drives undergo in-house testing and inspection. Based on the results, we will send the replacement to the address on file.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;Here is the latest SMART report for the drive in question:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;smartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.1.36-Unraid] (local build)\nCopyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nModel Family:     Western Digital Ultrastar DC HC530\nDevice Model:     WDC  WUH721414ALE6L4\nSerial Number:    9JHDH26T\nLU WWN Device Id: 5 000cca 258d3c488\nFirmware Version: LDGNW2L0\nUser Capacity:    14,000,519,643,136 bytes [14.0 TB]\nSector Sizes:     512 bytes logical, 4096 bytes physical\nRotation Rate:    7200 rpm\nForm Factor:      3.5 inches\nDevice is:        In smartctl database 7.3/5440\nATA Version is:   ACS-2, ATA8-ACS T13/1699-D revision 4\nSATA Version is:  SATA 3.2, 6.0 Gb/s (current: 6.0 Gb/s)\nLocal Time is:    Fri Jul 21 13:23:51 2023 EDT\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\nAAM feature is:   Unavailable\nAPM feature is:   Disabled\nRd look-ahead is: Enabled\nWrite cache is:   Enabled\nDSN feature is:   Unavailable\nATA Security is:  Disabled, frozen [SEC2]\nWt Cache Reorder: Enabled\n\n=== START OF READ SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nGeneral SMART Values:\nOffline data collection status:  (0x84) Offline data collection activity\n                    was suspended by an interrupting command from host.\n                    Auto Offline Data Collection: Enabled.\nSelf-test execution status:      ( 241) Self-test routine in progress...\n                    10% of test remaining.\nTotal time to complete Offline \ndata collection:        (  101) seconds.\nOffline data collection\ncapabilities:            (0x5b) SMART execute Offline immediate.\n                    Auto Offline data collection on/off support.\n                    Suspend Offline collection upon new\n                    command.\n                    Offline surface scan supported.\n                    Self-test supported.\n                    No Conveyance Self-test supported.\n                    Selective Self-test supported.\nSMART capabilities:            (0x0003) Saves SMART data before entering\n                    power-saving mode.\n                    Supports SMART auto save timer.\nError logging capability:        (0x01) Error logging supported.\n                    General Purpose Logging supported.\nShort self-test routine \nrecommended polling time:    (   2) minutes.\nExtended self-test routine\nrecommended polling time:    (1434) minutes.\nSCT capabilities:          (0x003d) SCT Status supported.\n                    SCT Error Recovery Control supported.\n                    SCT Feature Control supported.\n                    SCT Data Table supported.\n\nSMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE\n  1 Raw_Read_Error_Rate     PO-R--   100   100   001    -    0\n  2 Throughput_Performance  P-S---   137   137   054    -    92\n  3 Spin_Up_Time            POS---   084   084   001    -    332 (Average 330)\n  4 Start_Stop_Count        -O--C-   100   100   000    -    46\n  5 Reallocated_Sector_Ct   PO--CK   100   100   001    -    0\n  7 Seek_Error_Rate         PO-R--   100   100   001    -    0\n  8 Seek_Time_Performance   P-S---   128   128   020    -    18\n  9 Power_On_Hours          -O--C-   100   100   000    -    4490\n 10 Spin_Retry_Count        PO--C-   100   100   001    -    0\n 12 Power_Cycle_Count       -O--CK   100   100   000    -    43\n 22 Helium_Level            PO---K   100   100   025    -    100\n192 Power-Off_Retract_Count -O--CK   100   100   000    -    195\n193 Load_Cycle_Count        -O--C-   100   100   000    -    195\n194 Temperature_Celsius     -O----   058   058   000    -    36 (Min/Max 14/49)\n196 Reallocated_Event_Count -O--CK   100   100   000    -    0\n197 Current_Pending_Sector  -O---K   100   100   000    -    0\n198 Offline_Uncorrectable   ---R--   100   100   000    -    0\n199 UDMA_CRC_Error_Count    -O-R--   100   100   000    -    0\n                            ||||||_ K auto-keep\n                            |||||__ C event count\n                            ||||___ R error rate\n                            |||____ S speed/performance\n                            ||_____ O updated online\n                            |______ P prefailure warning\n\nGeneral Purpose Log Directory Version 1\nSMART           Log Directory Version 1 [multi-sector log support]\nAddress    Access  R/W   Size  Description\n0x00       GPL,SL  R/O      1  Log Directory\n0x01           SL  R/O      1  Summary SMART error log\n0x02           SL  R/O      1  Comprehensive SMART error log\n0x03       GPL     R/O      1  Ext. Comprehensive SMART error log\n0x04       GPL     R/O    256  Device Statistics log\n0x04       SL      R/O    255  Device Statistics log\n0x06           SL  R/O      1  SMART self-test log\n0x07       GPL     R/O      1  Extended self-test log\n0x08       GPL     R/O      2  Power Conditions log\n0x09           SL  R/W      1  Selective self-test log\n0x0c       GPL     R/O   5501  Pending Defects log\n0x10       GPL     R/O      1  NCQ Command Error log\n0x11       GPL     R/O      1  SATA Phy Event Counters log\n0x12       GPL     R/O      1  SATA NCQ Non-Data log\n0x13       GPL     R/O      1  SATA NCQ Send and Receive log\n0x15       GPL     R/W      1  Rebuild Assist log\n0x21       GPL     R/O      1  Write stream error log\n0x22       GPL     R/O      1  Read stream error log\n0x24       GPL     R/O    256  Current Device Internal Status Data log\n0x25       GPL     R/O    256  Saved Device Internal Status Data log\n0x2f       GPL     -        1  Set Sector Configuration\n0x30       GPL,SL  R/O      9  IDENTIFY DEVICE data log\n0x80-0x9f  GPL,SL  R/W     16  Host vendor specific log\n0xe0       GPL,SL  R/W      1  SCT Command/Status\n0xe1       GPL,SL  R/W      1  SCT Data Transfer\n\nSMART Extended Comprehensive Error Log Version: 1 (1 sectors)\nDevice Error Count: 6 (device log contains only the most recent 4 errors)\n    CR     = Command Register\n    FEATR  = Features Register\n    COUNT  = Count (was: Sector Count) Register\n    LBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n    LH     = LBA High (was: Cylinder High) Register    ]   LBA\n    LM     = LBA Mid (was: Cylinder Low) Register      ] Register\n    LL     = LBA Low (was: Sector Number) Register     ]\n    DV     = Device (was: Device/Head) Register\n    DC     = Device Control Register\n    ER     = Error register\n    ST     = Status register\nPowered_Up_Time is measured from power on, and printed as\nDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\nSS=sec, and sss=millisec. It &amp;quot;wraps&amp;quot; after 49.710 days.\n\nError 6 [1] occurred at disk power-on lifetime: 4391 hours (182 days + 23 hours)\n  When the command that caused the error occurred, the device was doing SMART Offline or Self-test.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 05 40 00 00 00 06 4a 43 9f 30 40 08  4d+00:40:26.202  READ FPDMA QUEUED\n  61 00 08 00 48 00 00 81 29 50 70 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n  61 00 20 00 40 00 00 81 1f 06 d8 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n  61 00 20 00 38 00 00 81 1e ac 38 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n  61 00 20 00 30 00 00 81 17 06 d8 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\nError 5 [0] occurred at disk power-on lifetime: 4391 hours (182 days + 23 hours)\n  When the command that caused the error occurred, the device was doing SMART Offline or Self-test.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 05 40 00 b8 00 06 4a 43 9f 30 40 08  4d+00:40:17.853  READ FPDMA QUEUED\n  60 05 40 00 f8 00 06 4a 43 b4 b0 40 08  4d+00:40:17.853  READ FPDMA QUEUED\n  60 01 60 00 e0 00 06 4a 43 b3 50 40 08  4d+00:40:17.852  READ FPDMA QUEUED\n  60 01 00 00 d8 00 06 4a 43 b2 50 40 08  4d+00:40:14.958  READ FPDMA QUEUED\n  60 03 60 00 d0 00 06 4a 43 ae f0 40 08  4d+00:40:14.948  READ FPDMA QUEUED\n\nError 4 [3] occurred at disk power-on lifetime: 4364 hours (181 days + 20 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 05 40 00 a8 00 06 4a 43 a0 f0 40 08  2d+21:40:35.149  READ FPDMA QUEUED\n  60 02 40 00 a0 00 06 4a 43 a6 30 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n  60 05 40 00 78 00 06 4a 43 a8 70 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n  60 03 f8 00 70 00 06 4a 43 ad b0 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n  60 03 98 00 68 00 06 4a 43 b1 a8 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\nError 3 [2] occurred at disk power-on lifetime: 4364 hours (181 days + 20 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 05 40 00 60 00 06 4a 43 a0 f0 40 08  2d+21:40:16.224  READ FPDMA QUEUED\n  60 04 b0 00 88 00 06 4a 43 b5 40 40 08  2d+21:40:16.224  READ FPDMA QUEUED\n  60 03 98 00 80 00 06 4a 43 b1 a8 40 08  2d+21:40:13.337  READ FPDMA QUEUED\n  60 03 f8 00 78 00 06 4a 43 ad b0 40 08  2d+21:40:13.336  READ FPDMA QUEUED\n  60 05 40 00 70 00 06 4a 43 a8 70 40 08  2d+21:40:13.331  READ FPDMA QUEUED\n\nSMART Extended Self-test Log Version: 1 (1 sectors)\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Short offline       Completed without error       00%      4393         -\n# 2  Short offline       Completed: read failure       90%      4364         27015749936\n# 3  Short offline       Completed: read failure       10%      4344         27015749936\n# 4  Short offline       Completed: read failure       90%      4342         27015749936\n# 5  Extended offline    Completed: read failure       90%      4342         27015749936\n# 6  Extended offline    Completed: read failure       90%      4342         27015749936\n# 7  Short offline       Completed without error       00%         0         -\n\nSMART Selective self-test log data structure revision number 1\n SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n    1        0        0  Not_testing\n    2        0        0  Not_testing\n    3        0        0  Not_testing\n    4        0        0  Not_testing\n    5        0        0  Not_testing\nSelective self-test flags (0x0):\n  After scanning selected spans, do NOT read-scan remainder of disk.\nIf Selective self-test is pending on power-up, resume after 0 minute delay.\n\nSCT Status Version:                  3\nSCT Version (vendor specific):       256 (0x0100)\nDevice State:                        DST executing in background (3)\nCurrent Temperature:                    36 Celsius\nPower Cycle Min/Max Temperature:     33/39 Celsius\nLifetime    Min/Max Temperature:     14/49 Celsius\nUnder/Over Temperature Limit Count:   0/0\nSMART Status:                        0xc24f (PASSED)\nMinimum supported ERC Time Limit:    65 (6.5 seconds)\n\nSCT Temperature History Version:     2\nTemperature Sampling Period:         1 minute\nTemperature Logging Interval:        1 minute\nMin/Max recommended Temperature:      0/60 Celsius\nMin/Max Temperature Limit:           -40/70 Celsius\nTemperature History Size (Index):    128 (74)\n\nIndex    Estimated Time   Temperature Celsius\n  75    2023-07-21 11:16    36  *****************\n ...    ..(126 skipped).    ..  *****************\n  74    2023-07-21 13:23    36  *****************\n\nSCT Error Recovery Control:\n           Read: Disabled\n          Write: Disabled\n\nDevice Statistics (GP Log 0x04)\nPage  Offset Size        Value Flags Description\n0x01  =====  =               =  ===  == General Statistics (rev 1) ==\n0x01  0x008  4              43  ---  Lifetime Power-On Resets\n0x01  0x010  4            4490  ---  Power-on Hours\n0x01  0x018  6     19732537033  ---  Logical Sectors Written\n0x01  0x020  6        67404062  ---  Number of Write Commands\n0x01  0x028  6    486533164956  ---  Logical Sectors Read\n0x01  0x030  6      2489352044  ---  Number of Read Commands\n0x01  0x038  6     16167059850  ---  Date and Time TimeStamp\n0x03  =====  =               =  ===  == Rotating Media Statistics (rev 1) ==\n0x03  0x008  4            4470  ---  Spindle Motor Power-on Hours\n0x03  0x010  4            4470  ---  Head Flying Hours\n0x03  0x018  4             195  ---  Head Load Events\n0x03  0x020  4               0  ---  Number of Reallocated Logical Sectors\n0x03  0x028  4              75  ---  Read Recovery Attempts\n0x03  0x030  4               1  ---  Number of Mechanical Start Failures\n0x04  =====  =               =  ===  == General Errors Statistics (rev 1) ==\n0x04  0x008  4               6  ---  Number of Reported Uncorrectable Errors\n0x04  0x010  4               0  ---  Resets Between Cmd Acceptance and Completion\n0x04  0x018  4               0  ---  Physical Element Status Changed\n0x05  =====  =               =  ===  == Temperature Statistics (rev 1) ==\n0x05  0x008  1              36  ---  Current Temperature\n0x05  0x010  1              36  N--  Average Short Term Temperature\n0x05  0x018  1              35  N--  Average Long Term Temperature\n0x05  0x020  1              49  ---  Highest Temperature\n0x05  0x028  1              14  ---  Lowest Temperature\n0x05  0x030  1              47  N--  Highest Average Short Term Temperature\n0x05  0x038  1              21  N--  Lowest Average Short Term Temperature\n0x05  0x040  1              46  N--  Highest Average Long Term Temperature\n0x05  0x048  1              23  N--  Lowest Average Long Term Temperature\n0x05  0x050  4               0  ---  Time in Over-Temperature\n0x05  0x058  1              60  ---  Specified Maximum Operating Temperature\n0x05  0x060  4               0  ---  Time in Under-Temperature\n0x05  0x068  1               0  ---  Specified Minimum Operating Temperature\n0x06  =====  =               =  ===  == Transport Statistics (rev 1) ==\n0x06  0x008  4             205  ---  Number of Hardware Resets\n0x06  0x010  4              44  ---  Number of ASR Events\n0x06  0x018  4               0  ---  Number of Interface CRC Errors\n0xff  =====  =               =  ===  == Vendor Specific Statistics (rev 1) ==\n                                |||_ C monitored condition met\n                                ||__ D supports DSN\n                                |___ N normalized value\n\nPending Defects log (GP Log 0x0c)\nNo Defects Logged\n\nSATA Phy Event Counters (GP Log 0x11)\nID      Size     Value  Description\n0x0001  2            0  Command failed due to ICRC error\n0x0002  2            1  R_ERR response for data FIS\n0x0003  2            1  R_ERR response for device-to-host data FIS\n0x0004  2            0  R_ERR response for host-to-device data FIS\n0x0005  2            0  R_ERR response for non-data FIS\n0x0006  2            0  R_ERR response for device-to-host non-data FIS\n0x0007  2            0  R_ERR response for host-to-device non-data FIS\n0x0008  2            0  Device-to-host non-data FIS retries\n0x0009  2           16  Transition from drive PhyRdy to drive PhyNRdy\n0x000a  2           13  Device-to-host register FISes sent due to a COMRESET\n0x000b  2            0  CRC errors within host-to-device FIS\n0x000d  2            0  Non-CRC errors within host-to-device FIS\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?auto=webp&amp;s=fb62a62917a4cd681adfb2ed102d12b3bffec423", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e680cc615c3529565359d358068446dc7a749710", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4cfc8914fb31e963b8c976aa90a4c5dc0bee375", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=391fbea0666df1013ebe77e5378d9f0a44d734d8", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec56cfb30b2d32af9c4e3eed80d13ab5050b0c85", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=09986a2c305d87e6ce0985806f324daf8a8eef2c", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=294710c01d4d85ae44879409901f4555b571eb26", "width": 1080, "height": 1080}], "variants": {}, "id": "BM5_QdAyNCLTMrgIxGNUsuyqSJL4D1S83f-YL0tXUE0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155ugtf", "is_robot_indexable": true, "report_reasons": null, "author": "halexh", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155ugtf/failing_drive_serverpartdeals_return_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155ugtf/failing_drive_serverpartdeals_return_process/", "subreddit_subscribers": 693639, "created_utc": 1689960560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know of any good 2D/3D file visualizers for Windows?\n\nI\u2019m revamping my file system, and I\u2019d love something that visualizes file organization in 2D or 3D space. Preferably something customizable. \n\nWinDirStat does it at a completely functional level, but I was hoping for something a little slicker that would allow me to arrange and organize my collections of files in a \u201croom\u201d of some sort, rather than a directory tree. Something of a virtual bookshelf or virtual library, but for my entire filing system. The organization would simply visualize the existing dir structure, not add its own.\n\nAnyone know of anything similar?", "author_fullname": "t2_zxdz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Virtual file visualizer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155quin", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689952476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of any good 2D/3D file visualizers for Windows?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m revamping my file system, and I\u2019d love something that visualizes file organization in 2D or 3D space. Preferably something customizable. &lt;/p&gt;\n\n&lt;p&gt;WinDirStat does it at a completely functional level, but I was hoping for something a little slicker that would allow me to arrange and organize my collections of files in a \u201croom\u201d of some sort, rather than a directory tree. Something of a virtual bookshelf or virtual library, but for my entire filing system. The organization would simply visualize the existing dir structure, not add its own.&lt;/p&gt;\n\n&lt;p&gt;Anyone know of anything similar?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155quin", "is_robot_indexable": true, "report_reasons": null, "author": "CCMadman", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155quin/virtual_file_visualizer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155quin/virtual_file_visualizer/", "subreddit_subscribers": 693639, "created_utc": 1689952476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Very new to NAS and Nextcloud;\nI am looking to create a NAS with my PI 4, to replace Google Photos for my family; and Nextcloud seems to be the best option with their iOS app.\n\nHowever, I would also like to access my files on my local network, and one of my smart home camera provider has the option to back up to a NAS, but I\u2019m not sure if Nextccloud will expose my Pi as a NAS, or would I have to install Samba on top?\n\nAny other recommendations will be welcome for any programs that will:\n1. Background backup the camera roll to my PI, even if they are not connected to my local WiFi; exactly like GooglePhotos or Dropbox. Can each user have their own folders or accounts? So they don\u2019t get mixed up?\n2. Uses the same protocol as typical NAS.\n3. Able to access files on Windows Explorer, as I don\u2019t like to use a web browser.\n4. Create shareable links will be a bonus, but not a must have.\n\nThank you very much!", "author_fullname": "t2_ld9gl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Popular ways to turn my raspberry pi into a Nas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156iwdi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690029015.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Very new to NAS and Nextcloud;\nI am looking to create a NAS with my PI 4, to replace Google Photos for my family; and Nextcloud seems to be the best option with their iOS app.&lt;/p&gt;\n\n&lt;p&gt;However, I would also like to access my files on my local network, and one of my smart home camera provider has the option to back up to a NAS, but I\u2019m not sure if Nextccloud will expose my Pi as a NAS, or would I have to install Samba on top?&lt;/p&gt;\n\n&lt;p&gt;Any other recommendations will be welcome for any programs that will:\n1. Background backup the camera roll to my PI, even if they are not connected to my local WiFi; exactly like GooglePhotos or Dropbox. Can each user have their own folders or accounts? So they don\u2019t get mixed up?\n2. Uses the same protocol as typical NAS.\n3. Able to access files on Windows Explorer, as I don\u2019t like to use a web browser.\n4. Create shareable links will be a bonus, but not a must have.&lt;/p&gt;\n\n&lt;p&gt;Thank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "156iwdi", "is_robot_indexable": true, "report_reasons": null, "author": "AlexKLMan", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/156iwdi/popular_ways_to_turn_my_raspberry_pi_into_a_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/156iwdi/popular_ways_to_turn_my_raspberry_pi_into_a_nas/", "subreddit_subscribers": 693639, "created_utc": 1690029015.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, so I'm not very tech savvy at all and I'm hoping to buy additional HHDs for my desktop.\n\nI've used up all the power and SATA connectors but my case has capacity for at least 3-4 more HHDs that I would like to use up. \n\nHowever, how would I go about doing this?  I have already tried to Google this but didn't really understand what I was reading; something about molex to power SATA Y cables; power SATA to power SATA y cables etc.  It all went over my head.  Please explain it like I'm a 5 year old!\n\nMy motherboard is a MSI ZH77A-G43 (MS-7758) if that helps.\n\nWith my setup how many more drives could I install and make use of.  If I have to I'll swap out my drives for larger ones but that would be a pain.\n\nThank you in advance!", "author_fullname": "t2_fozjfk1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Super noob question regarding HHD expansion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156i36z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690026711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, so I&amp;#39;m not very tech savvy at all and I&amp;#39;m hoping to buy additional HHDs for my desktop.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used up all the power and SATA connectors but my case has capacity for at least 3-4 more HHDs that I would like to use up. &lt;/p&gt;\n\n&lt;p&gt;However, how would I go about doing this?  I have already tried to Google this but didn&amp;#39;t really understand what I was reading; something about molex to power SATA Y cables; power SATA to power SATA y cables etc.  It all went over my head.  Please explain it like I&amp;#39;m a 5 year old!&lt;/p&gt;\n\n&lt;p&gt;My motherboard is a MSI ZH77A-G43 (MS-7758) if that helps.&lt;/p&gt;\n\n&lt;p&gt;With my setup how many more drives could I install and make use of.  If I have to I&amp;#39;ll swap out my drives for larger ones but that would be a pain.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "156i36z", "is_robot_indexable": true, "report_reasons": null, "author": "firvulag359", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/156i36z/super_noob_question_regarding_hhd_expansion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/156i36z/super_noob_question_regarding_hhd_expansion/", "subreddit_subscribers": 693639, "created_utc": 1690026711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have two Exos drives that are at the last 20 months of their 5 yr warranties. All my other drives have almost the full 5 year warranty left (Seagate Exos)\n\nWhen would it make any sense to replace the older drives and put them in a backup position? Is it wise to have drives loaded that have warranties set to expire around the same time for replacement purposes? Would it make any sense to combine the data on my 10tb and 12tb drives into say a newer 20 or 22tb?\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Having HDDs that warranty expire around the same time (Exos)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1566u3f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689990569.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have two Exos drives that are at the last 20 months of their 5 yr warranties. All my other drives have almost the full 5 year warranty left (Seagate Exos)&lt;/p&gt;\n\n&lt;p&gt;When would it make any sense to replace the older drives and put them in a backup position? Is it wise to have drives loaded that have warranties set to expire around the same time for replacement purposes? Would it make any sense to combine the data on my 10tb and 12tb drives into say a newer 20 or 22tb?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1566u3f", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1566u3f/having_hdds_that_warranty_expire_around_the_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1566u3f/having_hdds_that_warranty_expire_around_the_same/", "subreddit_subscribers": 693639, "created_utc": 1689990569.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Serverpartdeals currently has the 20TB exos OEM with 5 year SPD warranty (it was 3 year SPD warranty a few days ago) for $279.  They had the new 20TB exos 20 with 5 years manuf. warranty available couple of weeks ago (now out of stock).\n\nWhat's the difference between the 2 warranty? I am trying to decide if I should buy these OEM now or hold out for when the new drives are available.  \n\nTheir pricing is not always intuitive (for consumers). A few weeks back, they had their OEM at a lot higher price than the new drives (a couple hundred dollars more). If everything is the same, we should ALWAYS buy new drives (instead of OEM) right?\n\nThank you for your input.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_7nhiinlw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is SPD warranty vs manufacturer warranty (on hard drives)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156ggzg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690021575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Serverpartdeals currently has the 20TB exos OEM with 5 year SPD warranty (it was 3 year SPD warranty a few days ago) for $279.  They had the new 20TB exos 20 with 5 years manuf. warranty available couple of weeks ago (now out of stock).&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the difference between the 2 warranty? I am trying to decide if I should buy these OEM now or hold out for when the new drives are available.  &lt;/p&gt;\n\n&lt;p&gt;Their pricing is not always intuitive (for consumers). A few weeks back, they had their OEM at a lot higher price than the new drives (a couple hundred dollars more). If everything is the same, we should ALWAYS buy new drives (instead of OEM) right?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your input.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "156ggzg", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-Mathematician35", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/156ggzg/what_is_spd_warranty_vs_manufacturer_warranty_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/156ggzg/what_is_spd_warranty_vs_manufacturer_warranty_on/", "subreddit_subscribers": 693639, "created_utc": 1690021575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "$19 Off Lexar NM790 SSD 4TB PCIe Gen4 NVMe M.2 2280 Internal SSD, Up to 7400MB/s, Compatible with PS5, for Gamers and Creators, for $190.64 - lowest Price for 4TB\n\n10% Off fanxiang S770 2TB PCIe 4.0 NVMe SSD M.2 2280 Internal SSD with Heatsink, Up to 7300MB/s, Perfectly Compatible with PS5, for $85.49 (w/ coupon) - lowest Price for 2TB\n\n5% Off ADATA 2TB Premium SSD for PS5 PCIe Gen4 M.2 2280 Internal Gaming SSD, Up to 7400 MB/s, $94.99\n\n10% Off Silicon Power 2TB XS70 Nvme PCIe Gen4 M.2 2280 Internal Gaming SSD W/R Up to 7,300/6,800 MB/s, for $89.07\n\n61% Off WD\\_BLACK 2TB SN850X NVMe Internal Gaming SSD with Heatsink - Works with Playstation 5, Gen4 PCIe, M.2 2280, Up to 7,300 MB/s, for $119.99 \n\nsource - [https://twitter.com/HelpMeFindDeals/status/1682717243837382656](https://twitter.com/HelpMeFindDeals/status/1682717243837382656)\n\nThanks", "author_fullname": "t2_3wef6thkx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Today's 5 Best Gaming SSD (2TB &amp; 4TB) Deals 2023, from $85.49", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156i1jc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690026586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;$19 Off Lexar NM790 SSD 4TB PCIe Gen4 NVMe M.2 2280 Internal SSD, Up to 7400MB/s, Compatible with PS5, for Gamers and Creators, for $190.64 - lowest Price for 4TB&lt;/p&gt;\n\n&lt;p&gt;10% Off fanxiang S770 2TB PCIe 4.0 NVMe SSD M.2 2280 Internal SSD with Heatsink, Up to 7300MB/s, Perfectly Compatible with PS5, for $85.49 (w/ coupon) - lowest Price for 2TB&lt;/p&gt;\n\n&lt;p&gt;5% Off ADATA 2TB Premium SSD for PS5 PCIe Gen4 M.2 2280 Internal Gaming SSD, Up to 7400 MB/s, $94.99&lt;/p&gt;\n\n&lt;p&gt;10% Off Silicon Power 2TB XS70 Nvme PCIe Gen4 M.2 2280 Internal Gaming SSD W/R Up to 7,300/6,800 MB/s, for $89.07&lt;/p&gt;\n\n&lt;p&gt;61% Off WD_BLACK 2TB SN850X NVMe Internal Gaming SSD with Heatsink - Works with Playstation 5, Gen4 PCIe, M.2 2280, Up to 7,300 MB/s, for $119.99 &lt;/p&gt;\n\n&lt;p&gt;source - &lt;a href=\"https://twitter.com/HelpMeFindDeals/status/1682717243837382656\"&gt;https://twitter.com/HelpMeFindDeals/status/1682717243837382656&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-FtB_IGUweAaB4df1eZ2jHfsfZjfrqst2BFyUKJ9CG8.jpg?auto=webp&amp;s=e21cfa5364360699cff66e7c5df20006e7d572b4", "width": 690, "height": 1584}, "resolutions": [{"url": "https://external-preview.redd.it/-FtB_IGUweAaB4df1eZ2jHfsfZjfrqst2BFyUKJ9CG8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=394750357a3ec05bfaaed8a3c44531d007cad8fb", "width": 108, "height": 216}, {"url": "https://external-preview.redd.it/-FtB_IGUweAaB4df1eZ2jHfsfZjfrqst2BFyUKJ9CG8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6908a0232579b64a6e28a1b8eab60f90a47d31d", "width": 216, "height": 432}, {"url": "https://external-preview.redd.it/-FtB_IGUweAaB4df1eZ2jHfsfZjfrqst2BFyUKJ9CG8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad9831aeae97f1b22133803a92a54529cdfc6d0b", "width": 320, "height": 640}, {"url": "https://external-preview.redd.it/-FtB_IGUweAaB4df1eZ2jHfsfZjfrqst2BFyUKJ9CG8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f46f93de72b3b6b4302ea5b0aa089098dfe419b1", "width": 640, "height": 1280}], "variants": {}, "id": "n2ZR2cu5v_zBEbGewDmB3_dMdy6HUQJcYnCS7Y2iYtE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "156i1jc", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious_Command15", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/156i1jc/todays_5_best_gaming_ssd_2tb_4tb_deals_2023_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/156i1jc/todays_5_best_gaming_ssd_2tb_4tb_deals_2023_from/", "subreddit_subscribers": 693639, "created_utc": 1690026586.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}