{"kind": "Listing", "data": {"after": "t3_10emkmw", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background: I've been working with TB-scale data for several years. I realize this could sound like a DA/DS question, but almost all of the visualization products I have seen do not work well (at at all) at this scale or require significant pre-processing and customization, so I think this is appropriately a DE question. \n\nLet's say I have some data I want to visualize that is updated at an offline cadence, e.g. once a day. Parquet/Iceberg etc. New data is appended to daily partitions at a rate of \\~100GB/day. I have about a dozen dimensions and filters I would like to aggregate across or filter on, but the metrics themselves are somewhat basic (sums, counts, ratios, etc). Are there any visualization solutions that can handle interactive querying against data of this scale (interactive meaning the end user self-filter and update the visualization within a second or two)? What are the benefits/tradeoffs to current systems for this? Is this a use case to bring in a specialized OLAP cube product?", "author_fullname": "t2_7pxby", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who is doing large-scale visualization and dashboarding well?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10eov35", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673992650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background: I&amp;#39;ve been working with TB-scale data for several years. I realize this could sound like a DA/DS question, but almost all of the visualization products I have seen do not work well (at at all) at this scale or require significant pre-processing and customization, so I think this is appropriately a DE question. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I have some data I want to visualize that is updated at an offline cadence, e.g. once a day. Parquet/Iceberg etc. New data is appended to daily partitions at a rate of ~100GB/day. I have about a dozen dimensions and filters I would like to aggregate across or filter on, but the metrics themselves are somewhat basic (sums, counts, ratios, etc). Are there any visualization solutions that can handle interactive querying against data of this scale (interactive meaning the end user self-filter and update the visualization within a second or two)? What are the benefits/tradeoffs to current systems for this? Is this a use case to bring in a specialized OLAP cube product?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10eov35", "is_robot_indexable": true, "report_reasons": null, "author": "ColdPorridge", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10eov35/who_is_doing_largescale_visualization_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10eov35/who_is_doing_largescale_visualization_and/", "subreddit_subscribers": 86646, "created_utc": 1673992650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_m6gnxiuj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DW toolkit book by Ralph Kimball", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 96, "top_awarded_type": null, "hide_score": false, "name": "t3_10esybb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5ioEG9GaIPjg4DO8YEHXNaQLLEZ29VHNOBtRF9u2KaY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674002443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/yc0bxaypoqca1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?auto=webp&amp;v=enabled&amp;s=579f4c43ca548b1e0742912ba9dfea97c392c86f", "width": 2819, "height": 1953}, "resolutions": [{"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7d6aca1f1b6519a452a7852feed37a02fb924c0", "width": 108, "height": 74}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae5930e940d0713f43e6184c6c0df951e5af2353", "width": 216, "height": 149}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=954c43e1c94f190557f4541106f2dc24f4a5f1ea", "width": 320, "height": 221}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=726092b906848d30d07c31f9715bfbbd62fb79cd", "width": 640, "height": 443}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e790040401ee5a93fc61d0098c4843d73d6efb1", "width": 960, "height": 665}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7d765bedccbfc6d37bf7201318cec16c16e4926", "width": 1080, "height": 748}], "variants": {}, "id": "gKCsuiInCaKY56pQ3Kk6u9IsUHba7ufeNr9UMkEUlRw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10esybb", "is_robot_indexable": true, "report_reasons": null, "author": "rajekum512", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10esybb/dw_toolkit_book_by_ralph_kimball/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/yc0bxaypoqca1.jpg", "subreddit_subscribers": 86646, "created_utc": 1674002443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lets say ***a small company currently handles all data by sharing excel files over emails and has never heard the concept of a database***. \n\n***How do you transition this into a data warehouse?***\n\nDoes this mean now I have to write a lot of code to process a bunch of random csv files to turn them into a star schema?\n\nLet's say I managed to turn their csv files into a consistent set of tables... How do they access it? Do they need to hire a bunch of sql analysts to get the information? or Do you build some sort of dashboard out of it?  \n\n\nI know someone has suffered from this before.  \n\n\nHow would you approach it?  \nIs there a guide or a book for this?", "author_fullname": "t2_zs1xp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering for small companies.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10eurjm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674007202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say &lt;strong&gt;&lt;em&gt;a small company currently handles all data by sharing excel files over emails and has never heard the concept of a database&lt;/em&gt;&lt;/strong&gt;. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;How do you transition this into a data warehouse?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Does this mean now I have to write a lot of code to process a bunch of random csv files to turn them into a star schema?&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I managed to turn their csv files into a consistent set of tables... How do they access it? Do they need to hire a bunch of sql analysts to get the information? or Do you build some sort of dashboard out of it?  &lt;/p&gt;\n\n&lt;p&gt;I know someone has suffered from this before.  &lt;/p&gt;\n\n&lt;p&gt;How would you approach it?&lt;br/&gt;\nIs there a guide or a book for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10eurjm", "is_robot_indexable": true, "report_reasons": null, "author": "lFuckRedditl", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10eurjm/data_engineering_for_small_companies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10eurjm/data_engineering_for_small_companies/", "subreddit_subscribers": 86646, "created_utc": 1674007202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, is there a good implementation for Delta Sharing usable for on-prem deployments?\n\nThe reference implementation provided doesn't support adding new shares without modifying the configuration file and restarting the server, also there's no user management there.", "author_fullname": "t2_4clu4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Sharing on premise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ezch7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674020331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, is there a good implementation for Delta Sharing usable for on-prem deployments?&lt;/p&gt;\n\n&lt;p&gt;The reference implementation provided doesn&amp;#39;t support adding new shares without modifying the configuration file and restarting the server, also there&amp;#39;s no user management there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ezch7", "is_robot_indexable": true, "report_reasons": null, "author": "inteloid", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ezch7/delta_sharing_on_premise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ezch7/delta_sharing_on_premise/", "subreddit_subscribers": 86646, "created_utc": 1674020331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you document data products? I mean e.g. we use mainly Python for ETL jobs and our documentation consists of README in the git repo and sometimes (depending on the transformation complexity) also a Google Document describing the app, calculations, input and outputs etc. \n\nDo you have any better approach? I would really like to have some \u201cdocumentation framework\u201d that would have all data products described and explorable in one place. It would be nice to see products dependencies/lineage similar way you see in Airflow. I am thinking of some custom data catalog maybe, but firstly I would like to know what others do/have out there.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you document data products?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ekhrq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673982550.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you document data products? I mean e.g. we use mainly Python for ETL jobs and our documentation consists of README in the git repo and sometimes (depending on the transformation complexity) also a Google Document describing the app, calculations, input and outputs etc. &lt;/p&gt;\n\n&lt;p&gt;Do you have any better approach? I would really like to have some \u201cdocumentation framework\u201d that would have all data products described and explorable in one place. It would be nice to see products dependencies/lineage similar way you see in Airflow. I am thinking of some custom data catalog maybe, but firstly I would like to know what others do/have out there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ekhrq", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ekhrq/how_do_you_document_data_products/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ekhrq/how_do_you_document_data_products/", "subreddit_subscribers": 86646, "created_utc": 1673982550.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_502r5jht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to make a change, resume feedback / advice appreciated for junior DE role.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10f2eow", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Sy52Ie6JvJWizUvuv5xo1U7nvqu7OXkc1LzxFEdNZMA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674031173.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zbpozed52tca1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zbpozed52tca1.jpg?auto=webp&amp;v=enabled&amp;s=24840a9556acbb5748715338b28ad0e4a48a2087", "width": 1203, "height": 1723}, "resolutions": [{"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=538cffe182f66e8f7d78190dc8c2928e9437cb73", "width": 108, "height": 154}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3786ab72038cbe2a4d83014c4d792028820cde4f", "width": 216, "height": 309}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1718595b0b5c4e07764565ec79ed3e4fa6e79462", "width": 320, "height": 458}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b223390d8ac0a08a2cf930a9914404460722901", "width": 640, "height": 916}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d5d0f9c3f577c1340f4229ba95fcbb4ff2bda9d", "width": 960, "height": 1374}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2a40325e0f0927f7547c023f0dfe71f6a0c89ea", "width": 1080, "height": 1546}], "variants": {}, "id": "2QiMbQXoSVTA6udrYgV4EfBnJyQkKItlm1SrK4_k-Bs"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10f2eow", "is_robot_indexable": true, "report_reasons": null, "author": "toem033", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f2eow/looking_to_make_a_change_resume_feedback_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zbpozed52tca1.jpg", "subreddit_subscribers": 86646, "created_utc": 1674031173.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When you do a merge in SQL, is it doing a full table scan of the target? If so, isn\u2019t that something we want to avoid?", "author_fullname": "t2_5ukitegd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MERGE statement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10exb2o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674014249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When you do a merge in SQL, is it doing a full table scan of the target? If so, isn\u2019t that something we want to avoid?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10exb2o", "is_robot_indexable": true, "report_reasons": null, "author": "burningburnerbern", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10exb2o/merge_statement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10exb2o/merge_statement/", "subreddit_subscribers": 86646, "created_utc": 1674014249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to flatten the JSON file\n\nstructure of the file looks like this \n\n    root\n     |-- items: array (nullable = true)\n     |    |-- element: struct (containsNull = true)\n     |    |    |-- _links: struct (nullable = true)\n     |    |    |    |-- self: struct (nullable = true)\n     |    |    |    |    |-- href: string (nullable = true)\n     |    |    |-- code: string (nullable = true)\n     |    |    |-- labels: struct (nullable = true)\n     |    |    |    |-- bg_BG: string (nullable = true)\n     |    |    |    |-- cs_CZ: string (nullable = true)\n     |    |    |    |-- da_DK: string (nullable = true)\n     |    |    |    |-- de_AT: string (nullable = true)\n     |    |    |    |-- de_BE: string (nullable = true)\n     |    |    |    |-- de_CH: string (nullable = true)\n     |    |    |    |-- de_DE: string (nullable = true)\n     |    |    |    |-- el_GR: string (nullable = true)\n     |    |    |    |-- en_AU: string (nullable = true)\n     |    |    |    |-- en_CA: string (nullable = true)\n     |    |    |    |-- en_CH: string (nullable = true)\n     |    |    |    |-- en_GB: string (nullable = true)\n     |    |    |    |-- en_IE: string (nullable = true)\n     |    |    |    |-- en_IN: string (nullable = true)\n     |    |    |    |-- en_MY: string (nullable = true)\n     |    |    |    |-- en_NZ: string (nullable = true)\n     |    |    |    |-- en_PH: string (nullable = true)\n     |    |    |    |-- en_SG: string (nullable = true)\n     |    |    |    |-- en_US: string (nullable = true)\n     |    |    |    |-- en_US_America: string (nullable = true)\n     |    |    |    |-- es_ES: string (nullable = true)\n     |    |    |    |-- et_EE: string (nullable = true)\n     |    |    |    |-- fi_FI: string (nullable = true)\n     |    |    |    |-- fr_BE: string (nullable = true)\n     |    |    |    |-- fr_CH: string (nullable = true)\n     |    |    |    |-- fr_FR: string (nullable = true)\n     |    |    |    |-- hr_HR: string (nullable = true)\n     |    |    |    |-- hu_HU: string (nullable = true)\n     |    |    |    |-- id_ID: string (nullable = true)\n     |    |    |    |-- is_IS: string (nullable = true)\n     |    |    |    |-- it_CH: string (nullable = true)\n     |    |    |    |-- it_IT: string (nullable = true)\n     |    |    |    |-- ja_JP: string (nullable = true)\n     |    |    |    |-- ko_KR: string (nullable = true)\n     |    |    |    |-- lt_LT: string (nullable = true)\n     |    |    |    |-- lv_LV: string (nullable = true)\n     |    |    |    |-- ms_MY: string (nullable = true)\n     |    |    |    |-- nb_NO: string (nullable = true)\n     |    |    |    |-- nl_BE: string (nullable = true)\n     |    |    |    |-- nl_NL: string (nullable = true)\n     |    |    |    |-- nn_NO: string (nullable = true)\n     |    |    |    |-- pl_PL: string (nullable = true)\n     |    |    |    |-- pt_PT: string (nullable = true)\n     |    |    |    |-- ro_RO: string (nullable = true)\n     |    |    |    |-- ru_RU: string (nullable = true)\n     |    |    |    |-- sk_SK: string (nullable = true)\n     |    |    |    |-- sl_SI: string (nullable = true)\n     |    |    |    |-- sr_Cyrl_RS: string (nullable = true)\n     |    |    |    |-- sv_SE: string (nullable = true)\n     |    |    |    |-- th_TH: string (nullable = true)\n     |    |    |    |-- tr_TR: string (nullable = true)\n     |    |    |    |-- uk_UA: string (nullable = true)\n     |    |    |    |-- vi_VN: string (nullable = true)\n     |    |    |    |-- zh_CN: string (nullable = true)\n     |    |    |    |-- zh_TW: string (nullable = true)\n     |    |    |-- parent: string (nullable = true)\n     |    |    |-- updated: string (nullable = true)\n\nMy function is this \n\n    def flatten_df(nested_df):\n        flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'string']\n        nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'struct']\n    \n        flat_df = nested_df.select(flat_cols +\n                                   [col(nc+'.'+c).alias(nc+'_'+c)\n                                    for nc in nested_cols\n                                    for c in nested_df.select(nc+'.*').columns])\n        return flat_df\n    \n    # Top level hierarchy\n    df = df.select('_embedded.*')\n    #Reaching the lower level called \"items\"\n    df1 = df.select(explode(df.items).alias('required'))\n    # Creating dataframe which will be passed to flatten_df to flatten entire data under \"items\" hierarchy\n    df2 = df1.select('required.*')\n    final = flatten_df(df2)\n    display(final)\n\nthis function takes the `lables`   column keys and puts them as seperate columns ( please see the image [https://imgur.com/a/xa5VmCu](https://imgur.com/a/xa5VmCu) )\n\n&amp;#x200B;\n\nwhat I want is that function should give only two columns from `lables` column named Lable\\_key Lable\\_value  as shown in the image  [https://imgur.com/a/sTW9sSx](https://imgur.com/a/sTW9sSx)", "author_fullname": "t2_56g5f4cg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "flatten nested JSON file pyspark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10f1kh1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674027896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to flatten the JSON file&lt;/p&gt;\n\n&lt;p&gt;structure of the file looks like this &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;root\n |-- items: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- _links: struct (nullable = true)\n |    |    |    |-- self: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |-- code: string (nullable = true)\n |    |    |-- labels: struct (nullable = true)\n |    |    |    |-- bg_BG: string (nullable = true)\n |    |    |    |-- cs_CZ: string (nullable = true)\n |    |    |    |-- da_DK: string (nullable = true)\n |    |    |    |-- de_AT: string (nullable = true)\n |    |    |    |-- de_BE: string (nullable = true)\n |    |    |    |-- de_CH: string (nullable = true)\n |    |    |    |-- de_DE: string (nullable = true)\n |    |    |    |-- el_GR: string (nullable = true)\n |    |    |    |-- en_AU: string (nullable = true)\n |    |    |    |-- en_CA: string (nullable = true)\n |    |    |    |-- en_CH: string (nullable = true)\n |    |    |    |-- en_GB: string (nullable = true)\n |    |    |    |-- en_IE: string (nullable = true)\n |    |    |    |-- en_IN: string (nullable = true)\n |    |    |    |-- en_MY: string (nullable = true)\n |    |    |    |-- en_NZ: string (nullable = true)\n |    |    |    |-- en_PH: string (nullable = true)\n |    |    |    |-- en_SG: string (nullable = true)\n |    |    |    |-- en_US: string (nullable = true)\n |    |    |    |-- en_US_America: string (nullable = true)\n |    |    |    |-- es_ES: string (nullable = true)\n |    |    |    |-- et_EE: string (nullable = true)\n |    |    |    |-- fi_FI: string (nullable = true)\n |    |    |    |-- fr_BE: string (nullable = true)\n |    |    |    |-- fr_CH: string (nullable = true)\n |    |    |    |-- fr_FR: string (nullable = true)\n |    |    |    |-- hr_HR: string (nullable = true)\n |    |    |    |-- hu_HU: string (nullable = true)\n |    |    |    |-- id_ID: string (nullable = true)\n |    |    |    |-- is_IS: string (nullable = true)\n |    |    |    |-- it_CH: string (nullable = true)\n |    |    |    |-- it_IT: string (nullable = true)\n |    |    |    |-- ja_JP: string (nullable = true)\n |    |    |    |-- ko_KR: string (nullable = true)\n |    |    |    |-- lt_LT: string (nullable = true)\n |    |    |    |-- lv_LV: string (nullable = true)\n |    |    |    |-- ms_MY: string (nullable = true)\n |    |    |    |-- nb_NO: string (nullable = true)\n |    |    |    |-- nl_BE: string (nullable = true)\n |    |    |    |-- nl_NL: string (nullable = true)\n |    |    |    |-- nn_NO: string (nullable = true)\n |    |    |    |-- pl_PL: string (nullable = true)\n |    |    |    |-- pt_PT: string (nullable = true)\n |    |    |    |-- ro_RO: string (nullable = true)\n |    |    |    |-- ru_RU: string (nullable = true)\n |    |    |    |-- sk_SK: string (nullable = true)\n |    |    |    |-- sl_SI: string (nullable = true)\n |    |    |    |-- sr_Cyrl_RS: string (nullable = true)\n |    |    |    |-- sv_SE: string (nullable = true)\n |    |    |    |-- th_TH: string (nullable = true)\n |    |    |    |-- tr_TR: string (nullable = true)\n |    |    |    |-- uk_UA: string (nullable = true)\n |    |    |    |-- vi_VN: string (nullable = true)\n |    |    |    |-- zh_CN: string (nullable = true)\n |    |    |    |-- zh_TW: string (nullable = true)\n |    |    |-- parent: string (nullable = true)\n |    |    |-- updated: string (nullable = true)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;My function is this &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def flatten_df(nested_df):\n    flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == &amp;#39;string&amp;#39;]\n    nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == &amp;#39;struct&amp;#39;]\n\n    flat_df = nested_df.select(flat_cols +\n                               [col(nc+&amp;#39;.&amp;#39;+c).alias(nc+&amp;#39;_&amp;#39;+c)\n                                for nc in nested_cols\n                                for c in nested_df.select(nc+&amp;#39;.*&amp;#39;).columns])\n    return flat_df\n\n# Top level hierarchy\ndf = df.select(&amp;#39;_embedded.*&amp;#39;)\n#Reaching the lower level called &amp;quot;items&amp;quot;\ndf1 = df.select(explode(df.items).alias(&amp;#39;required&amp;#39;))\n# Creating dataframe which will be passed to flatten_df to flatten entire data under &amp;quot;items&amp;quot; hierarchy\ndf2 = df1.select(&amp;#39;required.*&amp;#39;)\nfinal = flatten_df(df2)\ndisplay(final)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;this function takes the &lt;code&gt;lables&lt;/code&gt;   column keys and puts them as seperate columns ( please see the image &lt;a href=\"https://imgur.com/a/xa5VmCu\"&gt;https://imgur.com/a/xa5VmCu&lt;/a&gt; )&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;what I want is that function should give only two columns from &lt;code&gt;lables&lt;/code&gt; column named Lable_key Lable_value  as shown in the image  &lt;a href=\"https://imgur.com/a/sTW9sSx\"&gt;https://imgur.com/a/sTW9sSx&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?auto=webp&amp;v=enabled&amp;s=7a58cc0ee2d095f64688402d8012d36b9b6a66d2", "width": 1317, "height": 184}, "resolutions": [{"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f66ce948572caabe1038462bad873d97fc1a6f00", "width": 108, "height": 15}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19b8cfa11924aba9692b30f9f090ba1b2bdc5cf6", "width": 216, "height": 30}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d770f580c8e9f5a3b75d918f355a185747d363e", "width": 320, "height": 44}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e91a890cf3c85f5d7f35fc36efa574576ea686b6", "width": 640, "height": 89}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca2e0618fd945164bb0ee5d1f453f615f3370c12", "width": 960, "height": 134}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d83982ce3a8ecd415011b397cfd95a71b1a3ba4", "width": 1080, "height": 150}], "variants": {}, "id": "-PIFhkx_4ZX0v6qeVW9b0POiuvOW5532KuWALDCw1dU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10f1kh1", "is_robot_indexable": true, "report_reasons": null, "author": "9gg6", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f1kh1/flatten_nested_json_file_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f1kh1/flatten_nested_json_file_pyspark/", "subreddit_subscribers": 86646, "created_utc": 1674027896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can somebody help? In over my head trying to run a product assessment for Databricks with almost no guidance for the use case. Would be helpful to hear what folks really like about the product + what questions people wish they had asked or better understood before becoming a customer", "author_fullname": "t2_tye5ydmn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks evaluation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10euyrz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674007714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can somebody help? In over my head trying to run a product assessment for Databricks with almost no guidance for the use case. Would be helpful to hear what folks really like about the product + what questions people wish they had asked or better understood before becoming a customer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10euyrz", "is_robot_indexable": true, "report_reasons": null, "author": "LuckyChopsSOS", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10euyrz/databricks_evaluation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10euyrz/databricks_evaluation/", "subreddit_subscribers": 86646, "created_utc": 1674007714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an external websocket APIs, some of them produce \\~1k events / second. My goal is to save them in database and don't lose any of the events. Currently I have only 1 listener for each API, that pushes the events to redis queue (with disk persistence ) and then I have 1 worker for each q to write to Postgres.\n\n&amp;#x200B;\n\nIt works fine, but when restarting the listener (pulling new commit or simple power outage problem) - I lose some events during this time. So I was thinking to run at least 2 separate nodes with listener for each API. That way I can restart them one by one and dont lose any events. But that means that redis q will have almost all events duplicated. What if I will run 4 listeners instead of 2? Writing this q to db with cause a lot of vacuum. Probably I can just leave the job of deduplicaiton to a worker, that remove all dups from batch he receive, before writing to db.\n\n&amp;#x200B;\n\nWhat is the way to handle that? As far as I understood Kafka and pulsar does only have message ID deduplication. And you can't set custom ids based on your content", "author_fullname": "t2_3ppayh15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Queue with content deduplication", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ei41t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673978808.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673976948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an external websocket APIs, some of them produce ~1k events / second. My goal is to save them in database and don&amp;#39;t lose any of the events. Currently I have only 1 listener for each API, that pushes the events to redis queue (with disk persistence ) and then I have 1 worker for each q to write to Postgres.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It works fine, but when restarting the listener (pulling new commit or simple power outage problem) - I lose some events during this time. So I was thinking to run at least 2 separate nodes with listener for each API. That way I can restart them one by one and dont lose any events. But that means that redis q will have almost all events duplicated. What if I will run 4 listeners instead of 2? Writing this q to db with cause a lot of vacuum. Probably I can just leave the job of deduplicaiton to a worker, that remove all dups from batch he receive, before writing to db.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is the way to handle that? As far as I understood Kafka and pulsar does only have message ID deduplication. And you can&amp;#39;t set custom ids based on your content&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ei41t", "is_robot_indexable": true, "report_reasons": null, "author": "dotaleaker", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ei41t/queue_with_content_deduplication/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ei41t/queue_with_content_deduplication/", "subreddit_subscribers": 86646, "created_utc": 1673976948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uo96pc9v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resume advice needed. Applying for Data Engineering roles. I have mentioned a couple of projects in my portfolio (70milan.github.io) to avoid mentioning it on my resume yet cant get all the information on a single page. Reduced font size to 10.5-11 pts, more suggestions appreciated.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10egk6a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/T0N6_lfidPVS48rX80qr0wpSGrW4PYmlfvvNLg3do8Q.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673973224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/g1mevnbu9oca1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/g1mevnbu9oca1.png?auto=webp&amp;v=enabled&amp;s=3e596692bec3a08e2b341393ad293a9b84a6697f", "width": 1080, "height": 2109}, "resolutions": [{"url": "https://preview.redd.it/g1mevnbu9oca1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02625a9372ce393e8c2cb92b5bfce88411f77097", "width": 108, "height": 210}, {"url": "https://preview.redd.it/g1mevnbu9oca1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2f181323f1414795b240f10e18f6dbdd862aa9a", "width": 216, "height": 421}, {"url": "https://preview.redd.it/g1mevnbu9oca1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73c23182414f17420323af47065bbb41310d4201", "width": 320, "height": 624}, {"url": "https://preview.redd.it/g1mevnbu9oca1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67a7cfe8d623389adfd606b527435833afc71144", "width": 640, "height": 1249}, {"url": "https://preview.redd.it/g1mevnbu9oca1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f157e0da899ce434d2440d624158ed36fa832b9c", "width": 960, "height": 1874}, {"url": "https://preview.redd.it/g1mevnbu9oca1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91110b26c772afb5c571e34f0c302097547b4c5d", "width": 1080, "height": 2109}], "variants": {}, "id": "zGs4iRzbDByVHiSEeV-DzXrD3k8sLiwSYbRqDk5n1vQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10egk6a", "is_robot_indexable": true, "report_reasons": null, "author": "purplexed247", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10egk6a/resume_advice_needed_applying_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/g1mevnbu9oca1.png", "subreddit_subscribers": 86646, "created_utc": 1673973224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've mostly been web dev in my career and only recently joined a team that is data eng. In web dev Blue Green deployments are super easy, since you just can route requests between blue and green at DNS or load balancer.   \n\n\nHowever most of the data pipeline stuff I've seen in my team is something like:   \n1. Poll or listen for new data  \n2. Do stuff  \n3. Write new data somewhere else  \n\n\nBlue/Green doesn't work here as much because the instances themselves are reading and writing continuously. So there is no external control mechanism like DNS, so you need internal controls which could work but may be brittle.  \n\n\nWhat patterns have you found successful for bringing up and swapping new versions of running pipeline services?  \n\n\nExtra context: Everything is pretty homegrown, ec2 style deployments.", "author_fullname": "t2_gn2ff6o9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Green Blue Style deployments for continuous data pipeline services?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10egxfp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673974134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve mostly been web dev in my career and only recently joined a team that is data eng. In web dev Blue Green deployments are super easy, since you just can route requests between blue and green at DNS or load balancer.   &lt;/p&gt;\n\n&lt;p&gt;However most of the data pipeline stuff I&amp;#39;ve seen in my team is something like:&lt;br/&gt;\n1. Poll or listen for new data&lt;br/&gt;\n2. Do stuff&lt;br/&gt;\n3. Write new data somewhere else  &lt;/p&gt;\n\n&lt;p&gt;Blue/Green doesn&amp;#39;t work here as much because the instances themselves are reading and writing continuously. So there is no external control mechanism like DNS, so you need internal controls which could work but may be brittle.  &lt;/p&gt;\n\n&lt;p&gt;What patterns have you found successful for bringing up and swapping new versions of running pipeline services?  &lt;/p&gt;\n\n&lt;p&gt;Extra context: Everything is pretty homegrown, ec2 style deployments.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10egxfp", "is_robot_indexable": true, "report_reasons": null, "author": "fthb1000000", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10egxfp/green_blue_style_deployments_for_continuous_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10egxfp/green_blue_style_deployments_for_continuous_data/", "subreddit_subscribers": 86646, "created_utc": 1673974134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7m65ddby", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hello!Long time lurker here.I have been in the same team for the past 4 years. First as a third party client and then got onboarded directly.Please critique my resume and give advice as it's been awhile since I stepped into jobsearch market.I'm also looking for anyone interested in mentoring me.Thnx", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10efcva", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AKgDXYNTpD1Jg8ZmtGbifv1jIXPWUUAPCoBA64qzJ2w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673970249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/403zvyhgimca1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/403zvyhgimca1.jpg?auto=webp&amp;v=enabled&amp;s=4b56fb6ced56a6c01b3b919c1fbbf7d4f945fdcc", "width": 1275, "height": 1650}, "resolutions": [{"url": "https://preview.redd.it/403zvyhgimca1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5041a3d29851faa868eeba65b30162d2a6ee8964", "width": 108, "height": 139}, {"url": "https://preview.redd.it/403zvyhgimca1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a44f194c6138a5402574c1eaecd18cf60e3caf58", "width": 216, "height": 279}, {"url": "https://preview.redd.it/403zvyhgimca1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f9a74f887a00689a8d2708580b065eb2e0c191f", "width": 320, "height": 414}, {"url": "https://preview.redd.it/403zvyhgimca1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb2134164d4b4158e52346f9f05167c5f37b3f14", "width": 640, "height": 828}, {"url": "https://preview.redd.it/403zvyhgimca1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6914230bcddacce5369210865337df2d8a7e1678", "width": 960, "height": 1242}, {"url": "https://preview.redd.it/403zvyhgimca1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c0785a7f2f3cafefa4b43cba32d99a0ccbb76fd", "width": 1080, "height": 1397}], "variants": {}, "id": "OaA2Q8gdn0nTL1p4-wL_cBJ9nte5JA8_XasuNcdkQ4Y"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10efcva", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive_Ball_965", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10efcva/hellolong_time_lurker_herei_have_been_in_the_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/403zvyhgimca1.jpg", "subreddit_subscribers": 86646, "created_utc": 1673970249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a subscription in Azure. It is running a VM. One of the VMs is windows, the other linux. Both are set up with python, and all of the things i need to run my ETL and anything else i would need within.\n\nFor instance. I have file delivery set up to receive and receive files. I am running python scripts to do all the various tasks. And task scheduler/cron to schedule said scripts. All of my logging is set up and i am able to email/text myself notifications/failures.\n\nI understand however that serverless functions are the way to cut down on costs. \n\nI have yet to figure out how to implement this.  What am i missing? Does anyone have any reading on setting up some serverless function to schedule a basic ETL task?", "author_fullname": "t2_5ch80", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure serverless ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10edqw4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673969203.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673966113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a subscription in Azure. It is running a VM. One of the VMs is windows, the other linux. Both are set up with python, and all of the things i need to run my ETL and anything else i would need within.&lt;/p&gt;\n\n&lt;p&gt;For instance. I have file delivery set up to receive and receive files. I am running python scripts to do all the various tasks. And task scheduler/cron to schedule said scripts. All of my logging is set up and i am able to email/text myself notifications/failures.&lt;/p&gt;\n\n&lt;p&gt;I understand however that serverless functions are the way to cut down on costs. &lt;/p&gt;\n\n&lt;p&gt;I have yet to figure out how to implement this.  What am i missing? Does anyone have any reading on setting up some serverless function to schedule a basic ETL task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10edqw4", "is_robot_indexable": true, "report_reasons": null, "author": "circusboy", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10edqw4/azure_serverless_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10edqw4/azure_serverless_etl/", "subreddit_subscribers": 86646, "created_utc": 1673966113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_alka6pjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resume Review request - BA to DE Pivot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10ezuz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.55, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9KYzoafqeDnG-_PpUWLOByjbMlz1QfJEuPzJVCCVOxo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674021917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/lrfnoo02tqca1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/lrfnoo02tqca1.png?auto=webp&amp;v=enabled&amp;s=91b8620f075e5ce896f4b5e14c0d567427cf6f54", "width": 422, "height": 516}, "resolutions": [{"url": "https://preview.redd.it/lrfnoo02tqca1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85d093ca93176a394919ff46e28fecee90627c97", "width": 108, "height": 132}, {"url": "https://preview.redd.it/lrfnoo02tqca1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51bfb89f9e5b6abaed0c5f1291aaf34be7a05594", "width": 216, "height": 264}, {"url": "https://preview.redd.it/lrfnoo02tqca1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5dbe508bd3270e09688cf52b82f2c1631f491b38", "width": 320, "height": 391}], "variants": {}, "id": "NKWY7CXzH_LGoyIUo72kU-hbkmDTtkN7Hugu4VkV8Wg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10ezuz9", "is_robot_indexable": true, "report_reasons": null, "author": "depressedbutsassy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ezuz9/resume_review_request_ba_to_de_pivot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/lrfnoo02tqca1.png", "subreddit_subscribers": 86646, "created_utc": 1674021917.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some time series data coming from different sources that have different levels of granularity/frequency\n\nSome data comes in monthly, some daily, some hourly \n\nIs it possible to store all this in one time series database? We like the querying benefits a time series db provides for time series but I wasn\u2019t sure if all the data has to be the same level of granularity", "author_fullname": "t2_7nbpziqx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time series databases with different granularity or multiple time series", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10elmce", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673985168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some time series data coming from different sources that have different levels of granularity/frequency&lt;/p&gt;\n\n&lt;p&gt;Some data comes in monthly, some daily, some hourly &lt;/p&gt;\n\n&lt;p&gt;Is it possible to store all this in one time series database? We like the querying benefits a time series db provides for time series but I wasn\u2019t sure if all the data has to be the same level of granularity&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10elmce", "is_robot_indexable": true, "report_reasons": null, "author": "Affectionate_Shine55", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10elmce/time_series_databases_with_different_granularity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10elmce/time_series_databases_with_different_granularity/", "subreddit_subscribers": 86646, "created_utc": 1673985168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Come join the first [Seattle Spark Meetup](https://www.meetup.com/Seattle-Spark-Meetup/events/290865855/?response=3&amp;action=rsvp&amp;utm_medium=email&amp;utm_source=braze_canvas&amp;utm_campaign=mmrk_alleng_event_announcement_prod_v7_en&amp;utm_term=promo&amp;utm_content=lp_meetup) of 2023 on Jan 31!\n\nhttps://preview.redd.it/p065ppb4inca1.jpg?width=612&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=21dbb8c3c9fca9f59257ce30f3d53bcec85076d4", "author_fullname": "t2_5dvfu5m2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seattle Spark Meetup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "media_metadata": {"p065ppb4inca1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 65, "x": 108, "u": "https://preview.redd.it/p065ppb4inca1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd9eb36775709b68eaee28f59ef7034a2892dba3"}, {"y": 130, "x": 216, "u": "https://preview.redd.it/p065ppb4inca1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92f406cec1da38d5a0a4a69addb38148a1b27cc9"}, {"y": 192, "x": 320, "u": "https://preview.redd.it/p065ppb4inca1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ecdb676d3374bcac03cad41c5a995896b60d556"}], "s": {"y": 369, "x": 612, "u": "https://preview.redd.it/p065ppb4inca1.jpg?width=612&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=21dbb8c3c9fca9f59257ce30f3d53bcec85076d4"}, "id": "p065ppb4inca1"}}, "name": "t3_10ek8ut", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5XJvvvK19Bd24JizxyEjIfnJoXEDbapKMQAvQ-ji-ck.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673981977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Come join the first &lt;a href=\"https://www.meetup.com/Seattle-Spark-Meetup/events/290865855/?response=3&amp;amp;action=rsvp&amp;amp;utm_medium=email&amp;amp;utm_source=braze_canvas&amp;amp;utm_campaign=mmrk_alleng_event_announcement_prod_v7_en&amp;amp;utm_term=promo&amp;amp;utm_content=lp_meetup\"&gt;Seattle Spark Meetup&lt;/a&gt; of 2023 on Jan 31!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/p065ppb4inca1.jpg?width=612&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=21dbb8c3c9fca9f59257ce30f3d53bcec85076d4\"&gt;https://preview.redd.it/p065ppb4inca1.jpg?width=612&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=21dbb8c3c9fca9f59257ce30f3d53bcec85076d4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "10ek8ut", "is_robot_indexable": true, "report_reasons": null, "author": "bp_ryan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10ek8ut/seattle_spark_meetup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ek8ut/seattle_spark_meetup/", "subreddit_subscribers": 86646, "created_utc": 1673981977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working as a data engineer/data integration for 8+ years from traditional ETL to build data warehouses to cloud (mainly GCP) in large Biotech and start-up Fintech. \n\nI wanna use these skillets to work at a non-profit for animal welfare or organization rather than working for corporations. I wanna contribute my skill to something more meaningful. \n\nI've been searching but it's a bit tricky to find these organizations or they even have job postings. \n\nWhat do you recommend? Or site I should look at? Beside LinkedIn, indeed, zip recruiter, etc...", "author_fullname": "t2_16lpc7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to search for job posting at animal non profit organizations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10f7xf7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674050016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as a data engineer/data integration for 8+ years from traditional ETL to build data warehouses to cloud (mainly GCP) in large Biotech and start-up Fintech. &lt;/p&gt;\n\n&lt;p&gt;I wanna use these skillets to work at a non-profit for animal welfare or organization rather than working for corporations. I wanna contribute my skill to something more meaningful. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been searching but it&amp;#39;s a bit tricky to find these organizations or they even have job postings. &lt;/p&gt;\n\n&lt;p&gt;What do you recommend? Or site I should look at? Beside LinkedIn, indeed, zip recruiter, etc...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10f7xf7", "is_robot_indexable": true, "report_reasons": null, "author": "Totoro328", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f7xf7/how_to_search_for_job_posting_at_animal_non/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f7xf7/how_to_search_for_job_posting_at_animal_non/", "subreddit_subscribers": 86646, "created_utc": 1674050016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# [Jailer Database Tools.](https://wisser.github.io/Jailer/)\n\nJailer is a tool for database subsetting and relational data browsing.\n\nIt creates small slices from your database and lets you navigate through your database following the relationships.Ideal for creating small samples of test data or for local problem analysis with relevant production data.\n\nThe Subsetter creates small slices from your database (consistent and reverentially intact) as SQL (topologically sorted), DbUnit records or XML. Ideal for creating small samples of test data or for local problem analysis with relevant production data.\n\nThe Data Browser lets you navigate through your database following the relationships (foreign key-based or user-defined) between tables.\n\n# Features\n\nExports consistent and reverentially intact row-sets from your productive database and imports the data into your development and test environment.\n\nImproves database performance by removing and archiving obsolete data without violating integrity.\n\nGenerates topologically sorted SQL-DML, hierarchically structured XML and DbUnit datasets.\n\nData Browsing. Navigate bidirectionally through the database by following foreign-key-based or user-defined relationships.\n\nSQL Console with code completion, syntax highlighting and database metadata visualization.\n\nA demo database is included with which you can get a first impression without any configuration effort.", "author_fullname": "t2_5sa5b0ia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New release of Jailer database tools publicized", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10f3lpo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674035839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;&lt;a href=\"https://wisser.github.io/Jailer/\"&gt;Jailer Database Tools.&lt;/a&gt;&lt;/h1&gt;\n\n&lt;p&gt;Jailer is a tool for database subsetting and relational data browsing.&lt;/p&gt;\n\n&lt;p&gt;It creates small slices from your database and lets you navigate through your database following the relationships.Ideal for creating small samples of test data or for local problem analysis with relevant production data.&lt;/p&gt;\n\n&lt;p&gt;The Subsetter creates small slices from your database (consistent and reverentially intact) as SQL (topologically sorted), DbUnit records or XML. Ideal for creating small samples of test data or for local problem analysis with relevant production data.&lt;/p&gt;\n\n&lt;p&gt;The Data Browser lets you navigate through your database following the relationships (foreign key-based or user-defined) between tables.&lt;/p&gt;\n\n&lt;h1&gt;Features&lt;/h1&gt;\n\n&lt;p&gt;Exports consistent and reverentially intact row-sets from your productive database and imports the data into your development and test environment.&lt;/p&gt;\n\n&lt;p&gt;Improves database performance by removing and archiving obsolete data without violating integrity.&lt;/p&gt;\n\n&lt;p&gt;Generates topologically sorted SQL-DML, hierarchically structured XML and DbUnit datasets.&lt;/p&gt;\n\n&lt;p&gt;Data Browsing. Navigate bidirectionally through the database by following foreign-key-based or user-defined relationships.&lt;/p&gt;\n\n&lt;p&gt;SQL Console with code completion, syntax highlighting and database metadata visualization.&lt;/p&gt;\n\n&lt;p&gt;A demo database is included with which you can get a first impression without any configuration effort.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "10f3lpo", "is_robot_indexable": true, "report_reasons": null, "author": "Plane-Discussion", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f3lpo/new_release_of_jailer_database_tools_publicized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f3lpo/new_release_of_jailer_database_tools_publicized/", "subreddit_subscribers": 86646, "created_utc": 1674035839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone out there have experience with these tools?  If so, how do they compare in your opinion?", "author_fullname": "t2_5lfqidpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alation vs. Atlan vs. Collibra", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10esiz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674001376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone out there have experience with these tools?  If so, how do they compare in your opinion?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10esiz9", "is_robot_indexable": true, "report_reasons": null, "author": "imani_TqiynAZU", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10esiz9/alation_vs_atlan_vs_collibra/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10esiz9/alation_vs_atlan_vs_collibra/", "subreddit_subscribers": 86646, "created_utc": 1674001376.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nTL;DR\n\nSole Data Engineer in the company is to propose a new architecture to gradually connect 30 ERP systems. There is a failed Data Vault project because it was designed as a Source System Data Vault and there are also errors in the technical implementation.\n\nThe existing technologies are Snowflake and Wherescape.\n\nThe question is whether another Data Vault project should be set up or whether the new DWH should be set up differently.\n\n&amp;#x200B;\n\nI am new in a company, the only one who has anything to do with data engineering/BI. The previous developer left, leaving behind a failed Data Vault project. I am Data Vault 2.0 certified, but have never had to implement it in practice. I say failed Data Vault project because it is 1:1 to the source system and the historisation does not work. So there is a source data vault (Anti Pattern) that does not even historise the data.\n\nThis was implemented with Wherescape as the automation tool and Snowflake as the database. No upstream data lake or anything similar.\n\n&amp;#x200B;\n\nI have been using Informatica PowerCenter and have strong SQL skills and know dimensional modelling, but I am no longer interested in no code tools like PowerCenter, Talent, SSIS etc. and would be happy to leave them behind.\n\n&amp;#x200B;\n\nThe company is based in the manufacturing sector and has a total of over 30 more or less small sites, the largest of which run their own ERP systems.\n\nThe goal is (and was) to first integrate the ERP systems of the two largest sites into one DWH and then connect other sites. The larger locations each use the same ERP system (MS Dynamics NAV on-prem) and the smaller locations, for example, MS Dynamics 365 Business Central (Cloud). The departments only need daily updated data, which is good because CDC mechanisms are not active in the ERP databases and the DB responsibles have not had anything to do with it yet.\n\nAs far as I can see, only Azure AD and VMs are used in Azure.\n\n&amp;#x200B;\n\nThe question for me now is how to proceed, as my supervisor has asked me to suggest which architecture and data model we want to use.\n\nHow would you approach the issue?\n\nI see the possibility of creating the DV model again with the help of external support and implementing it using Wherescape.\n\nAlternatively, it would be conceivable to load the data into Snowflake and build a persistent staging area (PSA) there and transform the whole thing using dbt. I would find dbt interesting because it is essentially SQL. But then it would have to be decided which tools would be used for extraction and orchestration.", "author_fullname": "t2_uyjws4s8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DWH architecture for the integration of 30 sites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ejp50", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673980681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;TL;DR&lt;/p&gt;\n\n&lt;p&gt;Sole Data Engineer in the company is to propose a new architecture to gradually connect 30 ERP systems. There is a failed Data Vault project because it was designed as a Source System Data Vault and there are also errors in the technical implementation.&lt;/p&gt;\n\n&lt;p&gt;The existing technologies are Snowflake and Wherescape.&lt;/p&gt;\n\n&lt;p&gt;The question is whether another Data Vault project should be set up or whether the new DWH should be set up differently.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am new in a company, the only one who has anything to do with data engineering/BI. The previous developer left, leaving behind a failed Data Vault project. I am Data Vault 2.0 certified, but have never had to implement it in practice. I say failed Data Vault project because it is 1:1 to the source system and the historisation does not work. So there is a source data vault (Anti Pattern) that does not even historise the data.&lt;/p&gt;\n\n&lt;p&gt;This was implemented with Wherescape as the automation tool and Snowflake as the database. No upstream data lake or anything similar.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have been using Informatica PowerCenter and have strong SQL skills and know dimensional modelling, but I am no longer interested in no code tools like PowerCenter, Talent, SSIS etc. and would be happy to leave them behind.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The company is based in the manufacturing sector and has a total of over 30 more or less small sites, the largest of which run their own ERP systems.&lt;/p&gt;\n\n&lt;p&gt;The goal is (and was) to first integrate the ERP systems of the two largest sites into one DWH and then connect other sites. The larger locations each use the same ERP system (MS Dynamics NAV on-prem) and the smaller locations, for example, MS Dynamics 365 Business Central (Cloud). The departments only need daily updated data, which is good because CDC mechanisms are not active in the ERP databases and the DB responsibles have not had anything to do with it yet.&lt;/p&gt;\n\n&lt;p&gt;As far as I can see, only Azure AD and VMs are used in Azure.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The question for me now is how to proceed, as my supervisor has asked me to suggest which architecture and data model we want to use.&lt;/p&gt;\n\n&lt;p&gt;How would you approach the issue?&lt;/p&gt;\n\n&lt;p&gt;I see the possibility of creating the DV model again with the help of external support and implementing it using Wherescape.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, it would be conceivable to load the data into Snowflake and build a persistent staging area (PSA) there and transform the whole thing using dbt. I would find dbt interesting because it is essentially SQL. But then it would have to be decided which tools would be used for extraction and orchestration.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ejp50", "is_robot_indexable": true, "report_reasons": null, "author": "Individual-Detail286", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ejp50/dwh_architecture_for_the_integration_of_30_sites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ejp50/dwh_architecture_for_the_integration_of_30_sites/", "subreddit_subscribers": 86646, "created_utc": 1673980681.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just asking this question conceptually as I don't have experience in this area yet, trying to do my research but i dont fully understand how this works yet.\n\nI'm imagining you have something like Debezium + Kafka stream doing CDC from some rdbms. When this arrives in your dwh im guessing you get a sequence of some kind of SQL DML statements (UPDATE, INSERT etc), or the equivalent in another format? Do you then have to parse and execute these line-by-line to reconstruct the current state in a raw dwh table? How does it work?", "author_fullname": "t2_18xb5x05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you reconstruct current state in DWh from a stream of CDC events?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ef640", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673969748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just asking this question conceptually as I don&amp;#39;t have experience in this area yet, trying to do my research but i dont fully understand how this works yet.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m imagining you have something like Debezium + Kafka stream doing CDC from some rdbms. When this arrives in your dwh im guessing you get a sequence of some kind of SQL DML statements (UPDATE, INSERT etc), or the equivalent in another format? Do you then have to parse and execute these line-by-line to reconstruct the current state in a raw dwh table? How does it work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ef640", "is_robot_indexable": true, "report_reasons": null, "author": "the-data-scientist", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ef640/how_do_you_reconstruct_current_state_in_dwh_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ef640/how_do_you_reconstruct_current_state_in_dwh_from/", "subreddit_subscribers": 86646, "created_utc": 1673969748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "10 evaluation criteria for Data Orchestrators", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10edrdl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1673966147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/posts/marclamberti_dataengineering-dataengineer-data-activity-7021114084053917696-3CAF?utm_source=share&amp;utm_medium=member_desktop", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10edrdl", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10edrdl/10_evaluation_criteria_for_data_orchestrators/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/posts/marclamberti_dataengineering-dataengineer-data-activity-7021114084053917696-3CAF?utm_source=share&amp;utm_medium=member_desktop", "subreddit_subscribers": 86646, "created_utc": 1673966147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all!\n\nI received an email response from a company I applied to that they would like to interview me for the role of \"Jr. Cloud Engineer\". I was ecstatic that I'm being considered, as I have minimal DE experience. It's also in a field that I'm very passionate about. \n\nHowever, I honestly don't really know what a \"cloud engineer\" is? Does it differ at all from a DE? \n\nThis is what the general job duties would be:\n\n* Identifying and implementing optimal cloud-based solutions for Sports IQ, including team education and training\n* Participating in the software development life cycle, including planning, requirements, development, testing, and quality assurance\n* Building tools and systems to improve the time to market of product offerings in partnership with Product, Trading, and Data Science\n* Troubleshooting incidents, identifying root causes, fixing and documenting problems, and implementing preventive measures\n* Orchestrating and automating cloud operations and processes\n* Collaborating with stakeholders across the business to balance competing objectives\n* Working with third-party vendors to meet business requirements.\n\nIn your opinion, what does this role actually sound like? Is there a more apropriate job title than what they have listed?\n\nAlso, the role is looking for someone with Google Cloud experience, but I only have experience in AWS and Azure. In AWS, I pretty much only use it for creating VM's. While on Azure, I deploy DB's to the cloud, monitor elastic pool data storage, use failover groups etc. Are there significant differences between Google Cloud and AWS/Azure I should be aware of prior to this interview?\n\nThank you for your help!", "author_fullname": "t2_1x7s010", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Cloud Engineer\" Interview - Is it Different from DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10edpwi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673966042.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all!&lt;/p&gt;\n\n&lt;p&gt;I received an email response from a company I applied to that they would like to interview me for the role of &amp;quot;Jr. Cloud Engineer&amp;quot;. I was ecstatic that I&amp;#39;m being considered, as I have minimal DE experience. It&amp;#39;s also in a field that I&amp;#39;m very passionate about. &lt;/p&gt;\n\n&lt;p&gt;However, I honestly don&amp;#39;t really know what a &amp;quot;cloud engineer&amp;quot; is? Does it differ at all from a DE? &lt;/p&gt;\n\n&lt;p&gt;This is what the general job duties would be:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Identifying and implementing optimal cloud-based solutions for Sports IQ, including team education and training&lt;/li&gt;\n&lt;li&gt;Participating in the software development life cycle, including planning, requirements, development, testing, and quality assurance&lt;/li&gt;\n&lt;li&gt;Building tools and systems to improve the time to market of product offerings in partnership with Product, Trading, and Data Science&lt;/li&gt;\n&lt;li&gt;Troubleshooting incidents, identifying root causes, fixing and documenting problems, and implementing preventive measures&lt;/li&gt;\n&lt;li&gt;Orchestrating and automating cloud operations and processes&lt;/li&gt;\n&lt;li&gt;Collaborating with stakeholders across the business to balance competing objectives&lt;/li&gt;\n&lt;li&gt;Working with third-party vendors to meet business requirements.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In your opinion, what does this role actually sound like? Is there a more apropriate job title than what they have listed?&lt;/p&gt;\n\n&lt;p&gt;Also, the role is looking for someone with Google Cloud experience, but I only have experience in AWS and Azure. In AWS, I pretty much only use it for creating VM&amp;#39;s. While on Azure, I deploy DB&amp;#39;s to the cloud, monitor elastic pool data storage, use failover groups etc. Are there significant differences between Google Cloud and AWS/Azure I should be aware of prior to this interview?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10edpwi", "is_robot_indexable": true, "report_reasons": null, "author": "HercHuntsdirty", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10edpwi/cloud_engineer_interview_is_it_different_from_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10edpwi/cloud_engineer_interview_is_it_different_from_de/", "subreddit_subscribers": 86646, "created_utc": 1673966042.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a company that does geographic data processing for the country. We have a project that uses a library, previously made by the company, that contains several processing modules, where each module does the prediction or analysis of different types of geographic events for small regions of the country. For each region that we are going to monitor, we create a new virtual machine and do a workflow with airflow, and choose the module that will be applied to that region. The problem is that this is obviously getting too bad to manage or scale. We would also like to integrate the part of creating new monitoring flows with our portal, allowing the end user to create them, rather than it being an internal process.\n\n\n\nI was thinking of changing this architecture to something like:\n\n- Create a table in the database to manage the stream control, where each row would have the information of the stream type, periodicity, region, module and if it is being processed by any machines. Ex:\n\n\n\nTable Worflows:\n\nID | Module | Region | Period | Status\n---|---|----|----|----\n0 | RainPrediction | 55AX | 1 hour | Processing\n1 | RainPrediction | 45TB | 1 hour | Free\n2 | TornadoPrediction | 55AX | 1 hour | Processing\n\n\nTable Workflow_Logs\n\nID | Worflow_ID | Started_DT | Finished_DT | Logs \n---|---|----|----|----    \n0 | 1 | 13:00 | 13:17 | Lorem Ipsum...\n\n\n\n- Instead of creating a machine to run each region of each module, I am thinking of adapting the lib to have a main entrypoint that receives parameters and can run the X algorithm for the Y region. Then this machine would scan the flow control table looking for the tasks that are not being processed in order to process them.\n\n\n\n- Adapt the frontend so that the user can insert a new row into this flow control table.\n\n\n\nThen when we need to scale up our algorithm we would just need to create a new container and this container would start reading and fetching unprocessed tasks. I think I am trying to reinvent the wheel by doing this and would like some suggestions that do not increase the complexity of the project too much. I have been reading about kafka but it seems to add a lot of complexity to the project.", "author_fullname": "t2_9ca451os", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions on how to improve the architecture that I am working on", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10emkmw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673987372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a company that does geographic data processing for the country. We have a project that uses a library, previously made by the company, that contains several processing modules, where each module does the prediction or analysis of different types of geographic events for small regions of the country. For each region that we are going to monitor, we create a new virtual machine and do a workflow with airflow, and choose the module that will be applied to that region. The problem is that this is obviously getting too bad to manage or scale. We would also like to integrate the part of creating new monitoring flows with our portal, allowing the end user to create them, rather than it being an internal process.&lt;/p&gt;\n\n&lt;p&gt;I was thinking of changing this architecture to something like:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Create a table in the database to manage the stream control, where each row would have the information of the stream type, periodicity, region, module and if it is being processed by any machines. Ex:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Table Worflows:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;Module&lt;/th&gt;\n&lt;th&gt;Region&lt;/th&gt;\n&lt;th&gt;Period&lt;/th&gt;\n&lt;th&gt;Status&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;0&lt;/td&gt;\n&lt;td&gt;RainPrediction&lt;/td&gt;\n&lt;td&gt;55AX&lt;/td&gt;\n&lt;td&gt;1 hour&lt;/td&gt;\n&lt;td&gt;Processing&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;RainPrediction&lt;/td&gt;\n&lt;td&gt;45TB&lt;/td&gt;\n&lt;td&gt;1 hour&lt;/td&gt;\n&lt;td&gt;Free&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;TornadoPrediction&lt;/td&gt;\n&lt;td&gt;55AX&lt;/td&gt;\n&lt;td&gt;1 hour&lt;/td&gt;\n&lt;td&gt;Processing&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Table Workflow_Logs&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;Worflow_ID&lt;/th&gt;\n&lt;th&gt;Started_DT&lt;/th&gt;\n&lt;th&gt;Finished_DT&lt;/th&gt;\n&lt;th&gt;Logs&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;0&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;13:00&lt;/td&gt;\n&lt;td&gt;13:17&lt;/td&gt;\n&lt;td&gt;Lorem Ipsum...&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Instead of creating a machine to run each region of each module, I am thinking of adapting the lib to have a main entrypoint that receives parameters and can run the X algorithm for the Y region. Then this machine would scan the flow control table looking for the tasks that are not being processed in order to process them.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Adapt the frontend so that the user can insert a new row into this flow control table.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Then when we need to scale up our algorithm we would just need to create a new container and this container would start reading and fetching unprocessed tasks. I think I am trying to reinvent the wheel by doing this and would like some suggestions that do not increase the complexity of the project too much. I have been reading about kafka but it seems to add a lot of complexity to the project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10emkmw", "is_robot_indexable": true, "report_reasons": null, "author": "MiserableAstronaut77", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10emkmw/suggestions_on_how_to_improve_the_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10emkmw/suggestions_on_how_to_improve_the_architecture/", "subreddit_subscribers": 86646, "created_utc": 1673987372.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}