{"kind": "Listing", "data": {"after": "t3_10edrdl", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background: I've been working with TB-scale data for several years. I realize this could sound like a DA/DS question, but almost all of the visualization products I have seen do not work well (at at all) at this scale or require significant pre-processing and customization, so I think this is appropriately a DE question. \n\nLet's say I have some data I want to visualize that is updated at an offline cadence, e.g. once a day. Parquet/Iceberg etc. New data is appended to daily partitions at a rate of \\~100GB/day. I have about a dozen dimensions and filters I would like to aggregate across or filter on, but the metrics themselves are somewhat basic (sums, counts, ratios, etc). Are there any visualization solutions that can handle interactive querying against data of this scale (interactive meaning the end user self-filter and update the visualization within a second or two)? What are the benefits/tradeoffs to current systems for this? Is this a use case to bring in a specialized OLAP cube product?", "author_fullname": "t2_7pxby", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who is doing large-scale visualization and dashboarding well?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10eov35", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673992650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background: I&amp;#39;ve been working with TB-scale data for several years. I realize this could sound like a DA/DS question, but almost all of the visualization products I have seen do not work well (at at all) at this scale or require significant pre-processing and customization, so I think this is appropriately a DE question. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I have some data I want to visualize that is updated at an offline cadence, e.g. once a day. Parquet/Iceberg etc. New data is appended to daily partitions at a rate of ~100GB/day. I have about a dozen dimensions and filters I would like to aggregate across or filter on, but the metrics themselves are somewhat basic (sums, counts, ratios, etc). Are there any visualization solutions that can handle interactive querying against data of this scale (interactive meaning the end user self-filter and update the visualization within a second or two)? What are the benefits/tradeoffs to current systems for this? Is this a use case to bring in a specialized OLAP cube product?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10eov35", "is_robot_indexable": true, "report_reasons": null, "author": "ColdPorridge", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10eov35/who_is_doing_largescale_visualization_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10eov35/who_is_doing_largescale_visualization_and/", "subreddit_subscribers": 86604, "created_utc": 1673992650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_m6gnxiuj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DW toolkit book by Ralph Kimball", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 96, "top_awarded_type": null, "hide_score": false, "name": "t3_10esybb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5ioEG9GaIPjg4DO8YEHXNaQLLEZ29VHNOBtRF9u2KaY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674002443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/yc0bxaypoqca1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?auto=webp&amp;v=enabled&amp;s=579f4c43ca548b1e0742912ba9dfea97c392c86f", "width": 2819, "height": 1953}, "resolutions": [{"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7d6aca1f1b6519a452a7852feed37a02fb924c0", "width": 108, "height": 74}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae5930e940d0713f43e6184c6c0df951e5af2353", "width": 216, "height": 149}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=954c43e1c94f190557f4541106f2dc24f4a5f1ea", "width": 320, "height": 221}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=726092b906848d30d07c31f9715bfbbd62fb79cd", "width": 640, "height": 443}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e790040401ee5a93fc61d0098c4843d73d6efb1", "width": 960, "height": 665}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7d765bedccbfc6d37bf7201318cec16c16e4926", "width": 1080, "height": 748}], "variants": {}, "id": "gKCsuiInCaKY56pQ3Kk6u9IsUHba7ufeNr9UMkEUlRw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10esybb", "is_robot_indexable": true, "report_reasons": null, "author": "rajekum512", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10esybb/dw_toolkit_book_by_ralph_kimball/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/yc0bxaypoqca1.jpg", "subreddit_subscribers": 86604, "created_utc": 1674002443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lets say ***a small company currently handles all data by sharing excel files over emails and has never heard the concept of a database***. \n\n***How do you transition this into a data warehouse?***\n\nDoes this mean now I have to write a lot of code to process a bunch of random csv files to turn them into a star schema?\n\nLet's say I managed to turn their csv files into a consistent set of tables... How do they access it? Do they need to hire a bunch of sql analysts to get the information? or Do you build some sort of dashboard out of it?  \n\n\nI know someone has suffered from this before.  \n\n\nHow would you approach it?  \nIs there a guide or a book for this?", "author_fullname": "t2_zs1xp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering for small companies.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10eurjm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674007202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say &lt;strong&gt;&lt;em&gt;a small company currently handles all data by sharing excel files over emails and has never heard the concept of a database&lt;/em&gt;&lt;/strong&gt;. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;How do you transition this into a data warehouse?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Does this mean now I have to write a lot of code to process a bunch of random csv files to turn them into a star schema?&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I managed to turn their csv files into a consistent set of tables... How do they access it? Do they need to hire a bunch of sql analysts to get the information? or Do you build some sort of dashboard out of it?  &lt;/p&gt;\n\n&lt;p&gt;I know someone has suffered from this before.  &lt;/p&gt;\n\n&lt;p&gt;How would you approach it?&lt;br/&gt;\nIs there a guide or a book for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10eurjm", "is_robot_indexable": true, "report_reasons": null, "author": "lFuckRedditl", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10eurjm/data_engineering_for_small_companies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10eurjm/data_engineering_for_small_companies/", "subreddit_subscribers": 86604, "created_utc": 1674007202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you document data products? I mean e.g. we use mainly Python for ETL jobs and our documentation consists of README in the git repo and sometimes (depending on the transformation complexity) also a Google Document describing the app, calculations, input and outputs etc. \n\nDo you have any better approach? I would really like to have some \u201cdocumentation framework\u201d that would have all data products described and explorable in one place. It would be nice to see products dependencies/lineage similar way you see in Airflow. I am thinking of some custom data catalog maybe, but firstly I would like to know what others do/have out there.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you document data products?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ekhrq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673982550.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you document data products? I mean e.g. we use mainly Python for ETL jobs and our documentation consists of README in the git repo and sometimes (depending on the transformation complexity) also a Google Document describing the app, calculations, input and outputs etc. &lt;/p&gt;\n\n&lt;p&gt;Do you have any better approach? I would really like to have some \u201cdocumentation framework\u201d that would have all data products described and explorable in one place. It would be nice to see products dependencies/lineage similar way you see in Airflow. I am thinking of some custom data catalog maybe, but firstly I would like to know what others do/have out there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ekhrq", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ekhrq/how_do_you_document_data_products/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ekhrq/how_do_you_document_data_products/", "subreddit_subscribers": 86604, "created_utc": 1673982550.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an external websocket APIs, some of them produce \\~1k events / second. My goal is to save them in database and don't lose any of the events. Currently I have only 1 listener for each API, that pushes the events to redis queue (with disk persistence ) and then I have 1 worker for each q to write to Postgres.\n\n&amp;#x200B;\n\nIt works fine, but when restarting the listener (pulling new commit or simple power outage problem) - I lose some events during this time. So I was thinking to run at least 2 separate nodes with listener for each API. That way I can restart them one by one and dont lose any events. But that means that redis q will have almost all events duplicated. What if I will run 4 listeners instead of 2? Writing this q to db with cause a lot of vacuum. Probably I can just leave the job of deduplicaiton to a worker, that remove all dups from batch he receive, before writing to db.\n\n&amp;#x200B;\n\nWhat is the way to handle that? As far as I understood Kafka and pulsar does only have message ID deduplication. And you can't set custom ids based on your content", "author_fullname": "t2_3ppayh15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Queue with content deduplication", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ei41t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673978808.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673976948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an external websocket APIs, some of them produce ~1k events / second. My goal is to save them in database and don&amp;#39;t lose any of the events. Currently I have only 1 listener for each API, that pushes the events to redis queue (with disk persistence ) and then I have 1 worker for each q to write to Postgres.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It works fine, but when restarting the listener (pulling new commit or simple power outage problem) - I lose some events during this time. So I was thinking to run at least 2 separate nodes with listener for each API. That way I can restart them one by one and dont lose any events. But that means that redis q will have almost all events duplicated. What if I will run 4 listeners instead of 2? Writing this q to db with cause a lot of vacuum. Probably I can just leave the job of deduplicaiton to a worker, that remove all dups from batch he receive, before writing to db.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is the way to handle that? As far as I understood Kafka and pulsar does only have message ID deduplication. And you can&amp;#39;t set custom ids based on your content&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ei41t", "is_robot_indexable": true, "report_reasons": null, "author": "dotaleaker", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ei41t/queue_with_content_deduplication/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ei41t/queue_with_content_deduplication/", "subreddit_subscribers": 86604, "created_utc": 1673976948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uo96pc9v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resume advice needed. Applying for Data Engineering roles. I have mentioned a couple of projects in my portfolio (70milan.github.io) to avoid mentioning it on my resume yet cant get all the information on a single page. Reduced font size to 10.5-11 pts, more suggestions appreciated.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10egk6a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/T0N6_lfidPVS48rX80qr0wpSGrW4PYmlfvvNLg3do8Q.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673973224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/g1mevnbu9oca1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/g1mevnbu9oca1.png?auto=webp&amp;v=enabled&amp;s=3e596692bec3a08e2b341393ad293a9b84a6697f", "width": 1080, "height": 2109}, "resolutions": [{"url": "https://preview.redd.it/g1mevnbu9oca1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02625a9372ce393e8c2cb92b5bfce88411f77097", "width": 108, "height": 210}, {"url": "https://preview.redd.it/g1mevnbu9oca1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2f181323f1414795b240f10e18f6dbdd862aa9a", "width": 216, "height": 421}, {"url": "https://preview.redd.it/g1mevnbu9oca1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73c23182414f17420323af47065bbb41310d4201", "width": 320, "height": 624}, {"url": "https://preview.redd.it/g1mevnbu9oca1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67a7cfe8d623389adfd606b527435833afc71144", "width": 640, "height": 1249}, {"url": "https://preview.redd.it/g1mevnbu9oca1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f157e0da899ce434d2440d624158ed36fa832b9c", "width": 960, "height": 1874}, {"url": "https://preview.redd.it/g1mevnbu9oca1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91110b26c772afb5c571e34f0c302097547b4c5d", "width": 1080, "height": 2109}], "variants": {}, "id": "zGs4iRzbDByVHiSEeV-DzXrD3k8sLiwSYbRqDk5n1vQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10egk6a", "is_robot_indexable": true, "report_reasons": null, "author": "purplexed247", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10egk6a/resume_advice_needed_applying_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/g1mevnbu9oca1.png", "subreddit_subscribers": 86604, "created_utc": 1673973224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/10eceg8)", "author_fullname": "t2_14i1sz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you have a Bachelors, Masters, or PhD in computer science or another STEM discipline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10eceg8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673962480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/10eceg8\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10eceg8", "is_robot_indexable": true, "report_reasons": null, "author": "pescennius", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1674221680279, "options": [{"text": "Yes", "id": "21069065"}, {"text": "No, but bootcamp or other certification", "id": "21069066"}, {"text": "No", "id": "21069067"}, {"text": "See Results", "id": "21069068"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 263, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10eceg8/do_you_have_a_bachelors_masters_or_phd_in/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/10eceg8/do_you_have_a_bachelors_masters_or_phd_in/", "subreddit_subscribers": 86604, "created_utc": 1673962480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_alka6pjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resume Review request - BA to DE Pivot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10ezuz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9KYzoafqeDnG-_PpUWLOByjbMlz1QfJEuPzJVCCVOxo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674021917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/lrfnoo02tqca1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/lrfnoo02tqca1.png?auto=webp&amp;v=enabled&amp;s=91b8620f075e5ce896f4b5e14c0d567427cf6f54", "width": 422, "height": 516}, "resolutions": [{"url": "https://preview.redd.it/lrfnoo02tqca1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85d093ca93176a394919ff46e28fecee90627c97", "width": 108, "height": 132}, {"url": "https://preview.redd.it/lrfnoo02tqca1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51bfb89f9e5b6abaed0c5f1291aaf34be7a05594", "width": 216, "height": 264}, {"url": "https://preview.redd.it/lrfnoo02tqca1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5dbe508bd3270e09688cf52b82f2c1631f491b38", "width": 320, "height": 391}], "variants": {}, "id": "NKWY7CXzH_LGoyIUo72kU-hbkmDTtkN7Hugu4VkV8Wg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10ezuz9", "is_robot_indexable": true, "report_reasons": null, "author": "depressedbutsassy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ezuz9/resume_review_request_ba_to_de_pivot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/lrfnoo02tqca1.png", "subreddit_subscribers": 86604, "created_utc": 1674021917.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've mostly been web dev in my career and only recently joined a team that is data eng. In web dev Blue Green deployments are super easy, since you just can route requests between blue and green at DNS or load balancer.   \n\n\nHowever most of the data pipeline stuff I've seen in my team is something like:   \n1. Poll or listen for new data  \n2. Do stuff  \n3. Write new data somewhere else  \n\n\nBlue/Green doesn't work here as much because the instances themselves are reading and writing continuously. So there is no external control mechanism like DNS, so you need internal controls which could work but may be brittle.  \n\n\nWhat patterns have you found successful for bringing up and swapping new versions of running pipeline services?  \n\n\nExtra context: Everything is pretty homegrown, ec2 style deployments.", "author_fullname": "t2_gn2ff6o9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Green Blue Style deployments for continuous data pipeline services?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10egxfp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673974134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve mostly been web dev in my career and only recently joined a team that is data eng. In web dev Blue Green deployments are super easy, since you just can route requests between blue and green at DNS or load balancer.   &lt;/p&gt;\n\n&lt;p&gt;However most of the data pipeline stuff I&amp;#39;ve seen in my team is something like:&lt;br/&gt;\n1. Poll or listen for new data&lt;br/&gt;\n2. Do stuff&lt;br/&gt;\n3. Write new data somewhere else  &lt;/p&gt;\n\n&lt;p&gt;Blue/Green doesn&amp;#39;t work here as much because the instances themselves are reading and writing continuously. So there is no external control mechanism like DNS, so you need internal controls which could work but may be brittle.  &lt;/p&gt;\n\n&lt;p&gt;What patterns have you found successful for bringing up and swapping new versions of running pipeline services?  &lt;/p&gt;\n\n&lt;p&gt;Extra context: Everything is pretty homegrown, ec2 style deployments.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10egxfp", "is_robot_indexable": true, "report_reasons": null, "author": "fthb1000000", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10egxfp/green_blue_style_deployments_for_continuous_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10egxfp/green_blue_style_deployments_for_continuous_data/", "subreddit_subscribers": 86604, "created_utc": 1673974134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7m65ddby", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hello!Long time lurker here.I have been in the same team for the past 4 years. First as a third party client and then got onboarded directly.Please critique my resume and give advice as it's been awhile since I stepped into jobsearch market.I'm also looking for anyone interested in mentoring me.Thnx", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10efcva", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AKgDXYNTpD1Jg8ZmtGbifv1jIXPWUUAPCoBA64qzJ2w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673970249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/403zvyhgimca1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/403zvyhgimca1.jpg?auto=webp&amp;v=enabled&amp;s=4b56fb6ced56a6c01b3b919c1fbbf7d4f945fdcc", "width": 1275, "height": 1650}, "resolutions": [{"url": "https://preview.redd.it/403zvyhgimca1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5041a3d29851faa868eeba65b30162d2a6ee8964", "width": 108, "height": 139}, {"url": "https://preview.redd.it/403zvyhgimca1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a44f194c6138a5402574c1eaecd18cf60e3caf58", "width": 216, "height": 279}, {"url": "https://preview.redd.it/403zvyhgimca1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f9a74f887a00689a8d2708580b065eb2e0c191f", "width": 320, "height": 414}, {"url": "https://preview.redd.it/403zvyhgimca1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb2134164d4b4158e52346f9f05167c5f37b3f14", "width": 640, "height": 828}, {"url": "https://preview.redd.it/403zvyhgimca1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6914230bcddacce5369210865337df2d8a7e1678", "width": 960, "height": 1242}, {"url": "https://preview.redd.it/403zvyhgimca1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c0785a7f2f3cafefa4b43cba32d99a0ccbb76fd", "width": 1080, "height": 1397}], "variants": {}, "id": "OaA2Q8gdn0nTL1p4-wL_cBJ9nte5JA8_XasuNcdkQ4Y"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10efcva", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive_Ball_965", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10efcva/hellolong_time_lurker_herei_have_been_in_the_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/403zvyhgimca1.jpg", "subreddit_subscribers": 86604, "created_utc": 1673970249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a subscription in Azure. It is running a VM. One of the VMs is windows, the other linux. Both are set up with python, and all of the things i need to run my ETL and anything else i would need within.\n\nFor instance. I have file delivery set up to receive and receive files. I am running python scripts to do all the various tasks. And task scheduler/cron to schedule said scripts. All of my logging is set up and i am able to email/text myself notifications/failures.\n\nI understand however that serverless functions are the way to cut down on costs. \n\nI have yet to figure out how to implement this.  What am i missing? Does anyone have any reading on setting up some serverless function to schedule a basic ETL task?", "author_fullname": "t2_5ch80", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure serverless ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10edqw4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673969203.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673966113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a subscription in Azure. It is running a VM. One of the VMs is windows, the other linux. Both are set up with python, and all of the things i need to run my ETL and anything else i would need within.&lt;/p&gt;\n\n&lt;p&gt;For instance. I have file delivery set up to receive and receive files. I am running python scripts to do all the various tasks. And task scheduler/cron to schedule said scripts. All of my logging is set up and i am able to email/text myself notifications/failures.&lt;/p&gt;\n\n&lt;p&gt;I understand however that serverless functions are the way to cut down on costs. &lt;/p&gt;\n\n&lt;p&gt;I have yet to figure out how to implement this.  What am i missing? Does anyone have any reading on setting up some serverless function to schedule a basic ETL task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10edqw4", "is_robot_indexable": true, "report_reasons": null, "author": "circusboy", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10edqw4/azure_serverless_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10edqw4/azure_serverless_etl/", "subreddit_subscribers": 86604, "created_utc": 1673966113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's say we have a customer dimension, and a new subject area is being integrated into DWH, with its own customer base. Like good data engineers we are, we follow best industry practices and start conforming new customer data into existing customer dimension. Since we're conforming different systems we can't match customers by id, so we decide to match them by next best thing, which is customer\\_name. Some customer\\_names in two systems are obviously the same customers in real world but have slighthly different names in our systems, for example \"Acme\" in our existing dimension and  \"Acme Inc.\" in new subject area. The way to reconcile the values during development is to manually review such cases and rename them in order to match them.\n\nHere are my questions to you guys:\n\n* Inevitably, such cases will keep occuring after development is done, with production data pipeline happily loading the DWH without any manual intervention. How on earth to reconcile such similar values automatically?\n* Can this even be automated? Or there MUST be a a person who will manually check such rows?\n* I've tried Fuzzy Lookup and it works OK for reducing the amount of manual work, but still there are rows that algorith didn't pick up but should've, so again there's a need for manual intervention, and it doesn't solve the fundamental problem\n* What happens where there are hundreds or thousands or rows to compare?\n\nThanks guys!", "author_fullname": "t2_vkq0hftr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Conformed Dimensions problem that keeps recurring on every project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10eac4a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673956085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say we have a customer dimension, and a new subject area is being integrated into DWH, with its own customer base. Like good data engineers we are, we follow best industry practices and start conforming new customer data into existing customer dimension. Since we&amp;#39;re conforming different systems we can&amp;#39;t match customers by id, so we decide to match them by next best thing, which is customer_name. Some customer_names in two systems are obviously the same customers in real world but have slighthly different names in our systems, for example &amp;quot;Acme&amp;quot; in our existing dimension and  &amp;quot;Acme Inc.&amp;quot; in new subject area. The way to reconcile the values during development is to manually review such cases and rename them in order to match them.&lt;/p&gt;\n\n&lt;p&gt;Here are my questions to you guys:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Inevitably, such cases will keep occuring after development is done, with production data pipeline happily loading the DWH without any manual intervention. How on earth to reconcile such similar values automatically?&lt;/li&gt;\n&lt;li&gt;Can this even be automated? Or there MUST be a a person who will manually check such rows?&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve tried Fuzzy Lookup and it works OK for reducing the amount of manual work, but still there are rows that algorith didn&amp;#39;t pick up but should&amp;#39;ve, so again there&amp;#39;s a need for manual intervention, and it doesn&amp;#39;t solve the fundamental problem&lt;/li&gt;\n&lt;li&gt;What happens where there are hundreds or thousands or rows to compare?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10eac4a", "is_robot_indexable": true, "report_reasons": null, "author": "moj_nick", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10eac4a/conformed_dimensions_problem_that_keeps_recurring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10eac4a/conformed_dimensions_problem_that_keeps_recurring/", "subreddit_subscribers": 86604, "created_utc": 1673956085.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, is there a good implementation for Delta Sharing usable for on-prem deployments?\n\nThe reference implementation provided doesn't support adding new shares without modifying the configuration file and restarting the server, also there's no user management there.", "author_fullname": "t2_4clu4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Sharing on premise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ezch7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674020331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, is there a good implementation for Delta Sharing usable for on-prem deployments?&lt;/p&gt;\n\n&lt;p&gt;The reference implementation provided doesn&amp;#39;t support adding new shares without modifying the configuration file and restarting the server, also there&amp;#39;s no user management there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ezch7", "is_robot_indexable": true, "report_reasons": null, "author": "inteloid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ezch7/delta_sharing_on_premise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ezch7/delta_sharing_on_premise/", "subreddit_subscribers": 86604, "created_utc": 1674020331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When you do a merge in SQL, is it doing a full table scan of the target? If so, isn\u2019t that something we want to avoid?", "author_fullname": "t2_5ukitegd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MERGE statement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10exb2o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674014249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When you do a merge in SQL, is it doing a full table scan of the target? If so, isn\u2019t that something we want to avoid?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10exb2o", "is_robot_indexable": true, "report_reasons": null, "author": "burningburnerbern", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10exb2o/merge_statement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10exb2o/merge_statement/", "subreddit_subscribers": 86604, "created_utc": 1674014249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can somebody help? In over my head trying to run a product assessment for Databricks with almost no guidance for the use case. Would be helpful to hear what folks really like about the product + what questions people wish they had asked or better understood before becoming a customer", "author_fullname": "t2_tye5ydmn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks evaluation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10euyrz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674007714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can somebody help? In over my head trying to run a product assessment for Databricks with almost no guidance for the use case. Would be helpful to hear what folks really like about the product + what questions people wish they had asked or better understood before becoming a customer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10euyrz", "is_robot_indexable": true, "report_reasons": null, "author": "LuckyChopsSOS", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10euyrz/databricks_evaluation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10euyrz/databricks_evaluation/", "subreddit_subscribers": 86604, "created_utc": 1674007714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some time series data coming from different sources that have different levels of granularity/frequency\n\nSome data comes in monthly, some daily, some hourly \n\nIs it possible to store all this in one time series database? We like the querying benefits a time series db provides for time series but I wasn\u2019t sure if all the data has to be the same level of granularity", "author_fullname": "t2_7nbpziqx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time series databases with different granularity or multiple time series", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10elmce", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673985168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some time series data coming from different sources that have different levels of granularity/frequency&lt;/p&gt;\n\n&lt;p&gt;Some data comes in monthly, some daily, some hourly &lt;/p&gt;\n\n&lt;p&gt;Is it possible to store all this in one time series database? We like the querying benefits a time series db provides for time series but I wasn\u2019t sure if all the data has to be the same level of granularity&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10elmce", "is_robot_indexable": true, "report_reasons": null, "author": "Affectionate_Shine55", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10elmce/time_series_databases_with_different_granularity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10elmce/time_series_databases_with_different_granularity/", "subreddit_subscribers": 86604, "created_utc": 1673985168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Come join the first [Seattle Spark Meetup](https://www.meetup.com/Seattle-Spark-Meetup/events/290865855/?response=3&amp;action=rsvp&amp;utm_medium=email&amp;utm_source=braze_canvas&amp;utm_campaign=mmrk_alleng_event_announcement_prod_v7_en&amp;utm_term=promo&amp;utm_content=lp_meetup) of 2023 on Jan 31!\n\nhttps://preview.redd.it/p065ppb4inca1.jpg?width=612&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=21dbb8c3c9fca9f59257ce30f3d53bcec85076d4", "author_fullname": "t2_5dvfu5m2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seattle Spark Meetup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "media_metadata": {"p065ppb4inca1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 65, "x": 108, "u": "https://preview.redd.it/p065ppb4inca1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd9eb36775709b68eaee28f59ef7034a2892dba3"}, {"y": 130, "x": 216, "u": "https://preview.redd.it/p065ppb4inca1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92f406cec1da38d5a0a4a69addb38148a1b27cc9"}, {"y": 192, "x": 320, "u": "https://preview.redd.it/p065ppb4inca1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ecdb676d3374bcac03cad41c5a995896b60d556"}], "s": {"y": 369, "x": 612, "u": "https://preview.redd.it/p065ppb4inca1.jpg?width=612&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=21dbb8c3c9fca9f59257ce30f3d53bcec85076d4"}, "id": "p065ppb4inca1"}}, "name": "t3_10ek8ut", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5XJvvvK19Bd24JizxyEjIfnJoXEDbapKMQAvQ-ji-ck.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673981977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Come join the first &lt;a href=\"https://www.meetup.com/Seattle-Spark-Meetup/events/290865855/?response=3&amp;amp;action=rsvp&amp;amp;utm_medium=email&amp;amp;utm_source=braze_canvas&amp;amp;utm_campaign=mmrk_alleng_event_announcement_prod_v7_en&amp;amp;utm_term=promo&amp;amp;utm_content=lp_meetup\"&gt;Seattle Spark Meetup&lt;/a&gt; of 2023 on Jan 31!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/p065ppb4inca1.jpg?width=612&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=21dbb8c3c9fca9f59257ce30f3d53bcec85076d4\"&gt;https://preview.redd.it/p065ppb4inca1.jpg?width=612&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=21dbb8c3c9fca9f59257ce30f3d53bcec85076d4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "10ek8ut", "is_robot_indexable": true, "report_reasons": null, "author": "bp_ryan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10ek8ut/seattle_spark_meetup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ek8ut/seattle_spark_meetup/", "subreddit_subscribers": 86604, "created_utc": 1673981977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. Kind of new to Metabase but loving the potential for easily building out useful dashboards. \nI needed to restrict access for a certain group of Metabase users to a subset of the data. I know there are access groups that can be restricted to collections. However, users can still modify questions in the collection to access data sources and records beyond the question. What I need for instance is to restrict a sales team to only data that relates to them i.e. tables in the data that relate to just sales data and even going further to let say limiting users to access/work with sales data for their specific region. This means restricting access to even filtered data level. This is just a generic example of what I'm trying to actually achieve.\nHow would I go accomplishing this?\nThanks!", "author_fullname": "t2_i8j5c85v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Restrict access to part of the data in Metabase", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10e8ja5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673949559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. Kind of new to Metabase but loving the potential for easily building out useful dashboards. \nI needed to restrict access for a certain group of Metabase users to a subset of the data. I know there are access groups that can be restricted to collections. However, users can still modify questions in the collection to access data sources and records beyond the question. What I need for instance is to restrict a sales team to only data that relates to them i.e. tables in the data that relate to just sales data and even going further to let say limiting users to access/work with sales data for their specific region. This means restricting access to even filtered data level. This is just a generic example of what I&amp;#39;m trying to actually achieve.\nHow would I go accomplishing this?\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10e8ja5", "is_robot_indexable": true, "report_reasons": null, "author": "Due-Wall-2799", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10e8ja5/restrict_access_to_part_of_the_data_in_metabase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10e8ja5/restrict_access_to_part_of_the_data_in_metabase/", "subreddit_subscribers": 86604, "created_utc": 1673949559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to flatten the JSON file\n\nstructure of the file looks like this \n\n    root\n     |-- items: array (nullable = true)\n     |    |-- element: struct (containsNull = true)\n     |    |    |-- _links: struct (nullable = true)\n     |    |    |    |-- self: struct (nullable = true)\n     |    |    |    |    |-- href: string (nullable = true)\n     |    |    |-- code: string (nullable = true)\n     |    |    |-- labels: struct (nullable = true)\n     |    |    |    |-- bg_BG: string (nullable = true)\n     |    |    |    |-- cs_CZ: string (nullable = true)\n     |    |    |    |-- da_DK: string (nullable = true)\n     |    |    |    |-- de_AT: string (nullable = true)\n     |    |    |    |-- de_BE: string (nullable = true)\n     |    |    |    |-- de_CH: string (nullable = true)\n     |    |    |    |-- de_DE: string (nullable = true)\n     |    |    |    |-- el_GR: string (nullable = true)\n     |    |    |    |-- en_AU: string (nullable = true)\n     |    |    |    |-- en_CA: string (nullable = true)\n     |    |    |    |-- en_CH: string (nullable = true)\n     |    |    |    |-- en_GB: string (nullable = true)\n     |    |    |    |-- en_IE: string (nullable = true)\n     |    |    |    |-- en_IN: string (nullable = true)\n     |    |    |    |-- en_MY: string (nullable = true)\n     |    |    |    |-- en_NZ: string (nullable = true)\n     |    |    |    |-- en_PH: string (nullable = true)\n     |    |    |    |-- en_SG: string (nullable = true)\n     |    |    |    |-- en_US: string (nullable = true)\n     |    |    |    |-- en_US_America: string (nullable = true)\n     |    |    |    |-- es_ES: string (nullable = true)\n     |    |    |    |-- et_EE: string (nullable = true)\n     |    |    |    |-- fi_FI: string (nullable = true)\n     |    |    |    |-- fr_BE: string (nullable = true)\n     |    |    |    |-- fr_CH: string (nullable = true)\n     |    |    |    |-- fr_FR: string (nullable = true)\n     |    |    |    |-- hr_HR: string (nullable = true)\n     |    |    |    |-- hu_HU: string (nullable = true)\n     |    |    |    |-- id_ID: string (nullable = true)\n     |    |    |    |-- is_IS: string (nullable = true)\n     |    |    |    |-- it_CH: string (nullable = true)\n     |    |    |    |-- it_IT: string (nullable = true)\n     |    |    |    |-- ja_JP: string (nullable = true)\n     |    |    |    |-- ko_KR: string (nullable = true)\n     |    |    |    |-- lt_LT: string (nullable = true)\n     |    |    |    |-- lv_LV: string (nullable = true)\n     |    |    |    |-- ms_MY: string (nullable = true)\n     |    |    |    |-- nb_NO: string (nullable = true)\n     |    |    |    |-- nl_BE: string (nullable = true)\n     |    |    |    |-- nl_NL: string (nullable = true)\n     |    |    |    |-- nn_NO: string (nullable = true)\n     |    |    |    |-- pl_PL: string (nullable = true)\n     |    |    |    |-- pt_PT: string (nullable = true)\n     |    |    |    |-- ro_RO: string (nullable = true)\n     |    |    |    |-- ru_RU: string (nullable = true)\n     |    |    |    |-- sk_SK: string (nullable = true)\n     |    |    |    |-- sl_SI: string (nullable = true)\n     |    |    |    |-- sr_Cyrl_RS: string (nullable = true)\n     |    |    |    |-- sv_SE: string (nullable = true)\n     |    |    |    |-- th_TH: string (nullable = true)\n     |    |    |    |-- tr_TR: string (nullable = true)\n     |    |    |    |-- uk_UA: string (nullable = true)\n     |    |    |    |-- vi_VN: string (nullable = true)\n     |    |    |    |-- zh_CN: string (nullable = true)\n     |    |    |    |-- zh_TW: string (nullable = true)\n     |    |    |-- parent: string (nullable = true)\n     |    |    |-- updated: string (nullable = true)\n\nMy function is this \n\n    def flatten_df(nested_df):\n        flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'string']\n        nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'struct']\n    \n        flat_df = nested_df.select(flat_cols +\n                                   [col(nc+'.'+c).alias(nc+'_'+c)\n                                    for nc in nested_cols\n                                    for c in nested_df.select(nc+'.*').columns])\n        return flat_df\n    \n    # Top level hierarchy\n    df = df.select('_embedded.*')\n    #Reaching the lower level called \"items\"\n    df1 = df.select(explode(df.items).alias('required'))\n    # Creating dataframe which will be passed to flatten_df to flatten entire data under \"items\" hierarchy\n    df2 = df1.select('required.*')\n    final = flatten_df(df2)\n    display(final)\n\nthis function takes the `lables`   column keys and puts them as seperate columns ( please see the image [https://imgur.com/a/xa5VmCu](https://imgur.com/a/xa5VmCu) )\n\n&amp;#x200B;\n\nwhat I want is that function should give only two columns from `lables` column named Lable\\_key Lable\\_value  as shown in the image  [https://imgur.com/a/sTW9sSx](https://imgur.com/a/sTW9sSx)", "author_fullname": "t2_56g5f4cg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "flatten nested JSON file pyspark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10f1kh1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674027896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to flatten the JSON file&lt;/p&gt;\n\n&lt;p&gt;structure of the file looks like this &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;root\n |-- items: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- _links: struct (nullable = true)\n |    |    |    |-- self: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |-- code: string (nullable = true)\n |    |    |-- labels: struct (nullable = true)\n |    |    |    |-- bg_BG: string (nullable = true)\n |    |    |    |-- cs_CZ: string (nullable = true)\n |    |    |    |-- da_DK: string (nullable = true)\n |    |    |    |-- de_AT: string (nullable = true)\n |    |    |    |-- de_BE: string (nullable = true)\n |    |    |    |-- de_CH: string (nullable = true)\n |    |    |    |-- de_DE: string (nullable = true)\n |    |    |    |-- el_GR: string (nullable = true)\n |    |    |    |-- en_AU: string (nullable = true)\n |    |    |    |-- en_CA: string (nullable = true)\n |    |    |    |-- en_CH: string (nullable = true)\n |    |    |    |-- en_GB: string (nullable = true)\n |    |    |    |-- en_IE: string (nullable = true)\n |    |    |    |-- en_IN: string (nullable = true)\n |    |    |    |-- en_MY: string (nullable = true)\n |    |    |    |-- en_NZ: string (nullable = true)\n |    |    |    |-- en_PH: string (nullable = true)\n |    |    |    |-- en_SG: string (nullable = true)\n |    |    |    |-- en_US: string (nullable = true)\n |    |    |    |-- en_US_America: string (nullable = true)\n |    |    |    |-- es_ES: string (nullable = true)\n |    |    |    |-- et_EE: string (nullable = true)\n |    |    |    |-- fi_FI: string (nullable = true)\n |    |    |    |-- fr_BE: string (nullable = true)\n |    |    |    |-- fr_CH: string (nullable = true)\n |    |    |    |-- fr_FR: string (nullable = true)\n |    |    |    |-- hr_HR: string (nullable = true)\n |    |    |    |-- hu_HU: string (nullable = true)\n |    |    |    |-- id_ID: string (nullable = true)\n |    |    |    |-- is_IS: string (nullable = true)\n |    |    |    |-- it_CH: string (nullable = true)\n |    |    |    |-- it_IT: string (nullable = true)\n |    |    |    |-- ja_JP: string (nullable = true)\n |    |    |    |-- ko_KR: string (nullable = true)\n |    |    |    |-- lt_LT: string (nullable = true)\n |    |    |    |-- lv_LV: string (nullable = true)\n |    |    |    |-- ms_MY: string (nullable = true)\n |    |    |    |-- nb_NO: string (nullable = true)\n |    |    |    |-- nl_BE: string (nullable = true)\n |    |    |    |-- nl_NL: string (nullable = true)\n |    |    |    |-- nn_NO: string (nullable = true)\n |    |    |    |-- pl_PL: string (nullable = true)\n |    |    |    |-- pt_PT: string (nullable = true)\n |    |    |    |-- ro_RO: string (nullable = true)\n |    |    |    |-- ru_RU: string (nullable = true)\n |    |    |    |-- sk_SK: string (nullable = true)\n |    |    |    |-- sl_SI: string (nullable = true)\n |    |    |    |-- sr_Cyrl_RS: string (nullable = true)\n |    |    |    |-- sv_SE: string (nullable = true)\n |    |    |    |-- th_TH: string (nullable = true)\n |    |    |    |-- tr_TR: string (nullable = true)\n |    |    |    |-- uk_UA: string (nullable = true)\n |    |    |    |-- vi_VN: string (nullable = true)\n |    |    |    |-- zh_CN: string (nullable = true)\n |    |    |    |-- zh_TW: string (nullable = true)\n |    |    |-- parent: string (nullable = true)\n |    |    |-- updated: string (nullable = true)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;My function is this &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def flatten_df(nested_df):\n    flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == &amp;#39;string&amp;#39;]\n    nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == &amp;#39;struct&amp;#39;]\n\n    flat_df = nested_df.select(flat_cols +\n                               [col(nc+&amp;#39;.&amp;#39;+c).alias(nc+&amp;#39;_&amp;#39;+c)\n                                for nc in nested_cols\n                                for c in nested_df.select(nc+&amp;#39;.*&amp;#39;).columns])\n    return flat_df\n\n# Top level hierarchy\ndf = df.select(&amp;#39;_embedded.*&amp;#39;)\n#Reaching the lower level called &amp;quot;items&amp;quot;\ndf1 = df.select(explode(df.items).alias(&amp;#39;required&amp;#39;))\n# Creating dataframe which will be passed to flatten_df to flatten entire data under &amp;quot;items&amp;quot; hierarchy\ndf2 = df1.select(&amp;#39;required.*&amp;#39;)\nfinal = flatten_df(df2)\ndisplay(final)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;this function takes the &lt;code&gt;lables&lt;/code&gt;   column keys and puts them as seperate columns ( please see the image &lt;a href=\"https://imgur.com/a/xa5VmCu\"&gt;https://imgur.com/a/xa5VmCu&lt;/a&gt; )&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;what I want is that function should give only two columns from &lt;code&gt;lables&lt;/code&gt; column named Lable_key Lable_value  as shown in the image  &lt;a href=\"https://imgur.com/a/sTW9sSx\"&gt;https://imgur.com/a/sTW9sSx&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?auto=webp&amp;v=enabled&amp;s=7a58cc0ee2d095f64688402d8012d36b9b6a66d2", "width": 1317, "height": 184}, "resolutions": [{"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f66ce948572caabe1038462bad873d97fc1a6f00", "width": 108, "height": 15}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19b8cfa11924aba9692b30f9f090ba1b2bdc5cf6", "width": 216, "height": 30}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d770f580c8e9f5a3b75d918f355a185747d363e", "width": 320, "height": 44}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e91a890cf3c85f5d7f35fc36efa574576ea686b6", "width": 640, "height": 89}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca2e0618fd945164bb0ee5d1f453f615f3370c12", "width": 960, "height": 134}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d83982ce3a8ecd415011b397cfd95a71b1a3ba4", "width": 1080, "height": 150}], "variants": {}, "id": "-PIFhkx_4ZX0v6qeVW9b0POiuvOW5532KuWALDCw1dU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10f1kh1", "is_robot_indexable": true, "report_reasons": null, "author": "9gg6", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f1kh1/flatten_nested_json_file_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f1kh1/flatten_nested_json_file_pyspark/", "subreddit_subscribers": 86604, "created_utc": 1674027896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone out there have experience with these tools?  If so, how do they compare in your opinion?", "author_fullname": "t2_5lfqidpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alation vs. Atlan vs. Collibra", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10esiz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674001376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone out there have experience with these tools?  If so, how do they compare in your opinion?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10esiz9", "is_robot_indexable": true, "report_reasons": null, "author": "imani_TqiynAZU", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10esiz9/alation_vs_atlan_vs_collibra/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10esiz9/alation_vs_atlan_vs_collibra/", "subreddit_subscribers": 86604, "created_utc": 1674001376.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a company that does geographic data processing for the country. We have a project that uses a library, previously made by the company, that contains several processing modules, where each module does the prediction or analysis of different types of geographic events for small regions of the country. For each region that we are going to monitor, we create a new virtual machine and do a workflow with airflow, and choose the module that will be applied to that region. The problem is that this is obviously getting too bad to manage or scale. We would also like to integrate the part of creating new monitoring flows with our portal, allowing the end user to create them, rather than it being an internal process.\n\n\n\nI was thinking of changing this architecture to something like:\n\n- Create a table in the database to manage the stream control, where each row would have the information of the stream type, periodicity, region, module and if it is being processed by any machines. Ex:\n\n\n\nTable Worflows:\n\nID | Module | Region | Period | Status\n---|---|----|----|----\n0 | RainPrediction | 55AX | 1 hour | Processing\n1 | RainPrediction | 45TB | 1 hour | Free\n2 | TornadoPrediction | 55AX | 1 hour | Processing\n\n\nTable Workflow_Logs\n\nID | Worflow_ID | Started_DT | Finished_DT | Logs \n---|---|----|----|----    \n0 | 1 | 13:00 | 13:17 | Lorem Ipsum...\n\n\n\n- Instead of creating a machine to run each region of each module, I am thinking of adapting the lib to have a main entrypoint that receives parameters and can run the X algorithm for the Y region. Then this machine would scan the flow control table looking for the tasks that are not being processed in order to process them.\n\n\n\n- Adapt the frontend so that the user can insert a new row into this flow control table.\n\n\n\nThen when we need to scale up our algorithm we would just need to create a new container and this container would start reading and fetching unprocessed tasks. I think I am trying to reinvent the wheel by doing this and would like some suggestions that do not increase the complexity of the project too much. I have been reading about kafka but it seems to add a lot of complexity to the project.", "author_fullname": "t2_9ca451os", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions on how to improve the architecture that I am working on", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10emkmw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673987372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a company that does geographic data processing for the country. We have a project that uses a library, previously made by the company, that contains several processing modules, where each module does the prediction or analysis of different types of geographic events for small regions of the country. For each region that we are going to monitor, we create a new virtual machine and do a workflow with airflow, and choose the module that will be applied to that region. The problem is that this is obviously getting too bad to manage or scale. We would also like to integrate the part of creating new monitoring flows with our portal, allowing the end user to create them, rather than it being an internal process.&lt;/p&gt;\n\n&lt;p&gt;I was thinking of changing this architecture to something like:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Create a table in the database to manage the stream control, where each row would have the information of the stream type, periodicity, region, module and if it is being processed by any machines. Ex:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Table Worflows:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;Module&lt;/th&gt;\n&lt;th&gt;Region&lt;/th&gt;\n&lt;th&gt;Period&lt;/th&gt;\n&lt;th&gt;Status&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;0&lt;/td&gt;\n&lt;td&gt;RainPrediction&lt;/td&gt;\n&lt;td&gt;55AX&lt;/td&gt;\n&lt;td&gt;1 hour&lt;/td&gt;\n&lt;td&gt;Processing&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;RainPrediction&lt;/td&gt;\n&lt;td&gt;45TB&lt;/td&gt;\n&lt;td&gt;1 hour&lt;/td&gt;\n&lt;td&gt;Free&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;TornadoPrediction&lt;/td&gt;\n&lt;td&gt;55AX&lt;/td&gt;\n&lt;td&gt;1 hour&lt;/td&gt;\n&lt;td&gt;Processing&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Table Workflow_Logs&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;Worflow_ID&lt;/th&gt;\n&lt;th&gt;Started_DT&lt;/th&gt;\n&lt;th&gt;Finished_DT&lt;/th&gt;\n&lt;th&gt;Logs&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;0&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;13:00&lt;/td&gt;\n&lt;td&gt;13:17&lt;/td&gt;\n&lt;td&gt;Lorem Ipsum...&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Instead of creating a machine to run each region of each module, I am thinking of adapting the lib to have a main entrypoint that receives parameters and can run the X algorithm for the Y region. Then this machine would scan the flow control table looking for the tasks that are not being processed in order to process them.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Adapt the frontend so that the user can insert a new row into this flow control table.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Then when we need to scale up our algorithm we would just need to create a new container and this container would start reading and fetching unprocessed tasks. I think I am trying to reinvent the wheel by doing this and would like some suggestions that do not increase the complexity of the project too much. I have been reading about kafka but it seems to add a lot of complexity to the project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10emkmw", "is_robot_indexable": true, "report_reasons": null, "author": "MiserableAstronaut77", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10emkmw/suggestions_on_how_to_improve_the_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10emkmw/suggestions_on_how_to_improve_the_architecture/", "subreddit_subscribers": 86604, "created_utc": 1673987372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nTL;DR\n\nSole Data Engineer in the company is to propose a new architecture to gradually connect 30 ERP systems. There is a failed Data Vault project because it was designed as a Source System Data Vault and there are also errors in the technical implementation.\n\nThe existing technologies are Snowflake and Wherescape.\n\nThe question is whether another Data Vault project should be set up or whether the new DWH should be set up differently.\n\n&amp;#x200B;\n\nI am new in a company, the only one who has anything to do with data engineering/BI. The previous developer left, leaving behind a failed Data Vault project. I am Data Vault 2.0 certified, but have never had to implement it in practice. I say failed Data Vault project because it is 1:1 to the source system and the historisation does not work. So there is a source data vault (Anti Pattern) that does not even historise the data.\n\nThis was implemented with Wherescape as the automation tool and Snowflake as the database. No upstream data lake or anything similar.\n\n&amp;#x200B;\n\nI have been using Informatica PowerCenter and have strong SQL skills and know dimensional modelling, but I am no longer interested in no code tools like PowerCenter, Talent, SSIS etc. and would be happy to leave them behind.\n\n&amp;#x200B;\n\nThe company is based in the manufacturing sector and has a total of over 30 more or less small sites, the largest of which run their own ERP systems.\n\nThe goal is (and was) to first integrate the ERP systems of the two largest sites into one DWH and then connect other sites. The larger locations each use the same ERP system (MS Dynamics NAV on-prem) and the smaller locations, for example, MS Dynamics 365 Business Central (Cloud). The departments only need daily updated data, which is good because CDC mechanisms are not active in the ERP databases and the DB responsibles have not had anything to do with it yet.\n\nAs far as I can see, only Azure AD and VMs are used in Azure.\n\n&amp;#x200B;\n\nThe question for me now is how to proceed, as my supervisor has asked me to suggest which architecture and data model we want to use.\n\nHow would you approach the issue?\n\nI see the possibility of creating the DV model again with the help of external support and implementing it using Wherescape.\n\nAlternatively, it would be conceivable to load the data into Snowflake and build a persistent staging area (PSA) there and transform the whole thing using dbt. I would find dbt interesting because it is essentially SQL. But then it would have to be decided which tools would be used for extraction and orchestration.", "author_fullname": "t2_uyjws4s8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DWH architecture for the integration of 30 sites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ejp50", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673980681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;TL;DR&lt;/p&gt;\n\n&lt;p&gt;Sole Data Engineer in the company is to propose a new architecture to gradually connect 30 ERP systems. There is a failed Data Vault project because it was designed as a Source System Data Vault and there are also errors in the technical implementation.&lt;/p&gt;\n\n&lt;p&gt;The existing technologies are Snowflake and Wherescape.&lt;/p&gt;\n\n&lt;p&gt;The question is whether another Data Vault project should be set up or whether the new DWH should be set up differently.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am new in a company, the only one who has anything to do with data engineering/BI. The previous developer left, leaving behind a failed Data Vault project. I am Data Vault 2.0 certified, but have never had to implement it in practice. I say failed Data Vault project because it is 1:1 to the source system and the historisation does not work. So there is a source data vault (Anti Pattern) that does not even historise the data.&lt;/p&gt;\n\n&lt;p&gt;This was implemented with Wherescape as the automation tool and Snowflake as the database. No upstream data lake or anything similar.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have been using Informatica PowerCenter and have strong SQL skills and know dimensional modelling, but I am no longer interested in no code tools like PowerCenter, Talent, SSIS etc. and would be happy to leave them behind.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The company is based in the manufacturing sector and has a total of over 30 more or less small sites, the largest of which run their own ERP systems.&lt;/p&gt;\n\n&lt;p&gt;The goal is (and was) to first integrate the ERP systems of the two largest sites into one DWH and then connect other sites. The larger locations each use the same ERP system (MS Dynamics NAV on-prem) and the smaller locations, for example, MS Dynamics 365 Business Central (Cloud). The departments only need daily updated data, which is good because CDC mechanisms are not active in the ERP databases and the DB responsibles have not had anything to do with it yet.&lt;/p&gt;\n\n&lt;p&gt;As far as I can see, only Azure AD and VMs are used in Azure.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The question for me now is how to proceed, as my supervisor has asked me to suggest which architecture and data model we want to use.&lt;/p&gt;\n\n&lt;p&gt;How would you approach the issue?&lt;/p&gt;\n\n&lt;p&gt;I see the possibility of creating the DV model again with the help of external support and implementing it using Wherescape.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, it would be conceivable to load the data into Snowflake and build a persistent staging area (PSA) there and transform the whole thing using dbt. I would find dbt interesting because it is essentially SQL. But then it would have to be decided which tools would be used for extraction and orchestration.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ejp50", "is_robot_indexable": true, "report_reasons": null, "author": "Individual-Detail286", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ejp50/dwh_architecture_for_the_integration_of_30_sites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ejp50/dwh_architecture_for_the_integration_of_30_sites/", "subreddit_subscribers": 86604, "created_utc": 1673980681.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I worked in ETL for a hospital for 3 years and left the job to become a Data Engineer and work in snowflake. However within 2 weeks of me getting to the new company they had massive layoffs and after a bit longer than that i was told my new position will not be retained. \n\nI have tons of experience with SQL and have worked in datastacks with Microsoft, IBM, Oracle, and Snowflake including datastage and SSIS. \n\n\nI was looking to broaden my horizons at my new position and go from being purely ETL to more of a catch all data guy and found a great fit for it until they had their layoffs.\n\nNow it seems like every position im applying for wants AWS or Azure experience which i just dont have. Does anyone have any recommendations for courses or certifications for me? Or just any general career advice? I am at a bit of a crossroads and have never done any sort of online certifications", "author_fullname": "t2_6ra3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best certifications for my current career trajectory.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10efafg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673970072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I worked in ETL for a hospital for 3 years and left the job to become a Data Engineer and work in snowflake. However within 2 weeks of me getting to the new company they had massive layoffs and after a bit longer than that i was told my new position will not be retained. &lt;/p&gt;\n\n&lt;p&gt;I have tons of experience with SQL and have worked in datastacks with Microsoft, IBM, Oracle, and Snowflake including datastage and SSIS. &lt;/p&gt;\n\n&lt;p&gt;I was looking to broaden my horizons at my new position and go from being purely ETL to more of a catch all data guy and found a great fit for it until they had their layoffs.&lt;/p&gt;\n\n&lt;p&gt;Now it seems like every position im applying for wants AWS or Azure experience which i just dont have. Does anyone have any recommendations for courses or certifications for me? Or just any general career advice? I am at a bit of a crossroads and have never done any sort of online certifications&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10efafg", "is_robot_indexable": true, "report_reasons": null, "author": "deemerritt", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10efafg/best_certifications_for_my_current_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10efafg/best_certifications_for_my_current_career/", "subreddit_subscribers": 86604, "created_utc": 1673970072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just asking this question conceptually as I don't have experience in this area yet, trying to do my research but i dont fully understand how this works yet.\n\nI'm imagining you have something like Debezium + Kafka stream doing CDC from some rdbms. When this arrives in your dwh im guessing you get a sequence of some kind of SQL DML statements (UPDATE, INSERT etc), or the equivalent in another format? Do you then have to parse and execute these line-by-line to reconstruct the current state in a raw dwh table? How does it work?", "author_fullname": "t2_18xb5x05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you reconstruct current state in DWh from a stream of CDC events?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ef640", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673969748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just asking this question conceptually as I don&amp;#39;t have experience in this area yet, trying to do my research but i dont fully understand how this works yet.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m imagining you have something like Debezium + Kafka stream doing CDC from some rdbms. When this arrives in your dwh im guessing you get a sequence of some kind of SQL DML statements (UPDATE, INSERT etc), or the equivalent in another format? Do you then have to parse and execute these line-by-line to reconstruct the current state in a raw dwh table? How does it work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ef640", "is_robot_indexable": true, "report_reasons": null, "author": "the-data-scientist", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ef640/how_do_you_reconstruct_current_state_in_dwh_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ef640/how_do_you_reconstruct_current_state_in_dwh_from/", "subreddit_subscribers": 86604, "created_utc": 1673969748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "10 evaluation criteria for Data Orchestrators", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10edrdl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1673966147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/posts/marclamberti_dataengineering-dataengineer-data-activity-7021114084053917696-3CAF?utm_source=share&amp;utm_medium=member_desktop", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10edrdl", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10edrdl/10_evaluation_criteria_for_data_orchestrators/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/posts/marclamberti_dataengineering-dataengineer-data-activity-7021114084053917696-3CAF?utm_source=share&amp;utm_medium=member_desktop", "subreddit_subscribers": 86604, "created_utc": 1673966147.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}