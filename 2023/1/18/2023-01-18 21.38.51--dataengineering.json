{"kind": "Listing", "data": {"after": "t3_10esiz9", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_m6gnxiuj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DW toolkit book by Ralph Kimball", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 96, "top_awarded_type": null, "hide_score": false, "name": "t3_10esybb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 66, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 66, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5ioEG9GaIPjg4DO8YEHXNaQLLEZ29VHNOBtRF9u2KaY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674002443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/yc0bxaypoqca1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?auto=webp&amp;v=enabled&amp;s=579f4c43ca548b1e0742912ba9dfea97c392c86f", "width": 2819, "height": 1953}, "resolutions": [{"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7d6aca1f1b6519a452a7852feed37a02fb924c0", "width": 108, "height": 74}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae5930e940d0713f43e6184c6c0df951e5af2353", "width": 216, "height": 149}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=954c43e1c94f190557f4541106f2dc24f4a5f1ea", "width": 320, "height": 221}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=726092b906848d30d07c31f9715bfbbd62fb79cd", "width": 640, "height": 443}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e790040401ee5a93fc61d0098c4843d73d6efb1", "width": 960, "height": 665}, {"url": "https://preview.redd.it/yc0bxaypoqca1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7d765bedccbfc6d37bf7201318cec16c16e4926", "width": 1080, "height": 748}], "variants": {}, "id": "gKCsuiInCaKY56pQ3Kk6u9IsUHba7ufeNr9UMkEUlRw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10esybb", "is_robot_indexable": true, "report_reasons": null, "author": "rajekum512", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10esybb/dw_toolkit_book_by_ralph_kimball/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/yc0bxaypoqca1.jpg", "subreddit_subscribers": 86673, "created_utc": 1674002443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background: I've been working with TB-scale data for several years. I realize this could sound like a DA/DS question, but almost all of the visualization products I have seen do not work well (at at all) at this scale or require significant pre-processing and customization, so I think this is appropriately a DE question. \n\nLet's say I have some data I want to visualize that is updated at an offline cadence, e.g. once a day. Parquet/Iceberg etc. New data is appended to daily partitions at a rate of \\~100GB/day. I have about a dozen dimensions and filters I would like to aggregate across or filter on, but the metrics themselves are somewhat basic (sums, counts, ratios, etc). Are there any visualization solutions that can handle interactive querying against data of this scale (interactive meaning the end user self-filter and update the visualization within a second or two)? What are the benefits/tradeoffs to current systems for this? Is this a use case to bring in a specialized OLAP cube product?", "author_fullname": "t2_7pxby", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who is doing large-scale visualization and dashboarding well?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10eov35", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673992650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background: I&amp;#39;ve been working with TB-scale data for several years. I realize this could sound like a DA/DS question, but almost all of the visualization products I have seen do not work well (at at all) at this scale or require significant pre-processing and customization, so I think this is appropriately a DE question. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I have some data I want to visualize that is updated at an offline cadence, e.g. once a day. Parquet/Iceberg etc. New data is appended to daily partitions at a rate of ~100GB/day. I have about a dozen dimensions and filters I would like to aggregate across or filter on, but the metrics themselves are somewhat basic (sums, counts, ratios, etc). Are there any visualization solutions that can handle interactive querying against data of this scale (interactive meaning the end user self-filter and update the visualization within a second or two)? What are the benefits/tradeoffs to current systems for this? Is this a use case to bring in a specialized OLAP cube product?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10eov35", "is_robot_indexable": true, "report_reasons": null, "author": "ColdPorridge", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10eov35/who_is_doing_largescale_visualization_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10eov35/who_is_doing_largescale_visualization_and/", "subreddit_subscribers": 86673, "created_utc": 1673992650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi all, its been a pretty awful day. Two months ago my boss transferred out to a new team, my team was integrated in with another. Sure enough this morning i woke up to an email letting me know im no longer a valued member of the team.   \n\n\nat this point im feeling in the pits. over the last few years ive gone above and beyond for this company, spending untold unpaid evening and weekend hours trying to push our projects forward.   \n\n\nnot sure about the next steps, but i felt like i needed to vent. if anyone out there knows of any DE opportunities, please DM me.", "author_fullname": "t2_706trkkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "just got laid off (FAANG)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fg07o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674069529.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi all, its been a pretty awful day. Two months ago my boss transferred out to a new team, my team was integrated in with another. Sure enough this morning i woke up to an email letting me know im no longer a valued member of the team.   &lt;/p&gt;\n\n&lt;p&gt;at this point im feeling in the pits. over the last few years ive gone above and beyond for this company, spending untold unpaid evening and weekend hours trying to push our projects forward.   &lt;/p&gt;\n\n&lt;p&gt;not sure about the next steps, but i felt like i needed to vent. if anyone out there knows of any DE opportunities, please DM me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10fg07o", "is_robot_indexable": true, "report_reasons": null, "author": "Foodwithfloyd", "discussion_type": null, "num_comments": 17, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fg07o/just_got_laid_off_faang/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fg07o/just_got_laid_off_faang/", "subreddit_subscribers": 86673, "created_utc": 1674069529.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lets say ***a small company currently handles all data by sharing excel files over emails and has never heard the concept of a database***. \n\n***How do you transition this into a data warehouse?***\n\nDoes this mean now I have to write a lot of code to process a bunch of random csv files to turn them into a star schema?\n\nLet's say I managed to turn their csv files into a consistent set of tables... How do they access it? Do they need to hire a bunch of sql analysts to get the information? or Do you build some sort of dashboard out of it?  \n\n\nI know someone has suffered from this before.  \n\n\nHow would you approach it?  \nIs there a guide or a book for this?", "author_fullname": "t2_zs1xp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering for small companies.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10eurjm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674007202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say &lt;strong&gt;&lt;em&gt;a small company currently handles all data by sharing excel files over emails and has never heard the concept of a database&lt;/em&gt;&lt;/strong&gt;. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;How do you transition this into a data warehouse?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Does this mean now I have to write a lot of code to process a bunch of random csv files to turn them into a star schema?&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I managed to turn their csv files into a consistent set of tables... How do they access it? Do they need to hire a bunch of sql analysts to get the information? or Do you build some sort of dashboard out of it?  &lt;/p&gt;\n\n&lt;p&gt;I know someone has suffered from this before.  &lt;/p&gt;\n\n&lt;p&gt;How would you approach it?&lt;br/&gt;\nIs there a guide or a book for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10eurjm", "is_robot_indexable": true, "report_reasons": null, "author": "lFuckRedditl", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10eurjm/data_engineering_for_small_companies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10eurjm/data_engineering_for_small_companies/", "subreddit_subscribers": 86673, "created_utc": 1674007202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We can leverage platforms like Coursera Plus and Udemy.", "author_fullname": "t2_5p0gzgth", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your favourite DBT training/learning resources in 2023? Moving part of our stack to DBT Cloud and opening up access to more data analysts. Gotta get these newbies trained up!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10f8yo6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674052844.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We can leverage platforms like Coursera Plus and Udemy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10f8yo6", "is_robot_indexable": true, "report_reasons": null, "author": "the-strange-ninja", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f8yo6/what_are_your_favourite_dbt_traininglearning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f8yo6/what_are_your_favourite_dbt_traininglearning/", "subreddit_subscribers": 86673, "created_utc": 1674052844.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, is there a good implementation for Delta Sharing usable for on-prem deployments?\n\nThe reference implementation provided doesn't support adding new shares without modifying the configuration file and restarting the server, also there's no user management there.", "author_fullname": "t2_4clu4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Sharing on premise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ezch7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674020331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, is there a good implementation for Delta Sharing usable for on-prem deployments?&lt;/p&gt;\n\n&lt;p&gt;The reference implementation provided doesn&amp;#39;t support adding new shares without modifying the configuration file and restarting the server, also there&amp;#39;s no user management there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ezch7", "is_robot_indexable": true, "report_reasons": null, "author": "inteloid", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ezch7/delta_sharing_on_premise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ezch7/delta_sharing_on_premise/", "subreddit_subscribers": 86673, "created_utc": 1674020331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working as a data engineer/data integration for 8+ years from traditional ETL to build data warehouses to cloud (mainly GCP) in large Biotech and start-up Fintech. \n\nI wanna use these skillets to work at a non-profit for animal welfare or organization rather than working for corporations. I wanna contribute my skill to something more meaningful. \n\nI've been searching but it's a bit tricky to find these organizations or they even have job postings. \n\nWhat do you recommend? Or site I should look at? Beside LinkedIn, indeed, zip recruiter, etc...", "author_fullname": "t2_16lpc7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to search for job posting at animal non profit organizations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10f7xf7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674050016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as a data engineer/data integration for 8+ years from traditional ETL to build data warehouses to cloud (mainly GCP) in large Biotech and start-up Fintech. &lt;/p&gt;\n\n&lt;p&gt;I wanna use these skillets to work at a non-profit for animal welfare or organization rather than working for corporations. I wanna contribute my skill to something more meaningful. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been searching but it&amp;#39;s a bit tricky to find these organizations or they even have job postings. &lt;/p&gt;\n\n&lt;p&gt;What do you recommend? Or site I should look at? Beside LinkedIn, indeed, zip recruiter, etc...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10f7xf7", "is_robot_indexable": true, "report_reasons": null, "author": "Totoro328", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f7xf7/how_to_search_for_job_posting_at_animal_non/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f7xf7/how_to_search_for_job_posting_at_animal_non/", "subreddit_subscribers": 86673, "created_utc": 1674050016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_502r5jht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to make a change, resume feedback / advice appreciated for junior DE role.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10f2eow", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Sy52Ie6JvJWizUvuv5xo1U7nvqu7OXkc1LzxFEdNZMA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674031173.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zbpozed52tca1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zbpozed52tca1.jpg?auto=webp&amp;v=enabled&amp;s=24840a9556acbb5748715338b28ad0e4a48a2087", "width": 1203, "height": 1723}, "resolutions": [{"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=538cffe182f66e8f7d78190dc8c2928e9437cb73", "width": 108, "height": 154}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3786ab72038cbe2a4d83014c4d792028820cde4f", "width": 216, "height": 309}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1718595b0b5c4e07764565ec79ed3e4fa6e79462", "width": 320, "height": 458}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b223390d8ac0a08a2cf930a9914404460722901", "width": 640, "height": 916}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d5d0f9c3f577c1340f4229ba95fcbb4ff2bda9d", "width": 960, "height": 1374}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2a40325e0f0927f7547c023f0dfe71f6a0c89ea", "width": 1080, "height": 1546}], "variants": {}, "id": "2QiMbQXoSVTA6udrYgV4EfBnJyQkKItlm1SrK4_k-Bs"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10f2eow", "is_robot_indexable": true, "report_reasons": null, "author": "toem033", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f2eow/looking_to_make_a_change_resume_feedback_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zbpozed52tca1.jpg", "subreddit_subscribers": 86673, "created_utc": 1674031173.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When you do a merge in SQL, is it doing a full table scan of the target? If so, isn\u2019t that something we want to avoid?", "author_fullname": "t2_5ukitegd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MERGE statement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10exb2o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674014249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When you do a merge in SQL, is it doing a full table scan of the target? If so, isn\u2019t that something we want to avoid?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10exb2o", "is_robot_indexable": true, "report_reasons": null, "author": "burningburnerbern", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10exb2o/merge_statement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10exb2o/merge_statement/", "subreddit_subscribers": 86673, "created_utc": 1674014249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to flatten the JSON file\n\nstructure of the file looks like this \n\n    root\n     |-- items: array (nullable = true)\n     |    |-- element: struct (containsNull = true)\n     |    |    |-- _links: struct (nullable = true)\n     |    |    |    |-- self: struct (nullable = true)\n     |    |    |    |    |-- href: string (nullable = true)\n     |    |    |-- code: string (nullable = true)\n     |    |    |-- labels: struct (nullable = true)\n     |    |    |    |-- bg_BG: string (nullable = true)\n     |    |    |    |-- cs_CZ: string (nullable = true)\n     |    |    |    |-- da_DK: string (nullable = true)\n     |    |    |    |-- de_AT: string (nullable = true)\n     |    |    |    |-- de_BE: string (nullable = true)\n     |    |    |    |-- de_CH: string (nullable = true)\n     |    |    |    |-- de_DE: string (nullable = true)\n     |    |    |    |-- el_GR: string (nullable = true)\n     |    |    |    |-- en_AU: string (nullable = true)\n     |    |    |    |-- en_CA: string (nullable = true)\n     |    |    |    |-- en_CH: string (nullable = true)\n     |    |    |    |-- en_GB: string (nullable = true)\n     |    |    |    |-- en_IE: string (nullable = true)\n     |    |    |    |-- en_IN: string (nullable = true)\n     |    |    |    |-- en_MY: string (nullable = true)\n     |    |    |    |-- en_NZ: string (nullable = true)\n     |    |    |    |-- en_PH: string (nullable = true)\n     |    |    |    |-- en_SG: string (nullable = true)\n     |    |    |    |-- en_US: string (nullable = true)\n     |    |    |    |-- en_US_America: string (nullable = true)\n     |    |    |    |-- es_ES: string (nullable = true)\n     |    |    |    |-- et_EE: string (nullable = true)\n     |    |    |    |-- fi_FI: string (nullable = true)\n     |    |    |    |-- fr_BE: string (nullable = true)\n     |    |    |    |-- fr_CH: string (nullable = true)\n     |    |    |    |-- fr_FR: string (nullable = true)\n     |    |    |    |-- hr_HR: string (nullable = true)\n     |    |    |    |-- hu_HU: string (nullable = true)\n     |    |    |    |-- id_ID: string (nullable = true)\n     |    |    |    |-- is_IS: string (nullable = true)\n     |    |    |    |-- it_CH: string (nullable = true)\n     |    |    |    |-- it_IT: string (nullable = true)\n     |    |    |    |-- ja_JP: string (nullable = true)\n     |    |    |    |-- ko_KR: string (nullable = true)\n     |    |    |    |-- lt_LT: string (nullable = true)\n     |    |    |    |-- lv_LV: string (nullable = true)\n     |    |    |    |-- ms_MY: string (nullable = true)\n     |    |    |    |-- nb_NO: string (nullable = true)\n     |    |    |    |-- nl_BE: string (nullable = true)\n     |    |    |    |-- nl_NL: string (nullable = true)\n     |    |    |    |-- nn_NO: string (nullable = true)\n     |    |    |    |-- pl_PL: string (nullable = true)\n     |    |    |    |-- pt_PT: string (nullable = true)\n     |    |    |    |-- ro_RO: string (nullable = true)\n     |    |    |    |-- ru_RU: string (nullable = true)\n     |    |    |    |-- sk_SK: string (nullable = true)\n     |    |    |    |-- sl_SI: string (nullable = true)\n     |    |    |    |-- sr_Cyrl_RS: string (nullable = true)\n     |    |    |    |-- sv_SE: string (nullable = true)\n     |    |    |    |-- th_TH: string (nullable = true)\n     |    |    |    |-- tr_TR: string (nullable = true)\n     |    |    |    |-- uk_UA: string (nullable = true)\n     |    |    |    |-- vi_VN: string (nullable = true)\n     |    |    |    |-- zh_CN: string (nullable = true)\n     |    |    |    |-- zh_TW: string (nullable = true)\n     |    |    |-- parent: string (nullable = true)\n     |    |    |-- updated: string (nullable = true)\n\nMy function is this \n\n    def flatten_df(nested_df):\n        flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'string']\n        nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'struct']\n    \n        flat_df = nested_df.select(flat_cols +\n                                   [col(nc+'.'+c).alias(nc+'_'+c)\n                                    for nc in nested_cols\n                                    for c in nested_df.select(nc+'.*').columns])\n        return flat_df\n    \n    # Top level hierarchy\n    df = df.select('_embedded.*')\n    #Reaching the lower level called \"items\"\n    df1 = df.select(explode(df.items).alias('required'))\n    # Creating dataframe which will be passed to flatten_df to flatten entire data under \"items\" hierarchy\n    df2 = df1.select('required.*')\n    final = flatten_df(df2)\n    display(final)\n\nthis function takes the `lables`   column keys and puts them as seperate columns ( please see the image [https://imgur.com/a/xa5VmCu](https://imgur.com/a/xa5VmCu) )\n\n&amp;#x200B;\n\nwhat I want is that function should give only two columns from `lables` column named Lable\\_key Lable\\_value  as shown in the image  [https://imgur.com/a/sTW9sSx](https://imgur.com/a/sTW9sSx)", "author_fullname": "t2_56g5f4cg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "flatten nested JSON file pyspark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10f1kh1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674027896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to flatten the JSON file&lt;/p&gt;\n\n&lt;p&gt;structure of the file looks like this &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;root\n |-- items: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- _links: struct (nullable = true)\n |    |    |    |-- self: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |-- code: string (nullable = true)\n |    |    |-- labels: struct (nullable = true)\n |    |    |    |-- bg_BG: string (nullable = true)\n |    |    |    |-- cs_CZ: string (nullable = true)\n |    |    |    |-- da_DK: string (nullable = true)\n |    |    |    |-- de_AT: string (nullable = true)\n |    |    |    |-- de_BE: string (nullable = true)\n |    |    |    |-- de_CH: string (nullable = true)\n |    |    |    |-- de_DE: string (nullable = true)\n |    |    |    |-- el_GR: string (nullable = true)\n |    |    |    |-- en_AU: string (nullable = true)\n |    |    |    |-- en_CA: string (nullable = true)\n |    |    |    |-- en_CH: string (nullable = true)\n |    |    |    |-- en_GB: string (nullable = true)\n |    |    |    |-- en_IE: string (nullable = true)\n |    |    |    |-- en_IN: string (nullable = true)\n |    |    |    |-- en_MY: string (nullable = true)\n |    |    |    |-- en_NZ: string (nullable = true)\n |    |    |    |-- en_PH: string (nullable = true)\n |    |    |    |-- en_SG: string (nullable = true)\n |    |    |    |-- en_US: string (nullable = true)\n |    |    |    |-- en_US_America: string (nullable = true)\n |    |    |    |-- es_ES: string (nullable = true)\n |    |    |    |-- et_EE: string (nullable = true)\n |    |    |    |-- fi_FI: string (nullable = true)\n |    |    |    |-- fr_BE: string (nullable = true)\n |    |    |    |-- fr_CH: string (nullable = true)\n |    |    |    |-- fr_FR: string (nullable = true)\n |    |    |    |-- hr_HR: string (nullable = true)\n |    |    |    |-- hu_HU: string (nullable = true)\n |    |    |    |-- id_ID: string (nullable = true)\n |    |    |    |-- is_IS: string (nullable = true)\n |    |    |    |-- it_CH: string (nullable = true)\n |    |    |    |-- it_IT: string (nullable = true)\n |    |    |    |-- ja_JP: string (nullable = true)\n |    |    |    |-- ko_KR: string (nullable = true)\n |    |    |    |-- lt_LT: string (nullable = true)\n |    |    |    |-- lv_LV: string (nullable = true)\n |    |    |    |-- ms_MY: string (nullable = true)\n |    |    |    |-- nb_NO: string (nullable = true)\n |    |    |    |-- nl_BE: string (nullable = true)\n |    |    |    |-- nl_NL: string (nullable = true)\n |    |    |    |-- nn_NO: string (nullable = true)\n |    |    |    |-- pl_PL: string (nullable = true)\n |    |    |    |-- pt_PT: string (nullable = true)\n |    |    |    |-- ro_RO: string (nullable = true)\n |    |    |    |-- ru_RU: string (nullable = true)\n |    |    |    |-- sk_SK: string (nullable = true)\n |    |    |    |-- sl_SI: string (nullable = true)\n |    |    |    |-- sr_Cyrl_RS: string (nullable = true)\n |    |    |    |-- sv_SE: string (nullable = true)\n |    |    |    |-- th_TH: string (nullable = true)\n |    |    |    |-- tr_TR: string (nullable = true)\n |    |    |    |-- uk_UA: string (nullable = true)\n |    |    |    |-- vi_VN: string (nullable = true)\n |    |    |    |-- zh_CN: string (nullable = true)\n |    |    |    |-- zh_TW: string (nullable = true)\n |    |    |-- parent: string (nullable = true)\n |    |    |-- updated: string (nullable = true)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;My function is this &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def flatten_df(nested_df):\n    flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == &amp;#39;string&amp;#39;]\n    nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == &amp;#39;struct&amp;#39;]\n\n    flat_df = nested_df.select(flat_cols +\n                               [col(nc+&amp;#39;.&amp;#39;+c).alias(nc+&amp;#39;_&amp;#39;+c)\n                                for nc in nested_cols\n                                for c in nested_df.select(nc+&amp;#39;.*&amp;#39;).columns])\n    return flat_df\n\n# Top level hierarchy\ndf = df.select(&amp;#39;_embedded.*&amp;#39;)\n#Reaching the lower level called &amp;quot;items&amp;quot;\ndf1 = df.select(explode(df.items).alias(&amp;#39;required&amp;#39;))\n# Creating dataframe which will be passed to flatten_df to flatten entire data under &amp;quot;items&amp;quot; hierarchy\ndf2 = df1.select(&amp;#39;required.*&amp;#39;)\nfinal = flatten_df(df2)\ndisplay(final)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;this function takes the &lt;code&gt;lables&lt;/code&gt;   column keys and puts them as seperate columns ( please see the image &lt;a href=\"https://imgur.com/a/xa5VmCu\"&gt;https://imgur.com/a/xa5VmCu&lt;/a&gt; )&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;what I want is that function should give only two columns from &lt;code&gt;lables&lt;/code&gt; column named Lable_key Lable_value  as shown in the image  &lt;a href=\"https://imgur.com/a/sTW9sSx\"&gt;https://imgur.com/a/sTW9sSx&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?auto=webp&amp;v=enabled&amp;s=7a58cc0ee2d095f64688402d8012d36b9b6a66d2", "width": 1317, "height": 184}, "resolutions": [{"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f66ce948572caabe1038462bad873d97fc1a6f00", "width": 108, "height": 15}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19b8cfa11924aba9692b30f9f090ba1b2bdc5cf6", "width": 216, "height": 30}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d770f580c8e9f5a3b75d918f355a185747d363e", "width": 320, "height": 44}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e91a890cf3c85f5d7f35fc36efa574576ea686b6", "width": 640, "height": 89}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca2e0618fd945164bb0ee5d1f453f615f3370c12", "width": 960, "height": 134}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d83982ce3a8ecd415011b397cfd95a71b1a3ba4", "width": 1080, "height": 150}], "variants": {}, "id": "-PIFhkx_4ZX0v6qeVW9b0POiuvOW5532KuWALDCw1dU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10f1kh1", "is_robot_indexable": true, "report_reasons": null, "author": "9gg6", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f1kh1/flatten_nested_json_file_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f1kh1/flatten_nested_json_file_pyspark/", "subreddit_subscribers": 86673, "created_utc": 1674027896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can somebody help? In over my head trying to run a product assessment for Databricks with almost no guidance for the use case. Would be helpful to hear what folks really like about the product + what questions people wish they had asked or better understood before becoming a customer", "author_fullname": "t2_tye5ydmn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks evaluation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10euyrz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674007714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can somebody help? In over my head trying to run a product assessment for Databricks with almost no guidance for the use case. Would be helpful to hear what folks really like about the product + what questions people wish they had asked or better understood before becoming a customer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10euyrz", "is_robot_indexable": true, "report_reasons": null, "author": "LuckyChopsSOS", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10euyrz/databricks_evaluation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10euyrz/databricks_evaluation/", "subreddit_subscribers": 86673, "created_utc": 1674007714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am an intern with a long leash. My current python script calls a series of apis and puts the code into a dataframe. Let's call this script 1 and df1. \n\n&amp;#x200B;\n\nEach day I would like this script to run on a schedule and then append either a csv file or SQL table. \n\nMy immediate group uses AWS, and someone in my group said I could go ahead and use their SQL server. My experience with SQL is MySQL workbench, and I've connected to an AWS server before. \n\n&amp;#x200B;\n\nI was able to simply run my notebook in ML studio in azure on my personal computer, but I guess that isn't configured at work so that isn't an option. We do have pyspark and databricks through azure. I have deployed to an EC2 in aws before for a different project, but got really lost with aws lamda trying to schedule and I don't have a linux background. \n\n&amp;#x200B;\n\nI feel comfortable in python and can do some ML, and I would like to just use scikit-learn in python for predictive analytics down the line. It seems like scheduling batches is a bit more drag and drop in azure. This pipeline would probably just update a csv in the data lake each day. I am leaning azure.\n\n&amp;#x200B;\n\nwhich platform is the easiest to run this pipeline and schedule the code to run once per day?", "author_fullname": "t2_pqpunsd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "azure or aws? daily batch api calls --&gt; pandas dataframe --&gt; sql or csv --&gt; powerbi", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10febor", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674065611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an intern with a long leash. My current python script calls a series of apis and puts the code into a dataframe. Let&amp;#39;s call this script 1 and df1. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Each day I would like this script to run on a schedule and then append either a csv file or SQL table. &lt;/p&gt;\n\n&lt;p&gt;My immediate group uses AWS, and someone in my group said I could go ahead and use their SQL server. My experience with SQL is MySQL workbench, and I&amp;#39;ve connected to an AWS server before. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was able to simply run my notebook in ML studio in azure on my personal computer, but I guess that isn&amp;#39;t configured at work so that isn&amp;#39;t an option. We do have pyspark and databricks through azure. I have deployed to an EC2 in aws before for a different project, but got really lost with aws lamda trying to schedule and I don&amp;#39;t have a linux background. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I feel comfortable in python and can do some ML, and I would like to just use scikit-learn in python for predictive analytics down the line. It seems like scheduling batches is a bit more drag and drop in azure. This pipeline would probably just update a csv in the data lake each day. I am leaning azure.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;which platform is the easiest to run this pipeline and schedule the code to run once per day?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10febor", "is_robot_indexable": true, "report_reasons": null, "author": "ClimatePhilosopher", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10febor/azure_or_aws_daily_batch_api_calls_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10febor/azure_or_aws_daily_batch_api_calls_pandas/", "subreddit_subscribers": 86673, "created_utc": 1674065611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# [Jailer Database Tools.](https://wisser.github.io/Jailer/)\n\nJailer is a tool for database subsetting and relational data browsing.\n\nIt creates small slices from your database and lets you navigate through your database following the relationships.Ideal for creating small samples of test data or for local problem analysis with relevant production data.\n\nThe Subsetter creates small slices from your database (consistent and reverentially intact) as SQL (topologically sorted), DbUnit records or XML. Ideal for creating small samples of test data or for local problem analysis with relevant production data.\n\nThe Data Browser lets you navigate through your database following the relationships (foreign key-based or user-defined) between tables.\n\n# Features\n\nExports consistent and reverentially intact row-sets from your productive database and imports the data into your development and test environment.\n\nImproves database performance by removing and archiving obsolete data without violating integrity.\n\nGenerates topologically sorted SQL-DML, hierarchically structured XML and DbUnit datasets.\n\nData Browsing. Navigate bidirectionally through the database by following foreign-key-based or user-defined relationships.\n\nSQL Console with code completion, syntax highlighting and database metadata visualization.\n\nA demo database is included with which you can get a first impression without any configuration effort.", "author_fullname": "t2_5sa5b0ia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New release of Jailer database tools publicized", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10f3lpo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674035839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;&lt;a href=\"https://wisser.github.io/Jailer/\"&gt;Jailer Database Tools.&lt;/a&gt;&lt;/h1&gt;\n\n&lt;p&gt;Jailer is a tool for database subsetting and relational data browsing.&lt;/p&gt;\n\n&lt;p&gt;It creates small slices from your database and lets you navigate through your database following the relationships.Ideal for creating small samples of test data or for local problem analysis with relevant production data.&lt;/p&gt;\n\n&lt;p&gt;The Subsetter creates small slices from your database (consistent and reverentially intact) as SQL (topologically sorted), DbUnit records or XML. Ideal for creating small samples of test data or for local problem analysis with relevant production data.&lt;/p&gt;\n\n&lt;p&gt;The Data Browser lets you navigate through your database following the relationships (foreign key-based or user-defined) between tables.&lt;/p&gt;\n\n&lt;h1&gt;Features&lt;/h1&gt;\n\n&lt;p&gt;Exports consistent and reverentially intact row-sets from your productive database and imports the data into your development and test environment.&lt;/p&gt;\n\n&lt;p&gt;Improves database performance by removing and archiving obsolete data without violating integrity.&lt;/p&gt;\n\n&lt;p&gt;Generates topologically sorted SQL-DML, hierarchically structured XML and DbUnit datasets.&lt;/p&gt;\n\n&lt;p&gt;Data Browsing. Navigate bidirectionally through the database by following foreign-key-based or user-defined relationships.&lt;/p&gt;\n\n&lt;p&gt;SQL Console with code completion, syntax highlighting and database metadata visualization.&lt;/p&gt;\n\n&lt;p&gt;A demo database is included with which you can get a first impression without any configuration effort.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "10f3lpo", "is_robot_indexable": true, "report_reasons": null, "author": "Plane-Discussion", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f3lpo/new_release_of_jailer_database_tools_publicized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f3lpo/new_release_of_jailer_database_tools_publicized/", "subreddit_subscribers": 86673, "created_utc": 1674035839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_alka6pjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resume Review request - BA to DE Pivot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10ezuz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.55, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9KYzoafqeDnG-_PpUWLOByjbMlz1QfJEuPzJVCCVOxo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674021917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/lrfnoo02tqca1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/lrfnoo02tqca1.png?auto=webp&amp;v=enabled&amp;s=91b8620f075e5ce896f4b5e14c0d567427cf6f54", "width": 422, "height": 516}, "resolutions": [{"url": "https://preview.redd.it/lrfnoo02tqca1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85d093ca93176a394919ff46e28fecee90627c97", "width": 108, "height": 132}, {"url": "https://preview.redd.it/lrfnoo02tqca1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51bfb89f9e5b6abaed0c5f1291aaf34be7a05594", "width": 216, "height": 264}, {"url": "https://preview.redd.it/lrfnoo02tqca1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5dbe508bd3270e09688cf52b82f2c1631f491b38", "width": 320, "height": 391}], "variants": {}, "id": "NKWY7CXzH_LGoyIUo72kU-hbkmDTtkN7Hugu4VkV8Wg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10ezuz9", "is_robot_indexable": true, "report_reasons": null, "author": "depressedbutsassy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ezuz9/resume_review_request_ba_to_de_pivot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/lrfnoo02tqca1.png", "subreddit_subscribers": 86673, "created_utc": 1674021917.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What does a career path look like for someone in a leadership role in the data field? What are the common job titles and responsibilities one can expect to have at each step, and what skills and qualifications are typically required to advance in this field? I'm a head of data in a scale-up but am not sure of what my career progression will look like. It's less straightforward than when working in \"traditional\" teams. Would like to hear about other's experience", "author_fullname": "t2_9blh4yzs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data leader career progression", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10fhyud", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674074170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What does a career path look like for someone in a leadership role in the data field? What are the common job titles and responsibilities one can expect to have at each step, and what skills and qualifications are typically required to advance in this field? I&amp;#39;m a head of data in a scale-up but am not sure of what my career progression will look like. It&amp;#39;s less straightforward than when working in &amp;quot;traditional&amp;quot; teams. Would like to hear about other&amp;#39;s experience&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10fhyud", "is_robot_indexable": true, "report_reasons": null, "author": "Strict_Algae3766", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fhyud/data_leader_career_progression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fhyud/data_leader_career_progression/", "subreddit_subscribers": 86673, "created_utc": 1674074170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lately, I finished reading the **Learning Spark: Lightning-Fast Data Analytics** book, a book that helped me to have a better understanding of **Apache** **Spark**.  \n\nMy new article goes over some of the takeaways I learned from reading the book, and my daily interaction with Spark for 6 months.  \n\nLink: https://aymane11.github.io/posts/apache-spark-gotchas/", "author_fullname": "t2_20xw1ryl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Spark Gotchas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10fhkkq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674073223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lately, I finished reading the &lt;strong&gt;Learning Spark: Lightning-Fast Data Analytics&lt;/strong&gt; book, a book that helped me to have a better understanding of &lt;strong&gt;Apache&lt;/strong&gt; &lt;strong&gt;Spark&lt;/strong&gt;.  &lt;/p&gt;\n\n&lt;p&gt;My new article goes over some of the takeaways I learned from reading the book, and my daily interaction with Spark for 6 months.  &lt;/p&gt;\n\n&lt;p&gt;Link: &lt;a href=\"https://aymane11.github.io/posts/apache-spark-gotchas/\"&gt;https://aymane11.github.io/posts/apache-spark-gotchas/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10fhkkq", "is_robot_indexable": true, "report_reasons": null, "author": "Enamya11", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fhkkq/apache_spark_gotchas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fhkkq/apache_spark_gotchas/", "subreddit_subscribers": 86673, "created_utc": 1674073223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm conducting a study on how I would use Apache Superset for an organization. I will have large amounts of data at source (Million of Rows scale) and about 600 Users.\n\n  \nWhat would be a good way of deploying Superset Google Cloud ? I'm thinking GKE but I have no idea about the right sizing, nor on how much would it cost.\n\n  \nIt would be helpful if you could share some experiences you had.\n\nThank you !", "author_fullname": "t2_98269xyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying Apache Superset on GKE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fgihh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674070723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m conducting a study on how I would use Apache Superset for an organization. I will have large amounts of data at source (Million of Rows scale) and about 600 Users.&lt;/p&gt;\n\n&lt;p&gt;What would be a good way of deploying Superset Google Cloud ? I&amp;#39;m thinking GKE but I have no idea about the right sizing, nor on how much would it cost.&lt;/p&gt;\n\n&lt;p&gt;It would be helpful if you could share some experiences you had.&lt;/p&gt;\n\n&lt;p&gt;Thank you !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fgihh", "is_robot_indexable": true, "report_reasons": null, "author": "Electronic-Mountain9", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fgihh/deploying_apache_superset_on_gke/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fgihh/deploying_apache_superset_on_gke/", "subreddit_subscribers": 86673, "created_utc": 1674070723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a little shill-y, but I think it\u2019s cool and I think others here will too.\n\nIf you haven\u2019t heard of Materialize, it\u2019s a database that incrementally updates query results as new data flows in from Kafka or Postgres logical replication. It\u2019s different from typical databases in that results are updated on *write* using a stream processing engine rather than recomputed from scratch on read. That means reads are typically super fast, even for really complicated views with lots of joins.\n\nOne of the first things I had to learn as a Field Engineer at Materialize was how to optimize SQL joins to help our customers save on memory (and $). To do that, I made a couple of updates to one of Frank McSherry\u2019s blogs, which were published today! I\u2019d love to see what you think!\n\nhttps://materialize.com/blog/delta-joins/", "author_fullname": "t2_5p00kusf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimize Joins in Materialize with Delta Queries and Late Materialization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ffdvx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674068433.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674068064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a little shill-y, but I think it\u2019s cool and I think others here will too.&lt;/p&gt;\n\n&lt;p&gt;If you haven\u2019t heard of Materialize, it\u2019s a database that incrementally updates query results as new data flows in from Kafka or Postgres logical replication. It\u2019s different from typical databases in that results are updated on &lt;em&gt;write&lt;/em&gt; using a stream processing engine rather than recomputed from scratch on read. That means reads are typically super fast, even for really complicated views with lots of joins.&lt;/p&gt;\n\n&lt;p&gt;One of the first things I had to learn as a Field Engineer at Materialize was how to optimize SQL joins to help our customers save on memory (and $). To do that, I made a couple of updates to one of Frank McSherry\u2019s blogs, which were published today! I\u2019d love to see what you think!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://materialize.com/blog/delta-joins/\"&gt;https://materialize.com/blog/delta-joins/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?auto=webp&amp;v=enabled&amp;s=9fc4396a318c2807c8b7eb9925d82c522295c4c5", "width": 1636, "height": 656}, "resolutions": [{"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c95088825ab701a767ae737724f85f494144ce90", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f02a3d2de233ef59877a3105038f7f04fbe9971", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1789d3f9b845c9675d57ff92fdc5f2501fa76646", "width": 320, "height": 128}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d892c0c41ed4dbb8ff6c9f253c903b791694699d", "width": 640, "height": 256}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=654e9ae06191dae53c2255b2275a46abd8b59c9b", "width": 960, "height": 384}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ed95ec3916adc3bcb738ac68f52e0aaa922bef6", "width": 1080, "height": 433}], "variants": {}, "id": "IMNd2US1USXqACJWc3sq8tqHd6qPrbtxC4Zq9e0nYj4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10ffdvx", "is_robot_indexable": true, "report_reasons": null, "author": "Chuck-Alt-Delete", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ffdvx/optimize_joins_in_materialize_with_delta_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ffdvx/optimize_joins_in_materialize_with_delta_queries/", "subreddit_subscribers": 86673, "created_utc": 1674068064.0, "num_crossposts": 4, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone been able to read secrets from a Keyvault?  Supposedly TokenLibrary.getsecret() is the way to go but I have not been able to get it to work.  Error: POST failed with 'Bad Request'.\n\nThe managed identity in Synapse has rights to read keys, as I am using the keyvault in pipelines and already have a linked service set up.\n\nAny ideas about how to get this up and running?  I'm trying to connect to a SQL databse via jdbc, and it connects fine, I'd just like to use a secret for the user/pw on the connection.", "author_fullname": "t2_4270ek0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synapse PySpark Notebook - get secret from key vault?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ff5ts", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674067533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone been able to read secrets from a Keyvault?  Supposedly TokenLibrary.getsecret() is the way to go but I have not been able to get it to work.  Error: POST failed with &amp;#39;Bad Request&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;The managed identity in Synapse has rights to read keys, as I am using the keyvault in pipelines and already have a linked service set up.&lt;/p&gt;\n\n&lt;p&gt;Any ideas about how to get this up and running?  I&amp;#39;m trying to connect to a SQL databse via jdbc, and it connects fine, I&amp;#39;d just like to use a secret for the user/pw on the connection.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ff5ts", "is_robot_indexable": true, "report_reasons": null, "author": "mr_electric_wizard", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ff5ts/synapse_pyspark_notebook_get_secret_from_key_vault/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ff5ts/synapse_pyspark_notebook_get_secret_from_key_vault/", "subreddit_subscribers": 86673, "created_utc": 1674067533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_49cfbl1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Talks Club talks to Versatile Data Kit contributor @ Open-Source Spotlight", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_10fbx3v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/beLo1BGcRpI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Open-Source Spotlight - Versatile Data Kit - Gabriel Georgiev\"&gt;&lt;/iframe&gt;", "width": 267, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Open-Source Spotlight - Versatile Data Kit - Gabriel Georgiev", "type": "video", "thumbnail_width": 480, "height": 200, "width": 267, "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/beLo1BGcRpI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Open-Source Spotlight - Versatile Data Kit - Gabriel Georgiev\"&gt;&lt;/iframe&gt;", "author_name": "DataTalksClub \u2b1b", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/beLo1BGcRpI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataTalksClub"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/beLo1BGcRpI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Open-Source Spotlight - Versatile Data Kit - Gabriel Georgiev\"&gt;&lt;/iframe&gt;", "width": 267, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/10fbx3v", "height": 200}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/04WtPwQrJJva4ART6tgg_zjwQ9AdsZlgCMZVOO1DkwA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674060054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/beLo1BGcRpI", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ql3y-kwq-QIWaWxu2JB6NPnQX0FfNh0rAnukmfXqgcY.jpg?auto=webp&amp;v=enabled&amp;s=9f38f3f80e5a0b9f11ce2bd9dff06ec4800b195c", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ql3y-kwq-QIWaWxu2JB6NPnQX0FfNh0rAnukmfXqgcY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c7cd44986e26434dd87405d6302e27957f19a2f", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ql3y-kwq-QIWaWxu2JB6NPnQX0FfNh0rAnukmfXqgcY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4c77ec271bdeccd46e55835e9ecd64cc8f42c98", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ql3y-kwq-QIWaWxu2JB6NPnQX0FfNh0rAnukmfXqgcY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5b3b9184c6eb23b05cc89193630a3bfbfdbb2f3", "width": 320, "height": 240}], "variants": {}, "id": "AdhBuGH_vpxQo7W8hcgEr28X3Zq-l0IrTj9yZcy-1oI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "10fbx3v", "is_robot_indexable": true, "report_reasons": null, "author": "zverulacis", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fbx3v/data_talks_club_talks_to_versatile_data_kit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/beLo1BGcRpI", "subreddit_subscribers": 86673, "created_utc": 1674060054.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Open-Source Spotlight - Versatile Data Kit - Gabriel Georgiev", "type": "video", "thumbnail_width": 480, "height": 200, "width": 267, "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/beLo1BGcRpI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Open-Source Spotlight - Versatile Data Kit - Gabriel Georgiev\"&gt;&lt;/iframe&gt;", "author_name": "DataTalksClub \u2b1b", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/beLo1BGcRpI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataTalksClub"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Oracle has a free tier which I want to use for a general cloud computing instance on multiple projects; its specs are more or less .48 Gbps for network bandwidth, 1GB RAM, and 20GB storage.\n\nOne of the things I plan on using it for is for a data engineering project which itself would deal with pulls from some API for data and transporting the data into a BiqQuery instance on GCP. I figured I'd deal with the complete scheduling through Airflow.\n\nThus the crux of the problem: Airflow normally installable through pip, and I normally use pip under miniconda just to deal with environments and whatnot more easily. Afaik, conda is too beefy for this thing and there are some open bugs on issues with such low memory. I can install pip and poetry or whatever to deal with it, but I don't have any experience with poetry and have heard its learning curve is up there. I'm not opposed to learning it if that's the recommended way.\n\nI'm also considering just getting a docker image up there, but then I wouldn't be sure of whether I should dockerize everything else that would be involved in this project and how much that would affect the puny VM.\n\n&amp;#x200B;\n\nThoughts?", "author_fullname": "t2_svn12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to deal with my environments/projects on a minimal VM?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fb2lx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674058049.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Oracle has a free tier which I want to use for a general cloud computing instance on multiple projects; its specs are more or less .48 Gbps for network bandwidth, 1GB RAM, and 20GB storage.&lt;/p&gt;\n\n&lt;p&gt;One of the things I plan on using it for is for a data engineering project which itself would deal with pulls from some API for data and transporting the data into a BiqQuery instance on GCP. I figured I&amp;#39;d deal with the complete scheduling through Airflow.&lt;/p&gt;\n\n&lt;p&gt;Thus the crux of the problem: Airflow normally installable through pip, and I normally use pip under miniconda just to deal with environments and whatnot more easily. Afaik, conda is too beefy for this thing and there are some open bugs on issues with such low memory. I can install pip and poetry or whatever to deal with it, but I don&amp;#39;t have any experience with poetry and have heard its learning curve is up there. I&amp;#39;m not opposed to learning it if that&amp;#39;s the recommended way.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also considering just getting a docker image up there, but then I wouldn&amp;#39;t be sure of whether I should dockerize everything else that would be involved in this project and how much that would affect the puny VM.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fb2lx", "is_robot_indexable": true, "report_reasons": null, "author": "paxmlank", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fb2lx/whats_the_best_way_to_deal_with_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fb2lx/whats_the_best_way_to_deal_with_my/", "subreddit_subscribers": 86673, "created_utc": 1674058049.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI produce the #TrueDataOps Podcast with host, Kent Graziano. [https://www.youtube.com/playlist?list=PLgdSafWhtnBhTbzhT2o9XbQ-O6En1gs3Q](https://www.youtube.com/playlist?list=PLgdSafWhtnBhTbzhT2o9XbQ-O6En1gs3Q)\n\nWe discuss all things DataOps with the people leading the field and bringing dataops to maturity. A few past guests we had on are Wayne Eckerson and Joe Reis.   \n\n\nMy question is, What are your thoughts? What are ways we can improve? Thank you in advance!", "author_fullname": "t2_12li6zgs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "#TrueDataOps Podcast - Feedback", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10f94yw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674053311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I produce the #TrueDataOps Podcast with host, Kent Graziano. &lt;a href=\"https://www.youtube.com/playlist?list=PLgdSafWhtnBhTbzhT2o9XbQ-O6En1gs3Q\"&gt;https://www.youtube.com/playlist?list=PLgdSafWhtnBhTbzhT2o9XbQ-O6En1gs3Q&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We discuss all things DataOps with the people leading the field and bringing dataops to maturity. A few past guests we had on are Wayne Eckerson and Joe Reis.   &lt;/p&gt;\n\n&lt;p&gt;My question is, What are your thoughts? What are ways we can improve? Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BcKe7vAvwB9RQLSftH9A0yYdRiBTYP_QqpLVKh8VrN4.jpg?auto=webp&amp;v=enabled&amp;s=f69215e4e8e2708116989bd8a467e5a73eaf9d2e", "width": 168, "height": 94}, "resolutions": [{"url": "https://external-preview.redd.it/BcKe7vAvwB9RQLSftH9A0yYdRiBTYP_QqpLVKh8VrN4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98deb0994b26eec0cc4e539fd4ec95262b080f25", "width": 108, "height": 60}], "variants": {}, "id": "ON1UkDE3vdDvk5Ry9ELGDqD9y1-GqL74aUzQrFKbBhM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10f94yw", "is_robot_indexable": true, "report_reasons": null, "author": "Dkreig", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f94yw/truedataops_podcast_feedback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f94yw/truedataops_podcast_feedback/", "subreddit_subscribers": 86673, "created_utc": 1674053311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_v3fb8jv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "I'm having around 2 YoE. Thought of making a switch within a year. Looking to get into top product companies. Any advice to optimise my resume would be appreciated. Thanks in advance. Also I'm confused about which resume to use. Single column or Two columns.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"e6le1l2qatca1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 163, "x": 108, "u": "https://preview.redd.it/e6le1l2qatca1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a539c6c6627ece027d4b7ce14a54e8b1ad56107"}, {"y": 327, "x": 216, "u": "https://preview.redd.it/e6le1l2qatca1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9948abdcf4e4928259900fe9474e811ae9c96b11"}, {"y": 485, "x": 320, "u": "https://preview.redd.it/e6le1l2qatca1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08171eb17c29bebd1b9cce7af911d203152b5b98"}, {"y": 970, "x": 640, "u": "https://preview.redd.it/e6le1l2qatca1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c81d9bba3335b643d611c8a90e20efa55eb1ded0"}, {"y": 1455, "x": 960, "u": "https://preview.redd.it/e6le1l2qatca1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c09185f4537352a80a249e45ab15cbe3b283719"}, {"y": 1637, "x": 1080, "u": "https://preview.redd.it/e6le1l2qatca1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2bd1a8c4bfe1cff24d13522a6e37149a66e7d9d8"}], "s": {"y": 1637, "x": 1080, "u": "https://preview.redd.it/e6le1l2qatca1.png?width=1080&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2f28740ad619acbd33d01293f2ed1368c6ab5ecc"}, "id": "e6le1l2qatca1"}, "hp6f6jcqatca1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 152, "x": 108, "u": "https://preview.redd.it/hp6f6jcqatca1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2cb3d40d10f42bb10a1adedd87fea87928471074"}, {"y": 304, "x": 216, "u": "https://preview.redd.it/hp6f6jcqatca1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7ffd64dd7d092de072f0f3d72e17b226cf4ab63"}, {"y": 451, "x": 320, "u": "https://preview.redd.it/hp6f6jcqatca1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb21e351447d2d9ab74f60933ec03724f4be099c"}, {"y": 903, "x": 640, "u": "https://preview.redd.it/hp6f6jcqatca1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8267624046544a7f2789348ec19847b9cec8d003"}, {"y": 1354, "x": 960, "u": "https://preview.redd.it/hp6f6jcqatca1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb984ca8c0c9da03a8921e6c5e726fa5411f25ad"}, {"y": 1524, "x": 1080, "u": "https://preview.redd.it/hp6f6jcqatca1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19f5477a79f7ec2125c81f24655752735932791b"}], "s": {"y": 1524, "x": 1080, "u": "https://preview.redd.it/hp6f6jcqatca1.png?width=1080&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ea4ef3342c33ae40be64db9dfab4d9ad21d5a978"}, "id": "hp6f6jcqatca1"}}, "name": "t3_10f8o4k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 1, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "e6le1l2qatca1", "id": 231063009}, {"media_id": "hp6f6jcqatca1", "id": 231063010}]}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-iBH-vqT0TqJrO6XIU0cyYKPlxuQ87K_fTXGmotX6Ko.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674052059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/10f8o4k", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10f8o4k", "is_robot_indexable": true, "report_reasons": null, "author": "CheesecakeFrosty20", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f8o4k/im_having_around_2_yoe_thought_of_making_a_switch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/10f8o4k", "subreddit_subscribers": 86673, "created_utc": 1674052059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm at a new position and currently understanding their architecture. Since it is my first time learning data engineering solutions, I wanted to ask for your help/input.\n\nCurrently there are different machines with different sensors that collect data, I think you can think of IoT and telemetry data, for instance. The HW/SW onboard sends a JSON file to the Azure Blob Storage Cloud. The JSON is formed with semi-structured data cointaining the composite key to identify the machine, the composite key to identify the sensor, the value of the data and its timestamp (and other things not important). At the moment a scheduled ELT routine take these JSON and rewrite CSV files to the Blob Storage, one .csv for one machine: the rows are measurements and the columns the sensors of the machine. The problem is that this structure is very sparse due to the different sampling frequency of the sensor, many columns are very sparse. Also later analytics have to download every machines files in the desired data range to check if a desired measure is collected or not.\n\nMy idea is first to use binary data files, for instance .parquet, then maybe make one file for each (machine x sensor) product, so that the \"tables\" are not sparse anymore. I don't know how to evaluate the overhead of this solutions taking into considerations that Azure Blob Storage already has a tree directory path where the trees are hourly files, e.g. maximum 3600 rows for each machine x sensor since our max frequency is 1s and slowest 5m.\n\nMy idea was to use the Blob Storage with a logical division, the JSON raw data as semi-structured and the output of the ELT routine as a \"Data Warehouse\" where I could easily query the interested binary data and index them by the desired rows (the desired datetime range, using the timestamp information as index).\n\nAm I missing something? Any input is appreciated", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure blob storage and design of its files (database)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10f2ijv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674031604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m at a new position and currently understanding their architecture. Since it is my first time learning data engineering solutions, I wanted to ask for your help/input.&lt;/p&gt;\n\n&lt;p&gt;Currently there are different machines with different sensors that collect data, I think you can think of IoT and telemetry data, for instance. The HW/SW onboard sends a JSON file to the Azure Blob Storage Cloud. The JSON is formed with semi-structured data cointaining the composite key to identify the machine, the composite key to identify the sensor, the value of the data and its timestamp (and other things not important). At the moment a scheduled ELT routine take these JSON and rewrite CSV files to the Blob Storage, one .csv for one machine: the rows are measurements and the columns the sensors of the machine. The problem is that this structure is very sparse due to the different sampling frequency of the sensor, many columns are very sparse. Also later analytics have to download every machines files in the desired data range to check if a desired measure is collected or not.&lt;/p&gt;\n\n&lt;p&gt;My idea is first to use binary data files, for instance .parquet, then maybe make one file for each (machine x sensor) product, so that the &amp;quot;tables&amp;quot; are not sparse anymore. I don&amp;#39;t know how to evaluate the overhead of this solutions taking into considerations that Azure Blob Storage already has a tree directory path where the trees are hourly files, e.g. maximum 3600 rows for each machine x sensor since our max frequency is 1s and slowest 5m.&lt;/p&gt;\n\n&lt;p&gt;My idea was to use the Blob Storage with a logical division, the JSON raw data as semi-structured and the output of the ELT routine as a &amp;quot;Data Warehouse&amp;quot; where I could easily query the interested binary data and index them by the desired rows (the desired datetime range, using the timestamp information as index).&lt;/p&gt;\n\n&lt;p&gt;Am I missing something? Any input is appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10f2ijv", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f2ijv/azure_blob_storage_and_design_of_its_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f2ijv/azure_blob_storage_and_design_of_its_files/", "subreddit_subscribers": 86673, "created_utc": 1674031604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone out there have experience with these tools?  If so, how do they compare in your opinion?", "author_fullname": "t2_5lfqidpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alation vs. Atlan vs. Collibra", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10esiz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674001376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone out there have experience with these tools?  If so, how do they compare in your opinion?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10esiz9", "is_robot_indexable": true, "report_reasons": null, "author": "imani_TqiynAZU", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10esiz9/alation_vs_atlan_vs_collibra/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10esiz9/alation_vs_atlan_vs_collibra/", "subreddit_subscribers": 86673, "created_utc": 1674001376.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}