{"kind": "Listing", "data": {"after": "t3_109b8h6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a PhD in Engineering and have very good knowledge of Python, SQL, and machine learning.\n\nCurrently, work as a data scientist in an insurance company (less than 1 year of job experience), but my plan is to get into Amazon or Meta as a data scientist as the next step. \n\nMy current data scientist position is mainly about data cleaning, building, and improving ML models using Python.\n\nI do not have that much experience in Cloud and Big Data frameworks such as Spark, and my current employer does not provide such possibilities either.\n\nMy plan is to learn cloud (AWS or GCP) and focus on Leet Code for this. I consider 12 months for improving my resume and boosting the required skills. Considering my knowledge in SQL, Python, and ML, do you think improving my knowledge/experience in Cloud and Leet Code is a good package for a job change to Amazon or Meta? Do you recommend any other skillset such as Spark, etc?\n\nThank you so much!", "author_fullname": "t2_h2udjs11", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Skills required for DS position at Meta/AMAZON", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109hp8q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 58, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 58, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673475500.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673475277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a PhD in Engineering and have very good knowledge of Python, SQL, and machine learning.&lt;/p&gt;\n\n&lt;p&gt;Currently, work as a data scientist in an insurance company (less than 1 year of job experience), but my plan is to get into Amazon or Meta as a data scientist as the next step. &lt;/p&gt;\n\n&lt;p&gt;My current data scientist position is mainly about data cleaning, building, and improving ML models using Python.&lt;/p&gt;\n\n&lt;p&gt;I do not have that much experience in Cloud and Big Data frameworks such as Spark, and my current employer does not provide such possibilities either.&lt;/p&gt;\n\n&lt;p&gt;My plan is to learn cloud (AWS or GCP) and focus on Leet Code for this. I consider 12 months for improving my resume and boosting the required skills. Considering my knowledge in SQL, Python, and ML, do you think improving my knowledge/experience in Cloud and Leet Code is a good package for a job change to Amazon or Meta? Do you recommend any other skillset such as Spark, etc?&lt;/p&gt;\n\n&lt;p&gt;Thank you so much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109hp8q", "is_robot_indexable": true, "report_reasons": null, "author": "Last-Revenue-660", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109hp8q/skills_required_for_ds_position_at_metaamazon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109hp8q/skills_required_for_ds_position_at_metaamazon/", "subreddit_subscribers": 836394, "created_utc": 1673475277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I maintain a numeric model with unit and integration \"spot tests\".  When I add a new feature that changes the model some of these tests will predictably fail.  I usually manually go through and update each affected test, ensure the \"failed\" value looks correct, and then use it for the new expected value.  Is there any more automated way to accomplish this, or other approaches to the problem?  Thanks!\n\nFor background I am using Python with Pytest.", "author_fullname": "t2_172zngvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to update many unit tests when a numeric model changes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109n88c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673489594.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673489276.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I maintain a numeric model with unit and integration &amp;quot;spot tests&amp;quot;.  When I add a new feature that changes the model some of these tests will predictably fail.  I usually manually go through and update each affected test, ensure the &amp;quot;failed&amp;quot; value looks correct, and then use it for the new expected value.  Is there any more automated way to accomplish this, or other approaches to the problem?  Thanks!&lt;/p&gt;\n\n&lt;p&gt;For background I am using Python with Pytest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109n88c", "is_robot_indexable": true, "report_reasons": null, "author": "Bertie_Woo", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109n88c/how_to_update_many_unit_tests_when_a_numeric/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109n88c/how_to_update_many_unit_tests_when_a_numeric/", "subreddit_subscribers": 836394, "created_utc": 1673489276.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi,\n\nI am currently a political science major about to graduate and I don't really like it. I've been getting into data science/data analysis recently by doing some courses on Coursera and EDX, and I'm loving it. I've always been an analytical thinker, and I'm great at finding patterns and connections, and I have great logical thinking skills.\n\nI am yet to learn Python, SQL, R, etc. more in-depth, but I have learned over 17 languages. Even if it doesn't seem like programming languages and natural languages have anything in common, I'd like to differ, since both of them require learning a different code, structure, and usage, so I'm used to organizing my ideas using different patterns.\n\nI have heard many stories of people in similar situations who came from fields completely unrelated to data science that managed to thrive upon doing some courses on the internet and maybe getting some certificates elsewhere. I am afraid that it's too late for me to even attempt to join the field and I'd like to know if there's anyone with an unconventional trajectory through data science.\n\nI know this is something I enjoy, and I would like to put to use my analytical/mathematical/logical thinking skills which in political science would be useless. I don't know, however, if this is within my realm of possibilities.\n\nI know most of you are math or engineering graduates, so I'd like to know if many of you are not.", "author_fullname": "t2_7usea7do", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What did you study at uni? (if anything at all)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109adcm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673458097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am currently a political science major about to graduate and I don&amp;#39;t really like it. I&amp;#39;ve been getting into data science/data analysis recently by doing some courses on Coursera and EDX, and I&amp;#39;m loving it. I&amp;#39;ve always been an analytical thinker, and I&amp;#39;m great at finding patterns and connections, and I have great logical thinking skills.&lt;/p&gt;\n\n&lt;p&gt;I am yet to learn Python, SQL, R, etc. more in-depth, but I have learned over 17 languages. Even if it doesn&amp;#39;t seem like programming languages and natural languages have anything in common, I&amp;#39;d like to differ, since both of them require learning a different code, structure, and usage, so I&amp;#39;m used to organizing my ideas using different patterns.&lt;/p&gt;\n\n&lt;p&gt;I have heard many stories of people in similar situations who came from fields completely unrelated to data science that managed to thrive upon doing some courses on the internet and maybe getting some certificates elsewhere. I am afraid that it&amp;#39;s too late for me to even attempt to join the field and I&amp;#39;d like to know if there&amp;#39;s anyone with an unconventional trajectory through data science.&lt;/p&gt;\n\n&lt;p&gt;I know this is something I enjoy, and I would like to put to use my analytical/mathematical/logical thinking skills which in political science would be useless. I don&amp;#39;t know, however, if this is within my realm of possibilities.&lt;/p&gt;\n\n&lt;p&gt;I know most of you are math or engineering graduates, so I&amp;#39;d like to know if many of you are not.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109adcm", "is_robot_indexable": true, "report_reasons": null, "author": "frootloop2000", "discussion_type": null, "num_comments": 72, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109adcm/what_did_you_study_at_uni_if_anything_at_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109adcm/what_did_you_study_at_uni_if_anything_at_all/", "subreddit_subscribers": 836394, "created_utc": 1673458097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Lots of organizations can benefit from ML but often struggle to find impactful projects. We wanted to share a simple but effective method for finding valuable applications that keep delivering value for our business: look for software rules that are already in production, but are suboptimal, and upgrade them with a ML solution.\n\nThis approach guarantees that you will work on something needed for the business as the rules are already in use. It will also guarantee that you will create value with ML as you are focusing on going from a working solution to optimized solution. Delivering a ML solution that creates value is a great way to prove that ML is relevant to your business and that further investments should be made into it. We believe ML still has lots of potential to improve businesses and consumer experiences across industries.\n\nWe wrote an article sharing a detailed example of this strategy that helped us to save thousands of orders from cancellation and make all sides of our marketplace happy. You can read about it [here](https://doordash.engineering/2023/01/10/how-doordash-upgraded-a-heuristic-with-ml-to-save-thousands-of-canceled-orders/). Please leave comments below if you find this useful.", "author_fullname": "t2_fm5qvxh7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A useful pattern we found for getting business impact with ML", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1098x1u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673454624.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lots of organizations can benefit from ML but often struggle to find impactful projects. We wanted to share a simple but effective method for finding valuable applications that keep delivering value for our business: look for software rules that are already in production, but are suboptimal, and upgrade them with a ML solution.&lt;/p&gt;\n\n&lt;p&gt;This approach guarantees that you will work on something needed for the business as the rules are already in use. It will also guarantee that you will create value with ML as you are focusing on going from a working solution to optimized solution. Delivering a ML solution that creates value is a great way to prove that ML is relevant to your business and that further investments should be made into it. We believe ML still has lots of potential to improve businesses and consumer experiences across industries.&lt;/p&gt;\n\n&lt;p&gt;We wrote an article sharing a detailed example of this strategy that helped us to save thousands of orders from cancellation and make all sides of our marketplace happy. You can read about it &lt;a href=\"https://doordash.engineering/2023/01/10/how-doordash-upgraded-a-heuristic-with-ml-to-save-thousands-of-canceled-orders/\"&gt;here&lt;/a&gt;. Please leave comments below if you find this useful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1098x1u", "is_robot_indexable": true, "report_reasons": null, "author": "Zealousideal-One8213", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1098x1u/a_useful_pattern_we_found_for_getting_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1098x1u/a_useful_pattern_we_found_for_getting_business/", "subreddit_subscribers": 836394, "created_utc": 1673454624.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi. I work as a junior data analyst and started my job recently. I have been asked to do a predictive model for a automobile insurance company.  This is the project :\n\nInsurance pricing policies at XYZ automobile company would like to be reviewed. In order to adjust current pricing policies, the company would like to have models for predicting both frequency and severity of claims. \n\nSolution expected :\n\nPredictive models that can be used for predicting future frequency and severity of claims.  \n\n\nI have past data with me. Can anyone help me how to approach this?", "author_fullname": "t2_t03wuws1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you approach this problem?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109rbom", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673501067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I work as a junior data analyst and started my job recently. I have been asked to do a predictive model for a automobile insurance company.  This is the project :&lt;/p&gt;\n\n&lt;p&gt;Insurance pricing policies at XYZ automobile company would like to be reviewed. In order to adjust current pricing policies, the company would like to have models for predicting both frequency and severity of claims. &lt;/p&gt;\n\n&lt;p&gt;Solution expected :&lt;/p&gt;\n\n&lt;p&gt;Predictive models that can be used for predicting future frequency and severity of claims.  &lt;/p&gt;\n\n&lt;p&gt;I have past data with me. Can anyone help me how to approach this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109rbom", "is_robot_indexable": true, "report_reasons": null, "author": "decisiondengindi", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109rbom/how_would_you_approach_this_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109rbom/how_would_you_approach_this_problem/", "subreddit_subscribers": 836394, "created_utc": 1673501067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "[The masters in question.](https://www.ntu.edu.sg/education/graduate-programme/master-of-science-in-analytics) \n\nI have a background in biology and a little organic chemistry (final semester in my bachelors) but realised I also appreciate math and programming during my internship and final year project. (I went through almost half of the chapters in linear algebra and its applications by David C. Lay, completing 80% of its associated problems and felt like it was as fun as playing Monster Hunter World or Guild Wars 2.)\n\nHowever, I am unable to find any placement statistics for the masters, is this considered a red flag?  Additionally, MH6211 Analytics Software I and MH6212 Analytics Software II cover SPS which according to some, is not really used in industry? I was also unable to find any reviews. I googled the degree on linkedin and found that some alumni who had the degree are still data analysts 4 years after the degree while others are data scientists. Are data analysts considered entry level positions compared to data scientists? Can someone point out anything that stands out in the curriculum?\n\nAm I better off with Georgia Tech\u2019s Online Master of Science in Analytics (OMS Analytics)? I do have some basic knowledge of linear algebra, programming with python and introductory classical physics and do prefer learning on my own. Then there is also the [masters in data science by the same university](https://www.ntu.edu.sg/education/graduate-programme/master-of-science-in-data-science-(msds)) but that seems to be too competitive for a non-CS major to get into. Or is the data science field too saturated right now for non-CS majors?\n\nThanks for reading this far and any advice or insight would be greatly appreciated!", "author_fullname": "t2_cnbql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some thoughts on NTU's master's in analytics program", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1099er5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673455847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.ntu.edu.sg/education/graduate-programme/master-of-science-in-analytics\"&gt;The masters in question.&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;I have a background in biology and a little organic chemistry (final semester in my bachelors) but realised I also appreciate math and programming during my internship and final year project. (I went through almost half of the chapters in linear algebra and its applications by David C. Lay, completing 80% of its associated problems and felt like it was as fun as playing Monster Hunter World or Guild Wars 2.)&lt;/p&gt;\n\n&lt;p&gt;However, I am unable to find any placement statistics for the masters, is this considered a red flag?  Additionally, MH6211 Analytics Software I and MH6212 Analytics Software II cover SPS which according to some, is not really used in industry? I was also unable to find any reviews. I googled the degree on linkedin and found that some alumni who had the degree are still data analysts 4 years after the degree while others are data scientists. Are data analysts considered entry level positions compared to data scientists? Can someone point out anything that stands out in the curriculum?&lt;/p&gt;\n\n&lt;p&gt;Am I better off with Georgia Tech\u2019s Online Master of Science in Analytics (OMS Analytics)? I do have some basic knowledge of linear algebra, programming with python and introductory classical physics and do prefer learning on my own. Then there is also the &lt;a href=\"https://www.ntu.edu.sg/education/graduate-programme/master-of-science-in-data-science-(msds\"&gt;masters in data science by the same university&lt;/a&gt;) but that seems to be too competitive for a non-CS major to get into. Or is the data science field too saturated right now for non-CS majors?&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading this far and any advice or insight would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/H2HOY1ddpbU7MPDlXd8uo_tov-3KE8a0jdwScMP-ohs.jpg?auto=webp&amp;v=enabled&amp;s=caaaa88965fa30fea504732213d4552afb7ae478", "width": 775, "height": 465}, "resolutions": [{"url": "https://external-preview.redd.it/H2HOY1ddpbU7MPDlXd8uo_tov-3KE8a0jdwScMP-ohs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61c1eb84db31b551e84c05dcf517bb3ad9ec00bb", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/H2HOY1ddpbU7MPDlXd8uo_tov-3KE8a0jdwScMP-ohs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90cb9b48703cccebc712bccd508d038f3f9e0de6", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/H2HOY1ddpbU7MPDlXd8uo_tov-3KE8a0jdwScMP-ohs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=438d577db514b5bd9e1ae96c1458a62ade87b6a8", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/H2HOY1ddpbU7MPDlXd8uo_tov-3KE8a0jdwScMP-ohs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f396488f2ffdd1870aee237dcd10b8896333311", "width": 640, "height": 384}], "variants": {}, "id": "Lei49j-xSbvhd-Dmd_ZniNF_o5oF4aV0VAaS39JSH6M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1099er5", "is_robot_indexable": true, "report_reasons": null, "author": "TheBHSP", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1099er5/need_some_thoughts_on_ntus_masters_in_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1099er5/need_some_thoughts_on_ntus_masters_in_analytics/", "subreddit_subscribers": 836394, "created_utc": 1673455847.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have done due diligence and cleaned and removed outliers in my dataset. \n\n*This was not the study I actually did but trying to get an answer conceptually. \n\nIn my data set, I am trying to see if there is a correlation between course certifications and income. \n\nSay I have two sources of \u201ccourse certifications\u201d. For example 1 comes from someone\u2019s linked in and the other their resume\u2019 (not practical I know).\n\nThere is a moderately low positive correlation when looking at both groups of certifications and income. However, the p values for the resume\u2019 certifications are statistically significant while the p values for the linked in certifications are not. \n\nWould this indicate that while not strongly correlated, the resume\u2019 certifications are more reliable than the linked in source?", "author_fullname": "t2_4k7lwxqc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Correlation Question (Beginner)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10a0k7n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673532641.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have done due diligence and cleaned and removed outliers in my dataset. &lt;/p&gt;\n\n&lt;p&gt;*This was not the study I actually did but trying to get an answer conceptually. &lt;/p&gt;\n\n&lt;p&gt;In my data set, I am trying to see if there is a correlation between course certifications and income. &lt;/p&gt;\n\n&lt;p&gt;Say I have two sources of \u201ccourse certifications\u201d. For example 1 comes from someone\u2019s linked in and the other their resume\u2019 (not practical I know).&lt;/p&gt;\n\n&lt;p&gt;There is a moderately low positive correlation when looking at both groups of certifications and income. However, the p values for the resume\u2019 certifications are statistically significant while the p values for the linked in certifications are not. &lt;/p&gt;\n\n&lt;p&gt;Would this indicate that while not strongly correlated, the resume\u2019 certifications are more reliable than the linked in source?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a0k7n", "is_robot_indexable": true, "report_reasons": null, "author": "Data_rulez", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a0k7n/correlation_question_beginner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a0k7n/correlation_question_beginner/", "subreddit_subscribers": 836394, "created_utc": 1673532641.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_mmzrq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's your recent impact in your organization as a DS? What problem did you solve?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109o36f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673491581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109o36f", "is_robot_indexable": true, "report_reasons": null, "author": "ndemir", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109o36f/whats_your_recent_impact_in_your_organization_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109o36f/whats_your_recent_impact_in_your_organization_as/", "subreddit_subscribers": 836394, "created_utc": 1673491581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a graduating senior at a university in the US (US citizen). I have always been interested in studying abroad, but due to covid and the nature of my major I was unable to find opportunities. I am now looking for jobs abroad, but the process has been complicated and uneventful. I go to a school that qualifies me for the UK High Potential Individual visa, but every job I've applied to in the UK has rejected me. I've also looked at jobs in Switzerland and France, but their immigration laws are incredibly strict. My question follows: As an American, have you ever worked abroad as a data scientist in another country, and if so, when in your career and how?", "author_fullname": "t2_3im2k46f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you (as an American) ever worked in another country?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109akwy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673458586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a graduating senior at a university in the US (US citizen). I have always been interested in studying abroad, but due to covid and the nature of my major I was unable to find opportunities. I am now looking for jobs abroad, but the process has been complicated and uneventful. I go to a school that qualifies me for the UK High Potential Individual visa, but every job I&amp;#39;ve applied to in the UK has rejected me. I&amp;#39;ve also looked at jobs in Switzerland and France, but their immigration laws are incredibly strict. My question follows: As an American, have you ever worked abroad as a data scientist in another country, and if so, when in your career and how?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "109akwy", "is_robot_indexable": true, "report_reasons": null, "author": "bc_951", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109akwy/have_you_as_an_american_ever_worked_in_another/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109akwy/have_you_as_an_american_ever_worked_in_another/", "subreddit_subscribers": 836394, "created_utc": 1673458586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am tasked with completing the creation/analysis of a database from a dataset I can find online. This is for my masters in data science. I want to use this project as a way to showcase my skills for hiring managers. Does anyone have any opinions on what data set analysis would impress them as a hiring manager? Or a topic idea?", "author_fullname": "t2_dz6ua8ns", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database Project Ideas that would impress you as a hiring manager:", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109ztfa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673530562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am tasked with completing the creation/analysis of a database from a dataset I can find online. This is for my masters in data science. I want to use this project as a way to showcase my skills for hiring managers. Does anyone have any opinions on what data set analysis would impress them as a hiring manager? Or a topic idea?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109ztfa", "is_robot_indexable": true, "report_reasons": null, "author": "Effective-Guava8142", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109ztfa/database_project_ideas_that_would_impress_you_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109ztfa/database_project_ideas_that_would_impress_you_as/", "subreddit_subscribers": 836394, "created_utc": 1673530562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_a0oh9j1x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Analytics Trends You Need To Know (2023) - more aimed at data analytics, but there are some useful insight for data scientists too!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_109zsr6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Fv0dlGGIKTQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Analytics Trends You Need To Know (2023)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Analytics Trends You Need To Know (2023)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Fv0dlGGIKTQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Analytics Trends You Need To Know (2023)\"&gt;&lt;/iframe&gt;", "author_name": "CareerFoundry", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Fv0dlGGIKTQ/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@careerfoundry"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Fv0dlGGIKTQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Analytics Trends You Need To Know (2023)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/109zsr6", "height": 200}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ggCFh5hgsjGjJmHjERm-YN0ggPiAlIzlDF-54LIglT4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673530508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/Fv0dlGGIKTQ", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JQIa9M7bNHKvqNmdML3EKFA5QWvdZ4xAinB8hiFmy5c.jpg?auto=webp&amp;v=enabled&amp;s=8c7f5c22206296758c15e8197ce680f6d8607f5e", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/JQIa9M7bNHKvqNmdML3EKFA5QWvdZ4xAinB8hiFmy5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d26a17cf53f6ac0e7b9d1ff47c1faee5e523548", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/JQIa9M7bNHKvqNmdML3EKFA5QWvdZ4xAinB8hiFmy5c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4dbadb482580ec1c57f3fcd49ababdac6d19f138", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/JQIa9M7bNHKvqNmdML3EKFA5QWvdZ4xAinB8hiFmy5c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=79fae16cc3acc39c9fa312bfdf1b5d5cea01fc96", "width": 320, "height": 240}], "variants": {}, "id": "c9MNbTtFpBZLoOhDahSwjo7TsPHiLTySu0VZTFMw2Lo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "109zsr6", "is_robot_indexable": true, "report_reasons": null, "author": "Inevitable-Narwhal15", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109zsr6/data_analytics_trends_you_need_to_know_2023_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/Fv0dlGGIKTQ", "subreddit_subscribers": 836394, "created_utc": 1673530508.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Analytics Trends You Need To Know (2023)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Fv0dlGGIKTQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Analytics Trends You Need To Know (2023)\"&gt;&lt;/iframe&gt;", "author_name": "CareerFoundry", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Fv0dlGGIKTQ/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@careerfoundry"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_t5hi6lze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No SQL VS SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_109vfp6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZxLDiLwopKt6SZm7xHGFF6u6MFxUkT5byEeH2O0HYSQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673515586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8rzuf00fzkba1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8rzuf00fzkba1.jpg?auto=webp&amp;v=enabled&amp;s=f05270654eaf9de920a63709ab009b367815b6f1", "width": 800, "height": 2000}, "resolutions": [{"url": "https://preview.redd.it/8rzuf00fzkba1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=088cad217a722876da7e2567d2e6148a13bafedb", "width": 108, "height": 216}, {"url": "https://preview.redd.it/8rzuf00fzkba1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4eff2cf270d124d9ab627c5f9245a6232ad9c566", "width": 216, "height": 432}, {"url": "https://preview.redd.it/8rzuf00fzkba1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c585c736156d7400c9f33a015e6ceaab9b547a5", "width": 320, "height": 640}, {"url": "https://preview.redd.it/8rzuf00fzkba1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45ab8d2fb4a1244f82128af1cef81cde4fd47080", "width": 640, "height": 1280}], "variants": {}, "id": "JL_PyzoB_6fghMl7qxb70-qf9YQMfXmkefF7_b8oglo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "109vfp6", "is_robot_indexable": true, "report_reasons": null, "author": "praveen-skillslash", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109vfp6/no_sql_vs_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8rzuf00fzkba1.jpg", "subreddit_subscribers": 836394, "created_utc": 1673515586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want it all, the eternal power of both worlds. I want great notebooks with latex, ml, data...\n\nBut nothing i tried gave me the complete feeling.\n\nVs code =&gt; Unfinished. Cannot export R to pdf, R is just a hassle to work with, no good variable explorer, kernel browser is just bad. Cannot see R variables, Too much options and so little documentation.\n\nJupyter =&gt; No documented code completion.\n\nRstudio =&gt; No documented code completion for python.\n\nJetbrains... F jetbucks.\n\nWhat do i do?", "author_fullname": "t2_4qvdgg7g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Im trying to have both R &amp; Python in one editor for notebooks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109hv6c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673476446.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673475661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want it all, the eternal power of both worlds. I want great notebooks with latex, ml, data...&lt;/p&gt;\n\n&lt;p&gt;But nothing i tried gave me the complete feeling.&lt;/p&gt;\n\n&lt;p&gt;Vs code =&amp;gt; Unfinished. Cannot export R to pdf, R is just a hassle to work with, no good variable explorer, kernel browser is just bad. Cannot see R variables, Too much options and so little documentation.&lt;/p&gt;\n\n&lt;p&gt;Jupyter =&amp;gt; No documented code completion.&lt;/p&gt;\n\n&lt;p&gt;Rstudio =&amp;gt; No documented code completion for python.&lt;/p&gt;\n\n&lt;p&gt;Jetbrains... F jetbucks.&lt;/p&gt;\n\n&lt;p&gt;What do i do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109hv6c", "is_robot_indexable": true, "report_reasons": null, "author": "Rootsyl", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109hv6c/im_trying_to_have_both_r_python_in_one_editor_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109hv6c/im_trying_to_have_both_r_python_in_one_editor_for/", "subreddit_subscribers": 836394, "created_utc": 1673475661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi there, I found an exciting Scikit-Learn cheat sheet covering everything from basic steps to implementing machine learning algorithms successfully. This is great for beginners! You can download it from here - https://linktr.ee/codehub.ninja", "author_fullname": "t2_2buje7as", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have found a complete Scikit-Learn Cheat Sheet for Beginners.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10991pv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673454931.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I found an exciting Scikit-Learn cheat sheet covering everything from basic steps to implementing machine learning algorithms successfully. This is great for beginners! You can download it from here - &lt;a href=\"https://linktr.ee/codehub.ninja\"&gt;https://linktr.ee/codehub.ninja&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3Jq8puCcZTWyqYf78ava6cep849k7FbQ9-d9ZkJZF9k.jpg?auto=webp&amp;v=enabled&amp;s=061a8c1f59d2990d00c2a50007165ec250440495", "width": 180, "height": 180}, "resolutions": [{"url": "https://external-preview.redd.it/3Jq8puCcZTWyqYf78ava6cep849k7FbQ9-d9ZkJZF9k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c05cb03193f912e439101bb660e915fd92b9486a", "width": 108, "height": 108}], "variants": {}, "id": "bJzW2lZfHWph5R43dHg2rX-ZhGurVLEqwaXR7qY02Nk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10991pv", "is_robot_indexable": true, "report_reasons": null, "author": "ParticularBack", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10991pv/i_have_found_a_complete_scikitlearn_cheat_sheet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10991pv/i_have_found_a_complete_scikitlearn_cheat_sheet/", "subreddit_subscribers": 836394, "created_utc": 1673454931.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all! Are you a data enthusiast looking for opportunities to highlight your work and win cash prizes? Interested in researching how access to green space impacts public health? If the answer is yes, then the Green Space Data Challenge hosted by the Massive Data Institute at Georgetown\u2019s McCourt School of Public Policy is for you! \n\nUndergraduate students, graduate students, and early-career professionals are invited to participate in the data challenge during the entire month of February 2023 to create or improve indicators involving green space to better understand and measure community impact. \n\nParticipants will access various data sources on our Redivis platform in individual or team notebooks to transform green space data into community indicators. They will conduct their analyses and submit a short project narrative that describes the research question, analytic approach, and key findings. All submissions will be evaluated by judges based on the relevance, completeness, and quality of the submission. \n\nThere will be separate prize categories for submissions examining the effects of green space on the following subject areas: community health, community safety, specific populations, and physical environment. Winners can receive $5,000 for first place prizes, $2,000 for second place prizes, and $1,000 for third place prizes. Winners will also be invited to present their project at a webinar hosted by the Association of Public Data Users (APDU) and at APDU\u2019s annual conference in July 2023. \n\n[**Click here to learn more about the data challenge and to register.**](https://mdi.georgetown.edu/pbi/greenspace/)\n\nPlease help us spread the word and invite anyone eligible in your networks!\n\n&amp;#x200B;\n\nhttps://preview.redd.it/rn0wdqrxagba1.png?width=1600&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9fb427c8637d463775a890b650350944b8b25c65", "author_fullname": "t2_vi8w67ce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Green Space Data Challenge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"rn0wdqrxagba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/rn0wdqrxagba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff7104992aee187a7dafe7ba1e171e63b6849c52"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/rn0wdqrxagba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5384406bfa4f7de4a4649189f1cc7c2d580a33d2"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/rn0wdqrxagba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=477a38731b64a3f2ffd98a057f27e8691d4f081c"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/rn0wdqrxagba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37329f855bde1499e14295074b612a3753000f7f"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/rn0wdqrxagba1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fa66a7671eb8da709f98d693ad45abd57d64623e"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/rn0wdqrxagba1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f4a001281fbcafa624c41f94540534904c01e93"}], "s": {"y": 900, "x": 1600, "u": "https://preview.redd.it/rn0wdqrxagba1.png?width=1600&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9fb427c8637d463775a890b650350944b8b25c65"}, "id": "rn0wdqrxagba1"}}, "name": "t3_109asiq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7xVk10fDnvzwtPQ4UpraSRHFywOPm68Txuo_92T9Ubg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673459084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! Are you a data enthusiast looking for opportunities to highlight your work and win cash prizes? Interested in researching how access to green space impacts public health? If the answer is yes, then the Green Space Data Challenge hosted by the Massive Data Institute at Georgetown\u2019s McCourt School of Public Policy is for you! &lt;/p&gt;\n\n&lt;p&gt;Undergraduate students, graduate students, and early-career professionals are invited to participate in the data challenge during the entire month of February 2023 to create or improve indicators involving green space to better understand and measure community impact. &lt;/p&gt;\n\n&lt;p&gt;Participants will access various data sources on our Redivis platform in individual or team notebooks to transform green space data into community indicators. They will conduct their analyses and submit a short project narrative that describes the research question, analytic approach, and key findings. All submissions will be evaluated by judges based on the relevance, completeness, and quality of the submission. &lt;/p&gt;\n\n&lt;p&gt;There will be separate prize categories for submissions examining the effects of green space on the following subject areas: community health, community safety, specific populations, and physical environment. Winners can receive $5,000 for first place prizes, $2,000 for second place prizes, and $1,000 for third place prizes. Winners will also be invited to present their project at a webinar hosted by the Association of Public Data Users (APDU) and at APDU\u2019s annual conference in July 2023. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://mdi.georgetown.edu/pbi/greenspace/\"&gt;&lt;strong&gt;Click here to learn more about the data challenge and to register.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please help us spread the word and invite anyone eligible in your networks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rn0wdqrxagba1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9fb427c8637d463775a890b650350944b8b25c65\"&gt;https://preview.redd.it/rn0wdqrxagba1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9fb427c8637d463775a890b650350944b8b25c65&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109asiq", "is_robot_indexable": true, "report_reasons": null, "author": "georgetownpbi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109asiq/green_space_data_challenge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109asiq/green_space_data_challenge/", "subreddit_subscribers": 836394, "created_utc": 1673459084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello everyone,\nI am a junior data scientist, I worked on only one Data Science project. My objective is to learn the theoretical concepts and at the same time acquire programming knowledge. Can you suggest me some Kaggle competitions or datasets to start with. I want challenging ones to learn the subject the hard way.", "author_fullname": "t2_dwijd9yj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kaggle datasets to suggest ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109aers", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673458195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,\nI am a junior data scientist, I worked on only one Data Science project. My objective is to learn the theoretical concepts and at the same time acquire programming knowledge. Can you suggest me some Kaggle competitions or datasets to start with. I want challenging ones to learn the subject the hard way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109aers", "is_robot_indexable": true, "report_reasons": null, "author": "No_Cardiologist_3158", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109aers/kaggle_datasets_to_suggest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109aers/kaggle_datasets_to_suggest/", "subreddit_subscribers": 836394, "created_utc": 1673458195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&amp;s=08).\n\n2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.\n\nEmergent Abilities in LLMs\n\nIn a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs will allow them, among other things, to:\n\n* Become better at math\n* Understand even more subtleties of human language\n* Stop hallucinating and answer truthfully\n* ...\n\n(See the plot on break-out performance below for a full list)\n\n**Some Context:**\n\nIf you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.\n\n**Why does this happen?**\n\nLLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.\n\nHence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).\n\nLet's look at the following sentence.\n\n\"The sum of two plus two is ...\"\n\nThe model figures out that the most likely missing word is \"four\".\n\nThe fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).\n\nThere are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example are words that change their meaning with context. When the model encounters the word \"bed\", it needs to figure out from the context, if the text is talking about a \"river bed\" or a \"bed\" to sleep in.\n\n**What they discovered:**\n\nFor smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (proxy for model size) is reached.\n\nThe figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.\n\n&amp;#x200B;\n\n[Break-Out Performance At Critical Scale](https://preview.redd.it/w7xffqjimmba1.png?width=800&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c573f6199db7b5ccebfa73ce8e58313e5ae27822)\n\nThey observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.\n\nLooking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.\n\n(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.\n\nThere is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.\n\n**So what does this mean exactly?**\n\nThis beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.\n\nHowever, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.\n\nSuch exciting times to be alive!\n\nIf you got down here, thank you! It was a privilege to make this for you.  \nAt **TheDecoding** \u2b55, I send out a thoughtful newsletter about ML research and the data economy once a week.  \nNo Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)", "author_fullname": "t2_az3v2qdw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Research From Google Shines Light On The Future Of Language Models \u2b55", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": true, "media_metadata": {"w7xffqjimmba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/w7xffqjimmba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=064271844381b5d30e9e64768dca602c28e79ca0"}, {"y": 142, "x": 216, "u": "https://preview.redd.it/w7xffqjimmba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1134d5c14f8b0c4bf0844668ea108b606c26037c"}, {"y": 211, "x": 320, "u": "https://preview.redd.it/w7xffqjimmba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e48a5a0a0b5a51ee2dbcfd6b97ce6a352252c0e3"}, {"y": 422, "x": 640, "u": "https://preview.redd.it/w7xffqjimmba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3c6d9e818e81b07e4aa3f6c5eb9a7eefd103691"}], "s": {"y": 528, "x": 800, "u": "https://preview.redd.it/w7xffqjimmba1.png?width=800&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c573f6199db7b5ccebfa73ce8e58313e5ae27822"}, "id": "w7xffqjimmba1"}}, "name": "t3_10a1mik", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HHMyocjc1mprY1KVeTd2oCjTsPTwuNrN1QlSqTydiEw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673535544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create &lt;a href=\"https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:%7E:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company\"&gt;billion-dollar companies&lt;/a&gt;, and most notably they helped us recognize the &lt;a href=\"https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&amp;amp;s=08\"&gt;divine nature of ducks&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.&lt;/p&gt;\n\n&lt;p&gt;Emergent Abilities in LLMs&lt;/p&gt;\n\n&lt;p&gt;In a recent &lt;a href=\"https://arxiv.org/pdf/2206.07682.pdf\"&gt;paper from Google Brain&lt;/a&gt;, Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs will allow them, among other things, to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Become better at math&lt;/li&gt;\n&lt;li&gt;Understand even more subtleties of human language&lt;/li&gt;\n&lt;li&gt;Stop hallucinating and answer truthfully&lt;/li&gt;\n&lt;li&gt;...&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;(See the plot on break-out performance below for a full list)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Some Context:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why does this happen?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;LLMs are commonly trained by &lt;a href=\"https://www.cs.ubc.ca/%7Eamuham01/LING530/papers/radford2018improving.pdf\"&gt;maximizing the likelihood&lt;/a&gt; over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.&lt;/p&gt;\n\n&lt;p&gt;Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s look at the following sentence.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;The sum of two plus two is ...&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The model figures out that the most likely missing word is &amp;quot;four&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated &lt;a href=\"https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&amp;amp;ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F\"&gt;LLMs begin to struggle&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example are words that change their meaning with context. When the model encounters the word &amp;quot;bed&amp;quot;, it needs to figure out from the context, if the text is talking about a &amp;quot;river bed&amp;quot; or a &amp;quot;bed&amp;quot; to sleep in.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What they discovered:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (proxy for model size) is reached.&lt;/p&gt;\n\n&lt;p&gt;The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/w7xffqjimmba1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c573f6199db7b5ccebfa73ce8e58313e5ae27822\"&gt;Break-Out Performance At Critical Scale&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei&amp;#39;s personal blog. There he &lt;a href=\"https://www.jasonwei.net/blog/emergence\"&gt;listed a total of 137&lt;/a&gt; emergent abilities observable in LLMs.&lt;/p&gt;\n\n&lt;p&gt;Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.&lt;/p&gt;\n\n&lt;p&gt;(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.&lt;/p&gt;\n\n&lt;p&gt;There is &lt;a href=\"https://arxiv.org/abs/2203.15556\"&gt;other research&lt;/a&gt; suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So what does this mean exactly?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.&lt;/p&gt;\n\n&lt;p&gt;However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.&lt;/p&gt;\n\n&lt;p&gt;Such exciting times to be alive!&lt;/p&gt;\n\n&lt;p&gt;If you got down here, thank you! It was a privilege to make this for you.&lt;br/&gt;\nAt &lt;strong&gt;TheDecoding&lt;/strong&gt; \u2b55, I send out a thoughtful newsletter about ML research and the data economy once a week.&lt;br/&gt;\nNo Spam. No Nonsense. &lt;a href=\"https://thedecoding.net/\"&gt;Click here to sign up!&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a1mik", "is_robot_indexable": true, "report_reasons": null, "author": "LesleyFair", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a1mik/new_research_from_google_shines_light_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a1mik/new_research_from_google_shines_light_on_the/", "subreddit_subscribers": 836394, "created_utc": 1673535544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Best analytics or data science societies or groups to join that are either non-regional or east coast? Want to join and to projects / competitions with friends from grad school but we are all dispersed across the country now. \n\nIk Kaggle but other than that. Something more of a society or group. (: thanks!", "author_fullname": "t2_5njlw0ic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data science / analytics societies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10a0ram", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673533171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Best analytics or data science societies or groups to join that are either non-regional or east coast? Want to join and to projects / competitions with friends from grad school but we are all dispersed across the country now. &lt;/p&gt;\n\n&lt;p&gt;Ik Kaggle but other than that. Something more of a society or group. (: thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a0ram", "is_robot_indexable": true, "report_reasons": null, "author": "sunflowerworms", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a0ram/data_science_analytics_societies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a0ram/data_science_analytics_societies/", "subreddit_subscribers": 836394, "created_utc": 1673533171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The ethical agency of AI developers (original research)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109wkgw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_jfttdvv", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "Ethics", "selftext": "ABSTRACT:  Public and academic discourse about the ethics of artificial intelligence, machine learning, and data science has largely focused on the algorithms and the companies deploying them. Little attention has been paid to the ethical agency of the developers. This study is the first of its kind that centers developers in the ethical environment. Semi-structured interviews with 40 developers about the ethics of being a developer revealed more than 20 themes, 3 of which are the subject of this paper: ethics in the occupational ecosystem, developer ethical agency, and the characteristics of an ethical developer. These themes reveal significant gaps between how developers perceive themselves and the reality of their work experiences. Their ethical agency is likewise variable. They have some authority to intervene for ethical reasons in systems they work on, but they\u00a0often do not realize just how many ethical decisions they make. Nonetheless, this study reveals a growing ethical wisdom in this community, one that needs to be surfaced and nurtured by engaging with developers.\n\nEdit: Link to paper: [The ethical agency of AI developers](https://doi.org/10.1007/s43681-022-00256-3)", "author_fullname": "t2_jfttdvv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The ethical agency of AI developers (original research)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/Ethics", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109ud0y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673519298.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673511463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Ethics", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ABSTRACT:  Public and academic discourse about the ethics of artificial intelligence, machine learning, and data science has largely focused on the algorithms and the companies deploying them. Little attention has been paid to the ethical agency of the developers. This study is the first of its kind that centers developers in the ethical environment. Semi-structured interviews with 40 developers about the ethics of being a developer revealed more than 20 themes, 3 of which are the subject of this paper: ethics in the occupational ecosystem, developer ethical agency, and the characteristics of an ethical developer. These themes reveal significant gaps between how developers perceive themselves and the reality of their work experiences. Their ethical agency is likewise variable. They have some authority to intervene for ethical reasons in systems they work on, but they\u00a0often do not realize just how many ethical decisions they make. Nonetheless, this study reveals a growing ethical wisdom in this community, one that needs to be surfaced and nurtured by engaging with developers.&lt;/p&gt;\n\n&lt;p&gt;Edit: Link to paper: &lt;a href=\"https://doi.org/10.1007/s43681-022-00256-3\"&gt;The ethical agency of AI developers&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ekrwspU8dhVO-Gnn-pudTtEvcYQ6xggm-42rEJaxXIg.jpg?auto=webp&amp;v=enabled&amp;s=723fca1d331bfd2943c22c21568c9a8d7123853f", "width": 200, "height": 265}, "resolutions": [{"url": "https://external-preview.redd.it/ekrwspU8dhVO-Gnn-pudTtEvcYQ6xggm-42rEJaxXIg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef16f53d04dae5d95f2b28ba05fcf5974a4a0a61", "width": 108, "height": 143}], "variants": {}, "id": "L7cwVOvzykKHWn5C5uo87ZlaWT1r8sw7LoCX2VHNMvo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qp8u", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109ud0y", "is_robot_indexable": true, "report_reasons": null, "author": "EverPersisting", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/Ethics/comments/109ud0y/the_ethical_agency_of_ai_developers_original/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/Ethics/comments/109ud0y/the_ethical_agency_of_ai_developers_original/", "subreddit_subscribers": 14083, "created_utc": 1673511463.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1673519748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Ethics", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/Ethics/comments/109ud0y/the_ethical_agency_of_ai_developers_original/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ekrwspU8dhVO-Gnn-pudTtEvcYQ6xggm-42rEJaxXIg.jpg?auto=webp&amp;v=enabled&amp;s=723fca1d331bfd2943c22c21568c9a8d7123853f", "width": 200, "height": 265}, "resolutions": [{"url": "https://external-preview.redd.it/ekrwspU8dhVO-Gnn-pudTtEvcYQ6xggm-42rEJaxXIg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef16f53d04dae5d95f2b28ba05fcf5974a4a0a61", "width": 108, "height": 143}], "variants": {}, "id": "L7cwVOvzykKHWn5C5uo87ZlaWT1r8sw7LoCX2VHNMvo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109wkgw", "is_robot_indexable": true, "report_reasons": null, "author": "EverPersisting", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_109ud0y", "author_flair_text_color": null, "permalink": "/r/datascience/comments/109wkgw/the_ethical_agency_of_ai_developers_original/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/Ethics/comments/109ud0y/the_ethical_agency_of_ai_developers_original/", "subreddit_subscribers": 836394, "created_utc": 1673519748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I want to predict a force plate using plantar pressure. The shape of the force plate data is a 15000x6 array, and the shape of the plantar pressure data is a 15000x89 array. I will use a regression model to predict the force plate data. When collecting data to synchronize the force plate data and plantar pressure, I will do time synchronization between the force plate and plantar pressure app. force plate and plantar pressure data will capture 50 data in 1 second. \n\n Force Plate Data: \n\n Data shape : (15000,6) \n\nhttps://preview.redd.it/y9ql48ijfjba1.png?width=525&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8b553ab76ab8e750e5b5e1106a1b969c0c3d2c25\n\n&amp;#x200B;\n\nhttps://preview.redd.it/w9t3dmvkfjba1.png?width=418&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2bcb75881b3a9c9719c6918e38f2987c9abea966\n\n Plantar Pressure Data : \n\n Data shape : (15000,89) \n\n \n\n    array([[0.0, 0.0, 91.0, 100.0, 74.0, 7.0, 0.0, 0.0, 0.0, 0.0, 45.0, 123.0, 126.0, 3.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 95.0, 104.0, 75.0, 33.0, 0.0, 0.0, 0.0, 0.0, 57.0, 117.0, 123.0, 113.0, 66.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58.0, 67.0, 46.0, 16.0, 0.0, 8.0, 0.0, 15.0, 51.0, 122.0, 122.0, 111.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 25.0, 31.0, 64.0, 82.0, 125.0, 124.0, 114.0, 18.0, 0.0, 0.0, 0.0, 24.0, 56.0, 105.0, 116.0, 124.0, 77.0, 0.0, 0.0, 0.0, 71.0, 61.0, 0.0, 0.0, 3.0, 0.0], \n    [0.0, 0.0, 91.0, 100.0, 74.0, 7.0, 0.0, 0.0, 0.0, 0.0, 45.0, 123.0, 126.0, 3.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 95.0, 104.0, 75.0, 33.0, 0.0, 0.0, 0.0, 0.0, 57.0, 117.0, 123.0, 113.0, 66.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.0, 68.0, 46.0, 12.0, 0.0, 3.0, 0.0, 15.0, 53.0, 124.0, 125.0, 115.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 25.0, 31.0, 64.0, 82.0, 125.0, 124.0, 114.0, 18.0, 0.0, 0.0, 0.0, 24.0, 56.0, 105.0, 116.0, 124.0, 77.0, 0.0, 0.0, 0.0, 71.0, 61.0, 0.0, 0.0, 3.0, 0.0], \n    [0.0, 0.0, 91.0, 100.0, 68.0, 6.0, 0.0, 0.0, 0.0, 0.0, 29.0, 118.0, 120.0, 2.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 89.0, 100.0, 72.0, 32.0, 0.0, 0.0, 0.0, 0.0, 51.0, 113.0, 118.0, 109.0, 61.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58.0, 67.0, 46.0, 16.0, 0.0, 8.0, 0.0, 15.0, 51.0, 122.0, 122.0, 111.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 25.0, 31.0, 64.0, 82.0, 125.0, 124.0, 114.0, 18.0, 0.0, 0.0, 0.0, 24.0, 56.0, 105.0, 116.0, 124.0, 77.0, 0.0, 0.0, 0.0, 71.0, 61.0, 0.0, 0.0, 3.0, 0.0]])\n\n Method 1: \n\nInput Data : (15000,89) \n\nOutput Data : (15000,6) \n\n&amp;#x200B;\n\nhttps://preview.redd.it/ixdl7jfsfjba1.png?width=829&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ce5bccd25493c9ae423d64263688b0a7e27de457\n\nhttps://preview.redd.it/uk9ofjetfjba1.png?width=1638&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3fd5cf29b5fabd4b9af9673b22dc56107bd28420\n\n&amp;#x200B;\n\nhttps://preview.redd.it/hxjzo2cufjba1.png?width=1638&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8aa108c55f6d2446c9021236f6f156b540e7a914\n\n I will get good results on training, but when I try using new data to predict it the results will be bad. then I thought maybe because the plantar pressure data are so similar, it would be hard for the model to recognize it. then I tried to add data sequences to each plantar pressure data. the data sequences that I added are 1-50, the reason I added these data sequences is so that the data on plantar pressure are slightly different from one another so that the model is easier to recognize when I test using new data and I enter 1-50 because every second plantar pressure will capture 50 data. after I added the data sequence to the plantar pressure, the shape of the plantar pressure data is (15000,90) \n\n Plantar Pressure data after adding data sequence: the shape of plantar pressure data before adding data sequence is (15000,89), the shape of plantar pressure data after adding data sequence is (15000,90) \n\nhttps://preview.redd.it/v3kg3h3xfjba1.png?width=1060&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ea1a77d3f8509138b9d17c368963e28d1310a28b\n\n Method 2: \n\nInput Data : (15000,90) \n\nOutput Data : (15000,6) \n\nhttps://preview.redd.it/9wp1ioazfjba1.png?width=829&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cae4ad561e4fa77dcb772be3502a67eb040752c3\n\nhttps://preview.redd.it/s7i9erzzfjba1.png?width=1632&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cb6a85df6bbc4c893fff2e2b04bbbd0d2c09cc3e\n\nhttps://preview.redd.it/oofh8gg0gjba1.png?width=1632&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=89c9eec256e8c7d577ec9f00f3e4ecfd91ad1904\n\n so after I added the sequence data to the plantar pressure data, the prediction results from the training data and testing data will be good. my question is whether this method is allowed in data science? If yes, what method I use is called and is there a reference that is the same as the method I used in existing research", "author_fullname": "t2_end0qlqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding Data sequences as unique data on dataset for regression model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "media_metadata": {"uk9ofjetfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5437739b475963a5a9f6195ad5f4858a294c756"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59acc5363ccf0a2b6c853b174b95c04e9a6e957c"}, {"y": 139, "x": 320, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aedce6935dfe7ebae88c4cf85881421911367ab3"}, {"y": 278, "x": 640, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f69f36228ea203026217e45d0ef6116466057254"}, {"y": 418, "x": 960, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08d17bbcbf3b1d6ea42f95c34ed10c402ae62bf6"}, {"y": 470, "x": 1080, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=070600d7a05db4e89ad8b6c42304e1b37bf3b236"}], "s": {"y": 714, "x": 1638, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=1638&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3fd5cf29b5fabd4b9af9673b22dc56107bd28420"}, "id": "uk9ofjetfjba1"}, "9wp1ioazfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 75, "x": 108, "u": "https://preview.redd.it/9wp1ioazfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f98b7193abad8309d06286f3f23b17ed2cd9a2eb"}, {"y": 151, "x": 216, "u": "https://preview.redd.it/9wp1ioazfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af2d7ad6c97168e834b23f1f83873a0dc4421963"}, {"y": 223, "x": 320, "u": "https://preview.redd.it/9wp1ioazfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f0aae23c07021179cc1420f65c2e92364a2020d"}, {"y": 447, "x": 640, "u": "https://preview.redd.it/9wp1ioazfjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f00a5ea38ceee17a37a9166d65be16e945590e2"}], "s": {"y": 580, "x": 829, "u": "https://preview.redd.it/9wp1ioazfjba1.png?width=829&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cae4ad561e4fa77dcb772be3502a67eb040752c3"}, "id": "9wp1ioazfjba1"}, "s7i9erzzfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2221ea7e415680ea01b12bcc384108a1c1bf8486"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3889641995ee00f85dc7ae75e4d9c77115c5bd96"}, {"y": 140, "x": 320, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8dbfb07965a5aa25f3b53bc77bb58334e26f00af"}, {"y": 280, "x": 640, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e56c952755c04954d8f12d2e22c2cfff03d9c0e3"}, {"y": 420, "x": 960, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46006ea6b759f65446c884c89c1dcb11d0de87fb"}, {"y": 472, "x": 1080, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fef61624cb8b3053df053fdf438c99e2a788956"}], "s": {"y": 714, "x": 1632, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=1632&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cb6a85df6bbc4c893fff2e2b04bbbd0d2c09cc3e"}, "id": "s7i9erzzfjba1"}, "ixdl7jfsfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 75, "x": 108, "u": "https://preview.redd.it/ixdl7jfsfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=383e993d42856a48839c77616026afcc62cdb268"}, {"y": 151, "x": 216, "u": "https://preview.redd.it/ixdl7jfsfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=488b5e63286b16749750237af30ddac958e01747"}, {"y": 223, "x": 320, "u": "https://preview.redd.it/ixdl7jfsfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dcc2db8fab04d162e4f7b739d98f4d33ad625a1"}, {"y": 447, "x": 640, "u": "https://preview.redd.it/ixdl7jfsfjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a4d92dc8333fffe1b9be699eba07143d34ea4be"}], "s": {"y": 580, "x": 829, "u": "https://preview.redd.it/ixdl7jfsfjba1.png?width=829&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ce5bccd25493c9ae423d64263688b0a7e27de457"}, "id": "ixdl7jfsfjba1"}, "y9ql48ijfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/y9ql48ijfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd11ccd7f7b08c6d1cd09c32d3afed55996f9177"}, {"y": 132, "x": 216, "u": "https://preview.redd.it/y9ql48ijfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2bef9a1f88b06013b82062a3ceff7e724144b8fe"}, {"y": 196, "x": 320, "u": "https://preview.redd.it/y9ql48ijfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eca434b59b30236ab127c6e11dbdfc906e80a1bb"}], "s": {"y": 322, "x": 525, "u": "https://preview.redd.it/y9ql48ijfjba1.png?width=525&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8b553ab76ab8e750e5b5e1106a1b969c0c3d2c25"}, "id": "y9ql48ijfjba1"}, "hxjzo2cufjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd9cbbae89b169c354a714f2bd3a50aa389bad97"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad5362f1d88f504745b1114d92a0638c5bf0add2"}, {"y": 139, "x": 320, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d385d8e6ae8d0b2203c6b0df2d52727326d64de"}, {"y": 278, "x": 640, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d4a1b3822416f9a43e4729eadce3709473dac77"}, {"y": 418, "x": 960, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36a4aadb5f813c1c440cdda91ffc32d07d6b9185"}, {"y": 470, "x": 1080, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=964cf2b4be6a2fc5f89a62b4cd380be539d8e2b1"}], "s": {"y": 714, "x": 1638, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=1638&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8aa108c55f6d2446c9021236f6f156b540e7a914"}, "id": "hxjzo2cufjba1"}, "w9t3dmvkfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 87, "x": 108, "u": "https://preview.redd.it/w9t3dmvkfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f722acd865ec29b3c6a6ff7bd4ea8bd901c17ce"}, {"y": 175, "x": 216, "u": "https://preview.redd.it/w9t3dmvkfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eaa46adece19735e43d9943f8645e574791bd75d"}, {"y": 260, "x": 320, "u": "https://preview.redd.it/w9t3dmvkfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=28143dc4c17ab39ed309bddd64bcf83ed14294c8"}], "s": {"y": 340, "x": 418, "u": "https://preview.redd.it/w9t3dmvkfjba1.png?width=418&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2bcb75881b3a9c9719c6918e38f2987c9abea966"}, "id": "w9t3dmvkfjba1"}, "v3kg3h3xfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 29, "x": 108, "u": "https://preview.redd.it/v3kg3h3xfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37fda3f21e43fb3bc32070e8e2f0d30d77584485"}, {"y": 58, "x": 216, "u": "https://preview.redd.it/v3kg3h3xfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1644212683a9dfce1efa9314d486bd70232b0914"}, {"y": 86, "x": 320, "u": "https://preview.redd.it/v3kg3h3xfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e19d6148083ef1e9801d9d97c88f87c1d2d1e5e"}, {"y": 172, "x": 640, "u": "https://preview.redd.it/v3kg3h3xfjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f67a719f355c48944e093456bacbaa865c5e15e"}, {"y": 258, "x": 960, "u": "https://preview.redd.it/v3kg3h3xfjba1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46a8ca89ef0bebaee58d1c85af71c4b84ea2f56b"}], "s": {"y": 285, "x": 1060, "u": "https://preview.redd.it/v3kg3h3xfjba1.png?width=1060&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ea1a77d3f8509138b9d17c368963e28d1310a28b"}, "id": "v3kg3h3xfjba1"}, "oofh8gg0gjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1287933e3022d96a75b8525c443bfe4c7c3f21c4"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=17264be14523f039eca041f58d9d6cdf9b2d8555"}, {"y": 140, "x": 320, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4a0992394be6a09976834f07ebc2d9619fffcc6"}, {"y": 280, "x": 640, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77640857ed93a43643569cb9c4018e7c30a9441f"}, {"y": 420, "x": 960, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=119bbf2b1839ecb6a0251b95427b63c62d7b6926"}, {"y": 472, "x": 1080, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4fe7e8b05ad82c3bf7d2ae3604e85479534566a3"}], "s": {"y": 714, "x": 1632, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=1632&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=89c9eec256e8c7d577ec9f00f3e4ecfd91ad1904"}, "id": "oofh8gg0gjba1"}}, "name": "t3_109pzie", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/s8FQzbwalBWYlLGJrCiyUxOnKlwTeHA-dc8hwFNpXBw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673496990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to predict a force plate using plantar pressure. The shape of the force plate data is a 15000x6 array, and the shape of the plantar pressure data is a 15000x89 array. I will use a regression model to predict the force plate data. When collecting data to synchronize the force plate data and plantar pressure, I will do time synchronization between the force plate and plantar pressure app. force plate and plantar pressure data will capture 50 data in 1 second. &lt;/p&gt;\n\n&lt;p&gt;Force Plate Data: &lt;/p&gt;\n\n&lt;p&gt;Data shape : (15000,6) &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/y9ql48ijfjba1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8b553ab76ab8e750e5b5e1106a1b969c0c3d2c25\"&gt;https://preview.redd.it/y9ql48ijfjba1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8b553ab76ab8e750e5b5e1106a1b969c0c3d2c25&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/w9t3dmvkfjba1.png?width=418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2bcb75881b3a9c9719c6918e38f2987c9abea966\"&gt;https://preview.redd.it/w9t3dmvkfjba1.png?width=418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2bcb75881b3a9c9719c6918e38f2987c9abea966&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Plantar Pressure Data : &lt;/p&gt;\n\n&lt;p&gt;Data shape : (15000,89) &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;array([[0.0, 0.0, 91.0, 100.0, 74.0, 7.0, 0.0, 0.0, 0.0, 0.0, 45.0, 123.0, 126.0, 3.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 95.0, 104.0, 75.0, 33.0, 0.0, 0.0, 0.0, 0.0, 57.0, 117.0, 123.0, 113.0, 66.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58.0, 67.0, 46.0, 16.0, 0.0, 8.0, 0.0, 15.0, 51.0, 122.0, 122.0, 111.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 25.0, 31.0, 64.0, 82.0, 125.0, 124.0, 114.0, 18.0, 0.0, 0.0, 0.0, 24.0, 56.0, 105.0, 116.0, 124.0, 77.0, 0.0, 0.0, 0.0, 71.0, 61.0, 0.0, 0.0, 3.0, 0.0], \n[0.0, 0.0, 91.0, 100.0, 74.0, 7.0, 0.0, 0.0, 0.0, 0.0, 45.0, 123.0, 126.0, 3.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 95.0, 104.0, 75.0, 33.0, 0.0, 0.0, 0.0, 0.0, 57.0, 117.0, 123.0, 113.0, 66.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.0, 68.0, 46.0, 12.0, 0.0, 3.0, 0.0, 15.0, 53.0, 124.0, 125.0, 115.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 25.0, 31.0, 64.0, 82.0, 125.0, 124.0, 114.0, 18.0, 0.0, 0.0, 0.0, 24.0, 56.0, 105.0, 116.0, 124.0, 77.0, 0.0, 0.0, 0.0, 71.0, 61.0, 0.0, 0.0, 3.0, 0.0], \n[0.0, 0.0, 91.0, 100.0, 68.0, 6.0, 0.0, 0.0, 0.0, 0.0, 29.0, 118.0, 120.0, 2.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 89.0, 100.0, 72.0, 32.0, 0.0, 0.0, 0.0, 0.0, 51.0, 113.0, 118.0, 109.0, 61.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58.0, 67.0, 46.0, 16.0, 0.0, 8.0, 0.0, 15.0, 51.0, 122.0, 122.0, 111.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 25.0, 31.0, 64.0, 82.0, 125.0, 124.0, 114.0, 18.0, 0.0, 0.0, 0.0, 24.0, 56.0, 105.0, 116.0, 124.0, 77.0, 0.0, 0.0, 0.0, 71.0, 61.0, 0.0, 0.0, 3.0, 0.0]])\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Method 1: &lt;/p&gt;\n\n&lt;p&gt;Input Data : (15000,89) &lt;/p&gt;\n\n&lt;p&gt;Output Data : (15000,6) &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ixdl7jfsfjba1.png?width=829&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ce5bccd25493c9ae423d64263688b0a7e27de457\"&gt;https://preview.redd.it/ixdl7jfsfjba1.png?width=829&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ce5bccd25493c9ae423d64263688b0a7e27de457&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uk9ofjetfjba1.png?width=1638&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3fd5cf29b5fabd4b9af9673b22dc56107bd28420\"&gt;https://preview.redd.it/uk9ofjetfjba1.png?width=1638&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3fd5cf29b5fabd4b9af9673b22dc56107bd28420&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hxjzo2cufjba1.png?width=1638&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8aa108c55f6d2446c9021236f6f156b540e7a914\"&gt;https://preview.redd.it/hxjzo2cufjba1.png?width=1638&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8aa108c55f6d2446c9021236f6f156b540e7a914&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I will get good results on training, but when I try using new data to predict it the results will be bad. then I thought maybe because the plantar pressure data are so similar, it would be hard for the model to recognize it. then I tried to add data sequences to each plantar pressure data. the data sequences that I added are 1-50, the reason I added these data sequences is so that the data on plantar pressure are slightly different from one another so that the model is easier to recognize when I test using new data and I enter 1-50 because every second plantar pressure will capture 50 data. after I added the data sequence to the plantar pressure, the shape of the plantar pressure data is (15000,90) &lt;/p&gt;\n\n&lt;p&gt;Plantar Pressure data after adding data sequence: the shape of plantar pressure data before adding data sequence is (15000,89), the shape of plantar pressure data after adding data sequence is (15000,90) &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v3kg3h3xfjba1.png?width=1060&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ea1a77d3f8509138b9d17c368963e28d1310a28b\"&gt;https://preview.redd.it/v3kg3h3xfjba1.png?width=1060&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ea1a77d3f8509138b9d17c368963e28d1310a28b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Method 2: &lt;/p&gt;\n\n&lt;p&gt;Input Data : (15000,90) &lt;/p&gt;\n\n&lt;p&gt;Output Data : (15000,6) &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9wp1ioazfjba1.png?width=829&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cae4ad561e4fa77dcb772be3502a67eb040752c3\"&gt;https://preview.redd.it/9wp1ioazfjba1.png?width=829&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cae4ad561e4fa77dcb772be3502a67eb040752c3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s7i9erzzfjba1.png?width=1632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cb6a85df6bbc4c893fff2e2b04bbbd0d2c09cc3e\"&gt;https://preview.redd.it/s7i9erzzfjba1.png?width=1632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cb6a85df6bbc4c893fff2e2b04bbbd0d2c09cc3e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/oofh8gg0gjba1.png?width=1632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=89c9eec256e8c7d577ec9f00f3e4ecfd91ad1904\"&gt;https://preview.redd.it/oofh8gg0gjba1.png?width=1632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=89c9eec256e8c7d577ec9f00f3e4ecfd91ad1904&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;so after I added the sequence data to the plantar pressure data, the prediction results from the training data and testing data will be good. my question is whether this method is allowed in data science? If yes, what method I use is called and is there a reference that is the same as the method I used in existing research&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109pzie", "is_robot_indexable": true, "report_reasons": null, "author": "Constant-Cranberry29", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109pzie/adding_data_sequences_as_unique_data_on_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109pzie/adding_data_sequences_as_unique_data_on_dataset/", "subreddit_subscribers": 836394, "created_utc": 1673496990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_74f3kq8g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "lots of tutorials on sentiment analysis but seems like no one goes from analysis to insights", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109lfsq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673484536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109lfsq", "is_robot_indexable": true, "report_reasons": null, "author": "Character-Education3", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109lfsq/lots_of_tutorials_on_sentiment_analysis_but_seems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109lfsq/lots_of_tutorials_on_sentiment_analysis_but_seems/", "subreddit_subscribers": 836394, "created_utc": 1673484536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, so at my work the sales team uses a WhatsApp group chat to communicate. The format is something like: order number, client name, picture of design selected and info about design (quantity for example), then the other product, followed by a dash line to indicate the order is over and then repeat same format for new order. \n\nNow we have one guy who\u2019s only task is to copy paste that data into word and print is as purchase order. I want to automate this process by directly somehow having people input the data into WhatsApp and it getting converted and pasted to word, so then end of the day all I have to do is to print all. \n\nAny help is much appreciated.", "author_fullname": "t2_4pu77q4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help to automate a WhatsApp group chat data to word", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109hp0r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673475262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, so at my work the sales team uses a WhatsApp group chat to communicate. The format is something like: order number, client name, picture of design selected and info about design (quantity for example), then the other product, followed by a dash line to indicate the order is over and then repeat same format for new order. &lt;/p&gt;\n\n&lt;p&gt;Now we have one guy who\u2019s only task is to copy paste that data into word and print is as purchase order. I want to automate this process by directly somehow having people input the data into WhatsApp and it getting converted and pasted to word, so then end of the day all I have to do is to print all. &lt;/p&gt;\n\n&lt;p&gt;Any help is much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109hp0r", "is_robot_indexable": true, "report_reasons": null, "author": "blazingdodo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109hp0r/need_help_to_automate_a_whatsapp_group_chat_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109hp0r/need_help_to_automate_a_whatsapp_group_chat_data/", "subreddit_subscribers": 836394, "created_utc": 1673475262.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I'm currently a freshman in uni looking to work in either a quant field or data/computer sciencewhen I graduate and was wondering what would be the best way or best roles to intern as for me to gain experience at this stage?\n\nMy goal is to gain experience and become competitive for a FAANG internship, since I'm at a uni not as prestigious as the top schools with top programs. I know I have to probably get some projects done on the side to build my resume, but I can do that on my free time. Any advice welcome.", "author_fullname": "t2_60mmzsx1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about gaining experience as a freshman.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109e8l1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673467102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m currently a freshman in uni looking to work in either a quant field or data/computer sciencewhen I graduate and was wondering what would be the best way or best roles to intern as for me to gain experience at this stage?&lt;/p&gt;\n\n&lt;p&gt;My goal is to gain experience and become competitive for a FAANG internship, since I&amp;#39;m at a uni not as prestigious as the top schools with top programs. I know I have to probably get some projects done on the side to build my resume, but I can do that on my free time. Any advice welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109e8l1", "is_robot_indexable": true, "report_reasons": null, "author": "koolaidfan2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109e8l1/question_about_gaining_experience_as_a_freshman/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109e8l1/question_about_gaining_experience_as_a_freshman/", "subreddit_subscribers": 836394, "created_utc": 1673467102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi there,   \n Say I have multiple stores selling multiple products, mulitiple categories, etc.  Say on X date, I see a drop in sales but I want the algorithm to say that this was caused by \"store X\" which didnt sell much that day, or it was caused by \"Product Y\" suddenly dropping that day.   \n\n\nHow would you approach this? I tried pycaret and it had an example for anomaly detection, but what I'm looking is to drill down the anomaly instead and find out which variables/predictors are causing the anomaly.   \n   \nThe data could look like (sales total would the be 'outcome variable'):  \ndate, store, product, country, .... ... sales\\_total", "author_fullname": "t2_b8qbf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Had a sale anomaly on X day. What time series algorithm/model should I use to find the variables causing the anomaly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109dxev", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673466359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;br/&gt;\n Say I have multiple stores selling multiple products, mulitiple categories, etc.  Say on X date, I see a drop in sales but I want the algorithm to say that this was caused by &amp;quot;store X&amp;quot; which didnt sell much that day, or it was caused by &amp;quot;Product Y&amp;quot; suddenly dropping that day.   &lt;/p&gt;\n\n&lt;p&gt;How would you approach this? I tried pycaret and it had an example for anomaly detection, but what I&amp;#39;m looking is to drill down the anomaly instead and find out which variables/predictors are causing the anomaly.   &lt;/p&gt;\n\n&lt;p&gt;The data could look like (sales total would the be &amp;#39;outcome variable&amp;#39;):&lt;br/&gt;\ndate, store, product, country, .... ... sales_total&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109dxev", "is_robot_indexable": true, "report_reasons": null, "author": "honest-teorema", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109dxev/had_a_sale_anomaly_on_x_day_what_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109dxev/had_a_sale_anomaly_on_x_day_what_time_series/", "subreddit_subscribers": 836394, "created_utc": 1673466359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying my hands on sentiment analysis and i have a data with a million rows but I cannot figure out how to identify the source of tweets. Any lead will be helpful.", "author_fullname": "t2_4xoz47my", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "identifying government response to covid 19 spring 2020 peak from twitter data for sentiment analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109b8h6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673460103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying my hands on sentiment analysis and i have a data with a million rows but I cannot figure out how to identify the source of tweets. Any lead will be helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109b8h6", "is_robot_indexable": true, "report_reasons": null, "author": "mandy42069", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109b8h6/identifying_government_response_to_covid_19/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109b8h6/identifying_government_response_to_covid_19/", "subreddit_subscribers": 836394, "created_utc": 1673460103.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}