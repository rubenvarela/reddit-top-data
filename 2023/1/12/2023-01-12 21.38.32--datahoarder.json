{"kind": "Listing", "data": {"after": "t3_10a26dk", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have my 40TB hoard of data backed up to Backblaze, and with the recent acquisition of two more drives I needed to wipe my storage pool to switch it over from a simple one to a parity one. Instead of making a local copy I decided to fetch the data back from Backblaze, and since I'm located in Europe, instead of ordering drives and paying duty for them I opted for the download method. (A series of mistakes, I'm aware, but it all seemed like a good idea at the time).\n\nThe process is deceptively simple if you've never actually tried to go through it - either download single files directly, or select what you need and prepare a .zip to download later.\n\nThe first thing you'll run into is the 500GB limit for a single .zip - a pain since it means you need to split up your data, but not an unreasonable limitation, if a little on the small side.\n\nThen you'll discover that there's absolutely zero assistance for you to split your data up - you need to manually pick out files and folders to include and watch the total size (and be aware that this 500GB is decimal). At that point you may also notice that the interface to prepare restores is... not very good - nobody at Backblaze seems to have heard the word \"asynchronous\" and the UI is blocked on requests to the backend, so not only do you not get instant feedback on your current archive size, you don't even see your checkboxes get checked until the requests complete.\n\nBut let's say you've checked what you need for your first batch, got close enough to 500GB and started preparing your .zip. So you go to prepare another. You click back to the Restore screen and, if you have your backup encrypted, it asks you for the encryption key again. Wait, didn't you just provide that? Well, yes, and your backup is decrypted, but on server 0002, and this time the load balancer decided to get you onto server 0014. Not a big deal. Unless you grabbed yourself a coffee in the meantime and now are staring at a login screen again because Backblaze has one of the shortest session expiration times I've seen (something like 20-30 minutes) and no \"Remember me\" button. This is a bit more of a big deal, or - as you might find out later - a very big deal.\n\nSo you prepare a few more batches, still with that same less than responsive interface, and eventually you hit the limit of 5 restores being prepared at once. So you wait. And you wait. Maybe hours, maybe as much as two days. For whatever reason restores that hit close to that 500GB mark take ages, much more than the same amount of data split across multiple 40-50 GB packs - I've had 40GB packages prepared in 5-6 minutes, while the 500GB ones took not 10, but more like 100 times more. Unless you hit a snag and the package just refuses to get prepared and you have to cancel it - I haven't had that happen often with large ones, but a bunch of times with small ones.\n\nYou've finally got one of those restores ready though, and the seven day clock to download it is ticking - so you go to download and it tells you to get yourself a Backblaze Downloader. You may ignore it now and find out that your download is capped at about 100-150 MBit even on your gigabit connection, or you may ignore it later when you've had first hand experience with the downloader. (Spoilers, I know). Let's say you listen and download the downloader - pointlessly, as it turns out, since it's already there along with your Backblaze installation.\n\nYou give it your username and password, OTP code and get a dropdown list of restores - so far, so good. You select one, pick a folder to download to, go with the recommended number of threads, and start downloading.\n\nAnd then you realize the downloader has the same problem as the UI with the \"async\" concept, except Windows really, *really* doesn't like apps hogging the UI thread. So 90 percent of the time the window is \"not responding\", the Close button may work eventually when it gets around to it, and the speed indicator is useless. (The progress bar turns out to be useless too as I've had downloads hit 100% with the bar lingering somewhere three quarters of the way in). If you've made a mistake of restoring to your C:\\ drive this is going to be even worse since that's also where the scratch files are being written, so your disk is hit with a barrage of multiple processes at once (the downloader calls them \"threads\"; that's not quite telling the whole story as they're entirely separate processes getting spawned per 40MB chunk and killed when they finish) writing scratch files, and the downloader appending them to your target file. And the downloader constantly looks like it's hanged, but it has not, unless it has because that happens sometimes as well and your nightly restore might have not gotten past ten percent.\n\nBut let's say you've downloaded your first batch and want to download another - except all you can do with the downloader is close it, then restart it, there's no way to get back to the selection screen. And you need to provide your credentials again. And the target folder has reset to the Desktop again. And there's no indication which restores you have or have not already downloaded.\n\nAnd while you've been marveling at that the unzip process has thrown a CRC error - which I really, *really* hope is just an issue with the zipping/downloading process and the actual data that's being stored on the servers is okay. If you've had the downloader hang on you there's a pretty much 100% chance you'll get that, if you've stopped and restarted the download you'll probably get hit by that as well, and even if everything went just fine it may still happen just because. If you're lucky it's just going to be one or two files and you can restore them separately, if you're not and it plowed over a more sensitive portion of the .zip the entire thing is likely worthless and needs to be redownloaded.\n\nSo you give up on the downloader and decide to download manually - and because of that 100-150 MBit cap you get yourself a download accelerator. Great! Except for the \"acceleration\" part, which for some reason works only up to some size - maybe that's some issue on my side, but I've tried multiple ones and I haven't gotten the big restores to download in parallel, only smaller ones.\n\nAnd even if you've gotten that download acceleration to work - remember that part about getting signed out after 30 minutes? Turns out this applies to the download link as well. And since download accelerators reestablish connections once they've finished a chunk, said connections are now getting redirected to the login page. I've tried three of those programs and neither of them managed to work that situation out, all of them eventually got all of their threads stuck and were not able to resume, leaving a dead download. And even if you don't care for the acceleration, I hope you didn't spend too much time setting up a queue of downloads (or go to bed afterwards), because that won't work either for the same reason.\n\nIronically, the best way to get the downloads working turned out to be just downloading them in the browser - setting up far smaller chunks, so that the still occasional CRC errors don't ruin your day, and downloading multiple files in parallel to saturate the connection. But it still requires multiple trips to the restore screen, you can't just spend an afternoon setting up all your restores because you only have seven days to download them and you need to set them up little by little, and you may still run into issues with the downloads or the resulting zip files.\n\nNow does it mean Backblaze is a bad service? I guess not - for the price it's still a steal, and there are other options to restore. If you're in the US the USB drives are more than likely going to be a great option with zero of the above hassle, if you can eat the egress fees B2 may be a viable option, and in the end I'm likely going to get my files out eventually. But it seems like a lot of people who get interested in Backblaze are in the same boat as me - they don't want to spend more than the monthly fee, may not have the deposit money or live too far away for the drive restore, and they might've heard of the restore process being a bit iffy but it can't be that bad, right?\n\nWell, it's exactly as bad as above, no more, no less - whether that's a dealbreaker is in the eye of the beholder, but it's better to know those things about the service you use before you end up depending on it for your data. I know the Backblaze team has been speaking of a better downloader which I'm hoping will not be vaporware, but even that aside there are so many things that should be such easy wins to fix - the session length issue, the downloader not hogging the UI thread, the artificial 500 GB limit - that it's really a bit disappointing that the current process is so miserable.", "author_fullname": "t2_epug6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Backblaze large restore experience (is miserable)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109kd3j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 375, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 375, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673481815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have my 40TB hoard of data backed up to Backblaze, and with the recent acquisition of two more drives I needed to wipe my storage pool to switch it over from a simple one to a parity one. Instead of making a local copy I decided to fetch the data back from Backblaze, and since I&amp;#39;m located in Europe, instead of ordering drives and paying duty for them I opted for the download method. (A series of mistakes, I&amp;#39;m aware, but it all seemed like a good idea at the time).&lt;/p&gt;\n\n&lt;p&gt;The process is deceptively simple if you&amp;#39;ve never actually tried to go through it - either download single files directly, or select what you need and prepare a .zip to download later.&lt;/p&gt;\n\n&lt;p&gt;The first thing you&amp;#39;ll run into is the 500GB limit for a single .zip - a pain since it means you need to split up your data, but not an unreasonable limitation, if a little on the small side.&lt;/p&gt;\n\n&lt;p&gt;Then you&amp;#39;ll discover that there&amp;#39;s absolutely zero assistance for you to split your data up - you need to manually pick out files and folders to include and watch the total size (and be aware that this 500GB is decimal). At that point you may also notice that the interface to prepare restores is... not very good - nobody at Backblaze seems to have heard the word &amp;quot;asynchronous&amp;quot; and the UI is blocked on requests to the backend, so not only do you not get instant feedback on your current archive size, you don&amp;#39;t even see your checkboxes get checked until the requests complete.&lt;/p&gt;\n\n&lt;p&gt;But let&amp;#39;s say you&amp;#39;ve checked what you need for your first batch, got close enough to 500GB and started preparing your .zip. So you go to prepare another. You click back to the Restore screen and, if you have your backup encrypted, it asks you for the encryption key again. Wait, didn&amp;#39;t you just provide that? Well, yes, and your backup is decrypted, but on server 0002, and this time the load balancer decided to get you onto server 0014. Not a big deal. Unless you grabbed yourself a coffee in the meantime and now are staring at a login screen again because Backblaze has one of the shortest session expiration times I&amp;#39;ve seen (something like 20-30 minutes) and no &amp;quot;Remember me&amp;quot; button. This is a bit more of a big deal, or - as you might find out later - a very big deal.&lt;/p&gt;\n\n&lt;p&gt;So you prepare a few more batches, still with that same less than responsive interface, and eventually you hit the limit of 5 restores being prepared at once. So you wait. And you wait. Maybe hours, maybe as much as two days. For whatever reason restores that hit close to that 500GB mark take ages, much more than the same amount of data split across multiple 40-50 GB packs - I&amp;#39;ve had 40GB packages prepared in 5-6 minutes, while the 500GB ones took not 10, but more like 100 times more. Unless you hit a snag and the package just refuses to get prepared and you have to cancel it - I haven&amp;#39;t had that happen often with large ones, but a bunch of times with small ones.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;ve finally got one of those restores ready though, and the seven day clock to download it is ticking - so you go to download and it tells you to get yourself a Backblaze Downloader. You may ignore it now and find out that your download is capped at about 100-150 MBit even on your gigabit connection, or you may ignore it later when you&amp;#39;ve had first hand experience with the downloader. (Spoilers, I know). Let&amp;#39;s say you listen and download the downloader - pointlessly, as it turns out, since it&amp;#39;s already there along with your Backblaze installation.&lt;/p&gt;\n\n&lt;p&gt;You give it your username and password, OTP code and get a dropdown list of restores - so far, so good. You select one, pick a folder to download to, go with the recommended number of threads, and start downloading.&lt;/p&gt;\n\n&lt;p&gt;And then you realize the downloader has the same problem as the UI with the &amp;quot;async&amp;quot; concept, except Windows really, &lt;em&gt;really&lt;/em&gt; doesn&amp;#39;t like apps hogging the UI thread. So 90 percent of the time the window is &amp;quot;not responding&amp;quot;, the Close button may work eventually when it gets around to it, and the speed indicator is useless. (The progress bar turns out to be useless too as I&amp;#39;ve had downloads hit 100% with the bar lingering somewhere three quarters of the way in). If you&amp;#39;ve made a mistake of restoring to your C:\\ drive this is going to be even worse since that&amp;#39;s also where the scratch files are being written, so your disk is hit with a barrage of multiple processes at once (the downloader calls them &amp;quot;threads&amp;quot;; that&amp;#39;s not quite telling the whole story as they&amp;#39;re entirely separate processes getting spawned per 40MB chunk and killed when they finish) writing scratch files, and the downloader appending them to your target file. And the downloader constantly looks like it&amp;#39;s hanged, but it has not, unless it has because that happens sometimes as well and your nightly restore might have not gotten past ten percent.&lt;/p&gt;\n\n&lt;p&gt;But let&amp;#39;s say you&amp;#39;ve downloaded your first batch and want to download another - except all you can do with the downloader is close it, then restart it, there&amp;#39;s no way to get back to the selection screen. And you need to provide your credentials again. And the target folder has reset to the Desktop again. And there&amp;#39;s no indication which restores you have or have not already downloaded.&lt;/p&gt;\n\n&lt;p&gt;And while you&amp;#39;ve been marveling at that the unzip process has thrown a CRC error - which I really, &lt;em&gt;really&lt;/em&gt; hope is just an issue with the zipping/downloading process and the actual data that&amp;#39;s being stored on the servers is okay. If you&amp;#39;ve had the downloader hang on you there&amp;#39;s a pretty much 100% chance you&amp;#39;ll get that, if you&amp;#39;ve stopped and restarted the download you&amp;#39;ll probably get hit by that as well, and even if everything went just fine it may still happen just because. If you&amp;#39;re lucky it&amp;#39;s just going to be one or two files and you can restore them separately, if you&amp;#39;re not and it plowed over a more sensitive portion of the .zip the entire thing is likely worthless and needs to be redownloaded.&lt;/p&gt;\n\n&lt;p&gt;So you give up on the downloader and decide to download manually - and because of that 100-150 MBit cap you get yourself a download accelerator. Great! Except for the &amp;quot;acceleration&amp;quot; part, which for some reason works only up to some size - maybe that&amp;#39;s some issue on my side, but I&amp;#39;ve tried multiple ones and I haven&amp;#39;t gotten the big restores to download in parallel, only smaller ones.&lt;/p&gt;\n\n&lt;p&gt;And even if you&amp;#39;ve gotten that download acceleration to work - remember that part about getting signed out after 30 minutes? Turns out this applies to the download link as well. And since download accelerators reestablish connections once they&amp;#39;ve finished a chunk, said connections are now getting redirected to the login page. I&amp;#39;ve tried three of those programs and neither of them managed to work that situation out, all of them eventually got all of their threads stuck and were not able to resume, leaving a dead download. And even if you don&amp;#39;t care for the acceleration, I hope you didn&amp;#39;t spend too much time setting up a queue of downloads (or go to bed afterwards), because that won&amp;#39;t work either for the same reason.&lt;/p&gt;\n\n&lt;p&gt;Ironically, the best way to get the downloads working turned out to be just downloading them in the browser - setting up far smaller chunks, so that the still occasional CRC errors don&amp;#39;t ruin your day, and downloading multiple files in parallel to saturate the connection. But it still requires multiple trips to the restore screen, you can&amp;#39;t just spend an afternoon setting up all your restores because you only have seven days to download them and you need to set them up little by little, and you may still run into issues with the downloads or the resulting zip files.&lt;/p&gt;\n\n&lt;p&gt;Now does it mean Backblaze is a bad service? I guess not - for the price it&amp;#39;s still a steal, and there are other options to restore. If you&amp;#39;re in the US the USB drives are more than likely going to be a great option with zero of the above hassle, if you can eat the egress fees B2 may be a viable option, and in the end I&amp;#39;m likely going to get my files out eventually. But it seems like a lot of people who get interested in Backblaze are in the same boat as me - they don&amp;#39;t want to spend more than the monthly fee, may not have the deposit money or live too far away for the drive restore, and they might&amp;#39;ve heard of the restore process being a bit iffy but it can&amp;#39;t be that bad, right?&lt;/p&gt;\n\n&lt;p&gt;Well, it&amp;#39;s exactly as bad as above, no more, no less - whether that&amp;#39;s a dealbreaker is in the eye of the beholder, but it&amp;#39;s better to know those things about the service you use before you end up depending on it for your data. I know the Backblaze team has been speaking of a better downloader which I&amp;#39;m hoping will not be vaporware, but even that aside there are so many things that should be such easy wins to fix - the session length issue, the downloader not hogging the UI thread, the artificial 500 GB limit - that it&amp;#39;s really a bit disappointing that the current process is so miserable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109kd3j", "is_robot_indexable": true, "report_reasons": null, "author": "Mivexil", "discussion_type": null, "num_comments": 200, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109kd3j/the_backblaze_large_restore_experience_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109kd3j/the_backblaze_large_restore_experience_is/", "subreddit_subscribers": 665566, "created_utc": 1673481815.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5z5rj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "YouTubers said they destroyed over 100 VHS tapes of an obscure 1987 movie to increase the value of their final copy. They sold it on eBay for $80,600.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_10a50y4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 340, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 340, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Qv6VDMJ9RIpS_qPBQ49h4TZuGnbi7vn-81cpkqngm2Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673543913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "insider.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.insider.com/youtubers-destroy-nukie-vhs-tape-collectable-ebay-sale-redlettermedia-2023-1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?auto=webp&amp;v=enabled&amp;s=eeec15dd0fc4a21f4b70116ef9efdaab0d529fca", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c58fe37eba227f4d81b2839e2086d8bf3e388eb5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22769873551441f606718b7c7af2fc76eaeba25b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4187a265dcbbb07b8e58d84e8bd6a093e2e80ee", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b27a59c96af5d6556352631ef545aaa0eb11992b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d7b3a5c0168f9b727303ef54e8c063e24841299", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be08317c9a35c9da4e5399b1c5ba6898750ecbd4", "width": 1080, "height": 540}], "variants": {}, "id": "xRZTtLBIe0INmo3ybLtCCz7LqQtFpG1yBU1egx6NDfk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a50y4", "is_robot_indexable": true, "report_reasons": null, "author": "ET2-SW", "discussion_type": null, "num_comments": 160, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a50y4/youtubers_said_they_destroyed_over_100_vhs_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.insider.com/youtubers-destroy-nukie-vhs-tape-collectable-ebay-sale-redlettermedia-2023-1", "subreddit_subscribers": 665566, "created_utc": 1673543913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just a funny story and a small reminder to properly handle your drives on disposal (not that I think anyone here would be this dumb). \n\nI grabbed a cheap used NAS online to upgrade me from running my drives in a regular RAID box. I didn't *need* to do it so I wasn't looking to spend a lot of money. Got a good deal, showed up with drives in it. Default admin passwords. \n\nI have all the data once belonging to a construction company, covering every aspect of their business, going back to at least 2013. Including proposals, diagrams, pictures of their work, addresses, invoices, receipts, credit card numbers, scans of deposited checks, and checking account info. Apparently someone named Katrina decided the recycling bin was good enough. \n\nLuckily for them, I am not an asshole. But holy hell does this make me feel better about my overly cautious way of dealing with old drives.", "author_fullname": "t2_hz5u5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bought used QNAP TS431+.. Previous owner left hsi whole company on the drives.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109h0eu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 62, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 62, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673473617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just a funny story and a small reminder to properly handle your drives on disposal (not that I think anyone here would be this dumb). &lt;/p&gt;\n\n&lt;p&gt;I grabbed a cheap used NAS online to upgrade me from running my drives in a regular RAID box. I didn&amp;#39;t &lt;em&gt;need&lt;/em&gt; to do it so I wasn&amp;#39;t looking to spend a lot of money. Got a good deal, showed up with drives in it. Default admin passwords. &lt;/p&gt;\n\n&lt;p&gt;I have all the data once belonging to a construction company, covering every aspect of their business, going back to at least 2013. Including proposals, diagrams, pictures of their work, addresses, invoices, receipts, credit card numbers, scans of deposited checks, and checking account info. Apparently someone named Katrina decided the recycling bin was good enough. &lt;/p&gt;\n\n&lt;p&gt;Luckily for them, I am not an asshole. But holy hell does this make me feel better about my overly cautious way of dealing with old drives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "109h0eu", "is_robot_indexable": true, "report_reasons": null, "author": "DeffNotTom", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109h0eu/bought_used_qnap_ts431_previous_owner_left_hsi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109h0eu/bought_used_qnap_ts431_previous_owner_left_hsi/", "subreddit_subscribers": 665566, "created_utc": 1673473617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Micron Unveils 9400 SSD: 30 TB Capacities With Best-In-Class Performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": false, "name": "t3_10a4nvv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/AC0ueZqYqdOMzCdm3ntxWB65qmLoxW0SUkHoeuH1Bds.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673543009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "wccftech.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://wccftech.com/micron-unveils-9400-ssd-30-tb-capacities-with-best-in-class-performance/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?auto=webp&amp;v=enabled&amp;s=c5683a71bf7f47729b0ff2c5cc693e5e54236a37", "width": 980, "height": 620}, "resolutions": [{"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f096790349c2231fec800a7e162ac38ff971e054", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c6c9f335d72366983fe496f7108b7ba824b5f74", "width": 216, "height": 136}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6be074be75dbd483261d4e50615feebc1a996a10", "width": 320, "height": 202}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14a9c87bb2670e5df83de7748e5ddaa8c8813a88", "width": 640, "height": 404}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5652b2fb0ca0e91c36cb053db1abddf73351f32", "width": 960, "height": 607}], "variants": {}, "id": "Z0sSJbkM797eMfo6AVw0nJG4sVsDUTAEeTjiWS9-Vlc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a4nvv", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10a4nvv/micron_unveils_9400_ssd_30_tb_capacities_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://wccftech.com/micron-unveils-9400-ssd-30-tb-capacities-with-best-in-class-performance/", "subreddit_subscribers": 665566, "created_utc": 1673543009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm not the type of person for this sub. All of my data is under a TB.... but I started playing around with 3d scanning and that's changing fast. 20gb scans. 150gb project files. \n\nI'm not wanting to spend too much or get too involved for now so this is the setup I'm thinking. \n\npurchase 3 HDD in the 4-6tb range. 2 go into an old computer running raid 1. map network drive and drag the projects over when I'm done with them. The third is kept somewhere else and brought over once a month to get updated\n\nto me it sounds like a good idea with more than 1 layer of defense. any opinions?", "author_fullname": "t2_v3z3kryc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wanting to get a sanity check for archive/backup idea", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109n680", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673489125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not the type of person for this sub. All of my data is under a TB.... but I started playing around with 3d scanning and that&amp;#39;s changing fast. 20gb scans. 150gb project files. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not wanting to spend too much or get too involved for now so this is the setup I&amp;#39;m thinking. &lt;/p&gt;\n\n&lt;p&gt;purchase 3 HDD in the 4-6tb range. 2 go into an old computer running raid 1. map network drive and drag the projects over when I&amp;#39;m done with them. The third is kept somewhere else and brought over once a month to get updated&lt;/p&gt;\n\n&lt;p&gt;to me it sounds like a good idea with more than 1 layer of defense. any opinions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109n680", "is_robot_indexable": true, "report_reasons": null, "author": "Optimal-Growth-5741", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109n680/wanting_to_get_a_sanity_check_for_archivebackup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109n680/wanting_to_get_a_sanity_check_for_archivebackup/", "subreddit_subscribers": 665566, "created_utc": 1673489125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for something like Google Photos with smart photo sorting (eg. face, screenshots, blurry shots)", "author_fullname": "t2_woqd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photo sorter recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109l5sr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673483820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for something like Google Photos with smart photo sorting (eg. face, screenshots, blurry shots)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109l5sr", "is_robot_indexable": true, "report_reasons": null, "author": "slaiyfer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109l5sr/photo_sorter_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109l5sr/photo_sorter_recommendations/", "subreddit_subscribers": 665566, "created_utc": 1673483820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've located and reuploaded a long lost UnleashX skin", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "name": "t3_109vz32", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_2b9r7ngd", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Jnjs0qw2eBd5aTzRu8i0sLwfd-qlwww-SgI8E2Y9eSE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "originalxbox", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/20rcv4cgtjba1.png?width=689&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2072dcd672302c1afa94268bcd3f1514386135f7\n\nI archived an UnleashX skin that was deleted off the internet suddenly. It was posted to Twitter by its creator Retro Game Rarities in 2020 and his account suddenly got deleted. I was able to find a link to it via google cache and I've since reuploaded it to [archive.org](https://archive.org)\n\n[https://archive.org/details/UnleashX-EmeraldX-skin](https://archive.org/details/UnleashX-EmeraldX-skin)\n\nIt's easily the coolest looking UnleashX skin I've ever seen. It was disappointing to discover it was so hastily deleted and purged off the internet.\n\n&amp;#x200B;\n\nThe creator's original video is here\n\nhttps://reddit.com/link/109ri3e/video/icgsgx97ujba1/player", "author_fullname": "t2_77nrtq1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've located and reuploaded a long lost UnleashX skin", "link_flair_richtext": [], "subreddit_name_prefixed": "r/originalxbox", "hidden": false, "pwls": 6, "link_flair_css_class": "tools", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "media_metadata": {"20rcv4cgtjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/20rcv4cgtjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=efe2643dd2b8fbcee893090d86edbc036005feda"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/20rcv4cgtjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c285fb7495f33678c3b1ed7089e7269b1a5d74b"}, {"y": 177, "x": 320, "u": "https://preview.redd.it/20rcv4cgtjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76a2a4d9e3b0066cf79801458a08be4ba34f33a4"}, {"y": 355, "x": 640, "u": "https://preview.redd.it/20rcv4cgtjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f108e3c16a23e91941d839c7142df34078624251"}], "s": {"y": 383, "x": 689, "u": "https://preview.redd.it/20rcv4cgtjba1.png?width=689&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2072dcd672302c1afa94268bcd3f1514386135f7"}, "id": "20rcv4cgtjba1"}, "icgsgx97ujba1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/109ri3e/asset/icgsgx97ujba1/DASHPlaylist.mpd?a=1676151512%2COWM1ZTE5YWJjNjQ3MWE4OWRjYzRjNmU2OTgyOWU2NjQ2YzU3M2E4YThlZGRhZDc2ZDNkOWIwYmNmMzdjMjg4Yw%3D%3D&amp;v=1&amp;f=sd", "x": 1280, "y": 720, "hlsUrl": "https://v.redd.it/link/109ri3e/asset/icgsgx97ujba1/HLSPlaylist.m3u8?a=1676151512%2CZjlkYjY3MDM0MWJmMmI2OGNhYzc2YzNlMWNhZjJlNjQ5YzhmZjc0ZTU3OTVkZDBkYTJhZjcwMjFkZmUzZjA3ZQ%3D%3D&amp;v=1&amp;f=sd", "id": "icgsgx97ujba1", "isGif": false}}, "name": "t3_109ri3e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Software &amp; Tools", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Jnjs0qw2eBd5aTzRu8i0sLwfd-qlwww-SgI8E2Y9eSE.jpg", "edited": 1673516612.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673501638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.originalxbox", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/20rcv4cgtjba1.png?width=689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2072dcd672302c1afa94268bcd3f1514386135f7\"&gt;https://preview.redd.it/20rcv4cgtjba1.png?width=689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2072dcd672302c1afa94268bcd3f1514386135f7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I archived an UnleashX skin that was deleted off the internet suddenly. It was posted to Twitter by its creator Retro Game Rarities in 2020 and his account suddenly got deleted. I was able to find a link to it via google cache and I&amp;#39;ve since reuploaded it to &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://archive.org/details/UnleashX-EmeraldX-skin\"&gt;https://archive.org/details/UnleashX-EmeraldX-skin&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s easily the coolest looking UnleashX skin I&amp;#39;ve ever seen. It was disappointing to discover it was so hastily deleted and purged off the internet.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The creator&amp;#39;s original video is here&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/109ri3e/video/icgsgx97ujba1/player\"&gt;https://reddit.com/link/109ri3e/video/icgsgx97ujba1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "65253322-f811-11ec-aadc-4685b9374796", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2rww7", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "109ri3e", "is_robot_indexable": true, "report_reasons": null, "author": "Archer_Jr", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/originalxbox/comments/109ri3e/ive_located_and_reuploaded_a_long_lost_unleashx/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/originalxbox/comments/109ri3e/ive_located_and_reuploaded_a_long_lost_unleashx/", "subreddit_subscribers": 50896, "created_utc": 1673501638.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1673517607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.originalxbox", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/originalxbox/comments/109ri3e/ive_located_and_reuploaded_a_long_lost_unleashx/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109vz32", "is_robot_indexable": true, "report_reasons": null, "author": "RustedBlade7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_109ri3e", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109vz32/ive_located_and_reuploaded_a_long_lost_unleashx/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/originalxbox/comments/109ri3e/ive_located_and_reuploaded_a_long_lost_unleashx/", "subreddit_subscribers": 665566, "created_utc": 1673517607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need to grab a VKontakte group's media hoard- specifically the photos. I'm not up to no good (much), it's just a public fanpage for a celeb but it goes back too many years to save everything individually. We're talking nearly 4500 pics, and I may want the video too, idk yet.\n\nI turn to the DH brains trust here to advise me- what is the best way to do this? Are there any dedicated bulk VK rippers? I have had a google and JDownloader is an option I've investigated, but it's not grabbing everything (I have a tech support Q pending on the JD sub, but I think im going to be S.O.O.L). I need *everything. E.v.e.r.y. t. h. i. n. g.*\n\nHelp a fangirl out, how do I do this?\n\nI can *maybe* deal with a program in Russian if I have to. I can't code, I'm looking for an off the shelf solution.\n\nETA- Internet Download Manager and DownAll from the sub wiki didn't get anywhere.", "author_fullname": "t2_5l13q7a9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to hoard a VK profile/group?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109voht", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673521458.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673516482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to grab a VKontakte group&amp;#39;s media hoard- specifically the photos. I&amp;#39;m not up to no good (much), it&amp;#39;s just a public fanpage for a celeb but it goes back too many years to save everything individually. We&amp;#39;re talking nearly 4500 pics, and I may want the video too, idk yet.&lt;/p&gt;\n\n&lt;p&gt;I turn to the DH brains trust here to advise me- what is the best way to do this? Are there any dedicated bulk VK rippers? I have had a google and JDownloader is an option I&amp;#39;ve investigated, but it&amp;#39;s not grabbing everything (I have a tech support Q pending on the JD sub, but I think im going to be S.O.O.L). I need &lt;em&gt;everything. E.v.e.r.y. t. h. i. n. g.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Help a fangirl out, how do I do this?&lt;/p&gt;\n\n&lt;p&gt;I can &lt;em&gt;maybe&lt;/em&gt; deal with a program in Russian if I have to. I can&amp;#39;t code, I&amp;#39;m looking for an off the shelf solution.&lt;/p&gt;\n\n&lt;p&gt;ETA- Internet Download Manager and DownAll from the sub wiki didn&amp;#39;t get anywhere.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109voht", "is_robot_indexable": true, "report_reasons": null, "author": "nectarine_pie", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109voht/best_way_to_hoard_a_vk_profilegroup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109voht/best_way_to_hoard_a_vk_profilegroup/", "subreddit_subscribers": 665566, "created_utc": 1673516482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI download a lot of content for archival purposes. These contents are typically downloaded as ZIP or RAR archives which then have to be extracted. I am wondering if there is any way I can automatically add read-only attributes to all files as they are being extracted (eg. as an option inside WinRAR or 7-Zip) instead of having to manually select the folder afterwards, then right-clicking and selecting the read-only checkbox (which sometimes I forget to do)\n\nThanks in advance!", "author_fullname": "t2_fcjv96r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Windows] Is there any way to automatically add read-only attributes to all files extracted by WinRAR or 7-Zip?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a3vd9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673541140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I download a lot of content for archival purposes. These contents are typically downloaded as ZIP or RAR archives which then have to be extracted. I am wondering if there is any way I can automatically add read-only attributes to all files as they are being extracted (eg. as an option inside WinRAR or 7-Zip) instead of having to manually select the folder afterwards, then right-clicking and selecting the read-only checkbox (which sometimes I forget to do)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a3vd9", "is_robot_indexable": true, "report_reasons": null, "author": "P650SE", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a3vd9/windows_is_there_any_way_to_automatically_add/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a3vd9/windows_is_there_any_way_to_automatically_add/", "subreddit_subscribers": 665566, "created_utc": 1673541140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got about 300,000 emails going back to 1993.  I love having the whole thing indexed and searchable for practical and nostalgic purposes, but the ratio of \"good\" to \"bad\" is ridiculous.  Even with gmail doing a lot of the heavy spam filtering, it's got a lot of garbage.\n\nDoes anyone have any pointers on how to efficiently spam filter large quantities of email spread across about 30 unix mbox files?  \n\nIf I had to, I'd be OK resorting to something somewhat iterative and convoluted (somehow feed all the mbox files to a mail server running on one of my linux boxes?) but it would be nice if there were tools out there to make this as easy as possible.", "author_fullname": "t2_tthrp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "30 years of saved email - spam filtering options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a02eo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673531289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got about 300,000 emails going back to 1993.  I love having the whole thing indexed and searchable for practical and nostalgic purposes, but the ratio of &amp;quot;good&amp;quot; to &amp;quot;bad&amp;quot; is ridiculous.  Even with gmail doing a lot of the heavy spam filtering, it&amp;#39;s got a lot of garbage.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any pointers on how to efficiently spam filter large quantities of email spread across about 30 unix mbox files?  &lt;/p&gt;\n\n&lt;p&gt;If I had to, I&amp;#39;d be OK resorting to something somewhat iterative and convoluted (somehow feed all the mbox files to a mail server running on one of my linux boxes?) but it would be nice if there were tools out there to make this as easy as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a02eo", "is_robot_indexable": true, "report_reasons": null, "author": "lectures", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a02eo/30_years_of_saved_email_spam_filtering_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a02eo/30_years_of_saved_email_spam_filtering_options/", "subreddit_subscribers": 665566, "created_utc": 1673531289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't actually have the hardware yet, I'm doing some research before purchase (always advisable lol). I'm interested in buying a used 3.2TB Micron 7300 MAX, but I want to bump that up to 3.84TB for more capacity since I won't be using anywhere near the listed 3DWPD spec. Hell, assuming that makes it equal to the \"Pro\" version of the drive, 1DWPD is still waaaay overkill.\n\nIt seems like Micron calls it \"Flex Capacity\" and it seems to be a part of \"Storage Executive Software\". Is this where I change the capacity? Can I even increase the capacity? I've heard of people reducing capacity to increase endurance, but I've never heard of someone doing the reverse.", "author_fullname": "t2_upalof7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I change the capacity / overprovisioning on a Micron 7300 MAX?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109z0cp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673528186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t actually have the hardware yet, I&amp;#39;m doing some research before purchase (always advisable lol). I&amp;#39;m interested in buying a used 3.2TB Micron 7300 MAX, but I want to bump that up to 3.84TB for more capacity since I won&amp;#39;t be using anywhere near the listed 3DWPD spec. Hell, assuming that makes it equal to the &amp;quot;Pro&amp;quot; version of the drive, 1DWPD is still waaaay overkill.&lt;/p&gt;\n\n&lt;p&gt;It seems like Micron calls it &amp;quot;Flex Capacity&amp;quot; and it seems to be a part of &amp;quot;Storage Executive Software&amp;quot;. Is this where I change the capacity? Can I even increase the capacity? I&amp;#39;ve heard of people reducing capacity to increase endurance, but I&amp;#39;ve never heard of someone doing the reverse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109z0cp", "is_robot_indexable": true, "report_reasons": null, "author": "Party_9001", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109z0cp/how_do_i_change_the_capacity_overprovisioning_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109z0cp/how_do_i_change_the_capacity_overprovisioning_on/", "subreddit_subscribers": 665566, "created_utc": 1673528186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI currently have about 1.3TB stored in Amazon Drive. With the retirement of that platform, I am looking to move to a new on-line option. I am currently thinking Google Drive may be the best fit at the best price.\n\nLooking ahead to the data migration piece.... Can someone suggest options that would allow me to transfer these files directly from the Amazon to the Google platform? Downloading and re-uploading will be a very  s l o w   p   r   o   c   e   s   s   with my internet provider.\n\nTIA!", "author_fullname": "t2_9jlhrl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestions: Data migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109h2xs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673473789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I currently have about 1.3TB stored in Amazon Drive. With the retirement of that platform, I am looking to move to a new on-line option. I am currently thinking Google Drive may be the best fit at the best price.&lt;/p&gt;\n\n&lt;p&gt;Looking ahead to the data migration piece.... Can someone suggest options that would allow me to transfer these files directly from the Amazon to the Google platform? Downloading and re-uploading will be a very  s l o w   p   r   o   c   e   s   s   with my internet provider.&lt;/p&gt;\n\n&lt;p&gt;TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109h2xs", "is_robot_indexable": true, "report_reasons": null, "author": "PerpetuallyPerplxed", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109h2xs/need_suggestions_data_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109h2xs/need_suggestions_data_migration/", "subreddit_subscribers": 665566, "created_utc": 1673473789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "title says it all , been trying many github scripts but nothing works", "author_fullname": "t2_aj30rvtl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "any way to download photo album from weibo ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109x642", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673521913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title says it all , been trying many github scripts but nothing works&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109x642", "is_robot_indexable": true, "report_reasons": null, "author": "Digital-Nuke", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109x642/any_way_to_download_photo_album_from_weibo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109x642/any_way_to_download_photo_album_from_weibo/", "subreddit_subscribers": 665566, "created_utc": 1673521913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\nI'm looking for a webui tool to manage my music folder.\n\nMy idea is something like that:\n\n\\- I tell the program where I will add new songs (like the rip folder)\n\n\\- It automatically detect new songs, search for tags and the move the song to the correct folder\n\n\\- from the webui I set the rules and edit metadata if needed.\n\n&amp;#x200B;\n\nDo you know anything that can do that?", "author_fullname": "t2_24u5cpux", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manage music folder from webui?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109jdnd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673479355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a webui tool to manage my music folder.&lt;/p&gt;\n\n&lt;p&gt;My idea is something like that:&lt;/p&gt;\n\n&lt;p&gt;- I tell the program where I will add new songs (like the rip folder)&lt;/p&gt;\n\n&lt;p&gt;- It automatically detect new songs, search for tags and the move the song to the correct folder&lt;/p&gt;\n\n&lt;p&gt;- from the webui I set the rules and edit metadata if needed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do you know anything that can do that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109jdnd", "is_robot_indexable": true, "report_reasons": null, "author": "TopdeckIsSkill", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109jdnd/manage_music_folder_from_webui/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109jdnd/manage_music_folder_from_webui/", "subreddit_subscribers": 665566, "created_utc": 1673479355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_bdd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is the patriot p210 2tb any good?? this will be my steam drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_10aakd0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8I0yMwHarJVyd2xFa1JZnaJiyIko-iBGr9nwCBzrxGc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673557244.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/d6yawtzafoba1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/d6yawtzafoba1.jpg?auto=webp&amp;v=enabled&amp;s=df80b947e9f9dfb8a7c7a2eaf3cc41dfb19c3bd5", "width": 2500, "height": 2500}, "resolutions": [{"url": "https://preview.redd.it/d6yawtzafoba1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c15ebea0ed8b6c7d035a0eb5d24456596b996d79", "width": 108, "height": 108}, {"url": "https://preview.redd.it/d6yawtzafoba1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3ebce75b1bec2b819e9a6f6a54681f88c3ca93e", "width": 216, "height": 216}, {"url": "https://preview.redd.it/d6yawtzafoba1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e0becd8d3cd1796920f1d410540593d838a2e0c", "width": 320, "height": 320}, {"url": "https://preview.redd.it/d6yawtzafoba1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b69a479ada2a4c5008b51f6b23c9d510538f7aa3", "width": 640, "height": 640}, {"url": "https://preview.redd.it/d6yawtzafoba1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a1520123d93770ffb026b916e2290a38706094e", "width": 960, "height": 960}, {"url": "https://preview.redd.it/d6yawtzafoba1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7c31e250df34caf59a6cf78d17dc4e4b1278e69c", "width": 1080, "height": 1080}], "variants": {}, "id": "Ol74G8lohFF_LoLtmsynNnmC6h-w4hTTqhe5QCm72wg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10aakd0", "is_robot_indexable": true, "report_reasons": null, "author": "rage9000", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10aakd0/is_the_patriot_p210_2tb_any_good_this_will_be_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/d6yawtzafoba1.jpg", "subreddit_subscribers": 665566, "created_utc": 1673557244.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "ive got a WD 12tb in desktop rig. When I try to open two particular folders on it [throws me this message](https://i.imgur.com/SQ08o7a.png). **note** I stole that image from somebody else in another forum with the same problem. my message looks identical to that just obviously with a different folder path. \n\ni loaded partedmagic usb boot and while inside the boot environment one of those folders is completely accessible and i can view and use the contents of it. so i changed the name of the folder and rebooted windows and the folder is fine now. but the other folder? it doesnt show up in partedmagic at all. its like it doesnt exist. but then i reboot back into windows and there it is again, but i cant access it without it giving me that error message. so weird and frustrating.\n\nnot sure if my OS was corrupted so i tried putting the hard drive in a different desktop and even loaded it on a mac via an external enclosure but no luck. folder still either doesnt exist or cant be accessed. \n\nim moving everything i can get off the drive via teracopy so i can full format it. i can live without the contents of that folder but still id like to save the the folder if anybody knows how before i give up.\n\nthanks for anybody that can help me.", "author_fullname": "t2_uhn37jxx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Windows 10) some folders on one of my drives are \"unavailable\" while others are fine. SMART tests come up fine... Anybody encountered this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10aajcd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673559209.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673557195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ive got a WD 12tb in desktop rig. When I try to open two particular folders on it &lt;a href=\"https://i.imgur.com/SQ08o7a.png\"&gt;throws me this message&lt;/a&gt;. &lt;strong&gt;note&lt;/strong&gt; I stole that image from somebody else in another forum with the same problem. my message looks identical to that just obviously with a different folder path. &lt;/p&gt;\n\n&lt;p&gt;i loaded partedmagic usb boot and while inside the boot environment one of those folders is completely accessible and i can view and use the contents of it. so i changed the name of the folder and rebooted windows and the folder is fine now. but the other folder? it doesnt show up in partedmagic at all. its like it doesnt exist. but then i reboot back into windows and there it is again, but i cant access it without it giving me that error message. so weird and frustrating.&lt;/p&gt;\n\n&lt;p&gt;not sure if my OS was corrupted so i tried putting the hard drive in a different desktop and even loaded it on a mac via an external enclosure but no luck. folder still either doesnt exist or cant be accessed. &lt;/p&gt;\n\n&lt;p&gt;im moving everything i can get off the drive via teracopy so i can full format it. i can live without the contents of that folder but still id like to save the the folder if anybody knows how before i give up.&lt;/p&gt;\n\n&lt;p&gt;thanks for anybody that can help me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FEsLF3NsYPIGH6T6AgTmwYvbuE9XQ68-N6_BbhNd_nY.png?auto=webp&amp;v=enabled&amp;s=2be013fea68f20dbe6f4163b6e486eed4bd8df0d", "width": 715, "height": 194}, "resolutions": [{"url": "https://external-preview.redd.it/FEsLF3NsYPIGH6T6AgTmwYvbuE9XQ68-N6_BbhNd_nY.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc33d509f9f9005831db8dcd25ed6110bb73a4b5", "width": 108, "height": 29}, {"url": "https://external-preview.redd.it/FEsLF3NsYPIGH6T6AgTmwYvbuE9XQ68-N6_BbhNd_nY.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c50962b225720f25193fb4cba68cc03ab3749eb5", "width": 216, "height": 58}, {"url": "https://external-preview.redd.it/FEsLF3NsYPIGH6T6AgTmwYvbuE9XQ68-N6_BbhNd_nY.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1fe57502d21d7ae69637b7b3c663fddbdbd8e56", "width": 320, "height": 86}, {"url": "https://external-preview.redd.it/FEsLF3NsYPIGH6T6AgTmwYvbuE9XQ68-N6_BbhNd_nY.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8866099ecbf3cc08035035400d73378f04bb42fc", "width": 640, "height": 173}], "variants": {}, "id": "ekN92S2jlLjd1FPlBiPJOHCIJyBrfKL9VTc3LeAvKJA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10aajcd", "is_robot_indexable": true, "report_reasons": null, "author": "woodpilecake", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10aajcd/windows_10_some_folders_on_one_of_my_drives_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10aajcd/windows_10_some_folders_on_one_of_my_drives_are/", "subreddit_subscribers": 665566, "created_utc": 1673557195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys,\n\nI am not that much of a hoarder rather a storage/compute/backup specialist.\n\nLast couple of months I found that torrent trackers don't have movies in my native language. So for me the tube is best source of movies and even 4k nature videos. Can you share with me what proof to be best way for you to download YT videos/playlist both 720p and 4k?\n\nPS\n\nHere is a snip from session that will time out:\n\nuser@host:\\~$ ls -lh\n\ntotal 235M\n\n\\-rw-rw-r-- 1 user 235M Jan  4 21:02 '\u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm.part'\n\nuser@host:\\~$ rm '\u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm.part'\n\nuser@host:\\~$ youtube-dl -f 315 [https://www.youtube.com/watch?v=LCDDzLK-sys](https://www.youtube.com/watch?v=LCDDzLK-sys)\n\n\\[youtube\\] LCDDzLK-sys: Downloading webpage\n\n\\[download\\] Destination: \u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm\n\n\\[download\\]   0.9% of 2.76GiB at 70.46KiB/s ETA 11:17:32", "author_fullname": "t2_db0rv0yp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you like to dump 4k Youtube videos ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10aa20l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673556026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I am not that much of a hoarder rather a storage/compute/backup specialist.&lt;/p&gt;\n\n&lt;p&gt;Last couple of months I found that torrent trackers don&amp;#39;t have movies in my native language. So for me the tube is best source of movies and even 4k nature videos. Can you share with me what proof to be best way for you to download YT videos/playlist both 720p and 4k?&lt;/p&gt;\n\n&lt;p&gt;PS&lt;/p&gt;\n\n&lt;p&gt;Here is a snip from session that will time out:&lt;/p&gt;\n\n&lt;p&gt;user@host:~$ ls -lh&lt;/p&gt;\n\n&lt;p&gt;total 235M&lt;/p&gt;\n\n&lt;p&gt;-rw-rw-r-- 1 user 235M Jan  4 21:02 &amp;#39;\u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm.part&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;user@host:~$ rm &amp;#39;\u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm.part&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;user@host:~$ youtube-dl -f 315 &lt;a href=\"https://www.youtube.com/watch?v=LCDDzLK-sys\"&gt;https://www.youtube.com/watch?v=LCDDzLK-sys&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[youtube] LCDDzLK-sys: Downloading webpage&lt;/p&gt;\n\n&lt;p&gt;[download] Destination: \u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm&lt;/p&gt;\n\n&lt;p&gt;[download]   0.9% of 2.76GiB at 70.46KiB/s ETA 11:17:32&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B_95ltT36M0Z_42W0ff5ZhRSP8_V3dIr00yEl4_xSyg.jpg?auto=webp&amp;v=enabled&amp;s=0750adbce2c293bab5e98c3db1fc417259085eff", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/B_95ltT36M0Z_42W0ff5ZhRSP8_V3dIr00yEl4_xSyg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a42c015f4b6d258225a8ce691de9bbef630ed1ca", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/B_95ltT36M0Z_42W0ff5ZhRSP8_V3dIr00yEl4_xSyg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9cd0fe9388dd7504c6be08fa00f019a728d63f3c", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/B_95ltT36M0Z_42W0ff5ZhRSP8_V3dIr00yEl4_xSyg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a55a3af333fb19a2e81021b7da43de8dd7ea89b", "width": 320, "height": 240}], "variants": {}, "id": "UW-6yU-dqtHKKqWFY9jvmOw8xW-z6xI7jzKq6jBnulw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "9TB TerraMaster", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10aa20l", "is_robot_indexable": true, "report_reasons": null, "author": "Accomplished-Eye1673", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10aa20l/how_do_you_like_to_dump_4k_youtube_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10aa20l/how_do_you_like_to_dump_4k_youtube_videos/", "subreddit_subscribers": 665566, "created_utc": 1673556026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Long time data hoarder (though I usually refer to myself as digital hoarder), but new to the group and not a NAS/RAID/Server expert, though with decent computer knowledge.\n\nI currently have 2 EOL Lenovo NAS (PX4-400D with 4x4TB and IX4-300D with 4x3TB) but would like to \"consolidate\" with truly large drives. According to this list below, the biggest drive they list is 6TB\n\n[https://download.lenovo.com/lenovoemc/eu/en/app/answers/detail/a\\_id/33754.html](https://download.lenovo.com/lenovoemc/eu/en/app/answers/detail/a_id/33754.html)\n\nI know the NAS systems are old and EOL, but they are just local storage of downloads and videos without installed apps or services that reach outside. Usually 1 user accessing the drive, sometimes 2. The data is not \"super precious\" and while a loss would hurt, it wouldn't ruin me. I am hoping to just make one the \"mother of all data\" throwing 4x12TB or something like that at the NAS in RAID-5.\n\nQ1: Is 6TB really the limit, or is this just what was available at the time the list was release (2015). Does anyone run either of them with bigger drives than that?\n\nQ2: Can I buy new drives in stages? For example get 2x12TB running RAID-1 (I know it will only have 12TB usable), then add another 12TB and convert the RAID-1 to RAID-5 to have 24TB usable. Then add another 12TB after that to get to 36TB? This is mainly because of my available funds, because they currently run at about $200 a piece. I understand that generally you can convert RAID-1 to RAID-5 but wondering about these older systems.\n\nQ3: They currently use dedicated NAS drives (WD Red), could I use Enterprise or Surveillance disks like EXOS if they are cheaper or is this risky? I understand I can generally run whatever I want but even after googling, I am not sure if the difference between the drives should worry me for this setup\n\nQ4: Seagate Ironwolf or WD Red?", "author_fullname": "t2_5ai3u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lenovo EMC questions about max drive size", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10a9cyw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673554366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long time data hoarder (though I usually refer to myself as digital hoarder), but new to the group and not a NAS/RAID/Server expert, though with decent computer knowledge.&lt;/p&gt;\n\n&lt;p&gt;I currently have 2 EOL Lenovo NAS (PX4-400D with 4x4TB and IX4-300D with 4x3TB) but would like to &amp;quot;consolidate&amp;quot; with truly large drives. According to this list below, the biggest drive they list is 6TB&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://download.lenovo.com/lenovoemc/eu/en/app/answers/detail/a_id/33754.html\"&gt;https://download.lenovo.com/lenovoemc/eu/en/app/answers/detail/a_id/33754.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I know the NAS systems are old and EOL, but they are just local storage of downloads and videos without installed apps or services that reach outside. Usually 1 user accessing the drive, sometimes 2. The data is not &amp;quot;super precious&amp;quot; and while a loss would hurt, it wouldn&amp;#39;t ruin me. I am hoping to just make one the &amp;quot;mother of all data&amp;quot; throwing 4x12TB or something like that at the NAS in RAID-5.&lt;/p&gt;\n\n&lt;p&gt;Q1: Is 6TB really the limit, or is this just what was available at the time the list was release (2015). Does anyone run either of them with bigger drives than that?&lt;/p&gt;\n\n&lt;p&gt;Q2: Can I buy new drives in stages? For example get 2x12TB running RAID-1 (I know it will only have 12TB usable), then add another 12TB and convert the RAID-1 to RAID-5 to have 24TB usable. Then add another 12TB after that to get to 36TB? This is mainly because of my available funds, because they currently run at about $200 a piece. I understand that generally you can convert RAID-1 to RAID-5 but wondering about these older systems.&lt;/p&gt;\n\n&lt;p&gt;Q3: They currently use dedicated NAS drives (WD Red), could I use Enterprise or Surveillance disks like EXOS if they are cheaper or is this risky? I understand I can generally run whatever I want but even after googling, I am not sure if the difference between the drives should worry me for this setup&lt;/p&gt;\n\n&lt;p&gt;Q4: Seagate Ironwolf or WD Red?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a9cyw", "is_robot_indexable": true, "report_reasons": null, "author": "saldridge", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a9cyw/lenovo_emc_questions_about_max_drive_size/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a9cyw/lenovo_emc_questions_about_max_drive_size/", "subreddit_subscribers": 665566, "created_utc": 1673554366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, posted this in the QNAP sub but thought it would be worth trying here as well\n\nHave a 4 bay QNAP 453BT3 connected to my Mac via thunderbolt 3. Have just upgraded it to 4 x 16tb ironwolfs and want to set it up a little differently to my previous config as I wasnt happy with the speeds I was getting previously with RAID 5, thick volume (310 write, 580 read).\n\nI was hoping Id see a bump in speed by going RAID 10, static volume, but its about the same as before (280 write, 530 read).\n\nWould be very open to any other tips as to how to get a better speed, been hoping for something closer to 600 on the write. Raid 0 is out of the question because Id like some form of protection against a drive failure and don\u2019t mind sacrificing the storage to go with RAID 10 if it ended up being a fast option.\n\nCan try and find a 10Gbe dock to give that a go, but the thunderbolt 3 connection is managing 550 on the write so it\u2019s clearly capable of that speed.\n\nFinally Im also down to try some SSD QTier or Caching but from what I\u2019ve read it doesn\u2019t sound like that\u2019s going to give me a big enough bump in speed over what I\u2019m seeing currently.\n\nAny help or hot tips about what I could be doing wrong would be very very appreciated\nThanks!", "author_fullname": "t2_248eq7zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Poor RAID speeds Thunderbolt 3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10a8zsk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673556335.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673553536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, posted this in the QNAP sub but thought it would be worth trying here as well&lt;/p&gt;\n\n&lt;p&gt;Have a 4 bay QNAP 453BT3 connected to my Mac via thunderbolt 3. Have just upgraded it to 4 x 16tb ironwolfs and want to set it up a little differently to my previous config as I wasnt happy with the speeds I was getting previously with RAID 5, thick volume (310 write, 580 read).&lt;/p&gt;\n\n&lt;p&gt;I was hoping Id see a bump in speed by going RAID 10, static volume, but its about the same as before (280 write, 530 read).&lt;/p&gt;\n\n&lt;p&gt;Would be very open to any other tips as to how to get a better speed, been hoping for something closer to 600 on the write. Raid 0 is out of the question because Id like some form of protection against a drive failure and don\u2019t mind sacrificing the storage to go with RAID 10 if it ended up being a fast option.&lt;/p&gt;\n\n&lt;p&gt;Can try and find a 10Gbe dock to give that a go, but the thunderbolt 3 connection is managing 550 on the write so it\u2019s clearly capable of that speed.&lt;/p&gt;\n\n&lt;p&gt;Finally Im also down to try some SSD QTier or Caching but from what I\u2019ve read it doesn\u2019t sound like that\u2019s going to give me a big enough bump in speed over what I\u2019m seeing currently.&lt;/p&gt;\n\n&lt;p&gt;Any help or hot tips about what I could be doing wrong would be very very appreciated\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a8zsk", "is_robot_indexable": true, "report_reasons": null, "author": "doco32", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a8zsk/poor_raid_speeds_thunderbolt_3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a8zsk/poor_raid_speeds_thunderbolt_3/", "subreddit_subscribers": 665566, "created_utc": 1673553536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Wonder if YouTube-DL would work, however this site isn\u2019t mainstream \n\nAny ideas? \n\nThanks", "author_fullname": "t2_jp6vscfv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download from a \u201cstreaming only\u201d site", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a8exh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673552079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wonder if YouTube-DL would work, however this site isn\u2019t mainstream &lt;/p&gt;\n\n&lt;p&gt;Any ideas? &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a8exh", "is_robot_indexable": true, "report_reasons": null, "author": "StoryThrowAway916", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a8exh/download_from_a_streaming_only_site/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a8exh/download_from_a_streaming_only_site/", "subreddit_subscribers": 665566, "created_utc": 1673552079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anybody know a service that offers large amounts of storage for very short periods of time and how much would this cost? cheers", "author_fullname": "t2_h36vzasl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to buy 2tb cloud hosted storage for a very short period of time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a7x3c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673550889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anybody know a service that offers large amounts of storage for very short periods of time and how much would this cost? cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a7x3c", "is_robot_indexable": true, "report_reasons": null, "author": "JustYourJoe_", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a7x3c/looking_to_buy_2tb_cloud_hosted_storage_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a7x3c/looking_to_buy_2tb_cloud_hosted_storage_for_a/", "subreddit_subscribers": 665566, "created_utc": 1673550889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have 14tb of data how should i creat encrypted container? \n\nAm i need to split those data in 1tb slot or its just have special software to mount each time? And from that mount i have to sync?\n\nIf i creat a container how software will sync data is anything change inside?\n\nI need to reupload whole 1 tb slot of container? And remember slots data is changed?", "author_fullname": "t2_10093u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to encrypt data to backup in cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a6s6m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673548186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 14tb of data how should i creat encrypted container? &lt;/p&gt;\n\n&lt;p&gt;Am i need to split those data in 1tb slot or its just have special software to mount each time? And from that mount i have to sync?&lt;/p&gt;\n\n&lt;p&gt;If i creat a container how software will sync data is anything change inside?&lt;/p&gt;\n\n&lt;p&gt;I need to reupload whole 1 tb slot of container? And remember slots data is changed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a6s6m", "is_robot_indexable": true, "report_reasons": null, "author": "robertniro1980", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a6s6m/what_is_the_best_way_to_encrypt_data_to_backup_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a6s6m/what_is_the_best_way_to_encrypt_data_to_backup_in/", "subreddit_subscribers": 665566, "created_utc": 1673548186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone found a software they like for managing a library of *arbitrary* files? I have Plex for Video, Kavita for ebooks, Paperless-ng for personal documents, etc.\n\nIs there anything like these tools but for any and all file types? Basically I'd want a tool that could read metadata, potentially file contexts for text, and also allow any file to be tagged \u00e0 la Gmail labels, making searching easy. If it's something common like HTML/PDF/mp4 it could just view the file in browser, but otherwise provide a file location reference or download link.\n\nAnyone aware of anything in this vein existing?", "author_fullname": "t2_32pyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Library Management Software for Arbitrary Files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a5ttt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673545888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone found a software they like for managing a library of &lt;em&gt;arbitrary&lt;/em&gt; files? I have Plex for Video, Kavita for ebooks, Paperless-ng for personal documents, etc.&lt;/p&gt;\n\n&lt;p&gt;Is there anything like these tools but for any and all file types? Basically I&amp;#39;d want a tool that could read metadata, potentially file contexts for text, and also allow any file to be tagged \u00e0 la Gmail labels, making searching easy. If it&amp;#39;s something common like HTML/PDF/mp4 it could just view the file in browser, but otherwise provide a file location reference or download link.&lt;/p&gt;\n\n&lt;p&gt;Anyone aware of anything in this vein existing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "12TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a5ttt", "is_robot_indexable": true, "report_reasons": null, "author": "eldridgea", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10a5ttt/library_management_software_for_arbitrary_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a5ttt/library_management_software_for_arbitrary_files/", "subreddit_subscribers": 665566, "created_utc": 1673545888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello there, I am looking into building my first NAS and am looking for advice on the kind of disk setup to go with. At this point I plan to reuse an older PC with TrueNAS and just need to get the drives. I might *possibly* end up moving to a prebuilt NAS device like from Synology in the future, depending on how I get on with my home build.\n \n \n \nMy use case is going to be primarily archive storage and a media server, used by only a couple of users at once. An external drive would be used to backup files from the NAS. \n \n \n \nMy concern now is whether I should go with 2x 6TB disks in RAID 1 (6TB usable), or 4x 2TB disks in RAID 5 / RAIDZ1 (also 6TB usable). Rookie numbers I know, but I can't justify spending 1000s on bigger drives at this point. \n\nThere is a difference of literally a couple of dollars between the two options above.\n \n \n \nResearching the different RAIDs, the cons are each one are:\n\n*RAID1:*\n\n* cost - half the potential storage is \"wasted\". -- This is the big one, essentially buying 2 things but only getting to use 1. Future upgrades are more expensive too, needing to buy pairs of larger dsks.\n\n* In the case of a failed drive, the system needs to be shut down to repair. -- This downtime is not a concern for me.\n\n*Raid 5:*\n\n* Rebuilding after a failure is a long process (days for large drives) and leaves the data vulnerable to further loss. This sounds pretty annoying. I'm not conerned with having the data accessable 24/7, so prefer the RAID1 in this respect.\n \n \n \nMaybe anoher option is no RAID at all? As I mentioned, my most important files (and likely a good chunk of whatever else) would be backed up elsewhere, so while downloading/copying stuff all over again would be a huge pain, it's not the end of the world. This would of course be half the cost, or I could get even more storage for just a bit more, but without any redundancy. Decisions decisions... I need a push in the right direction I think.", "author_fullname": "t2_3ycsbgx9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Disk / RAID Choice for First Build", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a2g8s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673538700.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673537625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there, I am looking into building my first NAS and am looking for advice on the kind of disk setup to go with. At this point I plan to reuse an older PC with TrueNAS and just need to get the drives. I might &lt;em&gt;possibly&lt;/em&gt; end up moving to a prebuilt NAS device like from Synology in the future, depending on how I get on with my home build.&lt;/p&gt;\n\n&lt;p&gt;My use case is going to be primarily archive storage and a media server, used by only a couple of users at once. An external drive would be used to backup files from the NAS. &lt;/p&gt;\n\n&lt;p&gt;My concern now is whether I should go with 2x 6TB disks in RAID 1 (6TB usable), or 4x 2TB disks in RAID 5 / RAIDZ1 (also 6TB usable). Rookie numbers I know, but I can&amp;#39;t justify spending 1000s on bigger drives at this point. &lt;/p&gt;\n\n&lt;p&gt;There is a difference of literally a couple of dollars between the two options above.&lt;/p&gt;\n\n&lt;p&gt;Researching the different RAIDs, the cons are each one are:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;RAID1:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;cost - half the potential storage is &amp;quot;wasted&amp;quot;. -- This is the big one, essentially buying 2 things but only getting to use 1. Future upgrades are more expensive too, needing to buy pairs of larger dsks.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;In the case of a failed drive, the system needs to be shut down to repair. -- This downtime is not a concern for me.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;em&gt;Raid 5:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Rebuilding after a failure is a long process (days for large drives) and leaves the data vulnerable to further loss. This sounds pretty annoying. I&amp;#39;m not conerned with having the data accessable 24/7, so prefer the RAID1 in this respect.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Maybe anoher option is no RAID at all? As I mentioned, my most important files (and likely a good chunk of whatever else) would be backed up elsewhere, so while downloading/copying stuff all over again would be a huge pain, it&amp;#39;s not the end of the world. This would of course be half the cost, or I could get even more storage for just a bit more, but without any redundancy. Decisions decisions... I need a push in the right direction I think.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a2g8s", "is_robot_indexable": true, "report_reasons": null, "author": "RogerTheBannister", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a2g8s/disk_raid_choice_for_first_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a2g8s/disk_raid_choice_for_first_build/", "subreddit_subscribers": 665566, "created_utc": 1673537625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a diy nas running windows server 2016 and a storage pool with 2 6TB WD Red drives set up as mirrored.  Yes I know this isn't the best or cleanest nas setup but for all my windows shares it's the easiest for me so I want to continue using storage spaces.\n\nMy question is, what happens when I add new drives to the existing pool that has 2 mirrored drives already.  I have on hand and want to add 2 12TB drives to the pool. Eventually I want to remove one by one the 6TB drives from the pool and replace them with larger newer drives.  Will storage spaces mirror the 2 12TB drives together? Will it copy the data from the 6TB drives to the 12TB drives or will it act like a 2x2 mirrored striped setup when the data is split between the 2 sets of drives?\n\nI would prefer more redundancy than total storage space.", "author_fullname": "t2_511438q9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage Spaces adding drives to existing mirrored pool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a26dk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673536931.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a diy nas running windows server 2016 and a storage pool with 2 6TB WD Red drives set up as mirrored.  Yes I know this isn&amp;#39;t the best or cleanest nas setup but for all my windows shares it&amp;#39;s the easiest for me so I want to continue using storage spaces.&lt;/p&gt;\n\n&lt;p&gt;My question is, what happens when I add new drives to the existing pool that has 2 mirrored drives already.  I have on hand and want to add 2 12TB drives to the pool. Eventually I want to remove one by one the 6TB drives from the pool and replace them with larger newer drives.  Will storage spaces mirror the 2 12TB drives together? Will it copy the data from the 6TB drives to the 12TB drives or will it act like a 2x2 mirrored striped setup when the data is split between the 2 sets of drives?&lt;/p&gt;\n\n&lt;p&gt;I would prefer more redundancy than total storage space.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a26dk", "is_robot_indexable": true, "report_reasons": null, "author": "Historical_Wheel1090", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a26dk/storage_spaces_adding_drives_to_existing_mirrored/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a26dk/storage_spaces_adding_drives_to_existing_mirrored/", "subreddit_subscribers": 665566, "created_utc": 1673536931.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}