{"kind": "Listing", "data": {"after": "t3_10a8b93", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a PhD in Engineering and have very good knowledge of Python, SQL, and machine learning.\n\nCurrently, work as a data scientist in an insurance company (less than 1 year of job experience), but my plan is to get into Amazon or Meta as a data scientist as the next step. \n\nMy current data scientist position is mainly about data cleaning, building, and improving ML models using Python.\n\nI do not have that much experience in Cloud and Big Data frameworks such as Spark, and my current employer does not provide such possibilities either.\n\nMy plan is to learn cloud (AWS or GCP) and focus on Leet Code for this. I consider 12 months for improving my resume and boosting the required skills. Considering my knowledge in SQL, Python, and ML, do you think improving my knowledge/experience in Cloud and Leet Code is a good package for a job change to Amazon or Meta? Do you recommend any other skillset such as Spark, etc?\n\nThank you so much!", "author_fullname": "t2_h2udjs11", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Skills required for DS position at Meta/AMAZON", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109hp8q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 78, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 78, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673475500.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673475277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a PhD in Engineering and have very good knowledge of Python, SQL, and machine learning.&lt;/p&gt;\n\n&lt;p&gt;Currently, work as a data scientist in an insurance company (less than 1 year of job experience), but my plan is to get into Amazon or Meta as a data scientist as the next step. &lt;/p&gt;\n\n&lt;p&gt;My current data scientist position is mainly about data cleaning, building, and improving ML models using Python.&lt;/p&gt;\n\n&lt;p&gt;I do not have that much experience in Cloud and Big Data frameworks such as Spark, and my current employer does not provide such possibilities either.&lt;/p&gt;\n\n&lt;p&gt;My plan is to learn cloud (AWS or GCP) and focus on Leet Code for this. I consider 12 months for improving my resume and boosting the required skills. Considering my knowledge in SQL, Python, and ML, do you think improving my knowledge/experience in Cloud and Leet Code is a good package for a job change to Amazon or Meta? Do you recommend any other skillset such as Spark, etc?&lt;/p&gt;\n\n&lt;p&gt;Thank you so much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109hp8q", "is_robot_indexable": true, "report_reasons": null, "author": "Last-Revenue-660", "discussion_type": null, "num_comments": 73, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109hp8q/skills_required_for_ds_position_at_metaamazon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109hp8q/skills_required_for_ds_position_at_metaamazon/", "subreddit_subscribers": 836482, "created_utc": 1673475277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&amp;s=08).\n\n2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.\n\nEmergent Abilities in LLMs\n\nIn a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs will allow them, among other things, to:\n\n* Become better at math\n* Understand even more subtleties of human language\n* Stop hallucinating and answer truthfully\n* ...\n\n(See the plot on break-out performance below for a full list)\n\n**Some Context:**\n\nIf you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.\n\n**Why does this happen?**\n\nLLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.\n\nHence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).\n\nLet's look at the following sentence.\n\n\"The sum of two plus two is ...\"\n\nThe model figures out that the most likely missing word is \"four\".\n\nThe fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).\n\nThere are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example are words that change their meaning with context. When the model encounters the word \"bed\", it needs to figure out from the context, if the text is talking about a \"river bed\" or a \"bed\" to sleep in.\n\n**What they discovered:**\n\nFor smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (proxy for model size) is reached.\n\nThe figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.\n\n&amp;#x200B;\n\n[Break-Out Performance At Critical Scale](https://preview.redd.it/w7xffqjimmba1.png?width=800&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c573f6199db7b5ccebfa73ce8e58313e5ae27822)\n\nThey observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.\n\nLooking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.\n\n(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.\n\nThere is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.\n\n**So what does this mean exactly?**\n\nThis beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.\n\nHowever, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.\n\nSuch exciting times to be alive!\n\nIf you got down here, thank you! It was a privilege to make this for you.  \nAt **TheDecoding** \u2b55, I send out a thoughtful newsletter about ML research and the data economy once a week.  \nNo Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)", "author_fullname": "t2_az3v2qdw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Research From Google Shines Light On The Future Of Language Models \u2b55", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "media_metadata": {"w7xffqjimmba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/w7xffqjimmba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=064271844381b5d30e9e64768dca602c28e79ca0"}, {"y": 142, "x": 216, "u": "https://preview.redd.it/w7xffqjimmba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1134d5c14f8b0c4bf0844668ea108b606c26037c"}, {"y": 211, "x": 320, "u": "https://preview.redd.it/w7xffqjimmba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e48a5a0a0b5a51ee2dbcfd6b97ce6a352252c0e3"}, {"y": 422, "x": 640, "u": "https://preview.redd.it/w7xffqjimmba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3c6d9e818e81b07e4aa3f6c5eb9a7eefd103691"}], "s": {"y": 528, "x": 800, "u": "https://preview.redd.it/w7xffqjimmba1.png?width=800&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c573f6199db7b5ccebfa73ce8e58313e5ae27822"}, "id": "w7xffqjimmba1"}}, "name": "t3_10a1mik", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HHMyocjc1mprY1KVeTd2oCjTsPTwuNrN1QlSqTydiEw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673535544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create &lt;a href=\"https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:%7E:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company\"&gt;billion-dollar companies&lt;/a&gt;, and most notably they helped us recognize the &lt;a href=\"https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&amp;amp;s=08\"&gt;divine nature of ducks&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.&lt;/p&gt;\n\n&lt;p&gt;Emergent Abilities in LLMs&lt;/p&gt;\n\n&lt;p&gt;In a recent &lt;a href=\"https://arxiv.org/pdf/2206.07682.pdf\"&gt;paper from Google Brain&lt;/a&gt;, Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs will allow them, among other things, to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Become better at math&lt;/li&gt;\n&lt;li&gt;Understand even more subtleties of human language&lt;/li&gt;\n&lt;li&gt;Stop hallucinating and answer truthfully&lt;/li&gt;\n&lt;li&gt;...&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;(See the plot on break-out performance below for a full list)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Some Context:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why does this happen?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;LLMs are commonly trained by &lt;a href=\"https://www.cs.ubc.ca/%7Eamuham01/LING530/papers/radford2018improving.pdf\"&gt;maximizing the likelihood&lt;/a&gt; over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.&lt;/p&gt;\n\n&lt;p&gt;Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s look at the following sentence.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;The sum of two plus two is ...&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The model figures out that the most likely missing word is &amp;quot;four&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated &lt;a href=\"https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&amp;amp;ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F\"&gt;LLMs begin to struggle&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example are words that change their meaning with context. When the model encounters the word &amp;quot;bed&amp;quot;, it needs to figure out from the context, if the text is talking about a &amp;quot;river bed&amp;quot; or a &amp;quot;bed&amp;quot; to sleep in.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What they discovered:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (proxy for model size) is reached.&lt;/p&gt;\n\n&lt;p&gt;The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/w7xffqjimmba1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c573f6199db7b5ccebfa73ce8e58313e5ae27822\"&gt;Break-Out Performance At Critical Scale&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei&amp;#39;s personal blog. There he &lt;a href=\"https://www.jasonwei.net/blog/emergence\"&gt;listed a total of 137&lt;/a&gt; emergent abilities observable in LLMs.&lt;/p&gt;\n\n&lt;p&gt;Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.&lt;/p&gt;\n\n&lt;p&gt;(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.&lt;/p&gt;\n\n&lt;p&gt;There is &lt;a href=\"https://arxiv.org/abs/2203.15556\"&gt;other research&lt;/a&gt; suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So what does this mean exactly?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.&lt;/p&gt;\n\n&lt;p&gt;However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.&lt;/p&gt;\n\n&lt;p&gt;Such exciting times to be alive!&lt;/p&gt;\n\n&lt;p&gt;If you got down here, thank you! It was a privilege to make this for you.&lt;br/&gt;\nAt &lt;strong&gt;TheDecoding&lt;/strong&gt; \u2b55, I send out a thoughtful newsletter about ML research and the data economy once a week.&lt;br/&gt;\nNo Spam. No Nonsense. &lt;a href=\"https://thedecoding.net/\"&gt;Click here to sign up!&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a1mik", "is_robot_indexable": true, "report_reasons": null, "author": "LesleyFair", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a1mik/new_research_from_google_shines_light_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a1mik/new_research_from_google_shines_light_on_the/", "subreddit_subscribers": 836482, "created_utc": 1673535544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I maintain a numeric model with unit and integration \"spot tests\".  When I add a new feature that changes the model some of these tests will predictably fail.  I usually manually go through and update each affected test, ensure the \"failed\" value looks correct, and then use it for the new expected value.  Is there any more automated way to accomplish this, or other approaches to the problem?  Thanks!\n\nFor background I am using Python with Pytest.", "author_fullname": "t2_172zngvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to update many unit tests when a numeric model changes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109n88c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673489594.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673489276.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I maintain a numeric model with unit and integration &amp;quot;spot tests&amp;quot;.  When I add a new feature that changes the model some of these tests will predictably fail.  I usually manually go through and update each affected test, ensure the &amp;quot;failed&amp;quot; value looks correct, and then use it for the new expected value.  Is there any more automated way to accomplish this, or other approaches to the problem?  Thanks!&lt;/p&gt;\n\n&lt;p&gt;For background I am using Python with Pytest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109n88c", "is_robot_indexable": true, "report_reasons": null, "author": "Bertie_Woo", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109n88c/how_to_update_many_unit_tests_when_a_numeric/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109n88c/how_to_update_many_unit_tests_when_a_numeric/", "subreddit_subscribers": 836482, "created_utc": 1673489276.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys, I created this website to help data professionals to find jobs across the globe. I hope it helps someone [https://bestdatajobs.com/](https://bestdatajobs.com/).", "author_fullname": "t2_10i9ox", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job board focused on data-related positions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a4upc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673543476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I created this website to help data professionals to find jobs across the globe. I hope it helps someone &lt;a href=\"https://bestdatajobs.com/\"&gt;https://bestdatajobs.com/&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/E6f4YaDXyqldIIBAv3yB-4VsZWypEBIljQeS4xvd92A.jpg?auto=webp&amp;v=enabled&amp;s=7d33e0d0617204d2c5b7f6c085324f96ce32961c", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/E6f4YaDXyqldIIBAv3yB-4VsZWypEBIljQeS4xvd92A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fcad11b0baf1f4ebbb41ef423772d6051c03904d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/E6f4YaDXyqldIIBAv3yB-4VsZWypEBIljQeS4xvd92A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=715edf043dcaff30a192dffd7c964b48b3ed0864", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/E6f4YaDXyqldIIBAv3yB-4VsZWypEBIljQeS4xvd92A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06a2d915073801529b5259a67cd1d4ded3ed53b1", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/E6f4YaDXyqldIIBAv3yB-4VsZWypEBIljQeS4xvd92A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ffed8109990b53c78aa066de24fae92c86aa8133", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/E6f4YaDXyqldIIBAv3yB-4VsZWypEBIljQeS4xvd92A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9a7c8666996e93741be03d88ea20de29de04a0e", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/E6f4YaDXyqldIIBAv3yB-4VsZWypEBIljQeS4xvd92A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3eee43ede5929242c1e1e8a44b0e1120ef0fec88", "width": 1080, "height": 564}], "variants": {}, "id": "jbPfVNFmntGIzd22YlFuLy3hAAqKfIlgxxRvgMUZINU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a4upc", "is_robot_indexable": true, "report_reasons": null, "author": "campostqe", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a4upc/job_board_focused_on_datarelated_positions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a4upc/job_board_focused_on_datarelated_positions/", "subreddit_subscribers": 836482, "created_utc": 1673543476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi. I work as a junior data analyst and started my job recently. I have been asked to do a predictive model for a automobile insurance company.  This is the project :\n\nInsurance pricing policies at XYZ automobile company would like to be reviewed. In order to adjust current pricing policies, the company would like to have models for predicting both frequency and severity of claims. \n\nSolution expected :\n\nPredictive models that can be used for predicting future frequency and severity of claims.  \n\n\nI have past data with me. Can anyone help me how to approach this?", "author_fullname": "t2_t03wuws1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you approach this problem?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109rbom", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673501067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I work as a junior data analyst and started my job recently. I have been asked to do a predictive model for a automobile insurance company.  This is the project :&lt;/p&gt;\n\n&lt;p&gt;Insurance pricing policies at XYZ automobile company would like to be reviewed. In order to adjust current pricing policies, the company would like to have models for predicting both frequency and severity of claims. &lt;/p&gt;\n\n&lt;p&gt;Solution expected :&lt;/p&gt;\n\n&lt;p&gt;Predictive models that can be used for predicting future frequency and severity of claims.  &lt;/p&gt;\n\n&lt;p&gt;I have past data with me. Can anyone help me how to approach this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109rbom", "is_robot_indexable": true, "report_reasons": null, "author": "decisiondengindi", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109rbom/how_would_you_approach_this_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109rbom/how_would_you_approach_this_problem/", "subreddit_subscribers": 836482, "created_utc": 1673501067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have done due diligence and cleaned and removed outliers in my dataset. \n\n*This was not the study I actually did but trying to get an answer conceptually. \n\nIn my data set, I am trying to see if there is a correlation between course certifications and income. \n\nSay I have two sources of \u201ccourse certifications\u201d. For example 1 comes from someone\u2019s linked in and the other their resume\u2019 (not practical I know).\n\nThere is a moderately low positive correlation when looking at both groups of certifications and income. However, the p values for the resume\u2019 certifications are statistically significant while the p values for the linked in certifications are not. \n\nWould this indicate that while not strongly correlated, the resume\u2019 certifications are more reliable than the linked in source?", "author_fullname": "t2_4k7lwxqc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Correlation Question (Beginner)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a0k7n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673532641.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have done due diligence and cleaned and removed outliers in my dataset. &lt;/p&gt;\n\n&lt;p&gt;*This was not the study I actually did but trying to get an answer conceptually. &lt;/p&gt;\n\n&lt;p&gt;In my data set, I am trying to see if there is a correlation between course certifications and income. &lt;/p&gt;\n\n&lt;p&gt;Say I have two sources of \u201ccourse certifications\u201d. For example 1 comes from someone\u2019s linked in and the other their resume\u2019 (not practical I know).&lt;/p&gt;\n\n&lt;p&gt;There is a moderately low positive correlation when looking at both groups of certifications and income. However, the p values for the resume\u2019 certifications are statistically significant while the p values for the linked in certifications are not. &lt;/p&gt;\n\n&lt;p&gt;Would this indicate that while not strongly correlated, the resume\u2019 certifications are more reliable than the linked in source?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a0k7n", "is_robot_indexable": true, "report_reasons": null, "author": "Data_rulez", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a0k7n/correlation_question_beginner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a0k7n/correlation_question_beginner/", "subreddit_subscribers": 836482, "created_utc": 1673532641.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Just 2 simple questions :\n\n1) Is data cleaning the most annoying part of the process?\n\n2) What alternative method do you use to clean your data other than pandas and Excel formulae ?", "author_fullname": "t2_4yke0pyg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi data dudes! Lemme know what you think...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a24yx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673536832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just 2 simple questions :&lt;/p&gt;\n\n&lt;p&gt;1) Is data cleaning the most annoying part of the process?&lt;/p&gt;\n\n&lt;p&gt;2) What alternative method do you use to clean your data other than pandas and Excel formulae ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a24yx", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical-You4014", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a24yx/hi_data_dudes_lemme_know_what_you_think/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a24yx/hi_data_dudes_lemme_know_what_you_think/", "subreddit_subscribers": 836482, "created_utc": 1673536832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_mmzrq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's your recent impact in your organization as a DS? What problem did you solve?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109o36f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673491581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109o36f", "is_robot_indexable": true, "report_reasons": null, "author": "ndemir", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109o36f/whats_your_recent_impact_in_your_organization_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109o36f/whats_your_recent_impact_in_your_organization_as/", "subreddit_subscribers": 836482, "created_utc": 1673491581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want it all, the eternal power of both worlds. I want great notebooks with latex, ml, data...\n\nBut nothing i tried gave me the complete feeling.\n\nVs code =&gt; Unfinished. Cannot export R to pdf, R is just a hassle to work with, no good variable explorer, kernel browser is just bad. Cannot see R variables, Too much options and so little documentation.\n\nJupyter =&gt; No documented code completion.\n\nRstudio =&gt; No documented code completion for python.\n\nJetbrains... F jetbucks.\n\nWhat do i do?", "author_fullname": "t2_4qvdgg7g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Im trying to have both R &amp; Python in one editor for notebooks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109hv6c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673476446.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673475661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want it all, the eternal power of both worlds. I want great notebooks with latex, ml, data...&lt;/p&gt;\n\n&lt;p&gt;But nothing i tried gave me the complete feeling.&lt;/p&gt;\n\n&lt;p&gt;Vs code =&amp;gt; Unfinished. Cannot export R to pdf, R is just a hassle to work with, no good variable explorer, kernel browser is just bad. Cannot see R variables, Too much options and so little documentation.&lt;/p&gt;\n\n&lt;p&gt;Jupyter =&amp;gt; No documented code completion.&lt;/p&gt;\n\n&lt;p&gt;Rstudio =&amp;gt; No documented code completion for python.&lt;/p&gt;\n\n&lt;p&gt;Jetbrains... F jetbucks.&lt;/p&gt;\n\n&lt;p&gt;What do i do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109hv6c", "is_robot_indexable": true, "report_reasons": null, "author": "Rootsyl", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109hv6c/im_trying_to_have_both_r_python_in_one_editor_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109hv6c/im_trying_to_have_both_r_python_in_one_editor_for/", "subreddit_subscribers": 836482, "created_utc": 1673475661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am tasked with completing the creation/analysis of a database from a dataset I can find online. This is for my masters in data science. I want to use this project as a way to showcase my skills for hiring managers. Does anyone have any opinions on what data set analysis would impress them as a hiring manager? Or a topic idea?", "author_fullname": "t2_dz6ua8ns", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database Project Ideas that would impress you as a hiring manager:", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109ztfa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673530562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am tasked with completing the creation/analysis of a database from a dataset I can find online. This is for my masters in data science. I want to use this project as a way to showcase my skills for hiring managers. Does anyone have any opinions on what data set analysis would impress them as a hiring manager? Or a topic idea?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109ztfa", "is_robot_indexable": true, "report_reasons": null, "author": "Effective-Guava8142", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109ztfa/database_project_ideas_that_would_impress_you_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109ztfa/database_project_ideas_that_would_impress_you_as/", "subreddit_subscribers": 836482, "created_utc": 1673530562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am trying to find a package that is capable of creating simple 3D plots of data held in pandas dataframes with the same ease of use as seaborn. I am trying to teach my coworkers how to do data analysis in python (they only know excel) and i do not want to overwhelm them with the need for coding knowhow so something plug-and-play like seaborn would be ideal. I have been searching high and low but i cannot find anything suitable. Vispy seems to be the best candidate so far but it's a little too powerful for what we need. I am almost considering adapting seaborn scatterplot function for our needs with mplot3d but that's gonna be debug hell so i would like to avoid it if possible.", "author_fullname": "t2_9x6sdtq7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anybody know an easy high-level plotting package in python similar to seaborn that is capable of making 3d scatterplots?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a2lwa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673538034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to find a package that is capable of creating simple 3D plots of data held in pandas dataframes with the same ease of use as seaborn. I am trying to teach my coworkers how to do data analysis in python (they only know excel) and i do not want to overwhelm them with the need for coding knowhow so something plug-and-play like seaborn would be ideal. I have been searching high and low but i cannot find anything suitable. Vispy seems to be the best candidate so far but it&amp;#39;s a little too powerful for what we need. I am almost considering adapting seaborn scatterplot function for our needs with mplot3d but that&amp;#39;s gonna be debug hell so i would like to avoid it if possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a2lwa", "is_robot_indexable": true, "report_reasons": null, "author": "vp_port", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a2lwa/does_anybody_know_an_easy_highlevel_plotting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a2lwa/does_anybody_know_an_easy_highlevel_plotting/", "subreddit_subscribers": 836482, "created_utc": 1673538034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I said easiest doesn't imply I think it is necessarily easy, just relatively easy lol.  \nAlso I'm doing a Phd in data science &amp; trying to do AI research. Stories appreciated.", "author_fullname": "t2_qrw52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the easiest way to become rich as a data scientist/AI researcher?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10aal6u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673557295.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I said easiest doesn&amp;#39;t imply I think it is necessarily easy, just relatively easy lol.&lt;br/&gt;\nAlso I&amp;#39;m doing a Phd in data science &amp;amp; trying to do AI research. Stories appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10aal6u", "is_robot_indexable": true, "report_reasons": null, "author": "Udon_noodles", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10aal6u/what_is_the_easiest_way_to_become_rich_as_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10aal6u/what_is_the_easiest_way_to_become_rich_as_a_data/", "subreddit_subscribers": 836482, "created_utc": 1673557295.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Best analytics or data science societies or groups to join that are either non-regional or east coast? Want to join and to projects / competitions with friends from grad school but we are all dispersed across the country now. \n\nIk Kaggle but other than that. Something more of a society or group. (: thanks!", "author_fullname": "t2_5njlw0ic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data science / analytics societies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a0ram", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673533171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Best analytics or data science societies or groups to join that are either non-regional or east coast? Want to join and to projects / competitions with friends from grad school but we are all dispersed across the country now. &lt;/p&gt;\n\n&lt;p&gt;Ik Kaggle but other than that. Something more of a society or group. (: thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a0ram", "is_robot_indexable": true, "report_reasons": null, "author": "sunflowerworms", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a0ram/data_science_analytics_societies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a0ram/data_science_analytics_societies/", "subreddit_subscribers": 836482, "created_utc": 1673533171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The ethical agency of AI developers (original research)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109wkgw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_jfttdvv", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "Ethics", "selftext": "ABSTRACT:  Public and academic discourse about the ethics of artificial intelligence, machine learning, and data science has largely focused on the algorithms and the companies deploying them. Little attention has been paid to the ethical agency of the developers. This study is the first of its kind that centers developers in the ethical environment. Semi-structured interviews with 40 developers about the ethics of being a developer revealed more than 20 themes, 3 of which are the subject of this paper: ethics in the occupational ecosystem, developer ethical agency, and the characteristics of an ethical developer. These themes reveal significant gaps between how developers perceive themselves and the reality of their work experiences. Their ethical agency is likewise variable. They have some authority to intervene for ethical reasons in systems they work on, but they\u00a0often do not realize just how many ethical decisions they make. Nonetheless, this study reveals a growing ethical wisdom in this community, one that needs to be surfaced and nurtured by engaging with developers.\n\nEdit: Link to paper: [The ethical agency of AI developers](https://doi.org/10.1007/s43681-022-00256-3)", "author_fullname": "t2_jfttdvv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The ethical agency of AI developers (original research)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/Ethics", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109ud0y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673519298.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673511463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Ethics", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ABSTRACT:  Public and academic discourse about the ethics of artificial intelligence, machine learning, and data science has largely focused on the algorithms and the companies deploying them. Little attention has been paid to the ethical agency of the developers. This study is the first of its kind that centers developers in the ethical environment. Semi-structured interviews with 40 developers about the ethics of being a developer revealed more than 20 themes, 3 of which are the subject of this paper: ethics in the occupational ecosystem, developer ethical agency, and the characteristics of an ethical developer. These themes reveal significant gaps between how developers perceive themselves and the reality of their work experiences. Their ethical agency is likewise variable. They have some authority to intervene for ethical reasons in systems they work on, but they\u00a0often do not realize just how many ethical decisions they make. Nonetheless, this study reveals a growing ethical wisdom in this community, one that needs to be surfaced and nurtured by engaging with developers.&lt;/p&gt;\n\n&lt;p&gt;Edit: Link to paper: &lt;a href=\"https://doi.org/10.1007/s43681-022-00256-3\"&gt;The ethical agency of AI developers&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ekrwspU8dhVO-Gnn-pudTtEvcYQ6xggm-42rEJaxXIg.jpg?auto=webp&amp;v=enabled&amp;s=723fca1d331bfd2943c22c21568c9a8d7123853f", "width": 200, "height": 265}, "resolutions": [{"url": "https://external-preview.redd.it/ekrwspU8dhVO-Gnn-pudTtEvcYQ6xggm-42rEJaxXIg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef16f53d04dae5d95f2b28ba05fcf5974a4a0a61", "width": 108, "height": 143}], "variants": {}, "id": "L7cwVOvzykKHWn5C5uo87ZlaWT1r8sw7LoCX2VHNMvo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qp8u", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109ud0y", "is_robot_indexable": true, "report_reasons": null, "author": "EverPersisting", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/Ethics/comments/109ud0y/the_ethical_agency_of_ai_developers_original/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/Ethics/comments/109ud0y/the_ethical_agency_of_ai_developers_original/", "subreddit_subscribers": 14086, "created_utc": 1673511463.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1673519748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Ethics", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/Ethics/comments/109ud0y/the_ethical_agency_of_ai_developers_original/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ekrwspU8dhVO-Gnn-pudTtEvcYQ6xggm-42rEJaxXIg.jpg?auto=webp&amp;v=enabled&amp;s=723fca1d331bfd2943c22c21568c9a8d7123853f", "width": 200, "height": 265}, "resolutions": [{"url": "https://external-preview.redd.it/ekrwspU8dhVO-Gnn-pudTtEvcYQ6xggm-42rEJaxXIg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef16f53d04dae5d95f2b28ba05fcf5974a4a0a61", "width": 108, "height": 143}], "variants": {}, "id": "L7cwVOvzykKHWn5C5uo87ZlaWT1r8sw7LoCX2VHNMvo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109wkgw", "is_robot_indexable": true, "report_reasons": null, "author": "EverPersisting", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_109ud0y", "author_flair_text_color": null, "permalink": "/r/datascience/comments/109wkgw/the_ethical_agency_of_ai_developers_original/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/Ethics/comments/109ud0y/the_ethical_agency_of_ai_developers_original/", "subreddit_subscribers": 836482, "created_utc": 1673519748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I am currently a graduate student in Computer Science in my final semester. I have 4 months of internship experience as a data scientist at a start-up and a 3.9 GPA. I have many projects under my belt, including a Google Data Analytics certification and strong mathematical skills that qualify me for entry-level roles as a data analyst or scientist. Despite my qualifications, I am having difficulty getting hired for new graduate roles in the US that I applied for in May/June. It's been demotivating to be rejected from every role I apply for. I understand that we are in a recession, but I am wondering if there is something I am missing or if the job market is really this difficult. I have linked my resume for reference, and I am working on getting references, but that has been challenging as well. I would greatly appreciate any suggestions or ideas on how to improve my job search or thoughts about my situation in general.  \n\n\nResume- [https://drive.google.com/file/d/1MMjc\\_RAjCeopMtcVBA0KxKwJLJKDD3mf/view?usp=sharing](https://drive.google.com/file/d/1MMjc_RAjCeopMtcVBA0KxKwJLJKDD3mf/view?usp=sharing)", "author_fullname": "t2_11otp57", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting constant rejections from all data analyst/Scientist positions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109uixy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673512110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a graduate student in Computer Science in my final semester. I have 4 months of internship experience as a data scientist at a start-up and a 3.9 GPA. I have many projects under my belt, including a Google Data Analytics certification and strong mathematical skills that qualify me for entry-level roles as a data analyst or scientist. Despite my qualifications, I am having difficulty getting hired for new graduate roles in the US that I applied for in May/June. It&amp;#39;s been demotivating to be rejected from every role I apply for. I understand that we are in a recession, but I am wondering if there is something I am missing or if the job market is really this difficult. I have linked my resume for reference, and I am working on getting references, but that has been challenging as well. I would greatly appreciate any suggestions or ideas on how to improve my job search or thoughts about my situation in general.  &lt;/p&gt;\n\n&lt;p&gt;Resume- &lt;a href=\"https://drive.google.com/file/d/1MMjc_RAjCeopMtcVBA0KxKwJLJKDD3mf/view?usp=sharing\"&gt;https://drive.google.com/file/d/1MMjc_RAjCeopMtcVBA0KxKwJLJKDD3mf/view?usp=sharing&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bE0KjjegRA2IMBgCWozXOr-aynjGl9NyDHyevrJzKfQ.jpg?auto=webp&amp;v=enabled&amp;s=aa8714799149106b2ad7921755743af46071a13f", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/bE0KjjegRA2IMBgCWozXOr-aynjGl9NyDHyevrJzKfQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=188332de0b4613cfcdec483b7df0db2b807361ba", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/bE0KjjegRA2IMBgCWozXOr-aynjGl9NyDHyevrJzKfQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9623ead6e31834e7031e431640316621a5d4c2a4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/bE0KjjegRA2IMBgCWozXOr-aynjGl9NyDHyevrJzKfQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2639d13090609d64177ff741dd5bed8574d69e5e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/bE0KjjegRA2IMBgCWozXOr-aynjGl9NyDHyevrJzKfQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d389192478d16b3a5e7653d2a57447e05d66491f", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/bE0KjjegRA2IMBgCWozXOr-aynjGl9NyDHyevrJzKfQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b73e74851827193d3e33bd4d16681cb6df7c221", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/bE0KjjegRA2IMBgCWozXOr-aynjGl9NyDHyevrJzKfQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f2731d22d6a9853aded0af778ebf2fb333e863c", "width": 1080, "height": 567}], "variants": {}, "id": "nZofFjNVbWzShnpicKzBug471bVZt6xVOyu5UQaUwGk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "109uixy", "is_robot_indexable": true, "report_reasons": null, "author": "ryanhiga2019", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109uixy/getting_constant_rejections_from_all_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109uixy/getting_constant_rejections_from_all_data/", "subreddit_subscribers": 836482, "created_utc": 1673512110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I want to predict a force plate using plantar pressure. The shape of the force plate data is a 15000x6 array, and the shape of the plantar pressure data is a 15000x89 array. I will use a regression model to predict the force plate data. When collecting data to synchronize the force plate data and plantar pressure, I will do time synchronization between the force plate and plantar pressure app. force plate and plantar pressure data will capture 50 data in 1 second. \n\n Force Plate Data: \n\n Data shape : (15000,6) \n\nhttps://preview.redd.it/y9ql48ijfjba1.png?width=525&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8b553ab76ab8e750e5b5e1106a1b969c0c3d2c25\n\n&amp;#x200B;\n\nhttps://preview.redd.it/w9t3dmvkfjba1.png?width=418&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2bcb75881b3a9c9719c6918e38f2987c9abea966\n\n Plantar Pressure Data : \n\n Data shape : (15000,89) \n\n \n\n    array([[0.0, 0.0, 91.0, 100.0, 74.0, 7.0, 0.0, 0.0, 0.0, 0.0, 45.0, 123.0, 126.0, 3.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 95.0, 104.0, 75.0, 33.0, 0.0, 0.0, 0.0, 0.0, 57.0, 117.0, 123.0, 113.0, 66.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58.0, 67.0, 46.0, 16.0, 0.0, 8.0, 0.0, 15.0, 51.0, 122.0, 122.0, 111.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 25.0, 31.0, 64.0, 82.0, 125.0, 124.0, 114.0, 18.0, 0.0, 0.0, 0.0, 24.0, 56.0, 105.0, 116.0, 124.0, 77.0, 0.0, 0.0, 0.0, 71.0, 61.0, 0.0, 0.0, 3.0, 0.0], \n    [0.0, 0.0, 91.0, 100.0, 74.0, 7.0, 0.0, 0.0, 0.0, 0.0, 45.0, 123.0, 126.0, 3.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 95.0, 104.0, 75.0, 33.0, 0.0, 0.0, 0.0, 0.0, 57.0, 117.0, 123.0, 113.0, 66.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.0, 68.0, 46.0, 12.0, 0.0, 3.0, 0.0, 15.0, 53.0, 124.0, 125.0, 115.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 25.0, 31.0, 64.0, 82.0, 125.0, 124.0, 114.0, 18.0, 0.0, 0.0, 0.0, 24.0, 56.0, 105.0, 116.0, 124.0, 77.0, 0.0, 0.0, 0.0, 71.0, 61.0, 0.0, 0.0, 3.0, 0.0], \n    [0.0, 0.0, 91.0, 100.0, 68.0, 6.0, 0.0, 0.0, 0.0, 0.0, 29.0, 118.0, 120.0, 2.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 89.0, 100.0, 72.0, 32.0, 0.0, 0.0, 0.0, 0.0, 51.0, 113.0, 118.0, 109.0, 61.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58.0, 67.0, 46.0, 16.0, 0.0, 8.0, 0.0, 15.0, 51.0, 122.0, 122.0, 111.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 25.0, 31.0, 64.0, 82.0, 125.0, 124.0, 114.0, 18.0, 0.0, 0.0, 0.0, 24.0, 56.0, 105.0, 116.0, 124.0, 77.0, 0.0, 0.0, 0.0, 71.0, 61.0, 0.0, 0.0, 3.0, 0.0]])\n\n Method 1: \n\nInput Data : (15000,89) \n\nOutput Data : (15000,6) \n\n&amp;#x200B;\n\nhttps://preview.redd.it/ixdl7jfsfjba1.png?width=829&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ce5bccd25493c9ae423d64263688b0a7e27de457\n\nhttps://preview.redd.it/uk9ofjetfjba1.png?width=1638&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3fd5cf29b5fabd4b9af9673b22dc56107bd28420\n\n&amp;#x200B;\n\nhttps://preview.redd.it/hxjzo2cufjba1.png?width=1638&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8aa108c55f6d2446c9021236f6f156b540e7a914\n\n I will get good results on training, but when I try using new data to predict it the results will be bad. then I thought maybe because the plantar pressure data are so similar, it would be hard for the model to recognize it. then I tried to add data sequences to each plantar pressure data. the data sequences that I added are 1-50, the reason I added these data sequences is so that the data on plantar pressure are slightly different from one another so that the model is easier to recognize when I test using new data and I enter 1-50 because every second plantar pressure will capture 50 data. after I added the data sequence to the plantar pressure, the shape of the plantar pressure data is (15000,90) \n\n Plantar Pressure data after adding data sequence: the shape of plantar pressure data before adding data sequence is (15000,89), the shape of plantar pressure data after adding data sequence is (15000,90) \n\nhttps://preview.redd.it/v3kg3h3xfjba1.png?width=1060&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ea1a77d3f8509138b9d17c368963e28d1310a28b\n\n Method 2: \n\nInput Data : (15000,90) \n\nOutput Data : (15000,6) \n\nhttps://preview.redd.it/9wp1ioazfjba1.png?width=829&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cae4ad561e4fa77dcb772be3502a67eb040752c3\n\nhttps://preview.redd.it/s7i9erzzfjba1.png?width=1632&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cb6a85df6bbc4c893fff2e2b04bbbd0d2c09cc3e\n\nhttps://preview.redd.it/oofh8gg0gjba1.png?width=1632&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=89c9eec256e8c7d577ec9f00f3e4ecfd91ad1904\n\n so after I added the sequence data to the plantar pressure data, the prediction results from the training data and testing data will be good. my question is whether this method is allowed in data science? If yes, what method I use is called and is there a reference that is the same as the method I used in existing research", "author_fullname": "t2_end0qlqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding Data sequences as unique data on dataset for regression model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "media_metadata": {"uk9ofjetfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5437739b475963a5a9f6195ad5f4858a294c756"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59acc5363ccf0a2b6c853b174b95c04e9a6e957c"}, {"y": 139, "x": 320, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aedce6935dfe7ebae88c4cf85881421911367ab3"}, {"y": 278, "x": 640, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f69f36228ea203026217e45d0ef6116466057254"}, {"y": 418, "x": 960, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08d17bbcbf3b1d6ea42f95c34ed10c402ae62bf6"}, {"y": 470, "x": 1080, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=070600d7a05db4e89ad8b6c42304e1b37bf3b236"}], "s": {"y": 714, "x": 1638, "u": "https://preview.redd.it/uk9ofjetfjba1.png?width=1638&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3fd5cf29b5fabd4b9af9673b22dc56107bd28420"}, "id": "uk9ofjetfjba1"}, "9wp1ioazfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 75, "x": 108, "u": "https://preview.redd.it/9wp1ioazfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f98b7193abad8309d06286f3f23b17ed2cd9a2eb"}, {"y": 151, "x": 216, "u": "https://preview.redd.it/9wp1ioazfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af2d7ad6c97168e834b23f1f83873a0dc4421963"}, {"y": 223, "x": 320, "u": "https://preview.redd.it/9wp1ioazfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f0aae23c07021179cc1420f65c2e92364a2020d"}, {"y": 447, "x": 640, "u": "https://preview.redd.it/9wp1ioazfjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f00a5ea38ceee17a37a9166d65be16e945590e2"}], "s": {"y": 580, "x": 829, "u": "https://preview.redd.it/9wp1ioazfjba1.png?width=829&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cae4ad561e4fa77dcb772be3502a67eb040752c3"}, "id": "9wp1ioazfjba1"}, "s7i9erzzfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2221ea7e415680ea01b12bcc384108a1c1bf8486"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3889641995ee00f85dc7ae75e4d9c77115c5bd96"}, {"y": 140, "x": 320, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8dbfb07965a5aa25f3b53bc77bb58334e26f00af"}, {"y": 280, "x": 640, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e56c952755c04954d8f12d2e22c2cfff03d9c0e3"}, {"y": 420, "x": 960, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46006ea6b759f65446c884c89c1dcb11d0de87fb"}, {"y": 472, "x": 1080, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fef61624cb8b3053df053fdf438c99e2a788956"}], "s": {"y": 714, "x": 1632, "u": "https://preview.redd.it/s7i9erzzfjba1.png?width=1632&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cb6a85df6bbc4c893fff2e2b04bbbd0d2c09cc3e"}, "id": "s7i9erzzfjba1"}, "ixdl7jfsfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 75, "x": 108, "u": "https://preview.redd.it/ixdl7jfsfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=383e993d42856a48839c77616026afcc62cdb268"}, {"y": 151, "x": 216, "u": "https://preview.redd.it/ixdl7jfsfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=488b5e63286b16749750237af30ddac958e01747"}, {"y": 223, "x": 320, "u": "https://preview.redd.it/ixdl7jfsfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dcc2db8fab04d162e4f7b739d98f4d33ad625a1"}, {"y": 447, "x": 640, "u": "https://preview.redd.it/ixdl7jfsfjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a4d92dc8333fffe1b9be699eba07143d34ea4be"}], "s": {"y": 580, "x": 829, "u": "https://preview.redd.it/ixdl7jfsfjba1.png?width=829&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ce5bccd25493c9ae423d64263688b0a7e27de457"}, "id": "ixdl7jfsfjba1"}, "y9ql48ijfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/y9ql48ijfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd11ccd7f7b08c6d1cd09c32d3afed55996f9177"}, {"y": 132, "x": 216, "u": "https://preview.redd.it/y9ql48ijfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2bef9a1f88b06013b82062a3ceff7e724144b8fe"}, {"y": 196, "x": 320, "u": "https://preview.redd.it/y9ql48ijfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eca434b59b30236ab127c6e11dbdfc906e80a1bb"}], "s": {"y": 322, "x": 525, "u": "https://preview.redd.it/y9ql48ijfjba1.png?width=525&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8b553ab76ab8e750e5b5e1106a1b969c0c3d2c25"}, "id": "y9ql48ijfjba1"}, "hxjzo2cufjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd9cbbae89b169c354a714f2bd3a50aa389bad97"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad5362f1d88f504745b1114d92a0638c5bf0add2"}, {"y": 139, "x": 320, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d385d8e6ae8d0b2203c6b0df2d52727326d64de"}, {"y": 278, "x": 640, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d4a1b3822416f9a43e4729eadce3709473dac77"}, {"y": 418, "x": 960, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36a4aadb5f813c1c440cdda91ffc32d07d6b9185"}, {"y": 470, "x": 1080, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=964cf2b4be6a2fc5f89a62b4cd380be539d8e2b1"}], "s": {"y": 714, "x": 1638, "u": "https://preview.redd.it/hxjzo2cufjba1.png?width=1638&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8aa108c55f6d2446c9021236f6f156b540e7a914"}, "id": "hxjzo2cufjba1"}, "w9t3dmvkfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 87, "x": 108, "u": "https://preview.redd.it/w9t3dmvkfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f722acd865ec29b3c6a6ff7bd4ea8bd901c17ce"}, {"y": 175, "x": 216, "u": "https://preview.redd.it/w9t3dmvkfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eaa46adece19735e43d9943f8645e574791bd75d"}, {"y": 260, "x": 320, "u": "https://preview.redd.it/w9t3dmvkfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=28143dc4c17ab39ed309bddd64bcf83ed14294c8"}], "s": {"y": 340, "x": 418, "u": "https://preview.redd.it/w9t3dmvkfjba1.png?width=418&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2bcb75881b3a9c9719c6918e38f2987c9abea966"}, "id": "w9t3dmvkfjba1"}, "v3kg3h3xfjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 29, "x": 108, "u": "https://preview.redd.it/v3kg3h3xfjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37fda3f21e43fb3bc32070e8e2f0d30d77584485"}, {"y": 58, "x": 216, "u": "https://preview.redd.it/v3kg3h3xfjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1644212683a9dfce1efa9314d486bd70232b0914"}, {"y": 86, "x": 320, "u": "https://preview.redd.it/v3kg3h3xfjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e19d6148083ef1e9801d9d97c88f87c1d2d1e5e"}, {"y": 172, "x": 640, "u": "https://preview.redd.it/v3kg3h3xfjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f67a719f355c48944e093456bacbaa865c5e15e"}, {"y": 258, "x": 960, "u": "https://preview.redd.it/v3kg3h3xfjba1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46a8ca89ef0bebaee58d1c85af71c4b84ea2f56b"}], "s": {"y": 285, "x": 1060, "u": "https://preview.redd.it/v3kg3h3xfjba1.png?width=1060&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ea1a77d3f8509138b9d17c368963e28d1310a28b"}, "id": "v3kg3h3xfjba1"}, "oofh8gg0gjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1287933e3022d96a75b8525c443bfe4c7c3f21c4"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=17264be14523f039eca041f58d9d6cdf9b2d8555"}, {"y": 140, "x": 320, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4a0992394be6a09976834f07ebc2d9619fffcc6"}, {"y": 280, "x": 640, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77640857ed93a43643569cb9c4018e7c30a9441f"}, {"y": 420, "x": 960, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=119bbf2b1839ecb6a0251b95427b63c62d7b6926"}, {"y": 472, "x": 1080, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4fe7e8b05ad82c3bf7d2ae3604e85479534566a3"}], "s": {"y": 714, "x": 1632, "u": "https://preview.redd.it/oofh8gg0gjba1.png?width=1632&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=89c9eec256e8c7d577ec9f00f3e4ecfd91ad1904"}, "id": "oofh8gg0gjba1"}}, "name": "t3_109pzie", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/s8FQzbwalBWYlLGJrCiyUxOnKlwTeHA-dc8hwFNpXBw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673496990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to predict a force plate using plantar pressure. The shape of the force plate data is a 15000x6 array, and the shape of the plantar pressure data is a 15000x89 array. I will use a regression model to predict the force plate data. When collecting data to synchronize the force plate data and plantar pressure, I will do time synchronization between the force plate and plantar pressure app. force plate and plantar pressure data will capture 50 data in 1 second. &lt;/p&gt;\n\n&lt;p&gt;Force Plate Data: &lt;/p&gt;\n\n&lt;p&gt;Data shape : (15000,6) &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/y9ql48ijfjba1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8b553ab76ab8e750e5b5e1106a1b969c0c3d2c25\"&gt;https://preview.redd.it/y9ql48ijfjba1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8b553ab76ab8e750e5b5e1106a1b969c0c3d2c25&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/w9t3dmvkfjba1.png?width=418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2bcb75881b3a9c9719c6918e38f2987c9abea966\"&gt;https://preview.redd.it/w9t3dmvkfjba1.png?width=418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2bcb75881b3a9c9719c6918e38f2987c9abea966&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Plantar Pressure Data : &lt;/p&gt;\n\n&lt;p&gt;Data shape : (15000,89) &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;array([[0.0, 0.0, 91.0, 100.0, 74.0, 7.0, 0.0, 0.0, 0.0, 0.0, 45.0, 123.0, 126.0, 3.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 95.0, 104.0, 75.0, 33.0, 0.0, 0.0, 0.0, 0.0, 57.0, 117.0, 123.0, 113.0, 66.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58.0, 67.0, 46.0, 16.0, 0.0, 8.0, 0.0, 15.0, 51.0, 122.0, 122.0, 111.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 25.0, 31.0, 64.0, 82.0, 125.0, 124.0, 114.0, 18.0, 0.0, 0.0, 0.0, 24.0, 56.0, 105.0, 116.0, 124.0, 77.0, 0.0, 0.0, 0.0, 71.0, 61.0, 0.0, 0.0, 3.0, 0.0], \n[0.0, 0.0, 91.0, 100.0, 74.0, 7.0, 0.0, 0.0, 0.0, 0.0, 45.0, 123.0, 126.0, 3.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 95.0, 104.0, 75.0, 33.0, 0.0, 0.0, 0.0, 0.0, 57.0, 117.0, 123.0, 113.0, 66.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.0, 68.0, 46.0, 12.0, 0.0, 3.0, 0.0, 15.0, 53.0, 124.0, 125.0, 115.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 25.0, 31.0, 64.0, 82.0, 125.0, 124.0, 114.0, 18.0, 0.0, 0.0, 0.0, 24.0, 56.0, 105.0, 116.0, 124.0, 77.0, 0.0, 0.0, 0.0, 71.0, 61.0, 0.0, 0.0, 3.0, 0.0], \n[0.0, 0.0, 91.0, 100.0, 68.0, 6.0, 0.0, 0.0, 0.0, 0.0, 29.0, 118.0, 120.0, 2.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 89.0, 100.0, 72.0, 32.0, 0.0, 0.0, 0.0, 0.0, 51.0, 113.0, 118.0, 109.0, 61.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58.0, 67.0, 46.0, 16.0, 0.0, 8.0, 0.0, 15.0, 51.0, 122.0, 122.0, 111.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 25.0, 31.0, 64.0, 82.0, 125.0, 124.0, 114.0, 18.0, 0.0, 0.0, 0.0, 24.0, 56.0, 105.0, 116.0, 124.0, 77.0, 0.0, 0.0, 0.0, 71.0, 61.0, 0.0, 0.0, 3.0, 0.0]])\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Method 1: &lt;/p&gt;\n\n&lt;p&gt;Input Data : (15000,89) &lt;/p&gt;\n\n&lt;p&gt;Output Data : (15000,6) &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ixdl7jfsfjba1.png?width=829&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ce5bccd25493c9ae423d64263688b0a7e27de457\"&gt;https://preview.redd.it/ixdl7jfsfjba1.png?width=829&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ce5bccd25493c9ae423d64263688b0a7e27de457&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uk9ofjetfjba1.png?width=1638&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3fd5cf29b5fabd4b9af9673b22dc56107bd28420\"&gt;https://preview.redd.it/uk9ofjetfjba1.png?width=1638&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3fd5cf29b5fabd4b9af9673b22dc56107bd28420&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hxjzo2cufjba1.png?width=1638&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8aa108c55f6d2446c9021236f6f156b540e7a914\"&gt;https://preview.redd.it/hxjzo2cufjba1.png?width=1638&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8aa108c55f6d2446c9021236f6f156b540e7a914&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I will get good results on training, but when I try using new data to predict it the results will be bad. then I thought maybe because the plantar pressure data are so similar, it would be hard for the model to recognize it. then I tried to add data sequences to each plantar pressure data. the data sequences that I added are 1-50, the reason I added these data sequences is so that the data on plantar pressure are slightly different from one another so that the model is easier to recognize when I test using new data and I enter 1-50 because every second plantar pressure will capture 50 data. after I added the data sequence to the plantar pressure, the shape of the plantar pressure data is (15000,90) &lt;/p&gt;\n\n&lt;p&gt;Plantar Pressure data after adding data sequence: the shape of plantar pressure data before adding data sequence is (15000,89), the shape of plantar pressure data after adding data sequence is (15000,90) &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v3kg3h3xfjba1.png?width=1060&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ea1a77d3f8509138b9d17c368963e28d1310a28b\"&gt;https://preview.redd.it/v3kg3h3xfjba1.png?width=1060&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ea1a77d3f8509138b9d17c368963e28d1310a28b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Method 2: &lt;/p&gt;\n\n&lt;p&gt;Input Data : (15000,90) &lt;/p&gt;\n\n&lt;p&gt;Output Data : (15000,6) &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9wp1ioazfjba1.png?width=829&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cae4ad561e4fa77dcb772be3502a67eb040752c3\"&gt;https://preview.redd.it/9wp1ioazfjba1.png?width=829&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cae4ad561e4fa77dcb772be3502a67eb040752c3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s7i9erzzfjba1.png?width=1632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cb6a85df6bbc4c893fff2e2b04bbbd0d2c09cc3e\"&gt;https://preview.redd.it/s7i9erzzfjba1.png?width=1632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cb6a85df6bbc4c893fff2e2b04bbbd0d2c09cc3e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/oofh8gg0gjba1.png?width=1632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=89c9eec256e8c7d577ec9f00f3e4ecfd91ad1904\"&gt;https://preview.redd.it/oofh8gg0gjba1.png?width=1632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=89c9eec256e8c7d577ec9f00f3e4ecfd91ad1904&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;so after I added the sequence data to the plantar pressure data, the prediction results from the training data and testing data will be good. my question is whether this method is allowed in data science? If yes, what method I use is called and is there a reference that is the same as the method I used in existing research&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109pzie", "is_robot_indexable": true, "report_reasons": null, "author": "Constant-Cranberry29", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109pzie/adding_data_sequences_as_unique_data_on_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109pzie/adding_data_sequences_as_unique_data_on_dataset/", "subreddit_subscribers": 836482, "created_utc": 1673496990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "i want to give myself opportunity for advancement in the future, would a management degree be worth it? if not, what other classes should i take?", "author_fullname": "t2_4oribc7j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is it worth it to double major in data science and management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10aakqf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673557268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i want to give myself opportunity for advancement in the future, would a management degree be worth it? if not, what other classes should i take?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10aakqf", "is_robot_indexable": true, "report_reasons": null, "author": "RyanHowardKapoor", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10aakqf/is_it_worth_it_to_double_major_in_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10aakqf/is_it_worth_it_to_double_major_in_data_science/", "subreddit_subscribers": 836482, "created_utc": 1673557268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Currently solving a transportation problem in CPLEX, need to implement a constraint that warehouse one can no longer serve customers 1-5.\n\nAny suggestions?", "author_fullname": "t2_e287cydl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transport problem constraint", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10a9ynx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673555806.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently solving a transportation problem in CPLEX, need to implement a constraint that warehouse one can no longer serve customers 1-5.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a9ynx", "is_robot_indexable": true, "report_reasons": null, "author": "Applesoranges124", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a9ynx/transport_problem_constraint/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a9ynx/transport_problem_constraint/", "subreddit_subscribers": 836482, "created_utc": 1673555806.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Just recently, I wrote up a guide on how to use [ChatGPT to build a website with Replit](https://buildspace.so/notes/chatgpt-replit-website?utm_source=r).\n\nGot some pretty good responses, so I decided to write + document more of the applications I'm discovering.\n\n**I'm actually really excited about this one, since I was in a graduate program for statistics.**\n\n[Here's the guide](https://buildspace.so/notes/chatgpt-data-science?utm_source=r) for doing data sci with ChatGPT\n\nThe tl;dr is that I show you some of the crazy data sci stuff ChatGPT can do:\n\n\\- Read and analyze raw CSV data. I just had to copy and paste.\n\n\\- It could tell what kind of data you're feeding it judging by the header columns!\n\n\\- It will give you the python/r code on how to run specific analysis.\n\n\\- It even knew how to use scikit-learn to run regression models \ud83e\udd2f (I mean, this makes sense since it's an AI tool lol).\n\nHonestly, this is just crazy to me.\n\n**Before I dropped out of graduate school for statistics, I often consulted non-technical researchers in the social sciences. It was always a pain for them to run datasets by themselves just to get some answers to their questions.**\n\nAlthough ChatGPT isn't perfect (and does make mistakes), it's crazy where the tool is going.\n\nI think this is really good news for a lot of people who are interested in doing research, but might feel too intimidated by needing to do stats. Obvi...some bad stuff could come from it. We'll see!\n\nhttps://preview.redd.it/ggd96gyhnnba1.png?width=619&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=336d66a381cceb0befe1614d221694d7a831ab31", "author_fullname": "t2_rp0am0gy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I wrote up a guide showing how to do Data Science with ChatGPT.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ggd96gyhnnba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 142, "x": 108, "u": "https://preview.redd.it/ggd96gyhnnba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5779bbd697a6c99463bb440c6215ad916a691f83"}, {"y": 285, "x": 216, "u": "https://preview.redd.it/ggd96gyhnnba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a482afc006aedd53f5e6eafc6e44eb95caf1282"}, {"y": 423, "x": 320, "u": "https://preview.redd.it/ggd96gyhnnba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=877afb8c364964484b19f5acfba0464008535d21"}], "s": {"y": 819, "x": 619, "u": "https://preview.redd.it/ggd96gyhnnba1.png?width=619&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=336d66a381cceb0befe1614d221694d7a831ab31"}, "id": "ggd96gyhnnba1"}}, "name": "t3_10a7kq4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.52, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NwGBiFzvMKnAVlPoLgKzu6aUKtRriWzQ7RZ_Nv6Q2Fw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1673550084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just recently, I wrote up a guide on how to use &lt;a href=\"https://buildspace.so/notes/chatgpt-replit-website?utm_source=r\"&gt;ChatGPT to build a website with Replit&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Got some pretty good responses, so I decided to write + document more of the applications I&amp;#39;m discovering.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I&amp;#39;m actually really excited about this one, since I was in a graduate program for statistics.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://buildspace.so/notes/chatgpt-data-science?utm_source=r\"&gt;Here&amp;#39;s the guide&lt;/a&gt; for doing data sci with ChatGPT&lt;/p&gt;\n\n&lt;p&gt;The tl;dr is that I show you some of the crazy data sci stuff ChatGPT can do:&lt;/p&gt;\n\n&lt;p&gt;- Read and analyze raw CSV data. I just had to copy and paste.&lt;/p&gt;\n\n&lt;p&gt;- It could tell what kind of data you&amp;#39;re feeding it judging by the header columns!&lt;/p&gt;\n\n&lt;p&gt;- It will give you the python/r code on how to run specific analysis.&lt;/p&gt;\n\n&lt;p&gt;- It even knew how to use scikit-learn to run regression models \ud83e\udd2f (I mean, this makes sense since it&amp;#39;s an AI tool lol).&lt;/p&gt;\n\n&lt;p&gt;Honestly, this is just crazy to me.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Before I dropped out of graduate school for statistics, I often consulted non-technical researchers in the social sciences. It was always a pain for them to run datasets by themselves just to get some answers to their questions.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Although ChatGPT isn&amp;#39;t perfect (and does make mistakes), it&amp;#39;s crazy where the tool is going.&lt;/p&gt;\n\n&lt;p&gt;I think this is really good news for a lot of people who are interested in doing research, but might feel too intimidated by needing to do stats. Obvi...some bad stuff could come from it. We&amp;#39;ll see!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ggd96gyhnnba1.png?width=619&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=336d66a381cceb0befe1614d221694d7a831ab31\"&gt;https://preview.redd.it/ggd96gyhnnba1.png?width=619&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=336d66a381cceb0befe1614d221694d7a831ab31&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bMmYVU5qbeHyacqdcjNNX_6ifH0w5YFDI-tqDoWS5H0.jpg?auto=webp&amp;v=enabled&amp;s=8ec4b54e7f1288e29fcab0cfe66782742f6f96a1", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/bMmYVU5qbeHyacqdcjNNX_6ifH0w5YFDI-tqDoWS5H0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0690be4f8d9aefcca0fdfcf5f070835826b39112", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/bMmYVU5qbeHyacqdcjNNX_6ifH0w5YFDI-tqDoWS5H0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d566c1d3b4ead76c66dbf4f0484c5d0a6122db8c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/bMmYVU5qbeHyacqdcjNNX_6ifH0w5YFDI-tqDoWS5H0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=548576729a71e074d0bc3f5fb8010d06f912b0dd", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/bMmYVU5qbeHyacqdcjNNX_6ifH0w5YFDI-tqDoWS5H0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0f539d22c093992d6cadaa5f696305a1f21683a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/bMmYVU5qbeHyacqdcjNNX_6ifH0w5YFDI-tqDoWS5H0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7267705d81cb9ecbefb47a767bcc8b7beaddb9f9", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/bMmYVU5qbeHyacqdcjNNX_6ifH0w5YFDI-tqDoWS5H0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1cc369d3bf529f1ad4a03bab1d4dc5348716f0b", "width": 1080, "height": 567}], "variants": {}, "id": "37JQnsF20Ok1m7gNTdPyXwo10tviNPSdLDxDrQ5FXbs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a7kq4", "is_robot_indexable": true, "report_reasons": null, "author": "Own-Anteater4164", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a7kq4/i_wrote_up_a_guide_showing_how_to_do_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a7kq4/i_wrote_up_a_guide_showing_how_to_do_data_science/", "subreddit_subscribers": 836482, "created_utc": 1673550084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "II\u2019m a grad student who works with remote sensing data and I\u2019m planning on transitioning into a data science role. I usually use Matlab but I know some Python. \n\nMy current issue is that my laptop sucks. It\u2019s a 2018 Macbook Pro but it\u2019s extremely slow and inefficient. What takes my PC that I use for research a day to run will take my laptop a couple of days. Or Matlab ends up crashing. My advisor told me its because Matlab doesn\u2019t work well with Macbooks but idk if thats true. Anyway, I just received this notification that my google drive isn\u2019t able to connect to my laptop anymore because the software is old, and I need the google drive for classes and other research needs. But for some reason, I can\u2019t update my software either. I\u2019m taking this as a sign that I need a new laptop. \n\nFor purposes of learning data science, am I better off getting an ok-ish laptop or should I splurge on a high performing laptop?", "author_fullname": "t2_h6gv3yd2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it worth it to splurge on a good laptop or should I settle for an ok computer if I\u2019m just learning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a6gam", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673547389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;II\u2019m a grad student who works with remote sensing data and I\u2019m planning on transitioning into a data science role. I usually use Matlab but I know some Python. &lt;/p&gt;\n\n&lt;p&gt;My current issue is that my laptop sucks. It\u2019s a 2018 Macbook Pro but it\u2019s extremely slow and inefficient. What takes my PC that I use for research a day to run will take my laptop a couple of days. Or Matlab ends up crashing. My advisor told me its because Matlab doesn\u2019t work well with Macbooks but idk if thats true. Anyway, I just received this notification that my google drive isn\u2019t able to connect to my laptop anymore because the software is old, and I need the google drive for classes and other research needs. But for some reason, I can\u2019t update my software either. I\u2019m taking this as a sign that I need a new laptop. &lt;/p&gt;\n\n&lt;p&gt;For purposes of learning data science, am I better off getting an ok-ish laptop or should I splurge on a high performing laptop?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a6gam", "is_robot_indexable": true, "report_reasons": null, "author": "Rude-Illustrator-884", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a6gam/is_it_worth_it_to_splurge_on_a_good_laptop_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a6gam/is_it_worth_it_to_splurge_on_a_good_laptop_or/", "subreddit_subscribers": 836482, "created_utc": 1673547389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello guys,\n\nJust looking for some advice. I'm working in a Fintech right now, and I am very advanced in a process for a very well known Pharmaceutical company. Do you think that Data Science is relevant in that sector? I like what I'm doing right now, the position seems similar on paper but I don't know if it's going to be more BI centric than DS. If any of you have some experience on the sector I would like to read what kind of problems are you tackling with DS.\n\nThanks.", "author_fullname": "t2_qlc00nj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning from Fintech to Pharma", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a2kn4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673537947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;/p&gt;\n\n&lt;p&gt;Just looking for some advice. I&amp;#39;m working in a Fintech right now, and I am very advanced in a process for a very well known Pharmaceutical company. Do you think that Data Science is relevant in that sector? I like what I&amp;#39;m doing right now, the position seems similar on paper but I don&amp;#39;t know if it&amp;#39;s going to be more BI centric than DS. If any of you have some experience on the sector I would like to read what kind of problems are you tackling with DS.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a2kn4", "is_robot_indexable": true, "report_reasons": null, "author": "lautaromgo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a2kn4/transitioning_from_fintech_to_pharma/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a2kn4/transitioning_from_fintech_to_pharma/", "subreddit_subscribers": 836482, "created_utc": 1673537947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_a0oh9j1x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Analytics Trends You Need To Know (2023) - more aimed at data analytics, but there are some useful insight for data scientists too!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_109zsr6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Fv0dlGGIKTQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Analytics Trends You Need To Know (2023)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Analytics Trends You Need To Know (2023)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Fv0dlGGIKTQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Analytics Trends You Need To Know (2023)\"&gt;&lt;/iframe&gt;", "author_name": "CareerFoundry", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Fv0dlGGIKTQ/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@careerfoundry"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Fv0dlGGIKTQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Analytics Trends You Need To Know (2023)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/109zsr6", "height": 200}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ggCFh5hgsjGjJmHjERm-YN0ggPiAlIzlDF-54LIglT4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673530508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/Fv0dlGGIKTQ", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JQIa9M7bNHKvqNmdML3EKFA5QWvdZ4xAinB8hiFmy5c.jpg?auto=webp&amp;v=enabled&amp;s=8c7f5c22206296758c15e8197ce680f6d8607f5e", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/JQIa9M7bNHKvqNmdML3EKFA5QWvdZ4xAinB8hiFmy5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d26a17cf53f6ac0e7b9d1ff47c1faee5e523548", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/JQIa9M7bNHKvqNmdML3EKFA5QWvdZ4xAinB8hiFmy5c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4dbadb482580ec1c57f3fcd49ababdac6d19f138", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/JQIa9M7bNHKvqNmdML3EKFA5QWvdZ4xAinB8hiFmy5c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=79fae16cc3acc39c9fa312bfdf1b5d5cea01fc96", "width": 320, "height": 240}], "variants": {}, "id": "c9MNbTtFpBZLoOhDahSwjo7TsPHiLTySu0VZTFMw2Lo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "109zsr6", "is_robot_indexable": true, "report_reasons": null, "author": "Inevitable-Narwhal15", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109zsr6/data_analytics_trends_you_need_to_know_2023_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/Fv0dlGGIKTQ", "subreddit_subscribers": 836482, "created_utc": 1673530508.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Analytics Trends You Need To Know (2023)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Fv0dlGGIKTQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Analytics Trends You Need To Know (2023)\"&gt;&lt;/iframe&gt;", "author_name": "CareerFoundry", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Fv0dlGGIKTQ/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@careerfoundry"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_74f3kq8g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "lots of tutorials on sentiment analysis but seems like no one goes from analysis to insights", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109lfsq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673484536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "109lfsq", "is_robot_indexable": true, "report_reasons": null, "author": "Character-Education3", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/109lfsq/lots_of_tutorials_on_sentiment_analysis_but_seems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/109lfsq/lots_of_tutorials_on_sentiment_analysis_but_seems/", "subreddit_subscribers": 836482, "created_utc": 1673484536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_qzy7otr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The true reason I chose to be a DS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "name": "t3_10a8tcr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4QE0XomqiNyHMnE_FDc_JP90XBhAxewLnC9FKndSrvE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673553069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/sfgoob5ikpba1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/sfgoob5ikpba1.jpg?auto=webp&amp;v=enabled&amp;s=e7761f151f3f04a5c00e62627df7188b919c239e", "width": 1000, "height": 500}, "resolutions": [{"url": "https://preview.redd.it/sfgoob5ikpba1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e59990c9c143c7ed6352b47820cde180e4aa78a0", "width": 108, "height": 54}, {"url": "https://preview.redd.it/sfgoob5ikpba1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82385dc1a0a41ad72cf1eea28209607b0a1a3acd", "width": 216, "height": 108}, {"url": "https://preview.redd.it/sfgoob5ikpba1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6c866f0e1e95eac371a4547ce617eed1c4f3231", "width": 320, "height": 160}, {"url": "https://preview.redd.it/sfgoob5ikpba1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d9f70f9bcb5d5434a18eaa48ae9acfac03db5f8", "width": 640, "height": 320}, {"url": "https://preview.redd.it/sfgoob5ikpba1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad48a191030fb1ddae5b8f5e62a5f282f565888b", "width": 960, "height": 480}], "variants": {}, "id": "8E-3SqUDrB68lVpGEqn6wzJL-BAnHeE6DZPk9xZ8yCw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a8tcr", "is_robot_indexable": true, "report_reasons": null, "author": "karaposu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a8tcr/the_true_reason_i_chose_to_be_a_ds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/sfgoob5ikpba1.jpg", "subreddit_subscribers": 836482, "created_utc": 1673553069.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\nHi Reddit,\n\nI\u2019m the new grad working for a very early stage startup. The CTO wants me to help him create a data lake and program a bot or something that will automate data pre-processing within the data lake. \n\nIs it possible or appropriate to automate data pre-processing? As I understand it, all datasets are different and would require careful analysis and bespoke treatment/manipulation in order to make it suitable for analysis?", "author_fullname": "t2_6dhdycki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can a bot be programmed to automate data pre-processing in a data lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a8b93", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673551831.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Reddit,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m the new grad working for a very early stage startup. The CTO wants me to help him create a data lake and program a bot or something that will automate data pre-processing within the data lake. &lt;/p&gt;\n\n&lt;p&gt;Is it possible or appropriate to automate data pre-processing? As I understand it, all datasets are different and would require careful analysis and bespoke treatment/manipulation in order to make it suitable for analysis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10a8b93", "is_robot_indexable": true, "report_reasons": null, "author": "throwawayldr08", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10a8b93/can_a_bot_be_programmed_to_automate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10a8b93/can_a_bot_be_programmed_to_automate_data/", "subreddit_subscribers": 836482, "created_utc": 1673551831.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}