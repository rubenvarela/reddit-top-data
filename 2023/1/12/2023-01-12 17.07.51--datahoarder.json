{"kind": "Listing", "data": {"after": "t3_109pfx6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have my 40TB hoard of data backed up to Backblaze, and with the recent acquisition of two more drives I needed to wipe my storage pool to switch it over from a simple one to a parity one. Instead of making a local copy I decided to fetch the data back from Backblaze, and since I'm located in Europe, instead of ordering drives and paying duty for them I opted for the download method. (A series of mistakes, I'm aware, but it all seemed like a good idea at the time).\n\nThe process is deceptively simple if you've never actually tried to go through it - either download single files directly, or select what you need and prepare a .zip to download later.\n\nThe first thing you'll run into is the 500GB limit for a single .zip - a pain since it means you need to split up your data, but not an unreasonable limitation, if a little on the small side.\n\nThen you'll discover that there's absolutely zero assistance for you to split your data up - you need to manually pick out files and folders to include and watch the total size (and be aware that this 500GB is decimal). At that point you may also notice that the interface to prepare restores is... not very good - nobody at Backblaze seems to have heard the word \"asynchronous\" and the UI is blocked on requests to the backend, so not only do you not get instant feedback on your current archive size, you don't even see your checkboxes get checked until the requests complete.\n\nBut let's say you've checked what you need for your first batch, got close enough to 500GB and started preparing your .zip. So you go to prepare another. You click back to the Restore screen and, if you have your backup encrypted, it asks you for the encryption key again. Wait, didn't you just provide that? Well, yes, and your backup is decrypted, but on server 0002, and this time the load balancer decided to get you onto server 0014. Not a big deal. Unless you grabbed yourself a coffee in the meantime and now are staring at a login screen again because Backblaze has one of the shortest session expiration times I've seen (something like 20-30 minutes) and no \"Remember me\" button. This is a bit more of a big deal, or - as you might find out later - a very big deal.\n\nSo you prepare a few more batches, still with that same less than responsive interface, and eventually you hit the limit of 5 restores being prepared at once. So you wait. And you wait. Maybe hours, maybe as much as two days. For whatever reason restores that hit close to that 500GB mark take ages, much more than the same amount of data split across multiple 40-50 GB packs - I've had 40GB packages prepared in 5-6 minutes, while the 500GB ones took not 10, but more like 100 times more. Unless you hit a snag and the package just refuses to get prepared and you have to cancel it - I haven't had that happen often with large ones, but a bunch of times with small ones.\n\nYou've finally got one of those restores ready though, and the seven day clock to download it is ticking - so you go to download and it tells you to get yourself a Backblaze Downloader. You may ignore it now and find out that your download is capped at about 100-150 MBit even on your gigabit connection, or you may ignore it later when you've had first hand experience with the downloader. (Spoilers, I know). Let's say you listen and download the downloader - pointlessly, as it turns out, since it's already there along with your Backblaze installation.\n\nYou give it your username and password, OTP code and get a dropdown list of restores - so far, so good. You select one, pick a folder to download to, go with the recommended number of threads, and start downloading.\n\nAnd then you realize the downloader has the same problem as the UI with the \"async\" concept, except Windows really, *really* doesn't like apps hogging the UI thread. So 90 percent of the time the window is \"not responding\", the Close button may work eventually when it gets around to it, and the speed indicator is useless. (The progress bar turns out to be useless too as I've had downloads hit 100% with the bar lingering somewhere three quarters of the way in). If you've made a mistake of restoring to your C:\\ drive this is going to be even worse since that's also where the scratch files are being written, so your disk is hit with a barrage of multiple processes at once (the downloader calls them \"threads\"; that's not quite telling the whole story as they're entirely separate processes getting spawned per 40MB chunk and killed when they finish) writing scratch files, and the downloader appending them to your target file. And the downloader constantly looks like it's hanged, but it has not, unless it has because that happens sometimes as well and your nightly restore might have not gotten past ten percent.\n\nBut let's say you've downloaded your first batch and want to download another - except all you can do with the downloader is close it, then restart it, there's no way to get back to the selection screen. And you need to provide your credentials again. And the target folder has reset to the Desktop again. And there's no indication which restores you have or have not already downloaded.\n\nAnd while you've been marveling at that the unzip process has thrown a CRC error - which I really, *really* hope is just an issue with the zipping/downloading process and the actual data that's being stored on the servers is okay. If you've had the downloader hang on you there's a pretty much 100% chance you'll get that, if you've stopped and restarted the download you'll probably get hit by that as well, and even if everything went just fine it may still happen just because. If you're lucky it's just going to be one or two files and you can restore them separately, if you're not and it plowed over a more sensitive portion of the .zip the entire thing is likely worthless and needs to be redownloaded.\n\nSo you give up on the downloader and decide to download manually - and because of that 100-150 MBit cap you get yourself a download accelerator. Great! Except for the \"acceleration\" part, which for some reason works only up to some size - maybe that's some issue on my side, but I've tried multiple ones and I haven't gotten the big restores to download in parallel, only smaller ones.\n\nAnd even if you've gotten that download acceleration to work - remember that part about getting signed out after 30 minutes? Turns out this applies to the download link as well. And since download accelerators reestablish connections once they've finished a chunk, said connections are now getting redirected to the login page. I've tried three of those programs and neither of them managed to work that situation out, all of them eventually got all of their threads stuck and were not able to resume, leaving a dead download. And even if you don't care for the acceleration, I hope you didn't spend too much time setting up a queue of downloads (or go to bed afterwards), because that won't work either for the same reason.\n\nIronically, the best way to get the downloads working turned out to be just downloading them in the browser - setting up far smaller chunks, so that the still occasional CRC errors don't ruin your day, and downloading multiple files in parallel to saturate the connection. But it still requires multiple trips to the restore screen, you can't just spend an afternoon setting up all your restores because you only have seven days to download them and you need to set them up little by little, and you may still run into issues with the downloads or the resulting zip files.\n\nNow does it mean Backblaze is a bad service? I guess not - for the price it's still a steal, and there are other options to restore. If you're in the US the USB drives are more than likely going to be a great option with zero of the above hassle, if you can eat the egress fees B2 may be a viable option, and in the end I'm likely going to get my files out eventually. But it seems like a lot of people who get interested in Backblaze are in the same boat as me - they don't want to spend more than the monthly fee, may not have the deposit money or live too far away for the drive restore, and they might've heard of the restore process being a bit iffy but it can't be that bad, right?\n\nWell, it's exactly as bad as above, no more, no less - whether that's a dealbreaker is in the eye of the beholder, but it's better to know those things about the service you use before you end up depending on it for your data. I know the Backblaze team has been speaking of a better downloader which I'm hoping will not be vaporware, but even that aside there are so many things that should be such easy wins to fix - the session length issue, the downloader not hogging the UI thread, the artificial 500 GB limit - that it's really a bit disappointing that the current process is so miserable.", "author_fullname": "t2_epug6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Backblaze large restore experience (is miserable)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109kd3j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 336, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 336, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673481815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have my 40TB hoard of data backed up to Backblaze, and with the recent acquisition of two more drives I needed to wipe my storage pool to switch it over from a simple one to a parity one. Instead of making a local copy I decided to fetch the data back from Backblaze, and since I&amp;#39;m located in Europe, instead of ordering drives and paying duty for them I opted for the download method. (A series of mistakes, I&amp;#39;m aware, but it all seemed like a good idea at the time).&lt;/p&gt;\n\n&lt;p&gt;The process is deceptively simple if you&amp;#39;ve never actually tried to go through it - either download single files directly, or select what you need and prepare a .zip to download later.&lt;/p&gt;\n\n&lt;p&gt;The first thing you&amp;#39;ll run into is the 500GB limit for a single .zip - a pain since it means you need to split up your data, but not an unreasonable limitation, if a little on the small side.&lt;/p&gt;\n\n&lt;p&gt;Then you&amp;#39;ll discover that there&amp;#39;s absolutely zero assistance for you to split your data up - you need to manually pick out files and folders to include and watch the total size (and be aware that this 500GB is decimal). At that point you may also notice that the interface to prepare restores is... not very good - nobody at Backblaze seems to have heard the word &amp;quot;asynchronous&amp;quot; and the UI is blocked on requests to the backend, so not only do you not get instant feedback on your current archive size, you don&amp;#39;t even see your checkboxes get checked until the requests complete.&lt;/p&gt;\n\n&lt;p&gt;But let&amp;#39;s say you&amp;#39;ve checked what you need for your first batch, got close enough to 500GB and started preparing your .zip. So you go to prepare another. You click back to the Restore screen and, if you have your backup encrypted, it asks you for the encryption key again. Wait, didn&amp;#39;t you just provide that? Well, yes, and your backup is decrypted, but on server 0002, and this time the load balancer decided to get you onto server 0014. Not a big deal. Unless you grabbed yourself a coffee in the meantime and now are staring at a login screen again because Backblaze has one of the shortest session expiration times I&amp;#39;ve seen (something like 20-30 minutes) and no &amp;quot;Remember me&amp;quot; button. This is a bit more of a big deal, or - as you might find out later - a very big deal.&lt;/p&gt;\n\n&lt;p&gt;So you prepare a few more batches, still with that same less than responsive interface, and eventually you hit the limit of 5 restores being prepared at once. So you wait. And you wait. Maybe hours, maybe as much as two days. For whatever reason restores that hit close to that 500GB mark take ages, much more than the same amount of data split across multiple 40-50 GB packs - I&amp;#39;ve had 40GB packages prepared in 5-6 minutes, while the 500GB ones took not 10, but more like 100 times more. Unless you hit a snag and the package just refuses to get prepared and you have to cancel it - I haven&amp;#39;t had that happen often with large ones, but a bunch of times with small ones.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;ve finally got one of those restores ready though, and the seven day clock to download it is ticking - so you go to download and it tells you to get yourself a Backblaze Downloader. You may ignore it now and find out that your download is capped at about 100-150 MBit even on your gigabit connection, or you may ignore it later when you&amp;#39;ve had first hand experience with the downloader. (Spoilers, I know). Let&amp;#39;s say you listen and download the downloader - pointlessly, as it turns out, since it&amp;#39;s already there along with your Backblaze installation.&lt;/p&gt;\n\n&lt;p&gt;You give it your username and password, OTP code and get a dropdown list of restores - so far, so good. You select one, pick a folder to download to, go with the recommended number of threads, and start downloading.&lt;/p&gt;\n\n&lt;p&gt;And then you realize the downloader has the same problem as the UI with the &amp;quot;async&amp;quot; concept, except Windows really, &lt;em&gt;really&lt;/em&gt; doesn&amp;#39;t like apps hogging the UI thread. So 90 percent of the time the window is &amp;quot;not responding&amp;quot;, the Close button may work eventually when it gets around to it, and the speed indicator is useless. (The progress bar turns out to be useless too as I&amp;#39;ve had downloads hit 100% with the bar lingering somewhere three quarters of the way in). If you&amp;#39;ve made a mistake of restoring to your C:\\ drive this is going to be even worse since that&amp;#39;s also where the scratch files are being written, so your disk is hit with a barrage of multiple processes at once (the downloader calls them &amp;quot;threads&amp;quot;; that&amp;#39;s not quite telling the whole story as they&amp;#39;re entirely separate processes getting spawned per 40MB chunk and killed when they finish) writing scratch files, and the downloader appending them to your target file. And the downloader constantly looks like it&amp;#39;s hanged, but it has not, unless it has because that happens sometimes as well and your nightly restore might have not gotten past ten percent.&lt;/p&gt;\n\n&lt;p&gt;But let&amp;#39;s say you&amp;#39;ve downloaded your first batch and want to download another - except all you can do with the downloader is close it, then restart it, there&amp;#39;s no way to get back to the selection screen. And you need to provide your credentials again. And the target folder has reset to the Desktop again. And there&amp;#39;s no indication which restores you have or have not already downloaded.&lt;/p&gt;\n\n&lt;p&gt;And while you&amp;#39;ve been marveling at that the unzip process has thrown a CRC error - which I really, &lt;em&gt;really&lt;/em&gt; hope is just an issue with the zipping/downloading process and the actual data that&amp;#39;s being stored on the servers is okay. If you&amp;#39;ve had the downloader hang on you there&amp;#39;s a pretty much 100% chance you&amp;#39;ll get that, if you&amp;#39;ve stopped and restarted the download you&amp;#39;ll probably get hit by that as well, and even if everything went just fine it may still happen just because. If you&amp;#39;re lucky it&amp;#39;s just going to be one or two files and you can restore them separately, if you&amp;#39;re not and it plowed over a more sensitive portion of the .zip the entire thing is likely worthless and needs to be redownloaded.&lt;/p&gt;\n\n&lt;p&gt;So you give up on the downloader and decide to download manually - and because of that 100-150 MBit cap you get yourself a download accelerator. Great! Except for the &amp;quot;acceleration&amp;quot; part, which for some reason works only up to some size - maybe that&amp;#39;s some issue on my side, but I&amp;#39;ve tried multiple ones and I haven&amp;#39;t gotten the big restores to download in parallel, only smaller ones.&lt;/p&gt;\n\n&lt;p&gt;And even if you&amp;#39;ve gotten that download acceleration to work - remember that part about getting signed out after 30 minutes? Turns out this applies to the download link as well. And since download accelerators reestablish connections once they&amp;#39;ve finished a chunk, said connections are now getting redirected to the login page. I&amp;#39;ve tried three of those programs and neither of them managed to work that situation out, all of them eventually got all of their threads stuck and were not able to resume, leaving a dead download. And even if you don&amp;#39;t care for the acceleration, I hope you didn&amp;#39;t spend too much time setting up a queue of downloads (or go to bed afterwards), because that won&amp;#39;t work either for the same reason.&lt;/p&gt;\n\n&lt;p&gt;Ironically, the best way to get the downloads working turned out to be just downloading them in the browser - setting up far smaller chunks, so that the still occasional CRC errors don&amp;#39;t ruin your day, and downloading multiple files in parallel to saturate the connection. But it still requires multiple trips to the restore screen, you can&amp;#39;t just spend an afternoon setting up all your restores because you only have seven days to download them and you need to set them up little by little, and you may still run into issues with the downloads or the resulting zip files.&lt;/p&gt;\n\n&lt;p&gt;Now does it mean Backblaze is a bad service? I guess not - for the price it&amp;#39;s still a steal, and there are other options to restore. If you&amp;#39;re in the US the USB drives are more than likely going to be a great option with zero of the above hassle, if you can eat the egress fees B2 may be a viable option, and in the end I&amp;#39;m likely going to get my files out eventually. But it seems like a lot of people who get interested in Backblaze are in the same boat as me - they don&amp;#39;t want to spend more than the monthly fee, may not have the deposit money or live too far away for the drive restore, and they might&amp;#39;ve heard of the restore process being a bit iffy but it can&amp;#39;t be that bad, right?&lt;/p&gt;\n\n&lt;p&gt;Well, it&amp;#39;s exactly as bad as above, no more, no less - whether that&amp;#39;s a dealbreaker is in the eye of the beholder, but it&amp;#39;s better to know those things about the service you use before you end up depending on it for your data. I know the Backblaze team has been speaking of a better downloader which I&amp;#39;m hoping will not be vaporware, but even that aside there are so many things that should be such easy wins to fix - the session length issue, the downloader not hogging the UI thread, the artificial 500 GB limit - that it&amp;#39;s really a bit disappointing that the current process is so miserable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109kd3j", "is_robot_indexable": true, "report_reasons": null, "author": "Mivexil", "discussion_type": null, "num_comments": 162, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109kd3j/the_backblaze_large_restore_experience_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109kd3j/the_backblaze_large_restore_experience_is/", "subreddit_subscribers": 665524, "created_utc": 1673481815.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just a funny story and a small reminder to properly handle your drives on disposal (not that I think anyone here would be this dumb). \n\nI grabbed a cheap used NAS online to upgrade me from running my drives in a regular RAID box. I didn't *need* to do it so I wasn't looking to spend a lot of money. Got a good deal, showed up with drives in it. Default admin passwords. \n\nI have all the data once belonging to a construction company, covering every aspect of their business, going back to at least 2013. Including proposals, diagrams, pictures of their work, addresses, invoices, receipts, credit card numbers, scans of deposited checks, and checking account info. Apparently someone named Katrina decided the recycling bin was good enough. \n\nLuckily for them, I am not an asshole. But holy hell does this make me feel better about my overly cautious way of dealing with old drives.", "author_fullname": "t2_hz5u5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bought used QNAP TS431+.. Previous owner left hsi whole company on the drives.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109h0eu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 53, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 53, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673473617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just a funny story and a small reminder to properly handle your drives on disposal (not that I think anyone here would be this dumb). &lt;/p&gt;\n\n&lt;p&gt;I grabbed a cheap used NAS online to upgrade me from running my drives in a regular RAID box. I didn&amp;#39;t &lt;em&gt;need&lt;/em&gt; to do it so I wasn&amp;#39;t looking to spend a lot of money. Got a good deal, showed up with drives in it. Default admin passwords. &lt;/p&gt;\n\n&lt;p&gt;I have all the data once belonging to a construction company, covering every aspect of their business, going back to at least 2013. Including proposals, diagrams, pictures of their work, addresses, invoices, receipts, credit card numbers, scans of deposited checks, and checking account info. Apparently someone named Katrina decided the recycling bin was good enough. &lt;/p&gt;\n\n&lt;p&gt;Luckily for them, I am not an asshole. But holy hell does this make me feel better about my overly cautious way of dealing with old drives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "109h0eu", "is_robot_indexable": true, "report_reasons": null, "author": "DeffNotTom", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109h0eu/bought_used_qnap_ts431_previous_owner_left_hsi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109h0eu/bought_used_qnap_ts431_previous_owner_left_hsi/", "subreddit_subscribers": 665524, "created_utc": 1673473617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_uyo43rx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building my first server ordered 4 ironwolf nas I got 2 and 2 with different labels all manufactured within 2 months of each other. Is there any difference in these models or is it it just the same drive with different sku that was sent ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_109bu22", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/c2jbX7b3UhyQurMFUhttmpCFURt2u611mr54l2OI89M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673461467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.imgur.com/3LFi29T.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3d2GuWizhnLfMrMqC38wBPtXuHOgHrNTQOtmIcUWvr0.jpg?auto=webp&amp;v=enabled&amp;s=6710305a9cd53d5664e90e5591beab4d5edacd63", "width": 1500, "height": 2000}, "resolutions": [{"url": "https://external-preview.redd.it/3d2GuWizhnLfMrMqC38wBPtXuHOgHrNTQOtmIcUWvr0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64050a9f5bc430f26fb1aedac51def6817a41cfd", "width": 108, "height": 144}, {"url": "https://external-preview.redd.it/3d2GuWizhnLfMrMqC38wBPtXuHOgHrNTQOtmIcUWvr0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c3cfd278a50b090624057c885b6ae0f1aca5bb1", "width": 216, "height": 288}, {"url": "https://external-preview.redd.it/3d2GuWizhnLfMrMqC38wBPtXuHOgHrNTQOtmIcUWvr0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30f98641b32882419cd8e5707e08babbff8c0ccc", "width": 320, "height": 426}, {"url": "https://external-preview.redd.it/3d2GuWizhnLfMrMqC38wBPtXuHOgHrNTQOtmIcUWvr0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=647e785ef4717773d796e26063513786711ad06e", "width": 640, "height": 853}, {"url": "https://external-preview.redd.it/3d2GuWizhnLfMrMqC38wBPtXuHOgHrNTQOtmIcUWvr0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49798d4dd8bf4d9770b9de0b2b668ad041f47f93", "width": 960, "height": 1280}, {"url": "https://external-preview.redd.it/3d2GuWizhnLfMrMqC38wBPtXuHOgHrNTQOtmIcUWvr0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=737def69e6194720c6a6b36d19b5d8575a583312", "width": 1080, "height": 1440}], "variants": {}, "id": "dGFmEl7G6uLJ9TlJnP-3Orf4-A5rCRB9aAQ2vW9aKBo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109bu22", "is_robot_indexable": true, "report_reasons": null, "author": "Yung_Gleesh_", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109bu22/building_my_first_server_ordered_4_ironwolf_nas_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.imgur.com/3LFi29T.jpg", "subreddit_subscribers": 665524, "created_utc": 1673461467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "an update to my previous post, I recently acquired 4 4tb external drives (WD, Elements). ran chkdsk on delivery, no problems found.\n\nstarted filling them with jdownloader until the 3rd one begins to lag terribly. asked here what the problem was, \"smr, working as intended\"\n\nonce downloads completed I figured I'd give chkdsk another pass just to be sure. 10 bad clusters found this time.\n\nI'm dumbfounded and angry at myself because I may not be able to return it despite the warranty (3rd world problems). I know I shouldn't trust it with important data, but considering I'm stuck with it, is there anything I can do? maybe zero fill?\n\nEDIT\n\nfull chkdsk log: https://i.imgur.com/Waab8Hb.png", "author_fullname": "t2_18grx8tx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "10 bad clusters on recently acquired WD external", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1099y9o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673459963.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673457091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;an update to my previous post, I recently acquired 4 4tb external drives (WD, Elements). ran chkdsk on delivery, no problems found.&lt;/p&gt;\n\n&lt;p&gt;started filling them with jdownloader until the 3rd one begins to lag terribly. asked here what the problem was, &amp;quot;smr, working as intended&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;once downloads completed I figured I&amp;#39;d give chkdsk another pass just to be sure. 10 bad clusters found this time.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m dumbfounded and angry at myself because I may not be able to return it despite the warranty (3rd world problems). I know I shouldn&amp;#39;t trust it with important data, but considering I&amp;#39;m stuck with it, is there anything I can do? maybe zero fill?&lt;/p&gt;\n\n&lt;p&gt;EDIT&lt;/p&gt;\n\n&lt;p&gt;full chkdsk log: &lt;a href=\"https://i.imgur.com/Waab8Hb.png\"&gt;https://i.imgur.com/Waab8Hb.png&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fcajxxOKZsh_N2O5HVekMRyELF0E2pDEbujRNs9DHKI.png?auto=webp&amp;v=enabled&amp;s=fa22f783b4db4bed916c680781e0f71926150b29", "width": 1324, "height": 916}, "resolutions": [{"url": "https://external-preview.redd.it/fcajxxOKZsh_N2O5HVekMRyELF0E2pDEbujRNs9DHKI.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=365328e4f0f15ae1640e0794ffeeb0506be978ce", "width": 108, "height": 74}, {"url": "https://external-preview.redd.it/fcajxxOKZsh_N2O5HVekMRyELF0E2pDEbujRNs9DHKI.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02df3d9cd0ffded9da82365fd3e75535b7254778", "width": 216, "height": 149}, {"url": "https://external-preview.redd.it/fcajxxOKZsh_N2O5HVekMRyELF0E2pDEbujRNs9DHKI.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7f5c08ca3023ea82e4cfcd3fd24e087bbc69ca4", "width": 320, "height": 221}, {"url": "https://external-preview.redd.it/fcajxxOKZsh_N2O5HVekMRyELF0E2pDEbujRNs9DHKI.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0691378505539a6abecb7007d2b64691ccd6bd29", "width": 640, "height": 442}, {"url": "https://external-preview.redd.it/fcajxxOKZsh_N2O5HVekMRyELF0E2pDEbujRNs9DHKI.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd404e36265ed7b36c9faa1a8b82675c1510d172", "width": 960, "height": 664}, {"url": "https://external-preview.redd.it/fcajxxOKZsh_N2O5HVekMRyELF0E2pDEbujRNs9DHKI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc1948ff24faecef9cda2940048d6d093a970c24", "width": 1080, "height": 747}], "variants": {}, "id": "WRxgf3zs3hd4T5edmv4gY-a8OnqkOammTTB4c1y_Lgc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1099y9o", "is_robot_indexable": true, "report_reasons": null, "author": "h-t-", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1099y9o/10_bad_clusters_on_recently_acquired_wd_external/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1099y9o/10_bad_clusters_on_recently_acquired_wd_external/", "subreddit_subscribers": 665524, "created_utc": 1673457091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm not the type of person for this sub. All of my data is under a TB.... but I started playing around with 3d scanning and that's changing fast. 20gb scans. 150gb project files. \n\nI'm not wanting to spend too much or get too involved for now so this is the setup I'm thinking. \n\npurchase 3 HDD in the 4-6tb range. 2 go into an old computer running raid 1. map network drive and drag the projects over when I'm done with them. The third is kept somewhere else and brought over once a month to get updated\n\nto me it sounds like a good idea with more than 1 layer of defense. any opinions?", "author_fullname": "t2_v3z3kryc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wanting to get a sanity check for archive/backup idea", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109n680", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673489125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not the type of person for this sub. All of my data is under a TB.... but I started playing around with 3d scanning and that&amp;#39;s changing fast. 20gb scans. 150gb project files. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not wanting to spend too much or get too involved for now so this is the setup I&amp;#39;m thinking. &lt;/p&gt;\n\n&lt;p&gt;purchase 3 HDD in the 4-6tb range. 2 go into an old computer running raid 1. map network drive and drag the projects over when I&amp;#39;m done with them. The third is kept somewhere else and brought over once a month to get updated&lt;/p&gt;\n\n&lt;p&gt;to me it sounds like a good idea with more than 1 layer of defense. any opinions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109n680", "is_robot_indexable": true, "report_reasons": null, "author": "Optimal-Growth-5741", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109n680/wanting_to_get_a_sanity_check_for_archivebackup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109n680/wanting_to_get_a_sanity_check_for_archivebackup/", "subreddit_subscribers": 665524, "created_utc": 1673489125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for something like Google Photos with smart photo sorting (eg. face, screenshots, blurry shots)", "author_fullname": "t2_woqd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photo sorter recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109l5sr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673483820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for something like Google Photos with smart photo sorting (eg. face, screenshots, blurry shots)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109l5sr", "is_robot_indexable": true, "report_reasons": null, "author": "slaiyfer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109l5sr/photo_sorter_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109l5sr/photo_sorter_recommendations/", "subreddit_subscribers": 665524, "created_utc": 1673483820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've located and reuploaded a long lost UnleashX skin", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "name": "t3_109vz32", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_2b9r7ngd", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Jnjs0qw2eBd5aTzRu8i0sLwfd-qlwww-SgI8E2Y9eSE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "originalxbox", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/20rcv4cgtjba1.png?width=689&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2072dcd672302c1afa94268bcd3f1514386135f7\n\nI archived an UnleashX skin that was deleted off the internet suddenly. It was posted to Twitter by its creator Retro Game Rarities in 2020 and his account suddenly got deleted. I was able to find a link to it via google cache and I've since reuploaded it to [archive.org](https://archive.org)\n\n[https://archive.org/details/UnleashX-EmeraldX-skin](https://archive.org/details/UnleashX-EmeraldX-skin)\n\nIt's easily the coolest looking UnleashX skin I've ever seen. It was disappointing to discover it was so hastily deleted and purged off the internet.\n\n&amp;#x200B;\n\nThe creator's original video is here\n\nhttps://reddit.com/link/109ri3e/video/icgsgx97ujba1/player", "author_fullname": "t2_77nrtq1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've located and reuploaded a long lost UnleashX skin", "link_flair_richtext": [], "subreddit_name_prefixed": "r/originalxbox", "hidden": false, "pwls": 6, "link_flair_css_class": "tools", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "media_metadata": {"20rcv4cgtjba1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/20rcv4cgtjba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=efe2643dd2b8fbcee893090d86edbc036005feda"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/20rcv4cgtjba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c285fb7495f33678c3b1ed7089e7269b1a5d74b"}, {"y": 177, "x": 320, "u": "https://preview.redd.it/20rcv4cgtjba1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76a2a4d9e3b0066cf79801458a08be4ba34f33a4"}, {"y": 355, "x": 640, "u": "https://preview.redd.it/20rcv4cgtjba1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f108e3c16a23e91941d839c7142df34078624251"}], "s": {"y": 383, "x": 689, "u": "https://preview.redd.it/20rcv4cgtjba1.png?width=689&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2072dcd672302c1afa94268bcd3f1514386135f7"}, "id": "20rcv4cgtjba1"}, "icgsgx97ujba1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/109ri3e/asset/icgsgx97ujba1/DASHPlaylist.mpd?a=1676135270%2CNjZkNjc4NTVjYjAwN2VkM2Y4MTVhMDcyZTVmYmM3OTE5NWIwZWVjNGMxNDliNTgzNDMwNGFlYjYzYzk1NzcyZg%3D%3D&amp;v=1&amp;f=sd", "x": 1280, "y": 720, "hlsUrl": "https://v.redd.it/link/109ri3e/asset/icgsgx97ujba1/HLSPlaylist.m3u8?a=1676135270%2CMjY1N2E2Y2FmMmQzYjllYWQ0MDMxOTkwODcyYjMxMzZiYzE1YzcxMzhlNTA2ZThhNzA0MjBkYjg2NWM4MGIyYw%3D%3D&amp;v=1&amp;f=sd", "id": "icgsgx97ujba1", "isGif": false}}, "name": "t3_109ri3e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Software &amp; Tools", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Jnjs0qw2eBd5aTzRu8i0sLwfd-qlwww-SgI8E2Y9eSE.jpg", "edited": 1673516612.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673501638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.originalxbox", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/20rcv4cgtjba1.png?width=689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2072dcd672302c1afa94268bcd3f1514386135f7\"&gt;https://preview.redd.it/20rcv4cgtjba1.png?width=689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2072dcd672302c1afa94268bcd3f1514386135f7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I archived an UnleashX skin that was deleted off the internet suddenly. It was posted to Twitter by its creator Retro Game Rarities in 2020 and his account suddenly got deleted. I was able to find a link to it via google cache and I&amp;#39;ve since reuploaded it to &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://archive.org/details/UnleashX-EmeraldX-skin\"&gt;https://archive.org/details/UnleashX-EmeraldX-skin&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s easily the coolest looking UnleashX skin I&amp;#39;ve ever seen. It was disappointing to discover it was so hastily deleted and purged off the internet.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The creator&amp;#39;s original video is here&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/109ri3e/video/icgsgx97ujba1/player\"&gt;https://reddit.com/link/109ri3e/video/icgsgx97ujba1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "65253322-f811-11ec-aadc-4685b9374796", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2rww7", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "109ri3e", "is_robot_indexable": true, "report_reasons": null, "author": "Archer_Jr", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/originalxbox/comments/109ri3e/ive_located_and_reuploaded_a_long_lost_unleashx/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/originalxbox/comments/109ri3e/ive_located_and_reuploaded_a_long_lost_unleashx/", "subreddit_subscribers": 50892, "created_utc": 1673501638.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1673517607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.originalxbox", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/originalxbox/comments/109ri3e/ive_located_and_reuploaded_a_long_lost_unleashx/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109vz32", "is_robot_indexable": true, "report_reasons": null, "author": "RustedBlade7", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_109ri3e", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109vz32/ive_located_and_reuploaded_a_long_lost_unleashx/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/originalxbox/comments/109ri3e/ive_located_and_reuploaded_a_long_lost_unleashx/", "subreddit_subscribers": 665524, "created_utc": 1673517607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't actually have the hardware yet, I'm doing some research before purchase (always advisable lol). I'm interested in buying a used 3.2TB Micron 7300 MAX, but I want to bump that up to 3.84TB for more capacity since I won't be using anywhere near the listed 3DWPD spec. Hell, assuming that makes it equal to the \"Pro\" version of the drive, 1DWPD is still waaaay overkill.\n\nIt seems like Micron calls it \"Flex Capacity\" and it seems to be a part of \"Storage Executive Software\". Is this where I change the capacity? Can I even increase the capacity? I've heard of people reducing capacity to increase endurance, but I've never heard of someone doing the reverse.", "author_fullname": "t2_upalof7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I change the capacity / overprovisioning on a Micron 7300 MAX?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109z0cp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673528186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t actually have the hardware yet, I&amp;#39;m doing some research before purchase (always advisable lol). I&amp;#39;m interested in buying a used 3.2TB Micron 7300 MAX, but I want to bump that up to 3.84TB for more capacity since I won&amp;#39;t be using anywhere near the listed 3DWPD spec. Hell, assuming that makes it equal to the &amp;quot;Pro&amp;quot; version of the drive, 1DWPD is still waaaay overkill.&lt;/p&gt;\n\n&lt;p&gt;It seems like Micron calls it &amp;quot;Flex Capacity&amp;quot; and it seems to be a part of &amp;quot;Storage Executive Software&amp;quot;. Is this where I change the capacity? Can I even increase the capacity? I&amp;#39;ve heard of people reducing capacity to increase endurance, but I&amp;#39;ve never heard of someone doing the reverse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109z0cp", "is_robot_indexable": true, "report_reasons": null, "author": "Party_9001", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109z0cp/how_do_i_change_the_capacity_overprovisioning_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109z0cp/how_do_i_change_the_capacity_overprovisioning_on/", "subreddit_subscribers": 665524, "created_utc": 1673528186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI currently have about 1.3TB stored in Amazon Drive. With the retirement of that platform, I am looking to move to a new on-line option. I am currently thinking Google Drive may be the best fit at the best price.\n\nLooking ahead to the data migration piece.... Can someone suggest options that would allow me to transfer these files directly from the Amazon to the Google platform? Downloading and re-uploading will be a very  s l o w   p   r   o   c   e   s   s   with my internet provider.\n\nTIA!", "author_fullname": "t2_9jlhrl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestions: Data migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109h2xs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673473789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I currently have about 1.3TB stored in Amazon Drive. With the retirement of that platform, I am looking to move to a new on-line option. I am currently thinking Google Drive may be the best fit at the best price.&lt;/p&gt;\n\n&lt;p&gt;Looking ahead to the data migration piece.... Can someone suggest options that would allow me to transfer these files directly from the Amazon to the Google platform? Downloading and re-uploading will be a very  s l o w   p   r   o   c   e   s   s   with my internet provider.&lt;/p&gt;\n\n&lt;p&gt;TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109h2xs", "is_robot_indexable": true, "report_reasons": null, "author": "PerpetuallyPerplxed", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109h2xs/need_suggestions_data_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109h2xs/need_suggestions_data_migration/", "subreddit_subscribers": 665524, "created_utc": 1673473789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got about 300,000 emails going back to 1993.  I love having the whole thing indexed and searchable for practical and nostalgic purposes, but the ratio of \"good\" to \"bad\" is ridiculous.  Even with gmail doing a lot of the heavy spam filtering, it's got a lot of garbage.\n\nDoes anyone have any pointers on how to efficiently spam filter large quantities of email spread across about 30 unix mbox files?  \n\nIf I had to, I'd be OK resorting to something somewhat iterative and convoluted (somehow feed all the mbox files to a mail server running on one of my linux boxes?) but it would be nice if there were tools out there to make this as easy as possible.", "author_fullname": "t2_tthrp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "30 years of saved email - spam filtering options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a02eo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673531289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got about 300,000 emails going back to 1993.  I love having the whole thing indexed and searchable for practical and nostalgic purposes, but the ratio of &amp;quot;good&amp;quot; to &amp;quot;bad&amp;quot; is ridiculous.  Even with gmail doing a lot of the heavy spam filtering, it&amp;#39;s got a lot of garbage.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any pointers on how to efficiently spam filter large quantities of email spread across about 30 unix mbox files?  &lt;/p&gt;\n\n&lt;p&gt;If I had to, I&amp;#39;d be OK resorting to something somewhat iterative and convoluted (somehow feed all the mbox files to a mail server running on one of my linux boxes?) but it would be nice if there were tools out there to make this as easy as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a02eo", "is_robot_indexable": true, "report_reasons": null, "author": "lectures", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a02eo/30_years_of_saved_email_spam_filtering_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a02eo/30_years_of_saved_email_spam_filtering_options/", "subreddit_subscribers": 665524, "created_utc": 1673531289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "title says it all , been trying many github scripts but nothing works", "author_fullname": "t2_aj30rvtl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "any way to download photo album from weibo ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109x642", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673521913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title says it all , been trying many github scripts but nothing works&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109x642", "is_robot_indexable": true, "report_reasons": null, "author": "Digital-Nuke", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109x642/any_way_to_download_photo_album_from_weibo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109x642/any_way_to_download_photo_album_from_weibo/", "subreddit_subscribers": 665524, "created_utc": 1673521913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need to grab a VKontakte group's media hoard- specifically the photos. I'm not up to no good (much), it's just a public fanpage for a celeb but it goes back too many years to save everything individually. We're talking nearly 4500 pics, and I may want the video too, idk yet.\n\nI turn to the DH brains trust here to advise me- what is the best way to do this? Are there any dedicated bulk VK rippers? I have had a google and JDownloader is an option I've investigated, but it's not grabbing everything (I have a tech support Q pending on the JD sub, but I think im going to be S.O.O.L). I need *everything. E.v.e.r.y. t. h. i. n. g.*\n\nHelp a fangirl out, how do I do this?\n\nI can *maybe* deal with a program in Russian if I have to. I can't code, I'm looking for an off the shelf solution.\n\nETA- Internet Download Manager and DownAll from the sub wiki didn't get anywhere.", "author_fullname": "t2_5l13q7a9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to hoard a VK profile/group?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109voht", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673521458.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673516482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to grab a VKontakte group&amp;#39;s media hoard- specifically the photos. I&amp;#39;m not up to no good (much), it&amp;#39;s just a public fanpage for a celeb but it goes back too many years to save everything individually. We&amp;#39;re talking nearly 4500 pics, and I may want the video too, idk yet.&lt;/p&gt;\n\n&lt;p&gt;I turn to the DH brains trust here to advise me- what is the best way to do this? Are there any dedicated bulk VK rippers? I have had a google and JDownloader is an option I&amp;#39;ve investigated, but it&amp;#39;s not grabbing everything (I have a tech support Q pending on the JD sub, but I think im going to be S.O.O.L). I need &lt;em&gt;everything. E.v.e.r.y. t. h. i. n. g.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Help a fangirl out, how do I do this?&lt;/p&gt;\n\n&lt;p&gt;I can &lt;em&gt;maybe&lt;/em&gt; deal with a program in Russian if I have to. I can&amp;#39;t code, I&amp;#39;m looking for an off the shelf solution.&lt;/p&gt;\n\n&lt;p&gt;ETA- Internet Download Manager and DownAll from the sub wiki didn&amp;#39;t get anywhere.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109voht", "is_robot_indexable": true, "report_reasons": null, "author": "nectarine_pie", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109voht/best_way_to_hoard_a_vk_profilegroup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109voht/best_way_to_hoard_a_vk_profilegroup/", "subreddit_subscribers": 665524, "created_utc": 1673516482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\nI'm looking for a webui tool to manage my music folder.\n\nMy idea is something like that:\n\n\\- I tell the program where I will add new songs (like the rip folder)\n\n\\- It automatically detect new songs, search for tags and the move the song to the correct folder\n\n\\- from the webui I set the rules and edit metadata if needed.\n\n&amp;#x200B;\n\nDo you know anything that can do that?", "author_fullname": "t2_24u5cpux", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manage music folder from webui?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109jdnd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673479355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a webui tool to manage my music folder.&lt;/p&gt;\n\n&lt;p&gt;My idea is something like that:&lt;/p&gt;\n\n&lt;p&gt;- I tell the program where I will add new songs (like the rip folder)&lt;/p&gt;\n\n&lt;p&gt;- It automatically detect new songs, search for tags and the move the song to the correct folder&lt;/p&gt;\n\n&lt;p&gt;- from the webui I set the rules and edit metadata if needed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do you know anything that can do that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109jdnd", "is_robot_indexable": true, "report_reasons": null, "author": "TopdeckIsSkill", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109jdnd/manage_music_folder_from_webui/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109jdnd/manage_music_folder_from_webui/", "subreddit_subscribers": 665524, "created_utc": 1673479355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "ArchiveBox only seems to allow a person to save *all* of one's bookmarks and I couldn't even do that because I got the following error:\n\nError while loading link! \\[1673463122.097194\\] \\[path to my browser's bookmarks file\\] \"None\"\n\n&amp;#x200B;\n\nThis error was shown after I used the following command:\n\narchivebox add --depth=1 \\[path to my browser's bookmarks file\\]\n\n&amp;#x200B;\n\nCan somebody instruct me on how to use ArchiveBox for this or give me a different method? Right now, I just want a copy of all sites in a certain bookmarks folder as 'HTML Only.' It wouldn't take me long to do it manually, but I figured I would finally try to do it automatically.", "author_fullname": "t2_emji9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "automatically save all pages in a bookmarks subfolder", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109d68v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673464571.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ArchiveBox only seems to allow a person to save &lt;em&gt;all&lt;/em&gt; of one&amp;#39;s bookmarks and I couldn&amp;#39;t even do that because I got the following error:&lt;/p&gt;\n\n&lt;p&gt;Error while loading link! [1673463122.097194] [path to my browser&amp;#39;s bookmarks file] &amp;quot;None&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This error was shown after I used the following command:&lt;/p&gt;\n\n&lt;p&gt;archivebox add --depth=1 [path to my browser&amp;#39;s bookmarks file]&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Can somebody instruct me on how to use ArchiveBox for this or give me a different method? Right now, I just want a copy of all sites in a certain bookmarks folder as &amp;#39;HTML Only.&amp;#39; It wouldn&amp;#39;t take me long to do it manually, but I figured I would finally try to do it automatically.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109d68v", "is_robot_indexable": true, "report_reasons": null, "author": "PA99", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109d68v/automatically_save_all_pages_in_a_bookmarks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109d68v/automatically_save_all_pages_in_a_bookmarks/", "subreddit_subscribers": 665524, "created_utc": 1673464571.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Micron Unveils 9400 SSD: 30 TB Capacities With Best-In-Class Performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": true, "name": "t3_10a4nvv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/AC0ueZqYqdOMzCdm3ntxWB65qmLoxW0SUkHoeuH1Bds.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673543009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "wccftech.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://wccftech.com/micron-unveils-9400-ssd-30-tb-capacities-with-best-in-class-performance/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?auto=webp&amp;v=enabled&amp;s=c5683a71bf7f47729b0ff2c5cc693e5e54236a37", "width": 980, "height": 620}, "resolutions": [{"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f096790349c2231fec800a7e162ac38ff971e054", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c6c9f335d72366983fe496f7108b7ba824b5f74", "width": 216, "height": 136}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6be074be75dbd483261d4e50615feebc1a996a10", "width": 320, "height": 202}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14a9c87bb2670e5df83de7748e5ddaa8c8813a88", "width": 640, "height": 404}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5652b2fb0ca0e91c36cb053db1abddf73351f32", "width": 960, "height": 607}], "variants": {}, "id": "Z0sSJbkM797eMfo6AVw0nJG4sVsDUTAEeTjiWS9-Vlc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a4nvv", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10a4nvv/micron_unveils_9400_ssd_30_tb_capacities_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://wccftech.com/micron-unveils-9400-ssd-30-tb-capacities-with-best-in-class-performance/", "subreddit_subscribers": 665524, "created_utc": 1673543009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI download a lot of content for archival purposes. These contents are typically downloaded as ZIP or RAR archives which then have to be extracted. I am wondering if there is any way I can automatically add read-only attributes to all files as they are being extracted (eg. as an option inside WinRAR or 7-Zip) instead of having to manually select the folder afterwards, then right-clicking and selecting the read-only checkbox (which sometimes I forget to do)\n\nThanks in advance!", "author_fullname": "t2_fcjv96r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Windows] Is there any way to automatically add read-only attributes to all files extracted by WinRAR or 7-Zip?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10a3vd9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673541140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I download a lot of content for archival purposes. These contents are typically downloaded as ZIP or RAR archives which then have to be extracted. I am wondering if there is any way I can automatically add read-only attributes to all files as they are being extracted (eg. as an option inside WinRAR or 7-Zip) instead of having to manually select the folder afterwards, then right-clicking and selecting the read-only checkbox (which sometimes I forget to do)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a3vd9", "is_robot_indexable": true, "report_reasons": null, "author": "P650SE", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a3vd9/windows_is_there_any_way_to_automatically_add/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a3vd9/windows_is_there_any_way_to_automatically_add/", "subreddit_subscribers": 665524, "created_utc": 1673541140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been trying to get myself to use the singlefile addon along with recoll to search them. That seems somewhat future proof in terms of the pages being readable and searchable in the future but I wanted to see if others perhaps had a better workflow/ideas?\n\nOne additional thing, dead links. SingleFile has a GREAT feature to save bookmarked pages, I have backups of old bookmark files but have discovered (duh i guess) there are many dead links so was wondering if there was a way to automate replacing of deadlinks with wayback machine links from the internet archive or perhaps google cache?", "author_fullname": "t2_f5t3js0e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you archive and search webpages?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10a3tme", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673541025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been trying to get myself to use the singlefile addon along with recoll to search them. That seems somewhat future proof in terms of the pages being readable and searchable in the future but I wanted to see if others perhaps had a better workflow/ideas?&lt;/p&gt;\n\n&lt;p&gt;One additional thing, dead links. SingleFile has a GREAT feature to save bookmarked pages, I have backups of old bookmark files but have discovered (duh i guess) there are many dead links so was wondering if there was a way to automate replacing of deadlinks with wayback machine links from the internet archive or perhaps google cache?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a3tme", "is_robot_indexable": true, "report_reasons": null, "author": "AlfieMcLuvin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a3tme/how_do_you_archive_and_search_webpages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a3tme/how_do_you_archive_and_search_webpages/", "subreddit_subscribers": 665524, "created_utc": 1673541025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a diy nas running windows server 2016 and a storage pool with 2 6TB WD Red drives set up as mirrored.  Yes I know this isn't the best or cleanest nas setup but for all my windows shares it's the easiest for me so I want to continue using storage spaces.\n\nMy question is, what happens when I add new drives to the existing pool that has 2 mirrored drives already.  I have on hand and want to add 2 12TB drives to the pool. Eventually I want to remove one by one the 6TB drives from the pool and replace them with larger newer drives.  Will storage spaces mirror the 2 12TB drives together? Will it copy the data from the 6TB drives to the 12TB drives or will it act like a 2x2 mirrored striped setup when the data is split between the 2 sets of drives?\n\nI would prefer more redundancy than total storage space.", "author_fullname": "t2_511438q9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage Spaces adding drives to existing mirrored pool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10a26dk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673536931.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a diy nas running windows server 2016 and a storage pool with 2 6TB WD Red drives set up as mirrored.  Yes I know this isn&amp;#39;t the best or cleanest nas setup but for all my windows shares it&amp;#39;s the easiest for me so I want to continue using storage spaces.&lt;/p&gt;\n\n&lt;p&gt;My question is, what happens when I add new drives to the existing pool that has 2 mirrored drives already.  I have on hand and want to add 2 12TB drives to the pool. Eventually I want to remove one by one the 6TB drives from the pool and replace them with larger newer drives.  Will storage spaces mirror the 2 12TB drives together? Will it copy the data from the 6TB drives to the 12TB drives or will it act like a 2x2 mirrored striped setup when the data is split between the 2 sets of drives?&lt;/p&gt;\n\n&lt;p&gt;I would prefer more redundancy than total storage space.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a26dk", "is_robot_indexable": true, "report_reasons": null, "author": "Historical_Wheel1090", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a26dk/storage_spaces_adding_drives_to_existing_mirrored/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a26dk/storage_spaces_adding_drives_to_existing_mirrored/", "subreddit_subscribers": 665524, "created_utc": 1673536931.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Like the title states, we're going to be moving (quite far) and I'm looking for recommendations for the best way to move the NAS and hard drives. I'm running a 920+ with 4 HDDs, can I bubblewrap the NAS, box it up and move with the drives installed or would it be advisable to remove the drives and individually bubblewrap and box them?\n\nFor what it matters, we're moving a long ways away, renting a uHaul pull behind trailer and will be traveling over bumpy / dirt roads for a not negligible portion of the drive. I'm leaning towards taking the hard drives out and individually wrapping them for transport.", "author_fullname": "t2_ckfoz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to transport Synology / Drives during a move", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a10ye", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673533916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like the title states, we&amp;#39;re going to be moving (quite far) and I&amp;#39;m looking for recommendations for the best way to move the NAS and hard drives. I&amp;#39;m running a 920+ with 4 HDDs, can I bubblewrap the NAS, box it up and move with the drives installed or would it be advisable to remove the drives and individually bubblewrap and box them?&lt;/p&gt;\n\n&lt;p&gt;For what it matters, we&amp;#39;re moving a long ways away, renting a uHaul pull behind trailer and will be traveling over bumpy / dirt roads for a not negligible portion of the drive. I&amp;#39;m leaning towards taking the hard drives out and individually wrapping them for transport.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a10ye", "is_robot_indexable": true, "report_reasons": null, "author": "offthewallness", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a10ye/how_to_transport_synology_drives_during_a_move/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a10ye/how_to_transport_synology_drives_during_a_move/", "subreddit_subscribers": 665524, "created_utc": 1673533916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There are some search engines out there that provide direct links to images (duck duck go, metager, brave that I know of) though they are still somewhat hidden (you have to click on the image to then see the direct link, and you have to scroll down to see more results) by java i guess it is? Anyway, its quite convenient to be able to specify what you want to see and then hover up the results :)\n\nAnyway know of a way to get past this?", "author_fullname": "t2_f5t3js0e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "scraping image search results?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109yn4c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673527007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are some search engines out there that provide direct links to images (duck duck go, metager, brave that I know of) though they are still somewhat hidden (you have to click on the image to then see the direct link, and you have to scroll down to see more results) by java i guess it is? Anyway, its quite convenient to be able to specify what you want to see and then hover up the results :)&lt;/p&gt;\n\n&lt;p&gt;Anyway know of a way to get past this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109yn4c", "is_robot_indexable": true, "report_reasons": null, "author": "AlfieMcLuvin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109yn4c/scraping_image_search_results/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109yn4c/scraping_image_search_results/", "subreddit_subscribers": 665524, "created_utc": 1673527007.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I\u2019m currently trying to recover a fan- animated video of Jack Stauber\u2019s \u201cBaby Hotline\u201d that I last saw around July 2022. Within the past few months, YouTube took it down and I didn\u2019t notice until I went to revisit it. It was well animated and what ultimately led me to liking this song, but it\u2019s impossible to search for the creator or title of the video and have some way of preserving its existence. \n\nI was only able to find an untitled link digging through my Google watch history but internet archives and google/international search bars, the usual avenues, have brought nothing. \n\nthe URL: https://youtube.com/watch?v=RDdsbNwTW0k\n\nIf anyone knows another way of sourcing video titles/information from dead links, I\u2019d be grateful.", "author_fullname": "t2_11778y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to recover an animated music video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109w9qz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673518662.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I\u2019m currently trying to recover a fan- animated video of Jack Stauber\u2019s \u201cBaby Hotline\u201d that I last saw around July 2022. Within the past few months, YouTube took it down and I didn\u2019t notice until I went to revisit it. It was well animated and what ultimately led me to liking this song, but it\u2019s impossible to search for the creator or title of the video and have some way of preserving its existence. &lt;/p&gt;\n\n&lt;p&gt;I was only able to find an untitled link digging through my Google watch history but internet archives and google/international search bars, the usual avenues, have brought nothing. &lt;/p&gt;\n\n&lt;p&gt;the URL: &lt;a href=\"https://youtube.com/watch?v=RDdsbNwTW0k\"&gt;https://youtube.com/watch?v=RDdsbNwTW0k&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If anyone knows another way of sourcing video titles/information from dead links, I\u2019d be grateful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109w9qz", "is_robot_indexable": true, "report_reasons": null, "author": "nyansensei888", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109w9qz/trying_to_recover_an_animated_music_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109w9qz/trying_to_recover_an_animated_music_video/", "subreddit_subscribers": 665524, "created_utc": 1673518662.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My 2tb harddrive is at the moment being used as a backup time machine for my mac. The problem is: it is suddenly disconnecting. Is that an issue with failing harddrives, with the enclosure or something else? Tested on different machines, all of them macs (I don\u2019t have a windows machine at disposal) should I remove all my backup from that drive? I have everything there.", "author_fullname": "t2_3knbe326", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got a 2 TB HDD 7200RPM (the big ones). But it has an issue and im not sure what is the cause:", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109w34j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673517992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My 2tb harddrive is at the moment being used as a backup time machine for my mac. The problem is: it is suddenly disconnecting. Is that an issue with failing harddrives, with the enclosure or something else? Tested on different machines, all of them macs (I don\u2019t have a windows machine at disposal) should I remove all my backup from that drive? I have everything there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109w34j", "is_robot_indexable": true, "report_reasons": null, "author": "Clipthecliph", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109w34j/got_a_2_tb_hdd_7200rpm_the_big_ones_but_it_has_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109w34j/got_a_2_tb_hdd_7200rpm_the_big_ones_but_it_has_an/", "subreddit_subscribers": 665524, "created_utc": 1673517992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi folks,\n\nI had a single 8TB drive failure out of my array, which I then switched out for a new 16TB drive. Problems started when I switched SATA channels on the 8TB drive, and it came back online. I then readded it to the storage spaces pool. (Stupid, I know, but I'm a greedy simpleton. Please take pity.)\n\nSince then, the pool has been stuck on \"Storage pool-Rebalance\". Output from PS \\`Get-StorageJob\\`:\n\n    Name | IsBackgroundTask | ElapsedTime | JobState | PercentComplete | BytesProcessed | BytesTotal\n    ----\n    Storage pool-Rebalance | True | 5012.09:03:37 | Shutting Down | 0 | 0 B | 8 GB\n    Nonresilient space-Repair | True | 1.01:14:57 | Suspended | 0 | 0 B | 256 GB\n\nFurther, the GUI shows something similar: [https://imgur.com/a/wcjcdla](https://imgur.com/a/wcjcdla), and \\`Stop-StorageJob -Name \"Storage pool-Rebalance\"\\` gives me:\n\n    Stop-StorageJob : Unspecified error\n    At line:1 char:1\n    + Stop-StorageJob -Name \"Storage pool-Rebalance\"\n    + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        + CategoryInfo          : NotSpecified: (MSFT_StorageJob...Windows/Sto...):ROOT/Microsoft/...MSFT_StorageJob) [Sto\n       p-StorageJob], CimException\n        + FullyQualifiedErrorId : HRESULT 0x80004005,Stop-StorageJob\n\nGoing into the Event Viewer, I have the same event over and over: [https://pastebin.com/GH44S4km](https://pastebin.com/GH44S4km) (but with RepairPhases increasing by 1 each time).\n\nMy sense is that if I can remove the drive for good, or alternatively reset the 'job queue' of the storage spaces, it will be able to recover from there. Any ideas?", "author_fullname": "t2_ph67y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage Spaces - stuck on Jobstate: Shutting Down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109qatw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673497936.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;I had a single 8TB drive failure out of my array, which I then switched out for a new 16TB drive. Problems started when I switched SATA channels on the 8TB drive, and it came back online. I then readded it to the storage spaces pool. (Stupid, I know, but I&amp;#39;m a greedy simpleton. Please take pity.)&lt;/p&gt;\n\n&lt;p&gt;Since then, the pool has been stuck on &amp;quot;Storage pool-Rebalance&amp;quot;. Output from PS `Get-StorageJob`:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Name | IsBackgroundTask | ElapsedTime | JobState | PercentComplete | BytesProcessed | BytesTotal\n----\nStorage pool-Rebalance | True | 5012.09:03:37 | Shutting Down | 0 | 0 B | 8 GB\nNonresilient space-Repair | True | 1.01:14:57 | Suspended | 0 | 0 B | 256 GB\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Further, the GUI shows something similar: &lt;a href=\"https://imgur.com/a/wcjcdla\"&gt;https://imgur.com/a/wcjcdla&lt;/a&gt;, and `Stop-StorageJob -Name &amp;quot;Storage pool-Rebalance&amp;quot;` gives me:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Stop-StorageJob : Unspecified error\nAt line:1 char:1\n+ Stop-StorageJob -Name &amp;quot;Storage pool-Rebalance&amp;quot;\n+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : NotSpecified: (MSFT_StorageJob...Windows/Sto...):ROOT/Microsoft/...MSFT_StorageJob) [Sto\n   p-StorageJob], CimException\n    + FullyQualifiedErrorId : HRESULT 0x80004005,Stop-StorageJob\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Going into the Event Viewer, I have the same event over and over: &lt;a href=\"https://pastebin.com/GH44S4km\"&gt;https://pastebin.com/GH44S4km&lt;/a&gt; (but with RepairPhases increasing by 1 each time).&lt;/p&gt;\n\n&lt;p&gt;My sense is that if I can remove the drive for good, or alternatively reset the &amp;#39;job queue&amp;#39; of the storage spaces, it will be able to recover from there. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YVFsfYRKZZeiIjNne5tFPDiIHN09K02P9y2gC7J1E2U.jpg?auto=webp&amp;v=enabled&amp;s=29a9a63637ba4d7d7351133779f5fa07adb1d870", "width": 1803, "height": 1924}, "resolutions": [{"url": "https://external-preview.redd.it/YVFsfYRKZZeiIjNne5tFPDiIHN09K02P9y2gC7J1E2U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2276873138d3e501787d9a79f3b15a5942741712", "width": 108, "height": 115}, {"url": "https://external-preview.redd.it/YVFsfYRKZZeiIjNne5tFPDiIHN09K02P9y2gC7J1E2U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=690183b4ac50dfee46b470f920c3d5120a499763", "width": 216, "height": 230}, {"url": "https://external-preview.redd.it/YVFsfYRKZZeiIjNne5tFPDiIHN09K02P9y2gC7J1E2U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d51ac3f5cb42667c31310b06bdc7054a1282549", "width": 320, "height": 341}, {"url": "https://external-preview.redd.it/YVFsfYRKZZeiIjNne5tFPDiIHN09K02P9y2gC7J1E2U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a3ae6c98d333d0485c622397379b8e788fab6a6", "width": 640, "height": 682}, {"url": "https://external-preview.redd.it/YVFsfYRKZZeiIjNne5tFPDiIHN09K02P9y2gC7J1E2U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=651f3888ff9af03bec825464ff23945959a00f6d", "width": 960, "height": 1024}, {"url": "https://external-preview.redd.it/YVFsfYRKZZeiIjNne5tFPDiIHN09K02P9y2gC7J1E2U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd9e9543a43ec482fa3feef62c78f643cd0333a6", "width": 1080, "height": 1152}], "variants": {}, "id": "QC9JE9G8hCpktX-YPJ4kEV8IJxPAON37FrYeq91Jsfc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109qatw", "is_robot_indexable": true, "report_reasons": null, "author": "cuss_sayer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109qatw/storage_spaces_stuck_on_jobstate_shutting_down/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109qatw/storage_spaces_stuck_on_jobstate_shutting_down/", "subreddit_subscribers": 665524, "created_utc": 1673497936.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI've been lurking around here for quite a while and I'm hoping someone could maybe help me in acquiring my first NAS.\n\nFirstly, here's what I would like to do. I'd like to have 4 bays for a start of 20TB with IronWolf drives that I already have. There will be Mac's and PC's connecting to this, and I'd like to be able to transfer files over the internet with something like SSH or Nextcloud. I will be storing mostly video and photo content on here. In the future I'd like to add Plex to it aswell, and it would be cool to have Dropbox support too. I don't need it to have any special power for transcoding; its hardware just needs to transfer files as fast as my network can (gigabit).\n\nMy old solution was a couple of large IronWolf's in my PC, but I've switched away to a MacBook and now use a couple of drive enclosures to connect to the drives here and there. (don't worry, the drives have been in great care.) This has been a temporary solution for a year now, and I now have the budget to get a proper NAS.\n\nI'd like everything to be pretty seamless and would not like to spend extra time troubleshooting issues in the future. I've worked with a QNAP before at my friend's place and kind of liked that, but I've never touched a Synology before. The consensus I've seen is that Synology is the better choice of an all-in-one since it has good software. So I think I'm in the right direction for an all-in-one, but I've been turned on a little bit to building my own, too. Plus, offerings like the DS920+ seems to be out of stock.\n\nI always thought that building a NAS from PC parts can yield more troubleshooting from something breaking (I think about issues I used to have building my PC's). But after seeing this subreddit and videos like those from Linus, it might be way more simple than I thought. Can someone with actual experience weigh into if this is true (and what you use)? I've seen TrueNAS and unRAID, and would be willing to lean into learning curves **if** **I don't have to troubleshoot so much in the future.** Maybe I'm wishing too much, but hey, I don't know. I would appreciate input from someone who knows more about this than me. Thanks.\n\nLuke", "author_fullname": "t2_ol7v3ls", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to buy or possibly build a new NAS for video and beyond", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109pzyf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673497909.0, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673497029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been lurking around here for quite a while and I&amp;#39;m hoping someone could maybe help me in acquiring my first NAS.&lt;/p&gt;\n\n&lt;p&gt;Firstly, here&amp;#39;s what I would like to do. I&amp;#39;d like to have 4 bays for a start of 20TB with IronWolf drives that I already have. There will be Mac&amp;#39;s and PC&amp;#39;s connecting to this, and I&amp;#39;d like to be able to transfer files over the internet with something like SSH or Nextcloud. I will be storing mostly video and photo content on here. In the future I&amp;#39;d like to add Plex to it aswell, and it would be cool to have Dropbox support too. I don&amp;#39;t need it to have any special power for transcoding; its hardware just needs to transfer files as fast as my network can (gigabit).&lt;/p&gt;\n\n&lt;p&gt;My old solution was a couple of large IronWolf&amp;#39;s in my PC, but I&amp;#39;ve switched away to a MacBook and now use a couple of drive enclosures to connect to the drives here and there. (don&amp;#39;t worry, the drives have been in great care.) This has been a temporary solution for a year now, and I now have the budget to get a proper NAS.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like everything to be pretty seamless and would not like to spend extra time troubleshooting issues in the future. I&amp;#39;ve worked with a QNAP before at my friend&amp;#39;s place and kind of liked that, but I&amp;#39;ve never touched a Synology before. The consensus I&amp;#39;ve seen is that Synology is the better choice of an all-in-one since it has good software. So I think I&amp;#39;m in the right direction for an all-in-one, but I&amp;#39;ve been turned on a little bit to building my own, too. Plus, offerings like the DS920+ seems to be out of stock.&lt;/p&gt;\n\n&lt;p&gt;I always thought that building a NAS from PC parts can yield more troubleshooting from something breaking (I think about issues I used to have building my PC&amp;#39;s). But after seeing this subreddit and videos like those from Linus, it might be way more simple than I thought. Can someone with actual experience weigh into if this is true (and what you use)? I&amp;#39;ve seen TrueNAS and unRAID, and would be willing to lean into learning curves &lt;strong&gt;if&lt;/strong&gt; &lt;strong&gt;I don&amp;#39;t have to troubleshoot so much in the future.&lt;/strong&gt; Maybe I&amp;#39;m wishing too much, but hey, I don&amp;#39;t know. I would appreciate input from someone who knows more about this than me. Thanks.&lt;/p&gt;\n\n&lt;p&gt;Luke&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109pzyf", "is_robot_indexable": true, "report_reasons": null, "author": "montana500", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/109pzyf/looking_to_buy_or_possibly_build_a_new_nas_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109pzyf/looking_to_buy_or_possibly_build_a_new_nas_for/", "subreddit_subscribers": 665524, "created_utc": 1673497029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Here someone says NAS is not a back up \n\nhttps://www.reddit.com/r/synology/comments/ga58cj/is_it_safe_to_use_a_nas_as_a_backup_instead_of_a/foxljta?utm_medium=android_app&amp;utm_source=share&amp;context=3\n\nHere a guy says to get a NAS \n\nhttps://www.reddit.com/r/torrents/comments/m6c2pk/where_do_you_store_your_larger_files/gr5067o?utm_medium=android_app&amp;utm_source=share&amp;context=3\n\nWould the better be to get 2 NAS'S or is that to expensive", "author_fullname": "t2_wk3o3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I don't know whether to have 3 external hard drives or a NAS I'm looking for the cheapest option", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109pfx6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673495402.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here someone says NAS is not a back up &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/synology/comments/ga58cj/is_it_safe_to_use_a_nas_as_a_backup_instead_of_a/foxljta?utm_medium=android_app&amp;amp;utm_source=share&amp;amp;context=3\"&gt;https://www.reddit.com/r/synology/comments/ga58cj/is_it_safe_to_use_a_nas_as_a_backup_instead_of_a/foxljta?utm_medium=android_app&amp;amp;utm_source=share&amp;amp;context=3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here a guy says to get a NAS &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/torrents/comments/m6c2pk/where_do_you_store_your_larger_files/gr5067o?utm_medium=android_app&amp;amp;utm_source=share&amp;amp;context=3\"&gt;https://www.reddit.com/r/torrents/comments/m6c2pk/where_do_you_store_your_larger_files/gr5067o?utm_medium=android_app&amp;amp;utm_source=share&amp;amp;context=3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would the better be to get 2 NAS&amp;#39;S or is that to expensive&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109pfx6", "is_robot_indexable": true, "report_reasons": null, "author": "CONFUS3D_DOTCOM", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109pfx6/i_dont_know_whether_to_have_3_external_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109pfx6/i_dont_know_whether_to_have_3_external_hard/", "subreddit_subscribers": 665524, "created_utc": 1673495402.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}