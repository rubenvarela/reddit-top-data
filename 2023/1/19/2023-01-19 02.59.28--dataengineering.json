{"kind": "Listing", "data": {"after": "t3_10ff5ts", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi all, its been a pretty awful day. Two months ago my boss transferred out to a new team, my team was integrated in with another. Sure enough this morning i woke up to an email letting me know im no longer a valued member of the team.   \n\n\nat this point im feeling in the pits. over the last few years ive gone above and beyond for this company, spending untold unpaid evening and weekend hours trying to push our projects forward.   \n\n\nnot sure about the next steps, but i felt like i needed to vent. if anyone out there knows of any DE opportunities, please DM me.", "author_fullname": "t2_706trkkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "just got laid off (FAANG)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fg07o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 155, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 155, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674069529.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi all, its been a pretty awful day. Two months ago my boss transferred out to a new team, my team was integrated in with another. Sure enough this morning i woke up to an email letting me know im no longer a valued member of the team.   &lt;/p&gt;\n\n&lt;p&gt;at this point im feeling in the pits. over the last few years ive gone above and beyond for this company, spending untold unpaid evening and weekend hours trying to push our projects forward.   &lt;/p&gt;\n\n&lt;p&gt;not sure about the next steps, but i felt like i needed to vent. if anyone out there knows of any DE opportunities, please DM me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10fg07o", "is_robot_indexable": true, "report_reasons": null, "author": "Foodwithfloyd", "discussion_type": null, "num_comments": 39, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fg07o/just_got_laid_off_faang/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fg07o/just_got_laid_off_faang/", "subreddit_subscribers": 86695, "created_utc": 1674069529.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We can leverage platforms like Coursera Plus and Udemy.", "author_fullname": "t2_5p0gzgth", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your favourite DBT training/learning resources in 2023? Moving part of our stack to DBT Cloud and opening up access to more data analysts. Gotta get these newbies trained up!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10f8yo6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674052844.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We can leverage platforms like Coursera Plus and Udemy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10f8yo6", "is_robot_indexable": true, "report_reasons": null, "author": "the-strange-ninja", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f8yo6/what_are_your_favourite_dbt_traininglearning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f8yo6/what_are_your_favourite_dbt_traininglearning/", "subreddit_subscribers": 86695, "created_utc": 1674052844.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working as a data engineer/data integration for 8+ years from traditional ETL to build data warehouses to cloud (mainly GCP) in large Biotech and start-up Fintech. \n\nI wanna use these skillets to work at a non-profit for animal welfare or organization rather than working for corporations. I wanna contribute my skill to something more meaningful. \n\nI've been searching but it's a bit tricky to find these organizations or they even have job postings. \n\nWhat do you recommend? Or site I should look at? Beside LinkedIn, indeed, zip recruiter, etc...", "author_fullname": "t2_16lpc7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to search for job posting at animal non profit organizations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10f7xf7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674050016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as a data engineer/data integration for 8+ years from traditional ETL to build data warehouses to cloud (mainly GCP) in large Biotech and start-up Fintech. &lt;/p&gt;\n\n&lt;p&gt;I wanna use these skillets to work at a non-profit for animal welfare or organization rather than working for corporations. I wanna contribute my skill to something more meaningful. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been searching but it&amp;#39;s a bit tricky to find these organizations or they even have job postings. &lt;/p&gt;\n\n&lt;p&gt;What do you recommend? Or site I should look at? Beside LinkedIn, indeed, zip recruiter, etc...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10f7xf7", "is_robot_indexable": true, "report_reasons": null, "author": "Totoro328", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f7xf7/how_to_search_for_job_posting_at_animal_non/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f7xf7/how_to_search_for_job_posting_at_animal_non/", "subreddit_subscribers": 86695, "created_utc": 1674050016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, is there a good implementation for Delta Sharing usable for on-prem deployments?\n\nThe reference implementation provided doesn't support adding new shares without modifying the configuration file and restarting the server, also there's no user management there.", "author_fullname": "t2_4clu4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Sharing on premise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ezch7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674020331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, is there a good implementation for Delta Sharing usable for on-prem deployments?&lt;/p&gt;\n\n&lt;p&gt;The reference implementation provided doesn&amp;#39;t support adding new shares without modifying the configuration file and restarting the server, also there&amp;#39;s no user management there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ezch7", "is_robot_indexable": true, "report_reasons": null, "author": "inteloid", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ezch7/delta_sharing_on_premise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ezch7/delta_sharing_on_premise/", "subreddit_subscribers": 86695, "created_utc": 1674020331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_502r5jht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to make a change, resume feedback / advice appreciated for junior DE role.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10f2eow", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Sy52Ie6JvJWizUvuv5xo1U7nvqu7OXkc1LzxFEdNZMA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674031173.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zbpozed52tca1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zbpozed52tca1.jpg?auto=webp&amp;v=enabled&amp;s=24840a9556acbb5748715338b28ad0e4a48a2087", "width": 1203, "height": 1723}, "resolutions": [{"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=538cffe182f66e8f7d78190dc8c2928e9437cb73", "width": 108, "height": 154}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3786ab72038cbe2a4d83014c4d792028820cde4f", "width": 216, "height": 309}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1718595b0b5c4e07764565ec79ed3e4fa6e79462", "width": 320, "height": 458}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b223390d8ac0a08a2cf930a9914404460722901", "width": 640, "height": 916}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d5d0f9c3f577c1340f4229ba95fcbb4ff2bda9d", "width": 960, "height": 1374}, {"url": "https://preview.redd.it/zbpozed52tca1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2a40325e0f0927f7547c023f0dfe71f6a0c89ea", "width": 1080, "height": 1546}], "variants": {}, "id": "2QiMbQXoSVTA6udrYgV4EfBnJyQkKItlm1SrK4_k-Bs"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10f2eow", "is_robot_indexable": true, "report_reasons": null, "author": "toem033", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f2eow/looking_to_make_a_change_resume_feedback_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zbpozed52tca1.jpg", "subreddit_subscribers": 86695, "created_utc": 1674031173.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When you do a merge in SQL, is it doing a full table scan of the target? If so, isn\u2019t that something we want to avoid?", "author_fullname": "t2_5ukitegd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MERGE statement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10exb2o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674014249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When you do a merge in SQL, is it doing a full table scan of the target? If so, isn\u2019t that something we want to avoid?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10exb2o", "is_robot_indexable": true, "report_reasons": null, "author": "burningburnerbern", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10exb2o/merge_statement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10exb2o/merge_statement/", "subreddit_subscribers": 86695, "created_utc": 1674014249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a little shill-y, but I think it\u2019s cool and I think others here will too.\n\nIf you haven\u2019t heard of Materialize, it\u2019s a database that incrementally updates query results as new data flows in from Kafka or Postgres logical replication. It\u2019s different from typical databases in that results are updated on *write* using a stream processing engine rather than recomputed from scratch on read. That means reads are typically super fast, even for really complicated views with lots of joins.\n\nOne of the first things I had to learn as a Field Engineer at Materialize was how to optimize SQL joins to help our customers save on memory (and $). To do that, I made a couple of updates to one of Frank McSherry\u2019s blogs, which were published today! I\u2019d love to see what you think!\n\nhttps://materialize.com/blog/delta-joins/", "author_fullname": "t2_5p00kusf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimize Joins in Materialize with Delta Queries and Late Materialization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ffdvx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674068433.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674068064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a little shill-y, but I think it\u2019s cool and I think others here will too.&lt;/p&gt;\n\n&lt;p&gt;If you haven\u2019t heard of Materialize, it\u2019s a database that incrementally updates query results as new data flows in from Kafka or Postgres logical replication. It\u2019s different from typical databases in that results are updated on &lt;em&gt;write&lt;/em&gt; using a stream processing engine rather than recomputed from scratch on read. That means reads are typically super fast, even for really complicated views with lots of joins.&lt;/p&gt;\n\n&lt;p&gt;One of the first things I had to learn as a Field Engineer at Materialize was how to optimize SQL joins to help our customers save on memory (and $). To do that, I made a couple of updates to one of Frank McSherry\u2019s blogs, which were published today! I\u2019d love to see what you think!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://materialize.com/blog/delta-joins/\"&gt;https://materialize.com/blog/delta-joins/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?auto=webp&amp;v=enabled&amp;s=9fc4396a318c2807c8b7eb9925d82c522295c4c5", "width": 1636, "height": 656}, "resolutions": [{"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c95088825ab701a767ae737724f85f494144ce90", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f02a3d2de233ef59877a3105038f7f04fbe9971", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1789d3f9b845c9675d57ff92fdc5f2501fa76646", "width": 320, "height": 128}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d892c0c41ed4dbb8ff6c9f253c903b791694699d", "width": 640, "height": 256}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=654e9ae06191dae53c2255b2275a46abd8b59c9b", "width": 960, "height": 384}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ed95ec3916adc3bcb738ac68f52e0aaa922bef6", "width": 1080, "height": 433}], "variants": {}, "id": "IMNd2US1USXqACJWc3sq8tqHd6qPrbtxC4Zq9e0nYj4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10ffdvx", "is_robot_indexable": true, "report_reasons": null, "author": "Chuck-Alt-Delete", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ffdvx/optimize_joins_in_materialize_with_delta_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ffdvx/optimize_joins_in_materialize_with_delta_queries/", "subreddit_subscribers": 86695, "created_utc": 1674068064.0, "num_crossposts": 4, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to flatten the JSON file\n\nstructure of the file looks like this \n\n    root\n     |-- items: array (nullable = true)\n     |    |-- element: struct (containsNull = true)\n     |    |    |-- _links: struct (nullable = true)\n     |    |    |    |-- self: struct (nullable = true)\n     |    |    |    |    |-- href: string (nullable = true)\n     |    |    |-- code: string (nullable = true)\n     |    |    |-- labels: struct (nullable = true)\n     |    |    |    |-- bg_BG: string (nullable = true)\n     |    |    |    |-- cs_CZ: string (nullable = true)\n     |    |    |    |-- da_DK: string (nullable = true)\n     |    |    |    |-- de_AT: string (nullable = true)\n     |    |    |    |-- de_BE: string (nullable = true)\n     |    |    |    |-- de_CH: string (nullable = true)\n     |    |    |    |-- de_DE: string (nullable = true)\n     |    |    |    |-- el_GR: string (nullable = true)\n     |    |    |    |-- en_AU: string (nullable = true)\n     |    |    |    |-- en_CA: string (nullable = true)\n     |    |    |    |-- en_CH: string (nullable = true)\n     |    |    |    |-- en_GB: string (nullable = true)\n     |    |    |    |-- en_IE: string (nullable = true)\n     |    |    |    |-- en_IN: string (nullable = true)\n     |    |    |    |-- en_MY: string (nullable = true)\n     |    |    |    |-- en_NZ: string (nullable = true)\n     |    |    |    |-- en_PH: string (nullable = true)\n     |    |    |    |-- en_SG: string (nullable = true)\n     |    |    |    |-- en_US: string (nullable = true)\n     |    |    |    |-- en_US_America: string (nullable = true)\n     |    |    |    |-- es_ES: string (nullable = true)\n     |    |    |    |-- et_EE: string (nullable = true)\n     |    |    |    |-- fi_FI: string (nullable = true)\n     |    |    |    |-- fr_BE: string (nullable = true)\n     |    |    |    |-- fr_CH: string (nullable = true)\n     |    |    |    |-- fr_FR: string (nullable = true)\n     |    |    |    |-- hr_HR: string (nullable = true)\n     |    |    |    |-- hu_HU: string (nullable = true)\n     |    |    |    |-- id_ID: string (nullable = true)\n     |    |    |    |-- is_IS: string (nullable = true)\n     |    |    |    |-- it_CH: string (nullable = true)\n     |    |    |    |-- it_IT: string (nullable = true)\n     |    |    |    |-- ja_JP: string (nullable = true)\n     |    |    |    |-- ko_KR: string (nullable = true)\n     |    |    |    |-- lt_LT: string (nullable = true)\n     |    |    |    |-- lv_LV: string (nullable = true)\n     |    |    |    |-- ms_MY: string (nullable = true)\n     |    |    |    |-- nb_NO: string (nullable = true)\n     |    |    |    |-- nl_BE: string (nullable = true)\n     |    |    |    |-- nl_NL: string (nullable = true)\n     |    |    |    |-- nn_NO: string (nullable = true)\n     |    |    |    |-- pl_PL: string (nullable = true)\n     |    |    |    |-- pt_PT: string (nullable = true)\n     |    |    |    |-- ro_RO: string (nullable = true)\n     |    |    |    |-- ru_RU: string (nullable = true)\n     |    |    |    |-- sk_SK: string (nullable = true)\n     |    |    |    |-- sl_SI: string (nullable = true)\n     |    |    |    |-- sr_Cyrl_RS: string (nullable = true)\n     |    |    |    |-- sv_SE: string (nullable = true)\n     |    |    |    |-- th_TH: string (nullable = true)\n     |    |    |    |-- tr_TR: string (nullable = true)\n     |    |    |    |-- uk_UA: string (nullable = true)\n     |    |    |    |-- vi_VN: string (nullable = true)\n     |    |    |    |-- zh_CN: string (nullable = true)\n     |    |    |    |-- zh_TW: string (nullable = true)\n     |    |    |-- parent: string (nullable = true)\n     |    |    |-- updated: string (nullable = true)\n\nMy function is this \n\n    def flatten_df(nested_df):\n        flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'string']\n        nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'struct']\n    \n        flat_df = nested_df.select(flat_cols +\n                                   [col(nc+'.'+c).alias(nc+'_'+c)\n                                    for nc in nested_cols\n                                    for c in nested_df.select(nc+'.*').columns])\n        return flat_df\n    \n    # Top level hierarchy\n    df = df.select('_embedded.*')\n    #Reaching the lower level called \"items\"\n    df1 = df.select(explode(df.items).alias('required'))\n    # Creating dataframe which will be passed to flatten_df to flatten entire data under \"items\" hierarchy\n    df2 = df1.select('required.*')\n    final = flatten_df(df2)\n    display(final)\n\nthis function takes the `lables`   column keys and puts them as seperate columns ( please see the image [https://imgur.com/a/xa5VmCu](https://imgur.com/a/xa5VmCu) )\n\n&amp;#x200B;\n\nwhat I want is that function should give only two columns from `lables` column named Lable\\_key Lable\\_value  as shown in the image  [https://imgur.com/a/sTW9sSx](https://imgur.com/a/sTW9sSx)", "author_fullname": "t2_56g5f4cg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "flatten nested JSON file pyspark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10f1kh1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674027896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to flatten the JSON file&lt;/p&gt;\n\n&lt;p&gt;structure of the file looks like this &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;root\n |-- items: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- _links: struct (nullable = true)\n |    |    |    |-- self: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |-- code: string (nullable = true)\n |    |    |-- labels: struct (nullable = true)\n |    |    |    |-- bg_BG: string (nullable = true)\n |    |    |    |-- cs_CZ: string (nullable = true)\n |    |    |    |-- da_DK: string (nullable = true)\n |    |    |    |-- de_AT: string (nullable = true)\n |    |    |    |-- de_BE: string (nullable = true)\n |    |    |    |-- de_CH: string (nullable = true)\n |    |    |    |-- de_DE: string (nullable = true)\n |    |    |    |-- el_GR: string (nullable = true)\n |    |    |    |-- en_AU: string (nullable = true)\n |    |    |    |-- en_CA: string (nullable = true)\n |    |    |    |-- en_CH: string (nullable = true)\n |    |    |    |-- en_GB: string (nullable = true)\n |    |    |    |-- en_IE: string (nullable = true)\n |    |    |    |-- en_IN: string (nullable = true)\n |    |    |    |-- en_MY: string (nullable = true)\n |    |    |    |-- en_NZ: string (nullable = true)\n |    |    |    |-- en_PH: string (nullable = true)\n |    |    |    |-- en_SG: string (nullable = true)\n |    |    |    |-- en_US: string (nullable = true)\n |    |    |    |-- en_US_America: string (nullable = true)\n |    |    |    |-- es_ES: string (nullable = true)\n |    |    |    |-- et_EE: string (nullable = true)\n |    |    |    |-- fi_FI: string (nullable = true)\n |    |    |    |-- fr_BE: string (nullable = true)\n |    |    |    |-- fr_CH: string (nullable = true)\n |    |    |    |-- fr_FR: string (nullable = true)\n |    |    |    |-- hr_HR: string (nullable = true)\n |    |    |    |-- hu_HU: string (nullable = true)\n |    |    |    |-- id_ID: string (nullable = true)\n |    |    |    |-- is_IS: string (nullable = true)\n |    |    |    |-- it_CH: string (nullable = true)\n |    |    |    |-- it_IT: string (nullable = true)\n |    |    |    |-- ja_JP: string (nullable = true)\n |    |    |    |-- ko_KR: string (nullable = true)\n |    |    |    |-- lt_LT: string (nullable = true)\n |    |    |    |-- lv_LV: string (nullable = true)\n |    |    |    |-- ms_MY: string (nullable = true)\n |    |    |    |-- nb_NO: string (nullable = true)\n |    |    |    |-- nl_BE: string (nullable = true)\n |    |    |    |-- nl_NL: string (nullable = true)\n |    |    |    |-- nn_NO: string (nullable = true)\n |    |    |    |-- pl_PL: string (nullable = true)\n |    |    |    |-- pt_PT: string (nullable = true)\n |    |    |    |-- ro_RO: string (nullable = true)\n |    |    |    |-- ru_RU: string (nullable = true)\n |    |    |    |-- sk_SK: string (nullable = true)\n |    |    |    |-- sl_SI: string (nullable = true)\n |    |    |    |-- sr_Cyrl_RS: string (nullable = true)\n |    |    |    |-- sv_SE: string (nullable = true)\n |    |    |    |-- th_TH: string (nullable = true)\n |    |    |    |-- tr_TR: string (nullable = true)\n |    |    |    |-- uk_UA: string (nullable = true)\n |    |    |    |-- vi_VN: string (nullable = true)\n |    |    |    |-- zh_CN: string (nullable = true)\n |    |    |    |-- zh_TW: string (nullable = true)\n |    |    |-- parent: string (nullable = true)\n |    |    |-- updated: string (nullable = true)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;My function is this &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def flatten_df(nested_df):\n    flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == &amp;#39;string&amp;#39;]\n    nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == &amp;#39;struct&amp;#39;]\n\n    flat_df = nested_df.select(flat_cols +\n                               [col(nc+&amp;#39;.&amp;#39;+c).alias(nc+&amp;#39;_&amp;#39;+c)\n                                for nc in nested_cols\n                                for c in nested_df.select(nc+&amp;#39;.*&amp;#39;).columns])\n    return flat_df\n\n# Top level hierarchy\ndf = df.select(&amp;#39;_embedded.*&amp;#39;)\n#Reaching the lower level called &amp;quot;items&amp;quot;\ndf1 = df.select(explode(df.items).alias(&amp;#39;required&amp;#39;))\n# Creating dataframe which will be passed to flatten_df to flatten entire data under &amp;quot;items&amp;quot; hierarchy\ndf2 = df1.select(&amp;#39;required.*&amp;#39;)\nfinal = flatten_df(df2)\ndisplay(final)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;this function takes the &lt;code&gt;lables&lt;/code&gt;   column keys and puts them as seperate columns ( please see the image &lt;a href=\"https://imgur.com/a/xa5VmCu\"&gt;https://imgur.com/a/xa5VmCu&lt;/a&gt; )&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;what I want is that function should give only two columns from &lt;code&gt;lables&lt;/code&gt; column named Lable_key Lable_value  as shown in the image  &lt;a href=\"https://imgur.com/a/sTW9sSx\"&gt;https://imgur.com/a/sTW9sSx&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?auto=webp&amp;v=enabled&amp;s=7a58cc0ee2d095f64688402d8012d36b9b6a66d2", "width": 1317, "height": 184}, "resolutions": [{"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f66ce948572caabe1038462bad873d97fc1a6f00", "width": 108, "height": 15}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19b8cfa11924aba9692b30f9f090ba1b2bdc5cf6", "width": 216, "height": 30}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d770f580c8e9f5a3b75d918f355a185747d363e", "width": 320, "height": 44}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e91a890cf3c85f5d7f35fc36efa574576ea686b6", "width": 640, "height": 89}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca2e0618fd945164bb0ee5d1f453f615f3370c12", "width": 960, "height": 134}, {"url": "https://external-preview.redd.it/tm3pGXLMNkap3E02C1aTihqrxwC3Dux36kk5J2fUaxo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d83982ce3a8ecd415011b397cfd95a71b1a3ba4", "width": 1080, "height": 150}], "variants": {}, "id": "-PIFhkx_4ZX0v6qeVW9b0POiuvOW5532KuWALDCw1dU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10f1kh1", "is_robot_indexable": true, "report_reasons": null, "author": "9gg6", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f1kh1/flatten_nested_json_file_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f1kh1/flatten_nested_json_file_pyspark/", "subreddit_subscribers": 86695, "created_utc": 1674027896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can somebody help? In over my head trying to run a product assessment for Databricks with almost no guidance for the use case. Would be helpful to hear what folks really like about the product + what questions people wish they had asked or better understood before becoming a customer", "author_fullname": "t2_tye5ydmn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks evaluation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10euyrz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674007714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can somebody help? In over my head trying to run a product assessment for Databricks with almost no guidance for the use case. Would be helpful to hear what folks really like about the product + what questions people wish they had asked or better understood before becoming a customer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10euyrz", "is_robot_indexable": true, "report_reasons": null, "author": "LuckyChopsSOS", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10euyrz/databricks_evaluation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10euyrz/databricks_evaluation/", "subreddit_subscribers": 86695, "created_utc": 1674007714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a backend engineer and try to work on the data import part for my company. we have a GraphQL server as a gateway between the client and the internal server. GraphQL is the only thing exposed to the outside world.  \n\n\nI am thinking about writing scripts to map the CSV file to the GraphQL mutation query and import.  \nBut I would like to have a better way to handle the import jobs. e.g. some import depends on the previous import result, retry when the job is fail...  \n\n\nI am looking at the Airflow tool now. I am wondering if it is a good starting point for me.  \n\n\nThank you.", "author_fullname": "t2_4hu7xbt4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "would airflow be good for this use case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10fo4ks", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674090265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a backend engineer and try to work on the data import part for my company. we have a GraphQL server as a gateway between the client and the internal server. GraphQL is the only thing exposed to the outside world.  &lt;/p&gt;\n\n&lt;p&gt;I am thinking about writing scripts to map the CSV file to the GraphQL mutation query and import.&lt;br/&gt;\nBut I would like to have a better way to handle the import jobs. e.g. some import depends on the previous import result, retry when the job is fail...  &lt;/p&gt;\n\n&lt;p&gt;I am looking at the Airflow tool now. I am wondering if it is a good starting point for me.  &lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fo4ks", "is_robot_indexable": true, "report_reasons": null, "author": "itsMoreAComment", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fo4ks/would_airflow_be_good_for_this_use_case/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fo4ks/would_airflow_be_good_for_this_use_case/", "subreddit_subscribers": 86695, "created_utc": 1674090265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am an intern with a long leash. My current python script calls a series of apis and puts the code into a dataframe. Let's call this script 1 and df1. \n\n&amp;#x200B;\n\nEach day I would like this script to run on a schedule and then append either a csv file or SQL table. \n\nMy immediate group uses AWS, and someone in my group said I could go ahead and use their SQL server. My experience with SQL is MySQL workbench, and I've connected to an AWS server before. \n\n&amp;#x200B;\n\nI was able to simply run my notebook in ML studio in azure on my personal computer, but I guess that isn't configured at work so that isn't an option. We do have pyspark and databricks through azure. I have deployed to an EC2 in aws before for a different project, but got really lost with aws lamda trying to schedule and I don't have a linux background. \n\n&amp;#x200B;\n\nI feel comfortable in python and can do some ML, and I would like to just use scikit-learn in python for predictive analytics down the line. It seems like scheduling batches is a bit more drag and drop in azure. This pipeline would probably just update a csv in the data lake each day. I am leaning azure.\n\n&amp;#x200B;\n\nwhich platform is the easiest to run this pipeline and schedule the code to run once per day?", "author_fullname": "t2_pqpunsd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "azure or aws? daily batch api calls --&gt; pandas dataframe --&gt; sql or csv --&gt; powerbi", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10febor", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674065611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an intern with a long leash. My current python script calls a series of apis and puts the code into a dataframe. Let&amp;#39;s call this script 1 and df1. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Each day I would like this script to run on a schedule and then append either a csv file or SQL table. &lt;/p&gt;\n\n&lt;p&gt;My immediate group uses AWS, and someone in my group said I could go ahead and use their SQL server. My experience with SQL is MySQL workbench, and I&amp;#39;ve connected to an AWS server before. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was able to simply run my notebook in ML studio in azure on my personal computer, but I guess that isn&amp;#39;t configured at work so that isn&amp;#39;t an option. We do have pyspark and databricks through azure. I have deployed to an EC2 in aws before for a different project, but got really lost with aws lamda trying to schedule and I don&amp;#39;t have a linux background. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I feel comfortable in python and can do some ML, and I would like to just use scikit-learn in python for predictive analytics down the line. It seems like scheduling batches is a bit more drag and drop in azure. This pipeline would probably just update a csv in the data lake each day. I am leaning azure.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;which platform is the easiest to run this pipeline and schedule the code to run once per day?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10febor", "is_robot_indexable": true, "report_reasons": null, "author": "ClimatePhilosopher", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10febor/azure_or_aws_daily_batch_api_calls_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10febor/azure_or_aws_daily_batch_api_calls_pandas/", "subreddit_subscribers": 86695, "created_utc": 1674065611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am helping a very small company to manage their sales data generated from a few sources(web stores). \n\nThe data is under 50k rows so ideally I would like to utilize cloud storage options that have a free tier for future scalability as they are growing.\n\nFor now I am thinking of using webhooks to start. \n\nAlso a little context, I am have experience as a analyst but would like to move into engineering and would want to use this as a little project to add in my portfolio.", "author_fullname": "t2_687fm3oi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple solution for very small datasets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fltqe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674084333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am helping a very small company to manage their sales data generated from a few sources(web stores). &lt;/p&gt;\n\n&lt;p&gt;The data is under 50k rows so ideally I would like to utilize cloud storage options that have a free tier for future scalability as they are growing.&lt;/p&gt;\n\n&lt;p&gt;For now I am thinking of using webhooks to start. &lt;/p&gt;\n\n&lt;p&gt;Also a little context, I am have experience as a analyst but would like to move into engineering and would want to use this as a little project to add in my portfolio.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fltqe", "is_robot_indexable": true, "report_reasons": null, "author": "mysterymalts", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fltqe/simple_solution_for_very_small_datasets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fltqe/simple_solution_for_very_small_datasets/", "subreddit_subscribers": 86695, "created_utc": 1674084333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What does a career path look like for someone in a leadership role in the data field? What are the common job titles and responsibilities one can expect to have at each step, and what skills and qualifications are typically required to advance in this field? I'm a head of data in a scale-up but am not sure of what my career progression will look like. It's less straightforward than when working in \"traditional\" teams. Would like to hear about other's experience", "author_fullname": "t2_9blh4yzs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data leader career progression", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fhyud", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674074170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What does a career path look like for someone in a leadership role in the data field? What are the common job titles and responsibilities one can expect to have at each step, and what skills and qualifications are typically required to advance in this field? I&amp;#39;m a head of data in a scale-up but am not sure of what my career progression will look like. It&amp;#39;s less straightforward than when working in &amp;quot;traditional&amp;quot; teams. Would like to hear about other&amp;#39;s experience&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10fhyud", "is_robot_indexable": true, "report_reasons": null, "author": "Strict_Algae3766", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fhyud/data_leader_career_progression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fhyud/data_leader_career_progression/", "subreddit_subscribers": 86695, "created_utc": 1674074170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# [Jailer Database Tools.](https://wisser.github.io/Jailer/)\n\nJailer is a tool for database subsetting and relational data browsing.\n\nIt creates small slices from your database and lets you navigate through your database following the relationships.Ideal for creating small samples of test data or for local problem analysis with relevant production data.\n\nThe Subsetter creates small slices from your database (consistent and reverentially intact) as SQL (topologically sorted), DbUnit records or XML. Ideal for creating small samples of test data or for local problem analysis with relevant production data.\n\nThe Data Browser lets you navigate through your database following the relationships (foreign key-based or user-defined) between tables.\n\n# Features\n\nExports consistent and reverentially intact row-sets from your productive database and imports the data into your development and test environment.\n\nImproves database performance by removing and archiving obsolete data without violating integrity.\n\nGenerates topologically sorted SQL-DML, hierarchically structured XML and DbUnit datasets.\n\nData Browsing. Navigate bidirectionally through the database by following foreign-key-based or user-defined relationships.\n\nSQL Console with code completion, syntax highlighting and database metadata visualization.\n\nA demo database is included with which you can get a first impression without any configuration effort.", "author_fullname": "t2_5sa5b0ia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New release of Jailer database tools publicized", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10f3lpo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674035839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;&lt;a href=\"https://wisser.github.io/Jailer/\"&gt;Jailer Database Tools.&lt;/a&gt;&lt;/h1&gt;\n\n&lt;p&gt;Jailer is a tool for database subsetting and relational data browsing.&lt;/p&gt;\n\n&lt;p&gt;It creates small slices from your database and lets you navigate through your database following the relationships.Ideal for creating small samples of test data or for local problem analysis with relevant production data.&lt;/p&gt;\n\n&lt;p&gt;The Subsetter creates small slices from your database (consistent and reverentially intact) as SQL (topologically sorted), DbUnit records or XML. Ideal for creating small samples of test data or for local problem analysis with relevant production data.&lt;/p&gt;\n\n&lt;p&gt;The Data Browser lets you navigate through your database following the relationships (foreign key-based or user-defined) between tables.&lt;/p&gt;\n\n&lt;h1&gt;Features&lt;/h1&gt;\n\n&lt;p&gt;Exports consistent and reverentially intact row-sets from your productive database and imports the data into your development and test environment.&lt;/p&gt;\n\n&lt;p&gt;Improves database performance by removing and archiving obsolete data without violating integrity.&lt;/p&gt;\n\n&lt;p&gt;Generates topologically sorted SQL-DML, hierarchically structured XML and DbUnit datasets.&lt;/p&gt;\n\n&lt;p&gt;Data Browsing. Navigate bidirectionally through the database by following foreign-key-based or user-defined relationships.&lt;/p&gt;\n\n&lt;p&gt;SQL Console with code completion, syntax highlighting and database metadata visualization.&lt;/p&gt;\n\n&lt;p&gt;A demo database is included with which you can get a first impression without any configuration effort.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "10f3lpo", "is_robot_indexable": true, "report_reasons": null, "author": "Plane-Discussion", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10f3lpo/new_release_of_jailer_database_tools_publicized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10f3lpo/new_release_of_jailer_database_tools_publicized/", "subreddit_subscribers": 86695, "created_utc": 1674035839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_alka6pjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resume Review request - BA to DE Pivot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10ezuz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9KYzoafqeDnG-_PpUWLOByjbMlz1QfJEuPzJVCCVOxo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674021917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/lrfnoo02tqca1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/lrfnoo02tqca1.png?auto=webp&amp;v=enabled&amp;s=91b8620f075e5ce896f4b5e14c0d567427cf6f54", "width": 422, "height": 516}, "resolutions": [{"url": "https://preview.redd.it/lrfnoo02tqca1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85d093ca93176a394919ff46e28fecee90627c97", "width": 108, "height": 132}, {"url": "https://preview.redd.it/lrfnoo02tqca1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51bfb89f9e5b6abaed0c5f1291aaf34be7a05594", "width": 216, "height": 264}, {"url": "https://preview.redd.it/lrfnoo02tqca1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5dbe508bd3270e09688cf52b82f2c1631f491b38", "width": 320, "height": 391}], "variants": {}, "id": "NKWY7CXzH_LGoyIUo72kU-hbkmDTtkN7Hugu4VkV8Wg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10ezuz9", "is_robot_indexable": true, "report_reasons": null, "author": "depressedbutsassy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ezuz9/resume_review_request_ba_to_de_pivot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/lrfnoo02tqca1.png", "subreddit_subscribers": 86695, "created_utc": 1674021917.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I did a small test and it seemed to be faster but when is the hash function even supposed to be used? \n\nIn star schema, surrogate keys are traditionally built using autoincrement. Is that still the best way in SF or was the hash function supposed to be used here? Assuming number of rows doesn\u2019t exceed a billion.", "author_fullname": "t2_kew1pm23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In Snowflake, is joining by a hash column more performant then joining by multiple text columns?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10fpc1f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674093596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did a small test and it seemed to be faster but when is the hash function even supposed to be used? &lt;/p&gt;\n\n&lt;p&gt;In star schema, surrogate keys are traditionally built using autoincrement. Is that still the best way in SF or was the hash function supposed to be used here? Assuming number of rows doesn\u2019t exceed a billion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fpc1f", "is_robot_indexable": true, "report_reasons": null, "author": "Critical_Chart_6513", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fpc1f/in_snowflake_is_joining_by_a_hash_column_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fpc1f/in_snowflake_is_joining_by_a_hash_column_more/", "subreddit_subscribers": 86695, "created_utc": 1674093596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in the biotech industry, where we actually generate a lot of our important data sources locally. Instruments like mass spectrometers and next generation sequencers make large datasets that are best analyzed in the cloud.\n\nI've been bouncing between a number of tools to automate data movement into the cloud, but I still feel like there's a better way to engineer it. I've used 3rd party tools (BioBright, TntDrive), and AWS services (Data Sync, Storage Gateway, CLI). But either the solution is too overengineered or it's easily breakable... I know this is a common use case, so I'm curious if others have found anything they like using?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you automate data movement into the cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fmx2h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674087097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in the biotech industry, where we actually generate a lot of our important data sources locally. Instruments like mass spectrometers and next generation sequencers make large datasets that are best analyzed in the cloud.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been bouncing between a number of tools to automate data movement into the cloud, but I still feel like there&amp;#39;s a better way to engineer it. I&amp;#39;ve used 3rd party tools (BioBright, TntDrive), and AWS services (Data Sync, Storage Gateway, CLI). But either the solution is too overengineered or it&amp;#39;s easily breakable... I know this is a common use case, so I&amp;#39;m curious if others have found anything they like using?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fmx2h", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fmx2h/how_do_you_automate_data_movement_into_the_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fmx2h/how_do_you_automate_data_movement_into_the_cloud/", "subreddit_subscribers": 86695, "created_utc": 1674087097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working on processing a large number (+/- 900) of smaller tables. These tables have to be loaded from a landing zone where they arrive in CSV format to a raw layer in delta format. \n\nThese tables have to be loaded on a daily basis and I'm currently using a synapse spark notebook to do this.\n\nMy first solution was to store all the tables in a list and then use a for loop to process these tables on by one. This was however taking a long time so I set up a multi threaded approach using the python multiprocessing library.\n\nThis lead to some significant performance gains but Im not sure that this is the best approach as the performance currently varies run by run.\n\nI'm also not sure how to optimally setup a spark cluster for such an use case.\n\n\nSo I was curious how you guys are currently solving a problem like this and if there are people also using multi threading how you then optimize the spark process for such a thing. Like do you have a large driver and less executors or vice versa and how do you determine the amount of threads to create.\n\nMany thanks in advance!", "author_fullname": "t2_71slaawn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you processing large numbers of smaller tables using spark.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fllzt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674083819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on processing a large number (+/- 900) of smaller tables. These tables have to be loaded from a landing zone where they arrive in CSV format to a raw layer in delta format. &lt;/p&gt;\n\n&lt;p&gt;These tables have to be loaded on a daily basis and I&amp;#39;m currently using a synapse spark notebook to do this.&lt;/p&gt;\n\n&lt;p&gt;My first solution was to store all the tables in a list and then use a for loop to process these tables on by one. This was however taking a long time so I set up a multi threaded approach using the python multiprocessing library.&lt;/p&gt;\n\n&lt;p&gt;This lead to some significant performance gains but Im not sure that this is the best approach as the performance currently varies run by run.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also not sure how to optimally setup a spark cluster for such an use case.&lt;/p&gt;\n\n&lt;p&gt;So I was curious how you guys are currently solving a problem like this and if there are people also using multi threading how you then optimize the spark process for such a thing. Like do you have a large driver and less executors or vice versa and how do you determine the amount of threads to create.&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fllzt", "is_robot_indexable": true, "report_reasons": null, "author": "blindbox2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fllzt/how_are_you_processing_large_numbers_of_smaller/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fllzt/how_are_you_processing_large_numbers_of_smaller/", "subreddit_subscribers": 86695, "created_utc": 1674083819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI'm relatively new to DynamoDB and I've been tasked with moving application from our DynamoDB database into a format that can be consumed by our existing snowflake data-lake. \n\nThis application is replacing an existing application that had a more traditional SQL/RDBMS style data back-end, and the lake layer was just a synchronized copy of the application layer.  \n\nI'm scratching my head at how to approach this. With a single table design the data is as far from normalized as it can get. Thankfully the dev team has created an urn as part of each item and I'm thinking I can use that to reconstruct the entities of the application (did I mention that the access patterns, and entities are only loosely documented?) \n\nThat leaves me with the following questions and open for advice?\n\n* How do I successfully investigate the heterogeneous data in DynamoDB to verify what entities I can stitch back together (I've tried Athena, but its unable to successfully crawl the data due to some fields being json objects / structs that are inconsistent).\n* Assuming I can extract the data (e.g. export to s3, or use Glue for some more sophisticated processing) how can I hope to consistently keep the s3 data lake up to date (Maybe use Kinesis and the associated lambda to find the \"record\" in s3 and update it accordingly)", "author_fullname": "t2_6d49ikq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading/Normalizing Data from Single Table Designed DynamoDB Consumption in AWS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fkdox", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674080856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m relatively new to DynamoDB and I&amp;#39;ve been tasked with moving application from our DynamoDB database into a format that can be consumed by our existing snowflake data-lake. &lt;/p&gt;\n\n&lt;p&gt;This application is replacing an existing application that had a more traditional SQL/RDBMS style data back-end, and the lake layer was just a synchronized copy of the application layer.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m scratching my head at how to approach this. With a single table design the data is as far from normalized as it can get. Thankfully the dev team has created an urn as part of each item and I&amp;#39;m thinking I can use that to reconstruct the entities of the application (did I mention that the access patterns, and entities are only loosely documented?) &lt;/p&gt;\n\n&lt;p&gt;That leaves me with the following questions and open for advice?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How do I successfully investigate the heterogeneous data in DynamoDB to verify what entities I can stitch back together (I&amp;#39;ve tried Athena, but its unable to successfully crawl the data due to some fields being json objects / structs that are inconsistent).&lt;/li&gt;\n&lt;li&gt;Assuming I can extract the data (e.g. export to s3, or use Glue for some more sophisticated processing) how can I hope to consistently keep the s3 data lake up to date (Maybe use Kinesis and the associated lambda to find the &amp;quot;record&amp;quot; in s3 and update it accordingly)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fkdox", "is_robot_indexable": true, "report_reasons": null, "author": "poppinstacks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fkdox/loadingnormalizing_data_from_single_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fkdox/loadingnormalizing_data_from_single_table/", "subreddit_subscribers": 86695, "created_utc": 1674080856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data scientist looking to upskill in data engineering. I'm hoping to get some knowledge from this subreddit. I have a small ML driven Dash app which reads data from a data warehouse and runs fine locally. \n\nI wish to containerised this app using Docker and host it on a cloud server. What is the most secure way to handle prodiving this app a service account to access my database? Can I hardcode it into my python code? Do I upload a copy of my sa.json file while containerising? Something else?\n\nAny help is appreciated.", "author_fullname": "t2_2gzsok4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice working with service accounts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fjczk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674078494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data scientist looking to upskill in data engineering. I&amp;#39;m hoping to get some knowledge from this subreddit. I have a small ML driven Dash app which reads data from a data warehouse and runs fine locally. &lt;/p&gt;\n\n&lt;p&gt;I wish to containerised this app using Docker and host it on a cloud server. What is the most secure way to handle prodiving this app a service account to access my database? Can I hardcode it into my python code? Do I upload a copy of my sa.json file while containerising? Something else?&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fjczk", "is_robot_indexable": true, "report_reasons": null, "author": "ciarandeceol1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fjczk/best_practice_working_with_service_accounts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fjczk/best_practice_working_with_service_accounts/", "subreddit_subscribers": 86695, "created_utc": 1674078494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand that SF is the out-of-the-box DWH that might cost a bit more but just works. And I've read that costs can be kept down if you manage data in a SF friendly way. \n\nWhat is that SF friendly way? Could anyone point me to a good brief about _how_ to ideally use SF? The nature of workflows that work best in that environment? \n\nJust beginning to look into it. Thanks for any pointers.", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's snowflake's deal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fj24x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674077851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand that SF is the out-of-the-box DWH that might cost a bit more but just works. And I&amp;#39;ve read that costs can be kept down if you manage data in a SF friendly way. &lt;/p&gt;\n\n&lt;p&gt;What is that SF friendly way? Could anyone point me to a good brief about &lt;em&gt;how&lt;/em&gt; to ideally use SF? The nature of workflows that work best in that environment? &lt;/p&gt;\n\n&lt;p&gt;Just beginning to look into it. Thanks for any pointers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fj24x", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fj24x/whats_snowflakes_deal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fj24x/whats_snowflakes_deal/", "subreddit_subscribers": 86695, "created_utc": 1674077851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lately, I finished reading the **Learning Spark: Lightning-Fast Data Analytics** book, a book that helped me to have a better understanding of **Apache** **Spark**.  \n\nMy new article goes over some of the takeaways I learned from reading the book, and my daily interaction with Spark for 6 months.  \n\nLink: https://aymane11.github.io/posts/apache-spark-gotchas/", "author_fullname": "t2_20xw1ryl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Spark Gotchas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fhkkq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674073223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lately, I finished reading the &lt;strong&gt;Learning Spark: Lightning-Fast Data Analytics&lt;/strong&gt; book, a book that helped me to have a better understanding of &lt;strong&gt;Apache&lt;/strong&gt; &lt;strong&gt;Spark&lt;/strong&gt;.  &lt;/p&gt;\n\n&lt;p&gt;My new article goes over some of the takeaways I learned from reading the book, and my daily interaction with Spark for 6 months.  &lt;/p&gt;\n\n&lt;p&gt;Link: &lt;a href=\"https://aymane11.github.io/posts/apache-spark-gotchas/\"&gt;https://aymane11.github.io/posts/apache-spark-gotchas/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10fhkkq", "is_robot_indexable": true, "report_reasons": null, "author": "Enamya11", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fhkkq/apache_spark_gotchas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fhkkq/apache_spark_gotchas/", "subreddit_subscribers": 86695, "created_utc": 1674073223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm conducting a study on how I would use Apache Superset for an organization. I will have large amounts of data at source (Million of Rows scale) and about 600 Users.\n\n  \nWhat would be a good way of deploying Superset Google Cloud ? I'm thinking GKE but I have no idea about the right sizing, nor on how much would it cost.\n\n  \nIt would be helpful if you could share some experiences you had.\n\nThank you !", "author_fullname": "t2_98269xyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying Apache Superset on GKE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fgihh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674070723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m conducting a study on how I would use Apache Superset for an organization. I will have large amounts of data at source (Million of Rows scale) and about 600 Users.&lt;/p&gt;\n\n&lt;p&gt;What would be a good way of deploying Superset Google Cloud ? I&amp;#39;m thinking GKE but I have no idea about the right sizing, nor on how much would it cost.&lt;/p&gt;\n\n&lt;p&gt;It would be helpful if you could share some experiences you had.&lt;/p&gt;\n\n&lt;p&gt;Thank you !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fgihh", "is_robot_indexable": true, "report_reasons": null, "author": "Electronic-Mountain9", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fgihh/deploying_apache_superset_on_gke/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fgihh/deploying_apache_superset_on_gke/", "subreddit_subscribers": 86695, "created_utc": 1674070723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to know if my company is the odd one put or if it is widespread for medium to large sized organizations to use Databricks, but not forking out the extra money for the premium features. I think we are missing out on some really beneficial features, but I am having a hard time convincing our governance team to sign off on the premium tier. So what is the concensus among the redditors in here?\n\nI am using the Azure flavor, if that is of relevance to any of you.", "author_fullname": "t2_rszqhgyr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anybody actually use databricks standard tier professionally?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fg1q2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674069627.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to know if my company is the odd one put or if it is widespread for medium to large sized organizations to use Databricks, but not forking out the extra money for the premium features. I think we are missing out on some really beneficial features, but I am having a hard time convincing our governance team to sign off on the premium tier. So what is the concensus among the redditors in here?&lt;/p&gt;\n\n&lt;p&gt;I am using the Azure flavor, if that is of relevance to any of you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fg1q2", "is_robot_indexable": true, "report_reasons": null, "author": "bjtho08", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fg1q2/does_anybody_actually_use_databricks_standard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fg1q2/does_anybody_actually_use_databricks_standard/", "subreddit_subscribers": 86695, "created_utc": 1674069627.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone been able to read secrets from a Keyvault?  Supposedly TokenLibrary.getsecret() is the way to go but I have not been able to get it to work.  Error: POST failed with 'Bad Request'.\n\nThe managed identity in Synapse has rights to read keys, as I am using the keyvault in pipelines and already have a linked service set up.\n\nAny ideas about how to get this up and running?  I'm trying to connect to a SQL databse via jdbc, and it connects fine, I'd just like to use a secret for the user/pw on the connection.", "author_fullname": "t2_4270ek0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synapse PySpark Notebook - get secret from key vault?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ff5ts", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674067533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone been able to read secrets from a Keyvault?  Supposedly TokenLibrary.getsecret() is the way to go but I have not been able to get it to work.  Error: POST failed with &amp;#39;Bad Request&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;The managed identity in Synapse has rights to read keys, as I am using the keyvault in pipelines and already have a linked service set up.&lt;/p&gt;\n\n&lt;p&gt;Any ideas about how to get this up and running?  I&amp;#39;m trying to connect to a SQL databse via jdbc, and it connects fine, I&amp;#39;d just like to use a secret for the user/pw on the connection.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ff5ts", "is_robot_indexable": true, "report_reasons": null, "author": "mr_electric_wizard", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ff5ts/synapse_pyspark_notebook_get_secret_from_key_vault/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ff5ts/synapse_pyspark_notebook_get_secret_from_key_vault/", "subreddit_subscribers": 86695, "created_utc": 1674067533.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}