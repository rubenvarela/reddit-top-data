{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Could be something very minor like a specific gotcha in a library or a major architecture decision that bit you in the behind.", "author_fullname": "t2_14i1sz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What mistake have you vowed never to repeat?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g3g1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674139484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could be something very minor like a specific gotcha in a library or a major architecture decision that bit you in the behind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10g3g1r", "is_robot_indexable": true, "report_reasons": null, "author": "pescennius", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g3g1r/what_mistake_have_you_vowed_never_to_repeat/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g3g1r/what_mistake_have_you_vowed_never_to_repeat/", "subreddit_subscribers": 86809, "created_utc": 1674139484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE community!  \nI've been working on the data migration to the star schema architecture, and here are a couple of questions I could not confidently decide how to proceed with.\n\n1. What are the common strategies for handling the loaded\\_at timestamp fields in dwh, when the target table gets the data from multiple different sources? I was thinking of creating a 'last' and/or the 'first' loaded\\_at fields, not sure if there's any better way.\n2. A similar question about specifying the 'source' field into the dimensions. When the dimension gets created out of two different sources, in the way that attributes of a single dimension record are coming from various sources, how do you usually handle the 'source' field? Here I though of two options, one was to simply indicate the main dimension record source, i.e. source from where the platform's unique identifier is coming. But this way we lose the source of additional dimension attributes.  \n\n\nThanks!", "author_fullname": "t2_gwqijajo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Star Schema - questions about loading the data from multiple sources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fzw1q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674129466.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE community!&lt;br/&gt;\nI&amp;#39;ve been working on the data migration to the star schema architecture, and here are a couple of questions I could not confidently decide how to proceed with.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What are the common strategies for handling the loaded_at timestamp fields in dwh, when the target table gets the data from multiple different sources? I was thinking of creating a &amp;#39;last&amp;#39; and/or the &amp;#39;first&amp;#39; loaded_at fields, not sure if there&amp;#39;s any better way.&lt;/li&gt;\n&lt;li&gt;A similar question about specifying the &amp;#39;source&amp;#39; field into the dimensions. When the dimension gets created out of two different sources, in the way that attributes of a single dimension record are coming from various sources, how do you usually handle the &amp;#39;source&amp;#39; field? Here I though of two options, one was to simply indicate the main dimension record source, i.e. source from where the platform&amp;#39;s unique identifier is coming. But this way we lose the source of additional dimension attributes.&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fzw1q", "is_robot_indexable": true, "report_reasons": null, "author": "orm_the_stalker", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fzw1q/star_schema_questions_about_loading_the_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fzw1q/star_schema_questions_about_loading_the_data_from/", "subreddit_subscribers": 86809, "created_utc": 1674129466.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I did a small test and it seemed to be faster but when is the hash function even supposed to be used? \n\nIn star schema, surrogate keys are traditionally built using autoincrement. Is that still the best way in SF or was the hash function supposed to be used here? Assuming number of rows doesn\u2019t exceed a billion.", "author_fullname": "t2_kew1pm23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In Snowflake, is joining by a hash column more performant then joining by multiple text columns?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fpc1f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674093596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did a small test and it seemed to be faster but when is the hash function even supposed to be used? &lt;/p&gt;\n\n&lt;p&gt;In star schema, surrogate keys are traditionally built using autoincrement. Is that still the best way in SF or was the hash function supposed to be used here? Assuming number of rows doesn\u2019t exceed a billion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fpc1f", "is_robot_indexable": true, "report_reasons": null, "author": "Critical_Chart_6513", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fpc1f/in_snowflake_is_joining_by_a_hash_column_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fpc1f/in_snowflake_is_joining_by_a_hash_column_more/", "subreddit_subscribers": 86809, "created_utc": 1674093596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_31ilk7t8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to speed up range joins joins in Snowflake by 300x - SELECT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "name": "t3_10g65bg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yVrMGv9ob-fHob-nsAMDUf1Wxgufd4sgTPddBt3tMfo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674145957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "select.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://select.dev/posts/snowflake-range-join-optimization", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?auto=webp&amp;v=enabled&amp;s=c6102393c1fc139ebe3a192d54baefc9269fa3d6", "width": 1200, "height": 733}, "resolutions": [{"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e24f19fe617b042b54367889609e4aa75d7d6135", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc01477758940100756397660527fd80cfa738ad", "width": 216, "height": 131}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2db23948c752c587caa1f5bd2ecbfb81b13c713f", "width": 320, "height": 195}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac762f60c4c6caf1fa44521997dda65fed7b824c", "width": 640, "height": 390}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=956b255f528f54123a9598503aa68e2eef59b025", "width": 960, "height": 586}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c8a705f31099be5dd86194da0794355c6a7b829", "width": 1080, "height": 659}], "variants": {}, "id": "Kl2clgMmndy65WOWu6IHcN4IPoSI4AphoMLolknNnyA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10g65bg", "is_robot_indexable": true, "report_reasons": null, "author": "ian-whitestone", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g65bg/how_to_speed_up_range_joins_joins_in_snowflake_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://select.dev/posts/snowflake-range-join-optimization", "subreddit_subscribers": 86809, "created_utc": 1674145957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a backend engineer and try to work on the data import part for my company. we have a GraphQL server as a gateway between the client and the internal server. GraphQL is the only thing exposed to the outside world.  \n\n\nI am thinking about writing scripts to map the CSV file to the GraphQL mutation query and import.  \nBut I would like to have a better way to handle the import jobs. e.g. some import depends on the previous import result, retry when the job is fail...  \n\n\nI am looking at the Airflow tool now. I am wondering if it is a good starting point for me.  \n\n\nThank you.", "author_fullname": "t2_4hu7xbt4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "would airflow be good for this use case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fo4ks", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674090265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a backend engineer and try to work on the data import part for my company. we have a GraphQL server as a gateway between the client and the internal server. GraphQL is the only thing exposed to the outside world.  &lt;/p&gt;\n\n&lt;p&gt;I am thinking about writing scripts to map the CSV file to the GraphQL mutation query and import.&lt;br/&gt;\nBut I would like to have a better way to handle the import jobs. e.g. some import depends on the previous import result, retry when the job is fail...  &lt;/p&gt;\n\n&lt;p&gt;I am looking at the Airflow tool now. I am wondering if it is a good starting point for me.  &lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fo4ks", "is_robot_indexable": true, "report_reasons": null, "author": "itsMoreAComment", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fo4ks/would_airflow_be_good_for_this_use_case/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fo4ks/would_airflow_be_good_for_this_use_case/", "subreddit_subscribers": 86809, "created_utc": 1674090265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j33i3nha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Requesting feedback on my resume - Data Engineer with a little over 2 years of experience, was laid off recently", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10g7wus", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": "transparent", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EYob2aCUBnKMaS02mKE8gOPld9yR-gCcrmYmsOtDUuw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674150081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zxbize3zd1da1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?auto=webp&amp;v=enabled&amp;s=9cca47ece1fc5ac2ac8ce349298c45c45aad2427", "width": 1700, "height": 2200}, "resolutions": [{"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be479760d91ee62bb8c1471afe5961e9285b2286", "width": 108, "height": 139}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0237c1248dafa3dfa220c7a14c15a52fa4896d13", "width": 216, "height": 279}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c234a772ce92e14f3642beb70560a6fc7e173be", "width": 320, "height": 414}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f5fc705a3bdac8a314c9016aa1ec77add7eed5f", "width": 640, "height": 828}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f68926ee615705f32c8cb6f0ee9fd672b2a9f16c", "width": 960, "height": 1242}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7aa391976255f8543c948d67f3fefd76e888af0", "width": 1080, "height": 1397}], "variants": {}, "id": "8eDElsZ4niQq2Lvas-ev5VOLUMDDsCuf4bubD9ehkf8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10g7wus", "is_robot_indexable": true, "report_reasons": null, "author": "CS_throwaway_DE", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10g7wus/requesting_feedback_on_my_resume_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zxbize3zd1da1.jpg", "subreddit_subscribers": 86809, "created_utc": 1674150081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working on processing a large number (+/- 900) of smaller tables. These tables have to be loaded from a landing zone where they arrive in CSV format to a raw layer in delta format. \n\nThese tables have to be loaded on a daily basis and I'm currently using a synapse spark notebook to do this.\n\nMy first solution was to store all the tables in a list and then use a for loop to process these tables on by one. This was however taking a long time so I set up a multi threaded approach using the python multiprocessing library.\n\nThis lead to some significant performance gains but Im not sure that this is the best approach as the performance currently varies run by run.\n\nI'm also not sure how to optimally setup a spark cluster for such an use case.\n\n\nSo I was curious how you guys are currently solving a problem like this and if there are people also using multi threading how you then optimize the spark process for such a thing. Like do you have a large driver and less executors or vice versa and how do you determine the amount of threads to create.\n\nMany thanks in advance!", "author_fullname": "t2_71slaawn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you processing large numbers of smaller tables using spark.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fllzt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674083819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on processing a large number (+/- 900) of smaller tables. These tables have to be loaded from a landing zone where they arrive in CSV format to a raw layer in delta format. &lt;/p&gt;\n\n&lt;p&gt;These tables have to be loaded on a daily basis and I&amp;#39;m currently using a synapse spark notebook to do this.&lt;/p&gt;\n\n&lt;p&gt;My first solution was to store all the tables in a list and then use a for loop to process these tables on by one. This was however taking a long time so I set up a multi threaded approach using the python multiprocessing library.&lt;/p&gt;\n\n&lt;p&gt;This lead to some significant performance gains but Im not sure that this is the best approach as the performance currently varies run by run.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also not sure how to optimally setup a spark cluster for such an use case.&lt;/p&gt;\n\n&lt;p&gt;So I was curious how you guys are currently solving a problem like this and if there are people also using multi threading how you then optimize the spark process for such a thing. Like do you have a large driver and less executors or vice versa and how do you determine the amount of threads to create.&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fllzt", "is_robot_indexable": true, "report_reasons": null, "author": "blindbox2", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fllzt/how_are_you_processing_large_numbers_of_smaller/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fllzt/how_are_you_processing_large_numbers_of_smaller/", "subreddit_subscribers": 86809, "created_utc": 1674083819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I kinda find the tools both use are same...so it makes both the roles same only ?", "author_fullname": "t2_99z52qfs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Data Engineers and Big Data Engineers same?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g7wyt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674150089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I kinda find the tools both use are same...so it makes both the roles same only ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10g7wyt", "is_robot_indexable": true, "report_reasons": null, "author": "Vortex_007s", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g7wyt/are_data_engineers_and_big_data_engineers_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g7wyt/are_data_engineers_and_big_data_engineers_same/", "subreddit_subscribers": 86809, "created_utc": 1674150089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was tasked to aggregate data in DynamoDB. It was only around 5GB of data. Is it better if I aggregate data using AWS Glue and load it to s3 as csv? Or should I export the table to s3 and query using athena?", "author_fullname": "t2_6542em0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Query from Athena or Transform in AWS Glue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fzw3l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674129471.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was tasked to aggregate data in DynamoDB. It was only around 5GB of data. Is it better if I aggregate data using AWS Glue and load it to s3 as csv? Or should I export the table to s3 and query using athena?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fzw3l", "is_robot_indexable": true, "report_reasons": null, "author": "EngrRhys", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fzw3l/query_from_athena_or_transform_in_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fzw3l/query_from_athena_or_transform_in_aws_glue/", "subreddit_subscribers": 86809, "created_utc": 1674129471.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know how to directly get the status of a specific sync job?\n\nThe round about way I\u2019m attempting to do this is by\n1. fetching all the logs of the extractions\n2. filtering them so that the extraction\u2019s job matches the name of the current job\n3. fetching the loads\n4. filtering them where the job name matches the current job name\n5. Parse the extraction logs to see if any of the streams for that extraction fetched any rows\n6. If a stream fetched rows based on the extraction log, then check the streams from the load\n7. If the load for a stream has a specific value designating it is done, and if it had rows extracted based on the logs, mark that stream as done\n8. Keep polling and once all streams are \u201cmarked\u201d as done, consider the job done\n\n\nProblem: a ton of api calls, and potential rate limiting.\n\nAnyone else solve this issue?\n\nHere are there docs: https://www.stitchdata.com/docs/developers/stitch-connect/api", "author_fullname": "t2_9aeao1ey", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stitch Connect API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ftnmc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674106423.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know how to directly get the status of a specific sync job?&lt;/p&gt;\n\n&lt;p&gt;The round about way I\u2019m attempting to do this is by\n1. fetching all the logs of the extractions\n2. filtering them so that the extraction\u2019s job matches the name of the current job\n3. fetching the loads\n4. filtering them where the job name matches the current job name\n5. Parse the extraction logs to see if any of the streams for that extraction fetched any rows\n6. If a stream fetched rows based on the extraction log, then check the streams from the load\n7. If the load for a stream has a specific value designating it is done, and if it had rows extracted based on the logs, mark that stream as done\n8. Keep polling and once all streams are \u201cmarked\u201d as done, consider the job done&lt;/p&gt;\n\n&lt;p&gt;Problem: a ton of api calls, and potential rate limiting.&lt;/p&gt;\n\n&lt;p&gt;Anyone else solve this issue?&lt;/p&gt;\n\n&lt;p&gt;Here are there docs: &lt;a href=\"https://www.stitchdata.com/docs/developers/stitch-connect/api\"&gt;https://www.stitchdata.com/docs/developers/stitch-connect/api&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "ex-Airbnb engineer (data, ML)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ftnmc", "is_robot_indexable": true, "report_reasons": null, "author": "ironplaneswalker", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10ftnmc/stitch_connect_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ftnmc/stitch_connect_api/", "subreddit_subscribers": 86809, "created_utc": 1674106423.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in the biotech industry, where we actually generate a lot of our important data sources locally. Instruments like mass spectrometers and next generation sequencers make large datasets that are best analyzed in the cloud.\n\nI've been bouncing between a number of tools to automate data movement into the cloud, but I still feel like there's a better way to engineer it. I've used 3rd party tools (BioBright, TntDrive), and AWS services (Data Sync, Storage Gateway, CLI). But either the solution is too overengineered or it's easily breakable... I know this is a common use case, so I'm curious if others have found anything they like using?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you automate data movement into the cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fmx2h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674087097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in the biotech industry, where we actually generate a lot of our important data sources locally. Instruments like mass spectrometers and next generation sequencers make large datasets that are best analyzed in the cloud.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been bouncing between a number of tools to automate data movement into the cloud, but I still feel like there&amp;#39;s a better way to engineer it. I&amp;#39;ve used 3rd party tools (BioBright, TntDrive), and AWS services (Data Sync, Storage Gateway, CLI). But either the solution is too overengineered or it&amp;#39;s easily breakable... I know this is a common use case, so I&amp;#39;m curious if others have found anything they like using?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fmx2h", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fmx2h/how_do_you_automate_data_movement_into_the_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fmx2h/how_do_you_automate_data_movement_into_the_cloud/", "subreddit_subscribers": 86809, "created_utc": 1674087097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am helping a very small company to manage their sales data generated from a few sources(web stores). \n\nThe data is under 50k rows so ideally I would like to utilize cloud storage options that have a free tier for future scalability as they are growing.\n\nFor now I am thinking of using webhooks to start. \n\nAlso a little context, I am have experience as a analyst but would like to move into engineering and would want to use this as a little project to add in my portfolio.", "author_fullname": "t2_687fm3oi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple solution for very small datasets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fltqe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674084333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am helping a very small company to manage their sales data generated from a few sources(web stores). &lt;/p&gt;\n\n&lt;p&gt;The data is under 50k rows so ideally I would like to utilize cloud storage options that have a free tier for future scalability as they are growing.&lt;/p&gt;\n\n&lt;p&gt;For now I am thinking of using webhooks to start. &lt;/p&gt;\n\n&lt;p&gt;Also a little context, I am have experience as a analyst but would like to move into engineering and would want to use this as a little project to add in my portfolio.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fltqe", "is_robot_indexable": true, "report_reasons": null, "author": "mysterymalts", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fltqe/simple_solution_for_very_small_datasets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fltqe/simple_solution_for_very_small_datasets/", "subreddit_subscribers": 86809, "created_utc": 1674084333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI'm relatively new to DynamoDB and I've been tasked with moving application from our DynamoDB database into a format that can be consumed by our existing snowflake data-lake. \n\nThis application is replacing an existing application that had a more traditional SQL/RDBMS style data back-end, and the lake layer was just a synchronized copy of the application layer.  \n\nI'm scratching my head at how to approach this. With a single table design the data is as far from normalized as it can get. Thankfully the dev team has created an urn as part of each item and I'm thinking I can use that to reconstruct the entities of the application (did I mention that the access patterns, and entities are only loosely documented?) \n\nThat leaves me with the following questions and open for advice?\n\n* How do I successfully investigate the heterogeneous data in DynamoDB to verify what entities I can stitch back together (I've tried Athena, but its unable to successfully crawl the data due to some fields being json objects / structs that are inconsistent).\n* Assuming I can extract the data (e.g. export to s3, or use Glue for some more sophisticated processing) how can I hope to consistently keep the s3 data lake up to date (Maybe use Kinesis and the associated lambda to find the \"record\" in s3 and update it accordingly)", "author_fullname": "t2_6d49ikq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading/Normalizing Data from Single Table Designed DynamoDB Consumption in AWS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fkdox", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674080856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m relatively new to DynamoDB and I&amp;#39;ve been tasked with moving application from our DynamoDB database into a format that can be consumed by our existing snowflake data-lake. &lt;/p&gt;\n\n&lt;p&gt;This application is replacing an existing application that had a more traditional SQL/RDBMS style data back-end, and the lake layer was just a synchronized copy of the application layer.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m scratching my head at how to approach this. With a single table design the data is as far from normalized as it can get. Thankfully the dev team has created an urn as part of each item and I&amp;#39;m thinking I can use that to reconstruct the entities of the application (did I mention that the access patterns, and entities are only loosely documented?) &lt;/p&gt;\n\n&lt;p&gt;That leaves me with the following questions and open for advice?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How do I successfully investigate the heterogeneous data in DynamoDB to verify what entities I can stitch back together (I&amp;#39;ve tried Athena, but its unable to successfully crawl the data due to some fields being json objects / structs that are inconsistent).&lt;/li&gt;\n&lt;li&gt;Assuming I can extract the data (e.g. export to s3, or use Glue for some more sophisticated processing) how can I hope to consistently keep the s3 data lake up to date (Maybe use Kinesis and the associated lambda to find the &amp;quot;record&amp;quot; in s3 and update it accordingly)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fkdox", "is_robot_indexable": true, "report_reasons": null, "author": "poppinstacks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fkdox/loadingnormalizing_data_from_single_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fkdox/loadingnormalizing_data_from_single_table/", "subreddit_subscribers": 86809, "created_utc": 1674080856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand that SF is the out-of-the-box DWH that might cost a bit more but just works. And I've read that costs can be kept down if you manage data in a SF friendly way. \n\nWhat is that SF friendly way? Could anyone point me to a good brief about _how_ to ideally use SF? The nature of workflows that work best in that environment? \n\nJust beginning to look into it. Thanks for any pointers.", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's snowflake's deal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fj24x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674077851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand that SF is the out-of-the-box DWH that might cost a bit more but just works. And I&amp;#39;ve read that costs can be kept down if you manage data in a SF friendly way. &lt;/p&gt;\n\n&lt;p&gt;What is that SF friendly way? Could anyone point me to a good brief about &lt;em&gt;how&lt;/em&gt; to ideally use SF? The nature of workflows that work best in that environment? &lt;/p&gt;\n\n&lt;p&gt;Just beginning to look into it. Thanks for any pointers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fj24x", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fj24x/whats_snowflakes_deal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fj24x/whats_snowflakes_deal/", "subreddit_subscribers": 86809, "created_utc": 1674077851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a pipeline that would look something like this:\n\n[Pipeline](https://preview.redd.it/aaaitll422da1.png?width=1662&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5e20276f445483765304633b7408ca66c642aed6)\n\nI intend to implement it in Prefect 2.0 but there are two things that I can't actually figure out:\n\n1. Flow B has a dependency on both flows A and X. However, flow X is triggered by a different process and X and A don't necessarily run at the same time (\\~15 min gap), can these two flows interact with each other and wait for each other to run a third flow given that both flows are running separately (i.e. separate deployment )\n2. Flow C is triggered by flow A on the 1st of every month, in normal circumstances, it would get executed, however, in occasional events, if it's Monday and also the 1st of the month, flow C should wait for B to finish, how does this work? I can just add an if-else or a similar implementation but wouldn't it mess up the radar (the visual flow representation) since the pythonic code can mess up with\n\nI would really appreciate it if anyone can lead me to the right path or maybe if there is any similar implementation, that'd be great too, Thanks!\n\nDiagram: You can consider the yellow box as job A and the green box as job B\n\nEdit 1: Flow C has no time schedule, it gets triggered from flow A, 2 PM is marked wrong\n\n^(Disclaimer: Apologies if this is a dumb question and doesn't relate to this sub, please feel free to remove this.)", "author_fullname": "t2_2xpdiv8k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can two different perfect flows interact with each other?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 99, "top_awarded_type": null, "hide_score": true, "media_metadata": {"aaaitll422da1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/aaaitll422da1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db84c3e387775b2ee0870dc3be0ddb74c8f8d822"}, {"y": 153, "x": 216, "u": "https://preview.redd.it/aaaitll422da1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbef575bd5ea417c89fb32a863f9afb688473127"}, {"y": 227, "x": 320, "u": "https://preview.redd.it/aaaitll422da1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=208c45f98ba15df36bd3fc701f3996f07e9c3e10"}, {"y": 454, "x": 640, "u": "https://preview.redd.it/aaaitll422da1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3fbe97132e10c73dfa67765d1e9ecf762fc55f86"}, {"y": 682, "x": 960, "u": "https://preview.redd.it/aaaitll422da1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cda3bd29b3866665a015e51bf8e002a449d8d276"}, {"y": 767, "x": 1080, "u": "https://preview.redd.it/aaaitll422da1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae6937b07c1287e4385f8702d8a9f6208849d7a4"}], "s": {"y": 1181, "x": 1662, "u": "https://preview.redd.it/aaaitll422da1.png?width=1662&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5e20276f445483765304633b7408ca66c642aed6"}, "id": "aaaitll422da1"}}, "name": "t3_10gbzvu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3uV6ZuGGEkvzaJvbz613V1a30VOQKMCXJO327EJSROs.jpg", "edited": 1674159639.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674159447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a pipeline that would look something like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/aaaitll422da1.png?width=1662&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5e20276f445483765304633b7408ca66c642aed6\"&gt;Pipeline&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I intend to implement it in Prefect 2.0 but there are two things that I can&amp;#39;t actually figure out:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Flow B has a dependency on both flows A and X. However, flow X is triggered by a different process and X and A don&amp;#39;t necessarily run at the same time (~15 min gap), can these two flows interact with each other and wait for each other to run a third flow given that both flows are running separately (i.e. separate deployment )&lt;/li&gt;\n&lt;li&gt;Flow C is triggered by flow A on the 1st of every month, in normal circumstances, it would get executed, however, in occasional events, if it&amp;#39;s Monday and also the 1st of the month, flow C should wait for B to finish, how does this work? I can just add an if-else or a similar implementation but wouldn&amp;#39;t it mess up the radar (the visual flow representation) since the pythonic code can mess up with&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I would really appreciate it if anyone can lead me to the right path or maybe if there is any similar implementation, that&amp;#39;d be great too, Thanks!&lt;/p&gt;\n\n&lt;p&gt;Diagram: You can consider the yellow box as job A and the green box as job B&lt;/p&gt;\n\n&lt;p&gt;Edit 1: Flow C has no time schedule, it gets triggered from flow A, 2 PM is marked wrong&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;Disclaimer: Apologies if this is a dumb question and doesn&amp;#39;t relate to this sub, please feel free to remove this.&lt;/sup&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "SSIS developer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10gbzvu", "is_robot_indexable": true, "report_reasons": null, "author": "_whitezetsu", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10gbzvu/can_two_different_perfect_flows_interact_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gbzvu/can_two_different_perfect_flows_interact_with/", "subreddit_subscribers": 86809, "created_utc": 1674159447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title\n\nI'm currently working as a Data Migration Analyst for a tech startup. I enjoy the job, as I'm developing a diverse set of skills. I work with a ton of SQL, JSON and Azure. In regards to Azure, I generally just deploy DB's to Azure and add them to elastic pools - nothing crazy.\n\nI'm currently working on my MS in DS while working full time. It's quite a bit of work, but I WFH so I do school work during the day as well. I also have an undergrad double major in Data Analytics &amp; Finance.\n\nI've been interviewing for a WFH  Jr. Cloud Engineer job. The company is in a field I'm extremely interested in and I was ecstatic to find out I got an interview. Below are the general job duties:\n\n* Identifying and implementing optimal cloud-based solutions, including team education and training\n* Participating in the software development life cycle, including planning, requirements, development, testing, and quality assurance\n* Building tools and systems to improve the time to market of product offerings in partnership with Product, Trading, and Data Science\n* Troubleshooting incidents, identifying root causes, fixing and documenting problems, and implementing preventive measures\n* Orchestrating and automating cloud operations and processes\n* Collaborating with stakeholders across the business to balance competing objectives\n* Working with third-party vendors to meet business requirements.\n\nAs I mentioned before, I don't have any experience in building a Cloud pipeline by any means. I understand the functionalities of  Cloud platforms and how they interact with SQL ide's, but nothing in terms of actual development.\n\nI had my first interview yesterday as a general screening to see my fit for the role and I think it went very well. I answered everything honestly and my resume doesn't make it seem like I'm more qualified than I actually am.\n\nToday I received multiple calls from the company asking if I saw their email and if I had time for a second interview today. The hiring manager told me their emails sometimes end up in spam/junk folders - which is exactly where it ended up. Long story short, I have another interview in 3 hours. I knew after yesterdays interview that they were very interested based on our conversations.\n\nHypothetically - lets say they give me an offer:\n\n* Firstly, does it seem like I have imposter syndrome? I worry that I'm just not qualified for this job and I won't deliver what they expect\n* Do I continue my MS if I'm getting legitimate experience as a Cloud Engineer?\n* If I do continue my MS, am I going to be able to manage the workload with this new role?\n* The expected pay is about $10,000 higher in the new role with a better job title - if my current job offers a similar pay raise and a new title + added responsibilities, should I just stay?\n\nThank you for reading and any advice would be awesome. I love the field of Data in general and the helpfulness of the community, you guys are the best.", "author_fullname": "t2_1x7s010", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Crisis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10gb99d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674158029.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674157724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working as a Data Migration Analyst for a tech startup. I enjoy the job, as I&amp;#39;m developing a diverse set of skills. I work with a ton of SQL, JSON and Azure. In regards to Azure, I generally just deploy DB&amp;#39;s to Azure and add them to elastic pools - nothing crazy.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on my MS in DS while working full time. It&amp;#39;s quite a bit of work, but I WFH so I do school work during the day as well. I also have an undergrad double major in Data Analytics &amp;amp; Finance.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been interviewing for a WFH  Jr. Cloud Engineer job. The company is in a field I&amp;#39;m extremely interested in and I was ecstatic to find out I got an interview. Below are the general job duties:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Identifying and implementing optimal cloud-based solutions, including team education and training&lt;/li&gt;\n&lt;li&gt;Participating in the software development life cycle, including planning, requirements, development, testing, and quality assurance&lt;/li&gt;\n&lt;li&gt;Building tools and systems to improve the time to market of product offerings in partnership with Product, Trading, and Data Science&lt;/li&gt;\n&lt;li&gt;Troubleshooting incidents, identifying root causes, fixing and documenting problems, and implementing preventive measures&lt;/li&gt;\n&lt;li&gt;Orchestrating and automating cloud operations and processes&lt;/li&gt;\n&lt;li&gt;Collaborating with stakeholders across the business to balance competing objectives&lt;/li&gt;\n&lt;li&gt;Working with third-party vendors to meet business requirements.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As I mentioned before, I don&amp;#39;t have any experience in building a Cloud pipeline by any means. I understand the functionalities of  Cloud platforms and how they interact with SQL ide&amp;#39;s, but nothing in terms of actual development.&lt;/p&gt;\n\n&lt;p&gt;I had my first interview yesterday as a general screening to see my fit for the role and I think it went very well. I answered everything honestly and my resume doesn&amp;#39;t make it seem like I&amp;#39;m more qualified than I actually am.&lt;/p&gt;\n\n&lt;p&gt;Today I received multiple calls from the company asking if I saw their email and if I had time for a second interview today. The hiring manager told me their emails sometimes end up in spam/junk folders - which is exactly where it ended up. Long story short, I have another interview in 3 hours. I knew after yesterdays interview that they were very interested based on our conversations.&lt;/p&gt;\n\n&lt;p&gt;Hypothetically - lets say they give me an offer:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Firstly, does it seem like I have imposter syndrome? I worry that I&amp;#39;m just not qualified for this job and I won&amp;#39;t deliver what they expect&lt;/li&gt;\n&lt;li&gt;Do I continue my MS if I&amp;#39;m getting legitimate experience as a Cloud Engineer?&lt;/li&gt;\n&lt;li&gt;If I do continue my MS, am I going to be able to manage the workload with this new role?&lt;/li&gt;\n&lt;li&gt;The expected pay is about $10,000 higher in the new role with a better job title - if my current job offers a similar pay raise and a new title + added responsibilities, should I just stay?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you for reading and any advice would be awesome. I love the field of Data in general and the helpfulness of the community, you guys are the best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10gb99d", "is_robot_indexable": true, "report_reasons": null, "author": "HercHuntsdirty", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gb99d/career_crisis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gb99d/career_crisis/", "subreddit_subscribers": 86809, "created_utc": 1674157724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_xfzc9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expert advice by ChatGPT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_10gb757", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2FtoYNodN%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D879&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2FtoYNodN&amp;image=https%3A%2F%2Fi.imgur.com%2FXAMXeQG.jpg%3Ffb&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"600\" height=\"741\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 600, "scrolling": false, "height": 741}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "imgur.com", "oembed": {"provider_url": "http://imgur.com", "description": "Post with 0 views. Expert advice by ChatGPT", "title": "Expert advice by ChatGPT", "url": "https://imgur.com/a/toYNodN", "type": "rich", "thumbnail_width": 600, "height": 741, "width": 600, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2FtoYNodN%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D879&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2FtoYNodN&amp;image=https%3A%2F%2Fi.imgur.com%2FXAMXeQG.jpg%3Ffb&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"600\" height=\"741\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Imgur", "thumbnail_url": "https://i.imgur.com/XAMXeQG.jpg?fb", "thumbnail_height": 315}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2FtoYNodN%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D879&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2FtoYNodN&amp;image=https%3A%2F%2Fi.imgur.com%2FXAMXeQG.jpg%3Ffb&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"600\" height=\"741\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 600, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/10gb757", "height": 741}, "link_flair_text": "Meme", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/4vCsWl2yVxN4UHklDWUJoBKOKP6mwctqMXNspaeq3G8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674157584.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgur.com/a/toYNodN", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/E0o1YBSdG_wyMSXji7FrnESTRtXw2_Vysyud2zbml9I.jpg?auto=webp&amp;v=enabled&amp;s=f097428be00702be17311ddc736475d50b3c5dbe", "width": 879, "height": 1026}, "resolutions": [{"url": "https://external-preview.redd.it/E0o1YBSdG_wyMSXji7FrnESTRtXw2_Vysyud2zbml9I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=87177e28e12788dfa9f90bdc96a12a690978e692", "width": 108, "height": 126}, {"url": "https://external-preview.redd.it/E0o1YBSdG_wyMSXji7FrnESTRtXw2_Vysyud2zbml9I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=baa63df8d3a2148f13781956a7b938db84ac78d7", "width": 216, "height": 252}, {"url": "https://external-preview.redd.it/E0o1YBSdG_wyMSXji7FrnESTRtXw2_Vysyud2zbml9I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d5027d061a4ace25cca64fb2d8b4fe9880399f6", "width": 320, "height": 373}, {"url": "https://external-preview.redd.it/E0o1YBSdG_wyMSXji7FrnESTRtXw2_Vysyud2zbml9I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0112840c122348d40098a6448a8386447b1e3565", "width": 640, "height": 747}], "variants": {}, "id": "5VHEjTfWs4cEr3kgxsDB4DFd7iTno5bdIcfHOfDJlAc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "10gb757", "is_robot_indexable": true, "report_reasons": null, "author": "dserban", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gb757/expert_advice_by_chatgpt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgur.com/a/toYNodN", "subreddit_subscribers": 86809, "created_utc": 1674157584.0, "num_crossposts": 0, "media": {"type": "imgur.com", "oembed": {"provider_url": "http://imgur.com", "description": "Post with 0 views. Expert advice by ChatGPT", "title": "Expert advice by ChatGPT", "url": "https://imgur.com/a/toYNodN", "type": "rich", "thumbnail_width": 600, "height": 741, "width": 600, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2FtoYNodN%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D879&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2FtoYNodN&amp;image=https%3A%2F%2Fi.imgur.com%2FXAMXeQG.jpg%3Ffb&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"600\" height=\"741\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Imgur", "thumbnail_url": "https://i.imgur.com/XAMXeQG.jpg?fb", "thumbnail_height": 315}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on calculating the total cost of ownership (TCO) for tools like Apache Kafka to determine when to build vs. buy.\n\nI'd love your feedback -- what am I missing? What did I underestimate/overestimate? How can I improve this?\n\n**First, the criteria to consider when calculating TCO:**\n\nUp-front costs\n\n* software cost &amp; licensing, if applicable\n* learning &amp; education\n* implementation &amp; testing (including data migration costs)\n* documentation &amp; knowledge sharing\n* customization\n\nOngoing costs\n\n* direct infrastructure costs (e.g., hosting &amp; storage)\n* backup infrastructure costs (e.g., failover &amp; additional AZs)\n* supporting infrastructure costs (e.g., monitoring &amp; alerting)\n* maintenance, patches/upgrades, &amp; support\n* feature additions\n\nTeam &amp; opportunity costs\n\n* hiring to replace the engineers now working with the new software\n* time spent on infrastructure that could otherwise be spent on core product\n\n**Now, an example using the above criteria:**\n\nDesired specs for our example deployment (I picked one of the smaller Heroku plans):\n\n* Capacity: 300GB\n* Retention: 2 weeks\n* vCPU: 4\n* Ram: 16GB\n* Brokers: 3\n\nAssuming an engineer has an all-in comp package of $200k/yr (this would obviously be different in every situation, for every geo), year one would look like:\n\n||**Building (on AWS)**|**Buying (Heroku)**|\n|:-|:-|:-|\n|software cost &amp; licensing|$0|$21,600|\n|learning &amp; education|$7,692 (2 eng \\* 1 week)|$3,846 (1 eng \\* 1 week)|\n|implementation &amp; testing|$15,384 (2 eng \\* 2 weeks)|$7,692 (1 eng \\* 1 week)|\n|infrastructure costs (see above specs)|$12,117.60|$0 (included in software cost)|\n|supporting infrastructure costs (monitoring, etc.)|$1,200/yr|$1,200/yr|\n|maintenance, patches/upgrades|$15,384 (2 eng \\* 2 weeks spread throughout the year)|$7,692 (1 eng \\* 2 weeks spread throughout the year)|\n|**Year 1 TCO**|**$51,777.60**|**$42,030**|\n\nDirectionally, this example seems correct.\n\nWhat do you think? What am I missing? What did I underestimate/overestimate? How can I improve this?\n\nThanks!", "author_fullname": "t2_fv515", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feedback Request: TCO Calculation for Apache Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gaucp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674156743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on calculating the total cost of ownership (TCO) for tools like Apache Kafka to determine when to build vs. buy.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love your feedback -- what am I missing? What did I underestimate/overestimate? How can I improve this?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;First, the criteria to consider when calculating TCO:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Up-front costs&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;software cost &amp;amp; licensing, if applicable&lt;/li&gt;\n&lt;li&gt;learning &amp;amp; education&lt;/li&gt;\n&lt;li&gt;implementation &amp;amp; testing (including data migration costs)&lt;/li&gt;\n&lt;li&gt;documentation &amp;amp; knowledge sharing&lt;/li&gt;\n&lt;li&gt;customization&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Ongoing costs&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;direct infrastructure costs (e.g., hosting &amp;amp; storage)&lt;/li&gt;\n&lt;li&gt;backup infrastructure costs (e.g., failover &amp;amp; additional AZs)&lt;/li&gt;\n&lt;li&gt;supporting infrastructure costs (e.g., monitoring &amp;amp; alerting)&lt;/li&gt;\n&lt;li&gt;maintenance, patches/upgrades, &amp;amp; support&lt;/li&gt;\n&lt;li&gt;feature additions&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Team &amp;amp; opportunity costs&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;hiring to replace the engineers now working with the new software&lt;/li&gt;\n&lt;li&gt;time spent on infrastructure that could otherwise be spent on core product&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Now, an example using the above criteria:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Desired specs for our example deployment (I picked one of the smaller Heroku plans):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Capacity: 300GB&lt;/li&gt;\n&lt;li&gt;Retention: 2 weeks&lt;/li&gt;\n&lt;li&gt;vCPU: 4&lt;/li&gt;\n&lt;li&gt;Ram: 16GB&lt;/li&gt;\n&lt;li&gt;Brokers: 3&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Assuming an engineer has an all-in comp package of $200k/yr (this would obviously be different in every situation, for every geo), year one would look like:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;Building (on AWS)&lt;/strong&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;Buying (Heroku)&lt;/strong&gt;&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;software cost &amp;amp; licensing&lt;/td&gt;\n&lt;td align=\"left\"&gt;$0&lt;/td&gt;\n&lt;td align=\"left\"&gt;$21,600&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;learning &amp;amp; education&lt;/td&gt;\n&lt;td align=\"left\"&gt;$7,692 (2 eng * 1 week)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$3,846 (1 eng * 1 week)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;implementation &amp;amp; testing&lt;/td&gt;\n&lt;td align=\"left\"&gt;$15,384 (2 eng * 2 weeks)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$7,692 (1 eng * 1 week)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;infrastructure costs (see above specs)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$12,117.60&lt;/td&gt;\n&lt;td align=\"left\"&gt;$0 (included in software cost)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;supporting infrastructure costs (monitoring, etc.)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$1,200/yr&lt;/td&gt;\n&lt;td align=\"left\"&gt;$1,200/yr&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;maintenance, patches/upgrades&lt;/td&gt;\n&lt;td align=\"left\"&gt;$15,384 (2 eng * 2 weeks spread throughout the year)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$7,692 (1 eng * 2 weeks spread throughout the year)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Year 1 TCO&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;$51,777.60&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;$42,030&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Directionally, this example seems correct.&lt;/p&gt;\n\n&lt;p&gt;What do you think? What am I missing? What did I underestimate/overestimate? How can I improve this?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10gaucp", "is_robot_indexable": true, "report_reasons": null, "author": "propjames", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gaucp/feedback_request_tco_calculation_for_apache_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gaucp/feedback_request_tco_calculation_for_apache_kafka/", "subreddit_subscribers": 86809, "created_utc": 1674156743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Same as the title.", "author_fullname": "t2_57e44nxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any way to load a public airtable into a pandas dataframe without using airtable api ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g9n7x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674153948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Same as the title.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10g9n7x", "is_robot_indexable": true, "report_reasons": null, "author": "RstarPhoneix", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g9n7x/any_way_to_load_a_public_airtable_into_a_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g9n7x/any_way_to_load_a_public_airtable_into_a_pandas/", "subreddit_subscribers": 86809, "created_utc": 1674153948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys \n\nI came in to a new company to fix some issues and design a modern OLAP system. Today I had my first standoff with a 61 year old SAP developer who developed a system that has major design flaws. They had 5 major issues last year for one single process and they implemented a very complicated workflow for basically extracting some data from excel, create a dataset and showing the dataset in SAP analytics cloud.\n\nHe told me he has 35 years of experience and he is so intelligent he doesnt need to use a pattern or modern technologies. He can do everything in ABAP and with transactions. Why should he even learn new technologies? \n\nWell I wouldnt have issues if the bloody thing worked but its just a bloody mess and I guess I am off to a rocky start. Any advise how I manage this guy?", "author_fullname": "t2_793tedn0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SAP Developer thinks he is boss", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g9ib2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674153637.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys &lt;/p&gt;\n\n&lt;p&gt;I came in to a new company to fix some issues and design a modern OLAP system. Today I had my first standoff with a 61 year old SAP developer who developed a system that has major design flaws. They had 5 major issues last year for one single process and they implemented a very complicated workflow for basically extracting some data from excel, create a dataset and showing the dataset in SAP analytics cloud.&lt;/p&gt;\n\n&lt;p&gt;He told me he has 35 years of experience and he is so intelligent he doesnt need to use a pattern or modern technologies. He can do everything in ABAP and with transactions. Why should he even learn new technologies? &lt;/p&gt;\n\n&lt;p&gt;Well I wouldnt have issues if the bloody thing worked but its just a bloody mess and I guess I am off to a rocky start. Any advise how I manage this guy?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10g9ib2", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Sentence-8542", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g9ib2/sap_developer_thinks_he_is_boss/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g9ib2/sap_developer_thinks_he_is_boss/", "subreddit_subscribers": 86809, "created_utc": 1674153637.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Tracking down all the args and import statements for providers was sometimes a pain. This VS Code extension has all of the providers loaded so you can autocomplete all of the operators and hooks. [https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates](https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates)", "author_fullname": "t2_shq2slu0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "VS Code extension for Airflow Provider packages: Airflow Templates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g7tsp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674149886.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tracking down all the args and import statements for providers was sometimes a pain. This VS Code extension has all of the providers loaded so you can autocomplete all of the operators and hooks. &lt;a href=\"https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates\"&gt;https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5Vwgalc18IqnYE7aIh59IAOgguPxfM4Agv9EhnfeWJc.jpg?auto=webp&amp;v=enabled&amp;s=f519d587d5d576b6d3824ecfeb08e435ac409c9d", "width": 140, "height": 139}, "resolutions": [{"url": "https://external-preview.redd.it/5Vwgalc18IqnYE7aIh59IAOgguPxfM4Agv9EhnfeWJc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dfd5cd2e7d92a6db1b462a32c97900911d11d93", "width": 108, "height": 107}], "variants": {}, "id": "vuJ3SPN1HSQh3YWOb1ZAo50TA1_Ol-O149S9O3Ne1Es"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10g7tsp", "is_robot_indexable": true, "report_reasons": null, "author": "compound-cluster", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g7tsp/vs_code_extension_for_airflow_provider_packages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g7tsp/vs_code_extension_for_airflow_provider_packages/", "subreddit_subscribers": 86809, "created_utc": 1674149886.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apologies if this question in basic. I understand that they're the ones who handle data before it reaches any data science/engineering teams. But it's still pretty confusing. if those guys are incharge of cleaning up the data, where to Database Admins/devs come in the picture. is there any pictictional representation for how each role is related to the entire data lifecycle journey?\n\nI'm on a data literacy project and need help figuring out how they relate to one another", "author_fullname": "t2_58hz9jwy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does Data Owner/Custodian/Steward correlate with Data Analyst/Scientists/Engineers/BI etc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g3ymx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674140758.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if this question in basic. I understand that they&amp;#39;re the ones who handle data before it reaches any data science/engineering teams. But it&amp;#39;s still pretty confusing. if those guys are incharge of cleaning up the data, where to Database Admins/devs come in the picture. is there any pictictional representation for how each role is related to the entire data lifecycle journey?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on a data literacy project and need help figuring out how they relate to one another&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10g3ymx", "is_robot_indexable": true, "report_reasons": null, "author": "kantaBane", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g3ymx/how_does_data_ownercustodiansteward_correlate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g3ymx/how_does_data_ownercustodiansteward_correlate/", "subreddit_subscribers": 86809, "created_utc": 1674140758.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do you see as the role of DBAs in managing a cloud based SAAS database solution? It seems like a lot of the traditional dba responsibilities (backups, indexing/distribution/compression, etc) are handled by the service provider these days.\n\nI\u2019m trying to draw up a guide of roles and responsibilities for managing a new Redshift serverless lake house and I\u2019m having trouble figuring out where data engineering responsibilities end and dba work begins in this world. If it helps I\u2019m at an organization with 5,000+ employees so we try to delineate responsibilities pretty strictly.\n\nWhat are your thoughts?", "author_fullname": "t2_tz8w2mw4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Role of DBAs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fo32l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674090153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you see as the role of DBAs in managing a cloud based SAAS database solution? It seems like a lot of the traditional dba responsibilities (backups, indexing/distribution/compression, etc) are handled by the service provider these days.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to draw up a guide of roles and responsibilities for managing a new Redshift serverless lake house and I\u2019m having trouble figuring out where data engineering responsibilities end and dba work begins in this world. If it helps I\u2019m at an organization with 5,000+ employees so we try to delineate responsibilities pretty strictly.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fo32l", "is_robot_indexable": true, "report_reasons": null, "author": "HouseOfSawdust", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fo32l/role_of_dbas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fo32l/role_of_dbas/", "subreddit_subscribers": 86809, "created_utc": 1674090153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data scientist looking to upskill in data engineering. I'm hoping to get some knowledge from this subreddit. I have a small ML driven Dash app which reads data from a data warehouse and runs fine locally. \n\nI wish to containerised this app using Docker and host it on a cloud server. What is the most secure way to handle prodiving this app a service account to access my database? Can I hardcode it into my python code? Do I upload a copy of my sa.json file while containerising? Something else?\n\nAny help is appreciated.", "author_fullname": "t2_2gzsok4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice working with service accounts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fjczk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674078494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data scientist looking to upskill in data engineering. I&amp;#39;m hoping to get some knowledge from this subreddit. I have a small ML driven Dash app which reads data from a data warehouse and runs fine locally. &lt;/p&gt;\n\n&lt;p&gt;I wish to containerised this app using Docker and host it on a cloud server. What is the most secure way to handle prodiving this app a service account to access my database? Can I hardcode it into my python code? Do I upload a copy of my sa.json file while containerising? Something else?&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fjczk", "is_robot_indexable": true, "report_reasons": null, "author": "ciarandeceol1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fjczk/best_practice_working_with_service_accounts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fjczk/best_practice_working_with_service_accounts/", "subreddit_subscribers": 86809, "created_utc": 1674078494.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}