{"kind": "Listing", "data": {"after": "t3_10fhkkq", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi all, its been a pretty awful day. Two months ago my boss transferred out to a new team, my team was integrated in with another. Sure enough this morning i woke up to an email letting me know im no longer a valued member of the team.   \n\n\nat this point im feeling in the pits. over the last few years ive gone above and beyond for this company, spending untold unpaid evening and weekend hours trying to push our projects forward.   \n\n\nnot sure about the next steps, but i felt like i needed to vent. if anyone out there knows of any DE opportunities, please DM me.", "author_fullname": "t2_706trkkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "just got laid off (FAANG)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fg07o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 262, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 262, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674069529.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi all, its been a pretty awful day. Two months ago my boss transferred out to a new team, my team was integrated in with another. Sure enough this morning i woke up to an email letting me know im no longer a valued member of the team.   &lt;/p&gt;\n\n&lt;p&gt;at this point im feeling in the pits. over the last few years ive gone above and beyond for this company, spending untold unpaid evening and weekend hours trying to push our projects forward.   &lt;/p&gt;\n\n&lt;p&gt;not sure about the next steps, but i felt like i needed to vent. if anyone out there knows of any DE opportunities, please DM me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10fg07o", "is_robot_indexable": true, "report_reasons": null, "author": "Foodwithfloyd", "discussion_type": null, "num_comments": 84, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fg07o/just_got_laid_off_faang/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fg07o/just_got_laid_off_faang/", "subreddit_subscribers": 86792, "created_utc": 1674069529.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a little shill-y, but I think it\u2019s cool and I think others here will too.\n\nIf you haven\u2019t heard of Materialize, it\u2019s a database that incrementally updates query results as new data flows in from Kafka or Postgres logical replication. It\u2019s different from typical databases in that results are updated on *write* using a stream processing engine rather than recomputed from scratch on read. That means reads are typically super fast, even for really complicated views with lots of joins.\n\nOne of the first things I had to learn as a Field Engineer at Materialize was how to optimize SQL joins to help our customers save on memory (and $). To do that, I made a couple of updates to one of Frank McSherry\u2019s blogs, which were published today! I\u2019d love to see what you think!\n\nhttps://materialize.com/blog/delta-joins/", "author_fullname": "t2_5p00kusf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimize Joins in Materialize with Delta Queries and Late Materialization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ffdvx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674068433.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674068064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a little shill-y, but I think it\u2019s cool and I think others here will too.&lt;/p&gt;\n\n&lt;p&gt;If you haven\u2019t heard of Materialize, it\u2019s a database that incrementally updates query results as new data flows in from Kafka or Postgres logical replication. It\u2019s different from typical databases in that results are updated on &lt;em&gt;write&lt;/em&gt; using a stream processing engine rather than recomputed from scratch on read. That means reads are typically super fast, even for really complicated views with lots of joins.&lt;/p&gt;\n\n&lt;p&gt;One of the first things I had to learn as a Field Engineer at Materialize was how to optimize SQL joins to help our customers save on memory (and $). To do that, I made a couple of updates to one of Frank McSherry\u2019s blogs, which were published today! I\u2019d love to see what you think!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://materialize.com/blog/delta-joins/\"&gt;https://materialize.com/blog/delta-joins/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?auto=webp&amp;v=enabled&amp;s=9fc4396a318c2807c8b7eb9925d82c522295c4c5", "width": 1636, "height": 656}, "resolutions": [{"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c95088825ab701a767ae737724f85f494144ce90", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f02a3d2de233ef59877a3105038f7f04fbe9971", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1789d3f9b845c9675d57ff92fdc5f2501fa76646", "width": 320, "height": 128}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d892c0c41ed4dbb8ff6c9f253c903b791694699d", "width": 640, "height": 256}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=654e9ae06191dae53c2255b2275a46abd8b59c9b", "width": 960, "height": 384}, {"url": "https://external-preview.redd.it/GOFNP7TuLgPYzA0tyGoxpTksbY5zu3otmvgDF6Nagws.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ed95ec3916adc3bcb738ac68f52e0aaa922bef6", "width": 1080, "height": 433}], "variants": {}, "id": "IMNd2US1USXqACJWc3sq8tqHd6qPrbtxC4Zq9e0nYj4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10ffdvx", "is_robot_indexable": true, "report_reasons": null, "author": "Chuck-Alt-Delete", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ffdvx/optimize_joins_in_materialize_with_delta_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ffdvx/optimize_joins_in_materialize_with_delta_queries/", "subreddit_subscribers": 86792, "created_utc": 1674068064.0, "num_crossposts": 4, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Could be something very minor like a specific gotcha in a library or a major architecture decision that bit you in the behind.", "author_fullname": "t2_14i1sz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What mistake have you vowed never to repeat?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g3g1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674139484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could be something very minor like a specific gotcha in a library or a major architecture decision that bit you in the behind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10g3g1r", "is_robot_indexable": true, "report_reasons": null, "author": "pescennius", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g3g1r/what_mistake_have_you_vowed_never_to_repeat/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g3g1r/what_mistake_have_you_vowed_never_to_repeat/", "subreddit_subscribers": 86792, "created_utc": 1674139484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE community!  \nI've been working on the data migration to the star schema architecture, and here are a couple of questions I could not confidently decide how to proceed with.\n\n1. What are the common strategies for handling the loaded\\_at timestamp fields in dwh, when the target table gets the data from multiple different sources? I was thinking of creating a 'last' and/or the 'first' loaded\\_at fields, not sure if there's any better way.\n2. A similar question about specifying the 'source' field into the dimensions. When the dimension gets created out of two different sources, in the way that attributes of a single dimension record are coming from various sources, how do you usually handle the 'source' field? Here I though of two options, one was to simply indicate the main dimension record source, i.e. source from where the platform's unique identifier is coming. But this way we lose the source of additional dimension attributes.  \n\n\nThanks!", "author_fullname": "t2_gwqijajo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Star Schema - questions about loading the data from multiple sources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fzw1q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674129466.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE community!&lt;br/&gt;\nI&amp;#39;ve been working on the data migration to the star schema architecture, and here are a couple of questions I could not confidently decide how to proceed with.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What are the common strategies for handling the loaded_at timestamp fields in dwh, when the target table gets the data from multiple different sources? I was thinking of creating a &amp;#39;last&amp;#39; and/or the &amp;#39;first&amp;#39; loaded_at fields, not sure if there&amp;#39;s any better way.&lt;/li&gt;\n&lt;li&gt;A similar question about specifying the &amp;#39;source&amp;#39; field into the dimensions. When the dimension gets created out of two different sources, in the way that attributes of a single dimension record are coming from various sources, how do you usually handle the &amp;#39;source&amp;#39; field? Here I though of two options, one was to simply indicate the main dimension record source, i.e. source from where the platform&amp;#39;s unique identifier is coming. But this way we lose the source of additional dimension attributes.&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fzw1q", "is_robot_indexable": true, "report_reasons": null, "author": "orm_the_stalker", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fzw1q/star_schema_questions_about_loading_the_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fzw1q/star_schema_questions_about_loading_the_data_from/", "subreddit_subscribers": 86792, "created_utc": 1674129466.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I did a small test and it seemed to be faster but when is the hash function even supposed to be used? \n\nIn star schema, surrogate keys are traditionally built using autoincrement. Is that still the best way in SF or was the hash function supposed to be used here? Assuming number of rows doesn\u2019t exceed a billion.", "author_fullname": "t2_kew1pm23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In Snowflake, is joining by a hash column more performant then joining by multiple text columns?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fpc1f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674093596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did a small test and it seemed to be faster but when is the hash function even supposed to be used? &lt;/p&gt;\n\n&lt;p&gt;In star schema, surrogate keys are traditionally built using autoincrement. Is that still the best way in SF or was the hash function supposed to be used here? Assuming number of rows doesn\u2019t exceed a billion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fpc1f", "is_robot_indexable": true, "report_reasons": null, "author": "Critical_Chart_6513", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fpc1f/in_snowflake_is_joining_by_a_hash_column_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fpc1f/in_snowflake_is_joining_by_a_hash_column_more/", "subreddit_subscribers": 86792, "created_utc": 1674093596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a backend engineer and try to work on the data import part for my company. we have a GraphQL server as a gateway between the client and the internal server. GraphQL is the only thing exposed to the outside world.  \n\n\nI am thinking about writing scripts to map the CSV file to the GraphQL mutation query and import.  \nBut I would like to have a better way to handle the import jobs. e.g. some import depends on the previous import result, retry when the job is fail...  \n\n\nI am looking at the Airflow tool now. I am wondering if it is a good starting point for me.  \n\n\nThank you.", "author_fullname": "t2_4hu7xbt4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "would airflow be good for this use case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fo4ks", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674090265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a backend engineer and try to work on the data import part for my company. we have a GraphQL server as a gateway between the client and the internal server. GraphQL is the only thing exposed to the outside world.  &lt;/p&gt;\n\n&lt;p&gt;I am thinking about writing scripts to map the CSV file to the GraphQL mutation query and import.&lt;br/&gt;\nBut I would like to have a better way to handle the import jobs. e.g. some import depends on the previous import result, retry when the job is fail...  &lt;/p&gt;\n\n&lt;p&gt;I am looking at the Airflow tool now. I am wondering if it is a good starting point for me.  &lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fo4ks", "is_robot_indexable": true, "report_reasons": null, "author": "itsMoreAComment", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fo4ks/would_airflow_be_good_for_this_use_case/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fo4ks/would_airflow_be_good_for_this_use_case/", "subreddit_subscribers": 86792, "created_utc": 1674090265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working on processing a large number (+/- 900) of smaller tables. These tables have to be loaded from a landing zone where they arrive in CSV format to a raw layer in delta format. \n\nThese tables have to be loaded on a daily basis and I'm currently using a synapse spark notebook to do this.\n\nMy first solution was to store all the tables in a list and then use a for loop to process these tables on by one. This was however taking a long time so I set up a multi threaded approach using the python multiprocessing library.\n\nThis lead to some significant performance gains but Im not sure that this is the best approach as the performance currently varies run by run.\n\nI'm also not sure how to optimally setup a spark cluster for such an use case.\n\n\nSo I was curious how you guys are currently solving a problem like this and if there are people also using multi threading how you then optimize the spark process for such a thing. Like do you have a large driver and less executors or vice versa and how do you determine the amount of threads to create.\n\nMany thanks in advance!", "author_fullname": "t2_71slaawn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you processing large numbers of smaller tables using spark.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fllzt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674083819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on processing a large number (+/- 900) of smaller tables. These tables have to be loaded from a landing zone where they arrive in CSV format to a raw layer in delta format. &lt;/p&gt;\n\n&lt;p&gt;These tables have to be loaded on a daily basis and I&amp;#39;m currently using a synapse spark notebook to do this.&lt;/p&gt;\n\n&lt;p&gt;My first solution was to store all the tables in a list and then use a for loop to process these tables on by one. This was however taking a long time so I set up a multi threaded approach using the python multiprocessing library.&lt;/p&gt;\n\n&lt;p&gt;This lead to some significant performance gains but Im not sure that this is the best approach as the performance currently varies run by run.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also not sure how to optimally setup a spark cluster for such an use case.&lt;/p&gt;\n\n&lt;p&gt;So I was curious how you guys are currently solving a problem like this and if there are people also using multi threading how you then optimize the spark process for such a thing. Like do you have a large driver and less executors or vice versa and how do you determine the amount of threads to create.&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fllzt", "is_robot_indexable": true, "report_reasons": null, "author": "blindbox2", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fllzt/how_are_you_processing_large_numbers_of_smaller/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fllzt/how_are_you_processing_large_numbers_of_smaller/", "subreddit_subscribers": 86792, "created_utc": 1674083819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What does a career path look like for someone in a leadership role in the data field? What are the common job titles and responsibilities one can expect to have at each step, and what skills and qualifications are typically required to advance in this field? I'm a head of data in a scale-up but am not sure of what my career progression will look like. It's less straightforward than when working in \"traditional\" teams. Would like to hear about other's experience", "author_fullname": "t2_9blh4yzs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data leader career progression", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fhyud", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674074170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What does a career path look like for someone in a leadership role in the data field? What are the common job titles and responsibilities one can expect to have at each step, and what skills and qualifications are typically required to advance in this field? I&amp;#39;m a head of data in a scale-up but am not sure of what my career progression will look like. It&amp;#39;s less straightforward than when working in &amp;quot;traditional&amp;quot; teams. Would like to hear about other&amp;#39;s experience&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10fhyud", "is_robot_indexable": true, "report_reasons": null, "author": "Strict_Algae3766", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fhyud/data_leader_career_progression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fhyud/data_leader_career_progression/", "subreddit_subscribers": 86792, "created_utc": 1674074170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_31ilk7t8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to speed up range joins joins in Snowflake by 300x - SELECT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "name": "t3_10g65bg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yVrMGv9ob-fHob-nsAMDUf1Wxgufd4sgTPddBt3tMfo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674145957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "select.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://select.dev/posts/snowflake-range-join-optimization", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?auto=webp&amp;v=enabled&amp;s=c6102393c1fc139ebe3a192d54baefc9269fa3d6", "width": 1200, "height": 733}, "resolutions": [{"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e24f19fe617b042b54367889609e4aa75d7d6135", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc01477758940100756397660527fd80cfa738ad", "width": 216, "height": 131}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2db23948c752c587caa1f5bd2ecbfb81b13c713f", "width": 320, "height": 195}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac762f60c4c6caf1fa44521997dda65fed7b824c", "width": 640, "height": 390}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=956b255f528f54123a9598503aa68e2eef59b025", "width": 960, "height": 586}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c8a705f31099be5dd86194da0794355c6a7b829", "width": 1080, "height": 659}], "variants": {}, "id": "Kl2clgMmndy65WOWu6IHcN4IPoSI4AphoMLolknNnyA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10g65bg", "is_robot_indexable": true, "report_reasons": null, "author": "ian-whitestone", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g65bg/how_to_speed_up_range_joins_joins_in_snowflake_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://select.dev/posts/snowflake-range-join-optimization", "subreddit_subscribers": 86792, "created_utc": 1674145957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm conducting a study on how I would use Apache Superset for an organization. I will have large amounts of data at source (Million of Rows scale) and about 600 Users.\n\n  \nWhat would be a good way of deploying Superset Google Cloud ? I'm thinking GKE but I have no idea about the right sizing, nor on how much would it cost.\n\n  \nIt would be helpful if you could share some experiences you had.\n\nThank you !", "author_fullname": "t2_98269xyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying Apache Superset on GKE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fgihh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674070723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m conducting a study on how I would use Apache Superset for an organization. I will have large amounts of data at source (Million of Rows scale) and about 600 Users.&lt;/p&gt;\n\n&lt;p&gt;What would be a good way of deploying Superset Google Cloud ? I&amp;#39;m thinking GKE but I have no idea about the right sizing, nor on how much would it cost.&lt;/p&gt;\n\n&lt;p&gt;It would be helpful if you could share some experiences you had.&lt;/p&gt;\n\n&lt;p&gt;Thank you !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fgihh", "is_robot_indexable": true, "report_reasons": null, "author": "Electronic-Mountain9", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fgihh/deploying_apache_superset_on_gke/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fgihh/deploying_apache_superset_on_gke/", "subreddit_subscribers": 86792, "created_utc": 1674070723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am an intern with a long leash. My current python script calls a series of apis and puts the code into a dataframe. Let's call this script 1 and df1. \n\n&amp;#x200B;\n\nEach day I would like this script to run on a schedule and then append either a csv file or SQL table. \n\nMy immediate group uses AWS, and someone in my group said I could go ahead and use their SQL server. My experience with SQL is MySQL workbench, and I've connected to an AWS server before. \n\n&amp;#x200B;\n\nI was able to simply run my notebook in ML studio in azure on my personal computer, but I guess that isn't configured at work so that isn't an option. We do have pyspark and databricks through azure. I have deployed to an EC2 in aws before for a different project, but got really lost with aws lamda trying to schedule and I don't have a linux background. \n\n&amp;#x200B;\n\nI feel comfortable in python and can do some ML, and I would like to just use scikit-learn in python for predictive analytics down the line. It seems like scheduling batches is a bit more drag and drop in azure. This pipeline would probably just update a csv in the data lake each day. I am leaning azure.\n\n&amp;#x200B;\n\nwhich platform is the easiest to run this pipeline and schedule the code to run once per day?", "author_fullname": "t2_pqpunsd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "azure or aws? daily batch api calls --&gt; pandas dataframe --&gt; sql or csv --&gt; powerbi", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10febor", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674065611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an intern with a long leash. My current python script calls a series of apis and puts the code into a dataframe. Let&amp;#39;s call this script 1 and df1. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Each day I would like this script to run on a schedule and then append either a csv file or SQL table. &lt;/p&gt;\n\n&lt;p&gt;My immediate group uses AWS, and someone in my group said I could go ahead and use their SQL server. My experience with SQL is MySQL workbench, and I&amp;#39;ve connected to an AWS server before. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was able to simply run my notebook in ML studio in azure on my personal computer, but I guess that isn&amp;#39;t configured at work so that isn&amp;#39;t an option. We do have pyspark and databricks through azure. I have deployed to an EC2 in aws before for a different project, but got really lost with aws lamda trying to schedule and I don&amp;#39;t have a linux background. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I feel comfortable in python and can do some ML, and I would like to just use scikit-learn in python for predictive analytics down the line. It seems like scheduling batches is a bit more drag and drop in azure. This pipeline would probably just update a csv in the data lake each day. I am leaning azure.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;which platform is the easiest to run this pipeline and schedule the code to run once per day?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10febor", "is_robot_indexable": true, "report_reasons": null, "author": "ClimatePhilosopher", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10febor/azure_or_aws_daily_batch_api_calls_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10febor/azure_or_aws_daily_batch_api_calls_pandas/", "subreddit_subscribers": 86792, "created_utc": 1674065611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j33i3nha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Requesting feedback on my resume - Data Engineer with a little over 2 years of experience, was laid off recently", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_10g7wus", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EYob2aCUBnKMaS02mKE8gOPld9yR-gCcrmYmsOtDUuw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674150081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zxbize3zd1da1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?auto=webp&amp;v=enabled&amp;s=9cca47ece1fc5ac2ac8ce349298c45c45aad2427", "width": 1700, "height": 2200}, "resolutions": [{"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be479760d91ee62bb8c1471afe5961e9285b2286", "width": 108, "height": 139}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0237c1248dafa3dfa220c7a14c15a52fa4896d13", "width": 216, "height": 279}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c234a772ce92e14f3642beb70560a6fc7e173be", "width": 320, "height": 414}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f5fc705a3bdac8a314c9016aa1ec77add7eed5f", "width": 640, "height": 828}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f68926ee615705f32c8cb6f0ee9fd672b2a9f16c", "width": 960, "height": 1242}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7aa391976255f8543c948d67f3fefd76e888af0", "width": 1080, "height": 1397}], "variants": {}, "id": "8eDElsZ4niQq2Lvas-ev5VOLUMDDsCuf4bubD9ehkf8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10g7wus", "is_robot_indexable": true, "report_reasons": null, "author": "CS_throwaway_DE", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10g7wus/requesting_feedback_on_my_resume_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zxbize3zd1da1.jpg", "subreddit_subscribers": 86792, "created_utc": 1674150081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was tasked to aggregate data in DynamoDB. It was only around 5GB of data. Is it better if I aggregate data using AWS Glue and load it to s3 as csv? Or should I export the table to s3 and query using athena?", "author_fullname": "t2_6542em0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Query from Athena or Transform in AWS Glue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fzw3l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674129471.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was tasked to aggregate data in DynamoDB. It was only around 5GB of data. Is it better if I aggregate data using AWS Glue and load it to s3 as csv? Or should I export the table to s3 and query using athena?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fzw3l", "is_robot_indexable": true, "report_reasons": null, "author": "EngrRhys", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fzw3l/query_from_athena_or_transform_in_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fzw3l/query_from_athena_or_transform_in_aws_glue/", "subreddit_subscribers": 86792, "created_utc": 1674129471.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know how to directly get the status of a specific sync job?\n\nThe round about way I\u2019m attempting to do this is by\n1. fetching all the logs of the extractions\n2. filtering them so that the extraction\u2019s job matches the name of the current job\n3. fetching the loads\n4. filtering them where the job name matches the current job name\n5. Parse the extraction logs to see if any of the streams for that extraction fetched any rows\n6. If a stream fetched rows based on the extraction log, then check the streams from the load\n7. If the load for a stream has a specific value designating it is done, and if it had rows extracted based on the logs, mark that stream as done\n8. Keep polling and once all streams are \u201cmarked\u201d as done, consider the job done\n\n\nProblem: a ton of api calls, and potential rate limiting.\n\nAnyone else solve this issue?\n\nHere are there docs: https://www.stitchdata.com/docs/developers/stitch-connect/api", "author_fullname": "t2_9aeao1ey", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stitch Connect API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ftnmc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674106423.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know how to directly get the status of a specific sync job?&lt;/p&gt;\n\n&lt;p&gt;The round about way I\u2019m attempting to do this is by\n1. fetching all the logs of the extractions\n2. filtering them so that the extraction\u2019s job matches the name of the current job\n3. fetching the loads\n4. filtering them where the job name matches the current job name\n5. Parse the extraction logs to see if any of the streams for that extraction fetched any rows\n6. If a stream fetched rows based on the extraction log, then check the streams from the load\n7. If the load for a stream has a specific value designating it is done, and if it had rows extracted based on the logs, mark that stream as done\n8. Keep polling and once all streams are \u201cmarked\u201d as done, consider the job done&lt;/p&gt;\n\n&lt;p&gt;Problem: a ton of api calls, and potential rate limiting.&lt;/p&gt;\n\n&lt;p&gt;Anyone else solve this issue?&lt;/p&gt;\n\n&lt;p&gt;Here are there docs: &lt;a href=\"https://www.stitchdata.com/docs/developers/stitch-connect/api\"&gt;https://www.stitchdata.com/docs/developers/stitch-connect/api&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "ex-Airbnb engineer (data, ML)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ftnmc", "is_robot_indexable": true, "report_reasons": null, "author": "ironplaneswalker", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10ftnmc/stitch_connect_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ftnmc/stitch_connect_api/", "subreddit_subscribers": 86792, "created_utc": 1674106423.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in the biotech industry, where we actually generate a lot of our important data sources locally. Instruments like mass spectrometers and next generation sequencers make large datasets that are best analyzed in the cloud.\n\nI've been bouncing between a number of tools to automate data movement into the cloud, but I still feel like there's a better way to engineer it. I've used 3rd party tools (BioBright, TntDrive), and AWS services (Data Sync, Storage Gateway, CLI). But either the solution is too overengineered or it's easily breakable... I know this is a common use case, so I'm curious if others have found anything they like using?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you automate data movement into the cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fmx2h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674087097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in the biotech industry, where we actually generate a lot of our important data sources locally. Instruments like mass spectrometers and next generation sequencers make large datasets that are best analyzed in the cloud.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been bouncing between a number of tools to automate data movement into the cloud, but I still feel like there&amp;#39;s a better way to engineer it. I&amp;#39;ve used 3rd party tools (BioBright, TntDrive), and AWS services (Data Sync, Storage Gateway, CLI). But either the solution is too overengineered or it&amp;#39;s easily breakable... I know this is a common use case, so I&amp;#39;m curious if others have found anything they like using?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fmx2h", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fmx2h/how_do_you_automate_data_movement_into_the_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fmx2h/how_do_you_automate_data_movement_into_the_cloud/", "subreddit_subscribers": 86792, "created_utc": 1674087097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am helping a very small company to manage their sales data generated from a few sources(web stores). \n\nThe data is under 50k rows so ideally I would like to utilize cloud storage options that have a free tier for future scalability as they are growing.\n\nFor now I am thinking of using webhooks to start. \n\nAlso a little context, I am have experience as a analyst but would like to move into engineering and would want to use this as a little project to add in my portfolio.", "author_fullname": "t2_687fm3oi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple solution for very small datasets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fltqe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674084333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am helping a very small company to manage their sales data generated from a few sources(web stores). &lt;/p&gt;\n\n&lt;p&gt;The data is under 50k rows so ideally I would like to utilize cloud storage options that have a free tier for future scalability as they are growing.&lt;/p&gt;\n\n&lt;p&gt;For now I am thinking of using webhooks to start. &lt;/p&gt;\n\n&lt;p&gt;Also a little context, I am have experience as a analyst but would like to move into engineering and would want to use this as a little project to add in my portfolio.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fltqe", "is_robot_indexable": true, "report_reasons": null, "author": "mysterymalts", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fltqe/simple_solution_for_very_small_datasets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fltqe/simple_solution_for_very_small_datasets/", "subreddit_subscribers": 86792, "created_utc": 1674084333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI'm relatively new to DynamoDB and I've been tasked with moving application from our DynamoDB database into a format that can be consumed by our existing snowflake data-lake. \n\nThis application is replacing an existing application that had a more traditional SQL/RDBMS style data back-end, and the lake layer was just a synchronized copy of the application layer.  \n\nI'm scratching my head at how to approach this. With a single table design the data is as far from normalized as it can get. Thankfully the dev team has created an urn as part of each item and I'm thinking I can use that to reconstruct the entities of the application (did I mention that the access patterns, and entities are only loosely documented?) \n\nThat leaves me with the following questions and open for advice?\n\n* How do I successfully investigate the heterogeneous data in DynamoDB to verify what entities I can stitch back together (I've tried Athena, but its unable to successfully crawl the data due to some fields being json objects / structs that are inconsistent).\n* Assuming I can extract the data (e.g. export to s3, or use Glue for some more sophisticated processing) how can I hope to consistently keep the s3 data lake up to date (Maybe use Kinesis and the associated lambda to find the \"record\" in s3 and update it accordingly)", "author_fullname": "t2_6d49ikq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading/Normalizing Data from Single Table Designed DynamoDB Consumption in AWS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fkdox", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674080856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m relatively new to DynamoDB and I&amp;#39;ve been tasked with moving application from our DynamoDB database into a format that can be consumed by our existing snowflake data-lake. &lt;/p&gt;\n\n&lt;p&gt;This application is replacing an existing application that had a more traditional SQL/RDBMS style data back-end, and the lake layer was just a synchronized copy of the application layer.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m scratching my head at how to approach this. With a single table design the data is as far from normalized as it can get. Thankfully the dev team has created an urn as part of each item and I&amp;#39;m thinking I can use that to reconstruct the entities of the application (did I mention that the access patterns, and entities are only loosely documented?) &lt;/p&gt;\n\n&lt;p&gt;That leaves me with the following questions and open for advice?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How do I successfully investigate the heterogeneous data in DynamoDB to verify what entities I can stitch back together (I&amp;#39;ve tried Athena, but its unable to successfully crawl the data due to some fields being json objects / structs that are inconsistent).&lt;/li&gt;\n&lt;li&gt;Assuming I can extract the data (e.g. export to s3, or use Glue for some more sophisticated processing) how can I hope to consistently keep the s3 data lake up to date (Maybe use Kinesis and the associated lambda to find the &amp;quot;record&amp;quot; in s3 and update it accordingly)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fkdox", "is_robot_indexable": true, "report_reasons": null, "author": "poppinstacks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fkdox/loadingnormalizing_data_from_single_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fkdox/loadingnormalizing_data_from_single_table/", "subreddit_subscribers": 86792, "created_utc": 1674080856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand that SF is the out-of-the-box DWH that might cost a bit more but just works. And I've read that costs can be kept down if you manage data in a SF friendly way. \n\nWhat is that SF friendly way? Could anyone point me to a good brief about _how_ to ideally use SF? The nature of workflows that work best in that environment? \n\nJust beginning to look into it. Thanks for any pointers.", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's snowflake's deal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fj24x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674077851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand that SF is the out-of-the-box DWH that might cost a bit more but just works. And I&amp;#39;ve read that costs can be kept down if you manage data in a SF friendly way. &lt;/p&gt;\n\n&lt;p&gt;What is that SF friendly way? Could anyone point me to a good brief about &lt;em&gt;how&lt;/em&gt; to ideally use SF? The nature of workflows that work best in that environment? &lt;/p&gt;\n\n&lt;p&gt;Just beginning to look into it. Thanks for any pointers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fj24x", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fj24x/whats_snowflakes_deal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fj24x/whats_snowflakes_deal/", "subreddit_subscribers": 86792, "created_utc": 1674077851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to know if my company is the odd one put or if it is widespread for medium to large sized organizations to use Databricks, but not forking out the extra money for the premium features. I think we are missing out on some really beneficial features, but I am having a hard time convincing our governance team to sign off on the premium tier. So what is the concensus among the redditors in here?\n\nI am using the Azure flavor, if that is of relevance to any of you.", "author_fullname": "t2_rszqhgyr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anybody actually use databricks standard tier professionally?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fg1q2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674069627.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to know if my company is the odd one put or if it is widespread for medium to large sized organizations to use Databricks, but not forking out the extra money for the premium features. I think we are missing out on some really beneficial features, but I am having a hard time convincing our governance team to sign off on the premium tier. So what is the concensus among the redditors in here?&lt;/p&gt;\n\n&lt;p&gt;I am using the Azure flavor, if that is of relevance to any of you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fg1q2", "is_robot_indexable": true, "report_reasons": null, "author": "bjtho08", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fg1q2/does_anybody_actually_use_databricks_standard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fg1q2/does_anybody_actually_use_databricks_standard/", "subreddit_subscribers": 86792, "created_utc": 1674069627.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I kinda find the tools both use are same...so it makes both the roles same only ?", "author_fullname": "t2_99z52qfs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Data Engineers and Big Data Engineers same?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10g7wyt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674150089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I kinda find the tools both use are same...so it makes both the roles same only ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10g7wyt", "is_robot_indexable": true, "report_reasons": null, "author": "Vortex_007s", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g7wyt/are_data_engineers_and_big_data_engineers_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g7wyt/are_data_engineers_and_big_data_engineers_same/", "subreddit_subscribers": 86792, "created_utc": 1674150089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Tracking down all the args and import statements for providers was sometimes a pain. This VS Code extension has all of the providers loaded so you can autocomplete all of the operators and hooks. [https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates](https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates)", "author_fullname": "t2_shq2slu0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "VS Code extension for Airflow Provider packages: Airflow Templates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10g7tsp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674149886.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tracking down all the args and import statements for providers was sometimes a pain. This VS Code extension has all of the providers loaded so you can autocomplete all of the operators and hooks. &lt;a href=\"https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates\"&gt;https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5Vwgalc18IqnYE7aIh59IAOgguPxfM4Agv9EhnfeWJc.jpg?auto=webp&amp;v=enabled&amp;s=f519d587d5d576b6d3824ecfeb08e435ac409c9d", "width": 140, "height": 139}, "resolutions": [{"url": "https://external-preview.redd.it/5Vwgalc18IqnYE7aIh59IAOgguPxfM4Agv9EhnfeWJc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dfd5cd2e7d92a6db1b462a32c97900911d11d93", "width": 108, "height": 107}], "variants": {}, "id": "vuJ3SPN1HSQh3YWOb1ZAo50TA1_Ol-O149S9O3Ne1Es"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10g7tsp", "is_robot_indexable": true, "report_reasons": null, "author": "compound-cluster", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g7tsp/vs_code_extension_for_airflow_provider_packages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g7tsp/vs_code_extension_for_airflow_provider_packages/", "subreddit_subscribers": 86792, "created_utc": 1674149886.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apologies if this question in basic. I understand that they're the ones who handle data before it reaches any data science/engineering teams. But it's still pretty confusing. if those guys are incharge of cleaning up the data, where to Database Admins/devs come in the picture. is there any pictictional representation for how each role is related to the entire data lifecycle journey?\n\nI'm on a data literacy project and need help figuring out how they relate to one another", "author_fullname": "t2_58hz9jwy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does Data Owner/Custodian/Steward correlate with Data Analyst/Scientists/Engineers/BI etc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g3ymx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674140758.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if this question in basic. I understand that they&amp;#39;re the ones who handle data before it reaches any data science/engineering teams. But it&amp;#39;s still pretty confusing. if those guys are incharge of cleaning up the data, where to Database Admins/devs come in the picture. is there any pictictional representation for how each role is related to the entire data lifecycle journey?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on a data literacy project and need help figuring out how they relate to one another&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10g3ymx", "is_robot_indexable": true, "report_reasons": null, "author": "kantaBane", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g3ymx/how_does_data_ownercustodiansteward_correlate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g3ymx/how_does_data_ownercustodiansteward_correlate/", "subreddit_subscribers": 86792, "created_utc": 1674140758.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do you see as the role of DBAs in managing a cloud based SAAS database solution? It seems like a lot of the traditional dba responsibilities (backups, indexing/distribution/compression, etc) are handled by the service provider these days.\n\nI\u2019m trying to draw up a guide of roles and responsibilities for managing a new Redshift serverless lake house and I\u2019m having trouble figuring out where data engineering responsibilities end and dba work begins in this world. If it helps I\u2019m at an organization with 5,000+ employees so we try to delineate responsibilities pretty strictly.\n\nWhat are your thoughts?", "author_fullname": "t2_tz8w2mw4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Role of DBAs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fo32l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674090153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you see as the role of DBAs in managing a cloud based SAAS database solution? It seems like a lot of the traditional dba responsibilities (backups, indexing/distribution/compression, etc) are handled by the service provider these days.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to draw up a guide of roles and responsibilities for managing a new Redshift serverless lake house and I\u2019m having trouble figuring out where data engineering responsibilities end and dba work begins in this world. If it helps I\u2019m at an organization with 5,000+ employees so we try to delineate responsibilities pretty strictly.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10fo32l", "is_robot_indexable": true, "report_reasons": null, "author": "HouseOfSawdust", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fo32l/role_of_dbas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fo32l/role_of_dbas/", "subreddit_subscribers": 86792, "created_utc": 1674090153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data scientist looking to upskill in data engineering. I'm hoping to get some knowledge from this subreddit. I have a small ML driven Dash app which reads data from a data warehouse and runs fine locally. \n\nI wish to containerised this app using Docker and host it on a cloud server. What is the most secure way to handle prodiving this app a service account to access my database? Can I hardcode it into my python code? Do I upload a copy of my sa.json file while containerising? Something else?\n\nAny help is appreciated.", "author_fullname": "t2_2gzsok4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice working with service accounts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fjczk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674078494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data scientist looking to upskill in data engineering. I&amp;#39;m hoping to get some knowledge from this subreddit. I have a small ML driven Dash app which reads data from a data warehouse and runs fine locally. &lt;/p&gt;\n\n&lt;p&gt;I wish to containerised this app using Docker and host it on a cloud server. What is the most secure way to handle prodiving this app a service account to access my database? Can I hardcode it into my python code? Do I upload a copy of my sa.json file while containerising? Something else?&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fjczk", "is_robot_indexable": true, "report_reasons": null, "author": "ciarandeceol1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fjczk/best_practice_working_with_service_accounts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fjczk/best_practice_working_with_service_accounts/", "subreddit_subscribers": 86792, "created_utc": 1674078494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lately, I finished reading the **Learning Spark: Lightning-Fast Data Analytics** book, a book that helped me to have a better understanding of **Apache** **Spark**.  \n\nMy new article goes over some of the takeaways I learned from reading the book, and my daily interaction with Spark for 6 months.  \n\nLink: https://aymane11.github.io/posts/apache-spark-gotchas/", "author_fullname": "t2_20xw1ryl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Spark Gotchas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fhkkq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674073223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lately, I finished reading the &lt;strong&gt;Learning Spark: Lightning-Fast Data Analytics&lt;/strong&gt; book, a book that helped me to have a better understanding of &lt;strong&gt;Apache&lt;/strong&gt; &lt;strong&gt;Spark&lt;/strong&gt;.  &lt;/p&gt;\n\n&lt;p&gt;My new article goes over some of the takeaways I learned from reading the book, and my daily interaction with Spark for 6 months.  &lt;/p&gt;\n\n&lt;p&gt;Link: &lt;a href=\"https://aymane11.github.io/posts/apache-spark-gotchas/\"&gt;https://aymane11.github.io/posts/apache-spark-gotchas/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10fhkkq", "is_robot_indexable": true, "report_reasons": null, "author": "Enamya11", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fhkkq/apache_spark_gotchas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fhkkq/apache_spark_gotchas/", "subreddit_subscribers": 86792, "created_utc": 1674073223.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}