{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have lots of free time as a single guy in my mid 20's. I started in engineering but have shifted into data work in the IoT space. I'd like to make more money this year and have a few ideas in mind and would like thoughts from other professionals?\n\n* **Continue working and enjoy my free time whatever way I see fit:** This is obviously the simplest option. I believe I'm making good strides at learning new things and exploring new technologies at this company. We are new in the IoT space for manufacturing data and I've been improving day by day at my job. After 9 months I asked for a raise that my boss wholeheartedly believes I deserve but it got caught up in HR bs. Allegedly there is still a chance I could get that raise but either way I doubt it will be anywhere near the amount I want (I should note I took less money for this role to learn IoT but have made massive strides and implemented some interesting ML use cases already). The reason I even see this as an option is my job does take quite a bit out of me but the holiday break has me feeling refreshed lol.\n* **Learn data skills and get certification in my own free time:** Go through ML and Azure classes and potentially work towards getting Azure certified for data engineering.\n* **Start A Data-Driven Blog:** Similar in style to fivethirtyeight except more cool charts and less reading for folks. Would want the target market to be people outside of the data world to be interested in the data-driven perspective on politics/sports as well.\n* **Create an iOS app:** Obviously not quite in the same realm but working on the back end will require some level of exercising my data engineering skills. This is something I've always wanted to do but have no experience with swift or app development at all. I have an idea but it seems immensely extensive for a newcomer.", "author_fullname": "t2_fgc39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should I be doing with my free time to advance my career? (Read Full Post)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_101yf4l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672721114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have lots of free time as a single guy in my mid 20&amp;#39;s. I started in engineering but have shifted into data work in the IoT space. I&amp;#39;d like to make more money this year and have a few ideas in mind and would like thoughts from other professionals?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Continue working and enjoy my free time whatever way I see fit:&lt;/strong&gt; This is obviously the simplest option. I believe I&amp;#39;m making good strides at learning new things and exploring new technologies at this company. We are new in the IoT space for manufacturing data and I&amp;#39;ve been improving day by day at my job. After 9 months I asked for a raise that my boss wholeheartedly believes I deserve but it got caught up in HR bs. Allegedly there is still a chance I could get that raise but either way I doubt it will be anywhere near the amount I want (I should note I took less money for this role to learn IoT but have made massive strides and implemented some interesting ML use cases already). The reason I even see this as an option is my job does take quite a bit out of me but the holiday break has me feeling refreshed lol.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Learn data skills and get certification in my own free time:&lt;/strong&gt; Go through ML and Azure classes and potentially work towards getting Azure certified for data engineering.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start A Data-Driven Blog:&lt;/strong&gt; Similar in style to fivethirtyeight except more cool charts and less reading for folks. Would want the target market to be people outside of the data world to be interested in the data-driven perspective on politics/sports as well.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Create an iOS app:&lt;/strong&gt; Obviously not quite in the same realm but working on the back end will require some level of exercising my data engineering skills. This is something I&amp;#39;ve always wanted to do but have no experience with swift or app development at all. I have an idea but it seems immensely extensive for a newcomer.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "101yf4l", "is_robot_indexable": true, "report_reasons": null, "author": "TheOnlinePolak", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/101yf4l/what_should_i_be_doing_with_my_free_time_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/101yf4l/what_should_i_be_doing_with_my_free_time_to/", "subreddit_subscribers": 85015, "created_utc": 1672721114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Clean/Manipulate data as it's scraped using Python OR Use Python only for scraping data then store the data in a db then clean/manipulate using SQL?", "author_fullname": "t2_45tfneon", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web Scrapers: Clean &amp; Manipulate data on the fly using python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1025tj2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672745605.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Clean/Manipulate data as it&amp;#39;s scraped using Python OR Use Python only for scraping data then store the data in a db then clean/manipulate using SQL?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1025tj2", "is_robot_indexable": true, "report_reasons": null, "author": "Administrative_Ad768", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1025tj2/web_scrapers_clean_manipulate_data_on_the_fly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1025tj2/web_scrapers_clean_manipulate_data_on_the_fly/", "subreddit_subscribers": 85015, "created_utc": 1672745605.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_t1h22hkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kafka Monitoring with eBPF: It\u2019s a Whole New Perspective", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_102akx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/LE63vVsI5tVLp0llaT9KnII28BGidJUtaVWdhi0nYhY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1672758817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "groundcover.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.groundcover.com/blog/monitoring-kafka-metrics", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L0JVKK2-tDjZnLCdKU7PpKH5tgsK_BsdV_nJM8NIaTI.jpg?auto=webp&amp;s=ecb451fc194e0514c0360661ec01d72443e8b633", "width": 396, "height": 228}, "resolutions": [{"url": "https://external-preview.redd.it/L0JVKK2-tDjZnLCdKU7PpKH5tgsK_BsdV_nJM8NIaTI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c259155e6376af6cccce9426fac5a9d0bef642b5", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/L0JVKK2-tDjZnLCdKU7PpKH5tgsK_BsdV_nJM8NIaTI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e12bcceff19fa53773510fc80fcfaa3e8c738c99", "width": 216, "height": 124}, {"url": "https://external-preview.redd.it/L0JVKK2-tDjZnLCdKU7PpKH5tgsK_BsdV_nJM8NIaTI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d4091a394bcf6adb845e29e87350774f595ba080", "width": 320, "height": 184}], "variants": {}, "id": "BTRHBYVRdKd12SyYrI4LWp1scN6XidF5akPQtKjLHew"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "102akx8", "is_robot_indexable": true, "report_reasons": null, "author": "woehoe198", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102akx8/kafka_monitoring_with_ebpf_its_a_whole_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.groundcover.com/blog/monitoring-kafka-metrics", "subreddit_subscribers": 85015, "created_utc": 1672758817.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_56xhg", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Writing a Python SQL engine from scratch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_102d1fm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 14, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://a.thumbs.redditmedia.com/rMvvwPPFFhs2jxlcu6tP1GD-PZXMyWdPD7jD4buqs-0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1672764933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/tobymao/sqlglot/blob/main/posts/python_sql_engine.md", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/h4PKF3ASWIvZ62zs5V-kqLS3aLPO3gQRBxRxXPl3DuU.jpg?auto=webp&amp;s=43e617540f37af8e02628265a93e6bbbe16c6f10", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/h4PKF3ASWIvZ62zs5V-kqLS3aLPO3gQRBxRxXPl3DuU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=17d1a577d41f5daef9b8ae6218688788eb796a5a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/h4PKF3ASWIvZ62zs5V-kqLS3aLPO3gQRBxRxXPl3DuU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=48ca067905e6272595a7da941976994a78133956", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/h4PKF3ASWIvZ62zs5V-kqLS3aLPO3gQRBxRxXPl3DuU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9a67563882a2fda63e4a35071bc880cdfa93c912", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/h4PKF3ASWIvZ62zs5V-kqLS3aLPO3gQRBxRxXPl3DuU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=30a53bb0d252f83935778261a3d81eba8a1d6eed", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/h4PKF3ASWIvZ62zs5V-kqLS3aLPO3gQRBxRxXPl3DuU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e0b28b69ce9b58732e4f29290dd0a4f8931ae14a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/h4PKF3ASWIvZ62zs5V-kqLS3aLPO3gQRBxRxXPl3DuU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6355573f1b27f1c6c8be8926c659a8d824637609", "width": 1080, "height": 540}], "variants": {}, "id": "Fz8DwcNym75LLQRoAIsbFYQffoaOTnLHS3B6fUr-iZQ"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "102d1fm", "is_robot_indexable": true, "report_reasons": null, "author": "captaintobs", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102d1fm/writing_a_python_sql_engine_from_scratch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/tobymao/sqlglot/blob/main/posts/python_sql_engine.md", "subreddit_subscribers": 85015, "created_utc": 1672764933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A previous poll [revealed BI](https://www.reddit.com/r/dataengineering/comments/z70cl2/what_is_the_primary_use_case_of_your_data/) as possibly the most common use case for readers of this sub. It will be interesting to see how/if this bias is reflected. \n\n[View Poll](https://www.reddit.com/poll/1029j2z)", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your organization's highest priority data engineering objective in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1029j2z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672756011.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A previous poll &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/z70cl2/what_is_the_primary_use_case_of_your_data/\"&gt;revealed BI&lt;/a&gt; as possibly the most common use case for readers of this sub. It will be interesting to see how/if this bias is reflected. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1029j2z\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1029j2z", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1673015211586, "options": [{"text": "Integrating New Data Sources", "id": "20786315"}, {"text": "Optimization (Performance, Cost, etc)", "id": "20786316"}, {"text": "Data Quality", "id": "20786317"}, {"text": "Data Apps/Products (Dashboards, bots, etc)", "id": "20786318"}, {"text": "Other or Combination of Above (Please Comment)", "id": "20786319"}, {"text": "See Results", "id": "20786320"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 135, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1029j2z/what_is_your_organizations_highest_priority_data/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1029j2z/what_is_your_organizations_highest_priority_data/", "subreddit_subscribers": 85015, "created_utc": 1672756011.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm doing some research and writing an article about red flags on Job Ads and Job Descriptions. \n\nI'm covering a few industries and job types and would love to know...\n\nWhen you engineering guys and gals are scanning the market, what are the red flags that make you think \"Hell no!\".\n\nWould appreciate any feedback from the community \ud83d\udc4b", "author_fullname": "t2_4fnvvjfr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on red flags", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1026t9c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672748617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing some research and writing an article about red flags on Job Ads and Job Descriptions. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m covering a few industries and job types and would love to know...&lt;/p&gt;\n\n&lt;p&gt;When you engineering guys and gals are scanning the market, what are the red flags that make you think &amp;quot;Hell no!&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Would appreciate any feedback from the community \ud83d\udc4b&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1026t9c", "is_robot_indexable": true, "report_reasons": null, "author": "benonomus", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1026t9c/question_on_red_flags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1026t9c/question_on_red_flags/", "subreddit_subscribers": 85015, "created_utc": 1672748617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you do it? What tools do you use?\n\nLet's say you have an incremental load pipeline, you alter the staging and the presentation layer tables by adding a column. You also Alter the stored procedures involved and import new column data (by full load since SPs take only last days/months). Since data is several millions row MAYBE it is better to have a temp table taking only id's and column of interest and then update from that instead of locking the table for indefinite amount of time. This operation adds complexity in automating deployment then.\n\nAnd this should be done with version control in place.\n\nWhat do you do to not have to go through all this manual work at every deployment in other envs? How do you do work your way around it?\n\nThanks", "author_fullname": "t2_d0ifg2cb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you version your on prem SQL data pipelines? And how do you deploy in different envs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102e4rl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1672767820.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672767551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you do it? What tools do you use?&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say you have an incremental load pipeline, you alter the staging and the presentation layer tables by adding a column. You also Alter the stored procedures involved and import new column data (by full load since SPs take only last days/months). Since data is several millions row MAYBE it is better to have a temp table taking only id&amp;#39;s and column of interest and then update from that instead of locking the table for indefinite amount of time. This operation adds complexity in automating deployment then.&lt;/p&gt;\n\n&lt;p&gt;And this should be done with version control in place.&lt;/p&gt;\n\n&lt;p&gt;What do you do to not have to go through all this manual work at every deployment in other envs? How do you do work your way around it?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "102e4rl", "is_robot_indexable": true, "report_reasons": null, "author": "Awkward-Cupcake6219", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102e4rl/how_do_you_version_your_on_prem_sql_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102e4rl/how_do_you_version_your_on_prem_sql_data/", "subreddit_subscribers": 85015, "created_utc": 1672767551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can anybody explain what does author mean by below statement  \n\n\n&gt; if possible, log the data you need from JOINs in the high cardinality data  \nThis eliminates the need for JOINs entirely and can make your pipelines much simpler and cheaper\n\nFor context here is the full post by author: [https://www.linkedin.com/posts/eczachly\\_dataengineering-activity-7015910484986007552-uByq?utm\\_source=share&amp;utm\\_medium=member\\_desktop](https://www.linkedin.com/posts/eczachly_dataengineering-activity-7015910484986007552-uByq?utm_source=share&amp;utm_medium=member_desktop)", "author_fullname": "t2_cu6opso3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Joins in hyperscale data processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1020z7d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1672729130.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anybody explain what does author mean by below statement  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;if possible, log the data you need from JOINs in the high cardinality data&lt;br/&gt;\nThis eliminates the need for JOINs entirely and can make your pipelines much simpler and cheaper&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;For context here is the full post by author: &lt;a href=\"https://www.linkedin.com/posts/eczachly_dataengineering-activity-7015910484986007552-uByq?utm_source=share&amp;amp;utm_medium=member_desktop\"&gt;https://www.linkedin.com/posts/eczachly_dataengineering-activity-7015910484986007552-uByq?utm_source=share&amp;amp;utm_medium=member_desktop&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?auto=webp&amp;s=18689041ed1ad1829e19b3f16bdf0619c3add354", "width": 1400, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd3104eb2954de61b63b4aded5889d7f481667e4", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c971a4bc289c8f96921326f627cebe12f3ac1660", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4aa8c0a35a98b8aba18c141f215320d65e017724", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b4185fb0b2f226c0a0e3e33b8be8f2575bf9ab5", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f9aebe95893f1568f8433661797ed06581b8b5b", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4632741c44875873f4766540a85113c0e703b906", "width": 1080, "height": 617}], "variants": {}, "id": "CjbMbFq2MEqSKWpNCjv-ipCLADmRBQ1ZCG3w2yy71f0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1020z7d", "is_robot_indexable": true, "report_reasons": null, "author": "bha159", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1020z7d/joins_in_hyperscale_data_processing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1020z7d/joins_in_hyperscale_data_processing/", "subreddit_subscribers": 85015, "created_utc": 1672729130.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm here at work writing a fair bit of python code to deal with json data and send info out to slack for stuff happening in SFTP. As I've been going through this I've realized I'm utilizing SQL, Python, JSON, HTTP, cyber security, data visualization, cloud, and devops. I was thinking I am so grateful data engineering has let me learn so much and I truly feel like I can wear so many hats. This may also be because I am on a small team at this company, but I get to be a DBA, machine learning engineer, backend engineer, cyber security engineer, and devops engineer work all under one title.\n\n&amp;#x200B;\n\nAnyone else feel the same? Do you like it? Hate it?", "author_fullname": "t2_b41hohwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you feel that data engineering is one of the largest encompassing fields?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_102gyzk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1672774883.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672774141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m here at work writing a fair bit of python code to deal with json data and send info out to slack for stuff happening in SFTP. As I&amp;#39;ve been going through this I&amp;#39;ve realized I&amp;#39;m utilizing SQL, Python, JSON, HTTP, cyber security, data visualization, cloud, and devops. I was thinking I am so grateful data engineering has let me learn so much and I truly feel like I can wear so many hats. This may also be because I am on a small team at this company, but I get to be a DBA, machine learning engineer, backend engineer, cyber security engineer, and devops engineer work all under one title.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anyone else feel the same? Do you like it? Hate it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "102gyzk", "is_robot_indexable": true, "report_reasons": null, "author": "Gutscazerk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/102gyzk/do_you_feel_that_data_engineering_is_one_of_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102gyzk/do_you_feel_that_data_engineering_is_one_of_the/", "subreddit_subscribers": 85015, "created_utc": 1672774141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my experience, our team had restricted the EMR spin up permissions to airflow-role only. Any DE will have to use an airflow dag to spin up an EMR cluster. And the dag will use an EMR operator to spin up and terminate the cluster in successive tasks. This is to ensure EMR is terminated right after the spark job is completed. \n\nOfcourse we can use cloudwatch to set alerts and what not. \n\nAre there any similar interesting strategies used by your teams to cut down cloud costs? I'm curious.", "author_fullname": "t2_msviuy1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some best practices in data engineering to cut cloud costs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_102gj8q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672773090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my experience, our team had restricted the EMR spin up permissions to airflow-role only. Any DE will have to use an airflow dag to spin up an EMR cluster. And the dag will use an EMR operator to spin up and terminate the cluster in successive tasks. This is to ensure EMR is terminated right after the spark job is completed. &lt;/p&gt;\n\n&lt;p&gt;Ofcourse we can use cloudwatch to set alerts and what not. &lt;/p&gt;\n\n&lt;p&gt;Are there any similar interesting strategies used by your teams to cut down cloud costs? I&amp;#39;m curious.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "102gj8q", "is_robot_indexable": true, "report_reasons": null, "author": "vino_and_data", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102gj8q/what_are_some_best_practices_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102gj8q/what_are_some_best_practices_in_data_engineering/", "subreddit_subscribers": 85015, "created_utc": 1672773090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI am trying to execute Redshift COPY command to copy data from S3 to Redshift table. In order to simulate a failure, dropped the table in Redshift database and executing the Lambda function. However, Lambda function is executing successfully and not sure where the data is getting copied to. The same COPY command execution fails in Redshift database with the non-existent table error. How to handle the same error when executed from Lambda function? Any help is appreciated.\n\nThanks.", "author_fullname": "t2_mspamalq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift COPY command in Lambda.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102a8a8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672757916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am trying to execute Redshift COPY command to copy data from S3 to Redshift table. In order to simulate a failure, dropped the table in Redshift database and executing the Lambda function. However, Lambda function is executing successfully and not sure where the data is getting copied to. The same COPY command execution fails in Redshift database with the non-existent table error. How to handle the same error when executed from Lambda function? Any help is appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "102a8a8", "is_robot_indexable": true, "report_reasons": null, "author": "Awsmason", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102a8a8/redshift_copy_command_in_lambda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102a8a8/redshift_copy_command_in_lambda/", "subreddit_subscribers": 85015, "created_utc": 1672757916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm confident in using Python and running SQL queries after university, but when it comes to tools and technologies, how long would it roughly take to be decent enough for a graduate/junior role? \n\nI've learnt a bit about pandas in university with my CS degree, and I'd like to learn about PySpark, Docker, Snowflake, and possibly a few others. I've seen some videos about how some places are happy to teach things but my \"Tools\" section for my CV is very empty, TBH. \n\nI'm going to bang out the applications for data engineering roles and was thinking of just picking up tools along the way and updating my CV. I've started reading \"[Fundamentals of Data Engineering](https://www.amazon.co.uk/Fundamentals-Data-Engineering-Robust-Systems/dp/1098108302)\" and am enjoying it a lot, so I think I'll gain a decent bit of knowledge (for DE) from there as well.", "author_fullname": "t2_v49pkl1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tools and Technologies for CV", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10267l9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672746864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m confident in using Python and running SQL queries after university, but when it comes to tools and technologies, how long would it roughly take to be decent enough for a graduate/junior role? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve learnt a bit about pandas in university with my CS degree, and I&amp;#39;d like to learn about PySpark, Docker, Snowflake, and possibly a few others. I&amp;#39;ve seen some videos about how some places are happy to teach things but my &amp;quot;Tools&amp;quot; section for my CV is very empty, TBH. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to bang out the applications for data engineering roles and was thinking of just picking up tools along the way and updating my CV. I&amp;#39;ve started reading &amp;quot;&lt;a href=\"https://www.amazon.co.uk/Fundamentals-Data-Engineering-Robust-Systems/dp/1098108302\"&gt;Fundamentals of Data Engineering&lt;/a&gt;&amp;quot; and am enjoying it a lot, so I think I&amp;#39;ll gain a decent bit of knowledge (for DE) from there as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CS Graduate", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10267l9", "is_robot_indexable": true, "report_reasons": null, "author": "Mapleess", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10267l9/tools_and_technologies_for_cv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10267l9/tools_and_technologies_for_cv/", "subreddit_subscribers": 85015, "created_utc": 1672746864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are building a data warehouse for our SaaS product data.\n\nThere are 2 methods to ingest data:\n\n1. Ingest Kafka events\n2. CDC from the relational database (when not possible to ingest events)\n\nIn both methods, we ingest data that is used for product purposes. The development teams are focused on product features and not on data that gets into the data warehouse so they might change the data we're ingesting in ways that will break the data warehouse.\n\nI would like to prevent developers from deploying breaking changes.\n\nIdeally, I would like to have tests that break if a developer changed an events schema in an incompatible way or ran a migration that changed a table which we're CDC'ing.\n\nWhat tools and processes can be used to do that?", "author_fullname": "t2_20muts76", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you prevent up stream developers from breaking the data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_102hhn8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672775384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are building a data warehouse for our SaaS product data.&lt;/p&gt;\n\n&lt;p&gt;There are 2 methods to ingest data:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ingest Kafka events&lt;/li&gt;\n&lt;li&gt;CDC from the relational database (when not possible to ingest events)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;In both methods, we ingest data that is used for product purposes. The development teams are focused on product features and not on data that gets into the data warehouse so they might change the data we&amp;#39;re ingesting in ways that will break the data warehouse.&lt;/p&gt;\n\n&lt;p&gt;I would like to prevent developers from deploying breaking changes.&lt;/p&gt;\n\n&lt;p&gt;Ideally, I would like to have tests that break if a developer changed an events schema in an incompatible way or ran a migration that changed a table which we&amp;#39;re CDC&amp;#39;ing.&lt;/p&gt;\n\n&lt;p&gt;What tools and processes can be used to do that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "102hhn8", "is_robot_indexable": true, "report_reasons": null, "author": "daramasala", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102hhn8/how_do_you_prevent_up_stream_developers_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102hhn8/how_do_you_prevent_up_stream_developers_from/", "subreddit_subscribers": 85015, "created_utc": 1672775384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone. I hope you are well, and happy new year. \n\nI am currently working on a project which makes use of BigQuery resources. I would like to have a much more in depth understanding of the cost of my project relating to BigQuery. GCP offers the billing UI which indicates the final monthly cost, however I am looking to understand which service accounts and data flows are costing the most amount of money to the BQ project. For this, I require an understanding of each of the cost associated with each flow. \n\nI have played around with INFORMATION\\_SCHEMA.JOBS table and converted the billed bytes to dollars, using the 5 dollars per TB formula. As well as worked with Audit logs, however I am falling into some issues. Particularly discrepancies between the expected costs from billed bytes and the actual costs at the end of the month. On average the expected cost from bytes billed tend to overestimate the cost by roughly 10-15%. \n\nIf anyone may know why this discrepancy exists or have any advice for how to approach this. I would be very appreciative. Thank you very much.", "author_fullname": "t2_7s8l0yk1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BigQuery Pricing using Information Schema.JOBS table.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1029ykt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672757175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I hope you are well, and happy new year. &lt;/p&gt;\n\n&lt;p&gt;I am currently working on a project which makes use of BigQuery resources. I would like to have a much more in depth understanding of the cost of my project relating to BigQuery. GCP offers the billing UI which indicates the final monthly cost, however I am looking to understand which service accounts and data flows are costing the most amount of money to the BQ project. For this, I require an understanding of each of the cost associated with each flow. &lt;/p&gt;\n\n&lt;p&gt;I have played around with INFORMATION_SCHEMA.JOBS table and converted the billed bytes to dollars, using the 5 dollars per TB formula. As well as worked with Audit logs, however I am falling into some issues. Particularly discrepancies between the expected costs from billed bytes and the actual costs at the end of the month. On average the expected cost from bytes billed tend to overestimate the cost by roughly 10-15%. &lt;/p&gt;\n\n&lt;p&gt;If anyone may know why this discrepancy exists or have any advice for how to approach this. I would be very appreciative. Thank you very much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1029ykt", "is_robot_indexable": true, "report_reasons": null, "author": "manfromthetaleb", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1029ykt/bigquery_pricing_using_information_schemajobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1029ykt/bigquery_pricing_using_information_schemajobs/", "subreddit_subscribers": 85015, "created_utc": 1672757175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi.\nWe are a streaming service and for every user action keep a log on our mongodb database.\nThese log include a few types of documents.\nLike :\nStream for how a user streamed a content.\nClick for where did they click.\nView what pages did they explore or what content was suggested to them.\n( all of them with time stamps) \n\nThese are stored in different collections. \n( we have collections 0.5 million documents insert per day to 5 million per day.) \n\n\nAnd for example Queries were like something like this.\n\nHow many seconds each content was streamed in a time period\nWhat pages were most seen in a time period\nWhat users searched in a time period.\n\n\nNow there are some new queries which I believe that mongodb can't handle.\nLike finding each user path in app until stream.( basically a join on all collections and aggregating based on users) \nStreams of users on a daily, weekly, monthly basis ( which is easy but on scale of 100 million documents gets pretty slow)\n\nSomeone suggested clickhouse but im not familiar with it in any way  so I did some reading and found timescale db \n\nI wanted to know which one would be better suited to our needs or if there is a better solution that I don't know about ( im not a data engineer but I just have to prepare the foundation for our team) \n\nThanks in advance ( and sorry if there are grammar mistakes not my native language)", "author_fullname": "t2_4klksp69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what database should I use? clickhouse or timescale or something else?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_101oy0w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672696474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi.\nWe are a streaming service and for every user action keep a log on our mongodb database.\nThese log include a few types of documents.\nLike :\nStream for how a user streamed a content.\nClick for where did they click.\nView what pages did they explore or what content was suggested to them.\n( all of them with time stamps) &lt;/p&gt;\n\n&lt;p&gt;These are stored in different collections. \n( we have collections 0.5 million documents insert per day to 5 million per day.) &lt;/p&gt;\n\n&lt;p&gt;And for example Queries were like something like this.&lt;/p&gt;\n\n&lt;p&gt;How many seconds each content was streamed in a time period\nWhat pages were most seen in a time period\nWhat users searched in a time period.&lt;/p&gt;\n\n&lt;p&gt;Now there are some new queries which I believe that mongodb can&amp;#39;t handle.\nLike finding each user path in app until stream.( basically a join on all collections and aggregating based on users) \nStreams of users on a daily, weekly, monthly basis ( which is easy but on scale of 100 million documents gets pretty slow)&lt;/p&gt;\n\n&lt;p&gt;Someone suggested clickhouse but im not familiar with it in any way  so I did some reading and found timescale db &lt;/p&gt;\n\n&lt;p&gt;I wanted to know which one would be better suited to our needs or if there is a better solution that I don&amp;#39;t know about ( im not a data engineer but I just have to prepare the foundation for our team) &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance ( and sorry if there are grammar mistakes not my native language)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "101oy0w", "is_robot_indexable": true, "report_reasons": null, "author": "Nhoan", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/101oy0w/what_database_should_i_use_clickhouse_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/101oy0w/what_database_should_i_use_clickhouse_or/", "subreddit_subscribers": 85015, "created_utc": 1672696474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm evaluating options for running a queue of N long-running jobs. They don't have to be finished sequentially, but our compute resources are limited so we can only process K jobs at a time. Every X days, the entire queue will be ran again. Each job will report information at the end. It'd be nice to have good monitoring for failures.\n\nWhat are current best practices for this type of thing? Is airflow still well supported for this type of thing?", "author_fullname": "t2_rs400v2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running recurring a queue of recurring long-running jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_102h9cs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672774817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m evaluating options for running a queue of N long-running jobs. They don&amp;#39;t have to be finished sequentially, but our compute resources are limited so we can only process K jobs at a time. Every X days, the entire queue will be ran again. Each job will report information at the end. It&amp;#39;d be nice to have good monitoring for failures.&lt;/p&gt;\n\n&lt;p&gt;What are current best practices for this type of thing? Is airflow still well supported for this type of thing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "102h9cs", "is_robot_indexable": true, "report_reasons": null, "author": "ship-flipper", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102h9cs/running_recurring_a_queue_of_recurring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102h9cs/running_recurring_a_queue_of_recurring/", "subreddit_subscribers": 85015, "created_utc": 1672774817.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some resources for interview preparation to answer questions such as \"How would you design a data warehouse?\" or \"How would you design a data pipeline?\"   \n\nThanks!!", "author_fullname": "t2_hfgzs7v2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Theoretical Resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102fnvv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672771060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some resources for interview preparation to answer questions such as &amp;quot;How would you design a data warehouse?&amp;quot; or &amp;quot;How would you design a data pipeline?&amp;quot;   &lt;/p&gt;\n\n&lt;p&gt;Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "102fnvv", "is_robot_indexable": true, "report_reasons": null, "author": "AlternativeDish5596", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102fnvv/interview_theoretical_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102fnvv/interview_theoretical_resources/", "subreddit_subscribers": 85015, "created_utc": 1672771060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How are folks using data \"time travel\" (aka temporal tables, history tables, etc) in the real world today? I've used it for backtesting signals in quant finance where we need to understand exactly the way the world looked at a given point-in-time.", "author_fullname": "t2_116kc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ask Reddit: Data Time Travel Use Cases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102fk82", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672770821.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are folks using data &amp;quot;time travel&amp;quot; (aka temporal tables, history tables, etc) in the real world today? I&amp;#39;ve used it for backtesting signals in quant finance where we need to understand exactly the way the world looked at a given point-in-time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "102fk82", "is_robot_indexable": true, "report_reasons": null, "author": "daeisfresh", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102fk82/ask_reddit_data_time_travel_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102fk82/ask_reddit_data_time_travel_use_cases/", "subreddit_subscribers": 85015, "created_utc": 1672770821.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3kxbd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Iceberg promises to change cloud-based data analytics - Adopted by Snowflake, Google and Cloudera, we look at why the Netflix-developed table format is important", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_102f2lc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": "#46d160", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0Srmt6nzjUOv5uMtihwHBDDvYu870YhRp48Xmw2GB1Y.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1672769702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "theregister.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.theregister.com/2023/01/03/apache_iceberg/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vpmO3WQNm-xbpWvJr-VoV4IaHQiUz4InQb_rH-m3it8.jpg?auto=webp&amp;s=ace2899b5f604e147825053fe19dfbcd4dec2ddf", "width": 648, "height": 432}, "resolutions": [{"url": "https://external-preview.redd.it/vpmO3WQNm-xbpWvJr-VoV4IaHQiUz4InQb_rH-m3it8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=59abd3b81132b28ec3bf0f394e8606dc63c28876", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/vpmO3WQNm-xbpWvJr-VoV4IaHQiUz4InQb_rH-m3it8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6dacd5907d5346db8fd240feb50aeebb189f6df0", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/vpmO3WQNm-xbpWvJr-VoV4IaHQiUz4InQb_rH-m3it8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9d26f2ea5846507e572eea33912c240131ca7a2", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/vpmO3WQNm-xbpWvJr-VoV4IaHQiUz4InQb_rH-m3it8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8799097a7441c9a9080e0d8be3ff50d5d482d97", "width": 640, "height": 426}], "variants": {}, "id": "lvFoT8qyKhCVnt3i0mB46e_6DHBh0Tp4GrtMpgbm_UE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "honorary mod | Snowflake", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "102f2lc", "is_robot_indexable": true, "report_reasons": null, "author": "fhoffa", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/102f2lc/apache_iceberg_promises_to_change_cloudbased_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.theregister.com/2023/01/03/apache_iceberg/", "subreddit_subscribers": 85015, "created_utc": 1672769702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're moving away from snowflake and moving to Azure (currently on AWS). Both of these decisions are already made so we can't simply move SF -&gt; Azure. \n\nRight now, internal reporting is very basic and this would be a near clean slate start. Our goal is to make analysts fairly independent on a given platform for exploratory data analysis, BI, dashboarding, etc. There is no rush to make them autonomous and they will have a dedicated engineer for the foreseeable future.\n\nTo that end though, they're proficient in SQL and are familiar with dbt, which is great. We could provide python support to them in the form of hiring someone or providing some engineering time.\n\nSo, for initial datawarehouse + basic analytics set up would you do a Databricks on Azure set up? Or a vanilla Azure Synapse + dbt set up?\n\nThe former seems to support immediate use cases and has a long runway for capabilities that might be down the line (BI -&gt; DS -&gt; ML, etc) and seems like it's an all in one solution. Though, it's Spark based and the team is not familiar with it.\n\nThe latter is a plain DWH set up and we could be fairly flexible with other pieces of it the analytics stack. As I understand it, this requires data to be stored in more proprietary ways though. \n\nTbh, I'd like to just set up a Postgresql instance in Azure and use that with dbt since our other prod DBs are also postgresql. I came from an AWS shop that used redshift + dbt. Also, it'd be nice to aggregate expertise in 1 technology. But I hear postgresql doesn't make for a great DWH?\n\nBarely beginning the project so would appreciate any tips.", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would you consider databricks, or vanilla synapse with these conditions in place?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102acms", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672758212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re moving away from snowflake and moving to Azure (currently on AWS). Both of these decisions are already made so we can&amp;#39;t simply move SF -&amp;gt; Azure. &lt;/p&gt;\n\n&lt;p&gt;Right now, internal reporting is very basic and this would be a near clean slate start. Our goal is to make analysts fairly independent on a given platform for exploratory data analysis, BI, dashboarding, etc. There is no rush to make them autonomous and they will have a dedicated engineer for the foreseeable future.&lt;/p&gt;\n\n&lt;p&gt;To that end though, they&amp;#39;re proficient in SQL and are familiar with dbt, which is great. We could provide python support to them in the form of hiring someone or providing some engineering time.&lt;/p&gt;\n\n&lt;p&gt;So, for initial datawarehouse + basic analytics set up would you do a Databricks on Azure set up? Or a vanilla Azure Synapse + dbt set up?&lt;/p&gt;\n\n&lt;p&gt;The former seems to support immediate use cases and has a long runway for capabilities that might be down the line (BI -&amp;gt; DS -&amp;gt; ML, etc) and seems like it&amp;#39;s an all in one solution. Though, it&amp;#39;s Spark based and the team is not familiar with it.&lt;/p&gt;\n\n&lt;p&gt;The latter is a plain DWH set up and we could be fairly flexible with other pieces of it the analytics stack. As I understand it, this requires data to be stored in more proprietary ways though. &lt;/p&gt;\n\n&lt;p&gt;Tbh, I&amp;#39;d like to just set up a Postgresql instance in Azure and use that with dbt since our other prod DBs are also postgresql. I came from an AWS shop that used redshift + dbt. Also, it&amp;#39;d be nice to aggregate expertise in 1 technology. But I hear postgresql doesn&amp;#39;t make for a great DWH?&lt;/p&gt;\n\n&lt;p&gt;Barely beginning the project so would appreciate any tips.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "102acms", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102acms/would_you_consider_databricks_or_vanilla_synapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102acms/would_you_consider_databricks_or_vanilla_synapse/", "subreddit_subscribers": 85015, "created_utc": 1672758212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!\n\nI'm still new to AWS/GCP technologies and I would like your input on how to solve this.\n\nI have an Aurora MySQL DB on AWS and I am currently importing the data to BigQuery (for analytics) using python scripts. I was reading I could use [Data Transfer](https://cloud.google.com/storage-transfer-service) instead. Data Transfer gets the data from S3 to GCP environment and then to BigQuery, but I'm stuck on how to bring the data from MySQL to S3.\n\nWhat is the best way to import MySQL data to S3?", "author_fullname": "t2_tcrhr5h0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Continuously import Aurora MySQL data into BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1027zxu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1672751850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m still new to AWS/GCP technologies and I would like your input on how to solve this.&lt;/p&gt;\n\n&lt;p&gt;I have an Aurora MySQL DB on AWS and I am currently importing the data to BigQuery (for analytics) using python scripts. I was reading I could use &lt;a href=\"https://cloud.google.com/storage-transfer-service\"&gt;Data Transfer&lt;/a&gt; instead. Data Transfer gets the data from S3 to GCP environment and then to BigQuery, but I&amp;#39;m stuck on how to bring the data from MySQL to S3.&lt;/p&gt;\n\n&lt;p&gt;What is the best way to import MySQL data to S3?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?auto=webp&amp;s=ada93f0d146c09556ac26cc3fa125a0aa7102150", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=05af26b5ec35c95ed95b5c40dbde3c1cc04dce06", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f36f0de65cdcbed3b705b8446710c7c83e0475e4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4067aebaadaec227b271d9c19c7af833c230fd32", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec46bff13e5c5bc616be11b375484d9d2a7fcbe8", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5db472c15d5717384ab8f8f64e9fd89efe70fa59", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fab49c1958487e16d598ede6d81140177c5c9a31", "width": 1080, "height": 567}], "variants": {}, "id": "DsiOIzUSicS_9zIKwMDQbNT2LOE1o29sSYs49HAmO_k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1027zxu", "is_robot_indexable": true, "report_reasons": null, "author": "Standard_Cloud_4288", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1027zxu/continuously_import_aurora_mysql_data_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1027zxu/continuously_import_aurora_mysql_data_into/", "subreddit_subscribers": 85015, "created_utc": 1672751850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So we're adding a super simple catalogue module that will be used for e-commerce.\n\nWe have a google spreadsheet of the items prepared for us by someone else in the team, and the values there resemble the following:\n\n|id|item_name|category|sub_category|brand|vendor|\n|:--|:--|:--:|:--:|:--:|:--:|\n|1|Example Shoes A|Apparel|Footwear|Nike|Vendor A|\n|2|Example Shoes B|Apparel|Footwear|Adidas|Vendor A|\n|3|Nice Football|Sports|Soccer|Puma|Vendor B|\n\nWe have a google spreadsheet with about ~400 rows having the above structure.\n\nWhat I'm trying to figure out, and this is perhaps trying to over-optimize, is **how these categories should be represented as models in a relational database**.\n\nThe columns of particular interest are:\n\n1. `category`\n2. `sub_category`\n3. `brand`\n4. `vendor`\n\nOur options are:\n\n- a) Store them as *foreign keys* that reference another table, \n- b) Store them as *enumerated values* (e.g. 0: Nike, 1: Puma)\n- c) Store them as *varchar text fields* in the table (e.g. \"Nike\", \"Puma\")\n\nFor `vendor`, we're pretty sure we want it to be a `vendor_id` column that references a `vendors` table, since we will eventually want a portal for someone to add more vendors to our e-commerce catalogue. \n\nBut we're not sure between between options b and c for the other columns. \n\n___\n## Our concerns\n\n- Is there ever any reason storing in plain text might be better than enumerated values? With varchar, it seems like it would make it easier to just run a rake task to read in new values whenever the sheet is updated, without needing to add more numeric keys. We don't expect this table to contain more than 2k rows, so the space-saving aspect of enums is quite miniscule.\n- Our front-end will have a `category` and `sub_category` dropdown filter, which will require the BE to pass to it the unique values available in those columns. Would this make the \"varchar\" column type less performant (since with enum, acquiring the distinct values of categories would be a lot quicker).\n    - Alternatively, would it be better practice to just have the categories and sub_category values hard-coded into the frontend so it's not making an extra API call to the backend?", "author_fullname": "t2_k01zajj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to create tables to store categorical values - when to use enumerated values VS static tables VS text columns.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1027svq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672751329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we&amp;#39;re adding a super simple catalogue module that will be used for e-commerce.&lt;/p&gt;\n\n&lt;p&gt;We have a google spreadsheet of the items prepared for us by someone else in the team, and the values there resemble the following:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;id&lt;/th&gt;\n&lt;th align=\"left\"&gt;item_name&lt;/th&gt;\n&lt;th align=\"center\"&gt;category&lt;/th&gt;\n&lt;th align=\"center\"&gt;sub_category&lt;/th&gt;\n&lt;th align=\"center\"&gt;brand&lt;/th&gt;\n&lt;th align=\"center\"&gt;vendor&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;Example Shoes A&lt;/td&gt;\n&lt;td align=\"center\"&gt;Apparel&lt;/td&gt;\n&lt;td align=\"center\"&gt;Footwear&lt;/td&gt;\n&lt;td align=\"center\"&gt;Nike&lt;/td&gt;\n&lt;td align=\"center\"&gt;Vendor A&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;Example Shoes B&lt;/td&gt;\n&lt;td align=\"center\"&gt;Apparel&lt;/td&gt;\n&lt;td align=\"center\"&gt;Footwear&lt;/td&gt;\n&lt;td align=\"center\"&gt;Adidas&lt;/td&gt;\n&lt;td align=\"center\"&gt;Vendor A&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nice Football&lt;/td&gt;\n&lt;td align=\"center\"&gt;Sports&lt;/td&gt;\n&lt;td align=\"center\"&gt;Soccer&lt;/td&gt;\n&lt;td align=\"center\"&gt;Puma&lt;/td&gt;\n&lt;td align=\"center\"&gt;Vendor B&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;We have a google spreadsheet with about ~400 rows having the above structure.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m trying to figure out, and this is perhaps trying to over-optimize, is &lt;strong&gt;how these categories should be represented as models in a relational database&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;The columns of particular interest are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;code&gt;category&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;sub_category&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;brand&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;vendor&lt;/code&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Our options are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;a) Store them as &lt;em&gt;foreign keys&lt;/em&gt; that reference another table, &lt;/li&gt;\n&lt;li&gt;b) Store them as &lt;em&gt;enumerated values&lt;/em&gt; (e.g. 0: Nike, 1: Puma)&lt;/li&gt;\n&lt;li&gt;c) Store them as &lt;em&gt;varchar text fields&lt;/em&gt; in the table (e.g. &amp;quot;Nike&amp;quot;, &amp;quot;Puma&amp;quot;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For &lt;code&gt;vendor&lt;/code&gt;, we&amp;#39;re pretty sure we want it to be a &lt;code&gt;vendor_id&lt;/code&gt; column that references a &lt;code&gt;vendors&lt;/code&gt; table, since we will eventually want a portal for someone to add more vendors to our e-commerce catalogue. &lt;/p&gt;\n\n&lt;p&gt;But we&amp;#39;re not sure between between options b and c for the other columns. &lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;Our concerns&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is there ever any reason storing in plain text might be better than enumerated values? With varchar, it seems like it would make it easier to just run a rake task to read in new values whenever the sheet is updated, without needing to add more numeric keys. We don&amp;#39;t expect this table to contain more than 2k rows, so the space-saving aspect of enums is quite miniscule.&lt;/li&gt;\n&lt;li&gt;Our front-end will have a &lt;code&gt;category&lt;/code&gt; and &lt;code&gt;sub_category&lt;/code&gt; dropdown filter, which will require the BE to pass to it the unique values available in those columns. Would this make the &amp;quot;varchar&amp;quot; column type less performant (since with enum, acquiring the distinct values of categories would be a lot quicker).\n\n&lt;ul&gt;\n&lt;li&gt;Alternatively, would it be better practice to just have the categories and sub_category values hard-coded into the frontend so it&amp;#39;s not making an extra API call to the backend?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1027svq", "is_robot_indexable": true, "report_reasons": null, "author": "Lostwhispers05", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1027svq/need_to_create_tables_to_store_categorical_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1027svq/need_to_create_tables_to_store_categorical_values/", "subreddit_subscribers": 85015, "created_utc": 1672751329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The Sprkl Operator is an improvement of the OpenTelemetry Operator. The OpenTelemetry Operator applies automatic library and infrastructure-level instrumentation. The Sprkl Operator provides the same functionality and integrates Sprkl instrumentation with it.  Join our beta - tell us your thoughts: [https://sprkl.dev/](https://sprkl.dev/)", "author_fullname": "t2_hissmiq8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My personal project: Automated logging for code changes - Cluster-level distributed tracing with personalization. If you guys can tell me your thoughts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1023gha", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1672737751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Sprkl Operator is an improvement of the OpenTelemetry Operator. The OpenTelemetry Operator applies automatic library and infrastructure-level instrumentation. The Sprkl Operator provides the same functionality and integrates Sprkl instrumentation with it.  Join our beta - tell us your thoughts: &lt;a href=\"https://sprkl.dev/\"&gt;https://sprkl.dev/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/na0QakHGPYr7wGgR8kcWq_QMraWiC63_AXJZawfqR5s.jpg?auto=webp&amp;s=b84bb8860ba325d0d852055684965c6f8a0be0da", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/na0QakHGPYr7wGgR8kcWq_QMraWiC63_AXJZawfqR5s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd07c567eb40445c7b0dae7c32ac07bf2d687714", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/na0QakHGPYr7wGgR8kcWq_QMraWiC63_AXJZawfqR5s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=182cd7a04da01fbb2330899563d80e79a7ce287f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/na0QakHGPYr7wGgR8kcWq_QMraWiC63_AXJZawfqR5s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f4926ffdd9f31d62bc7e43c02ff1dd825780eb87", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/na0QakHGPYr7wGgR8kcWq_QMraWiC63_AXJZawfqR5s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac8b3338519dfaf6d2689f64f5eddcc6e51f71f2", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/na0QakHGPYr7wGgR8kcWq_QMraWiC63_AXJZawfqR5s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f9fd9809ad0183ea78eddcacb4c6f7764bf389f5", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/na0QakHGPYr7wGgR8kcWq_QMraWiC63_AXJZawfqR5s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=60d59d289a2c4c345bf05ac448b26b0c0fe4bf8a", "width": 1080, "height": 567}], "variants": {}, "id": "JoWS26qq9r4QM5zkrrMH3d-Tmv56zgruB220rWYnO5g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1023gha", "is_robot_indexable": true, "report_reasons": null, "author": "Nice_Score_7552", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1023gha/my_personal_project_automated_logging_for_code/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1023gha/my_personal_project_automated_logging_for_code/", "subreddit_subscribers": 85015, "created_utc": 1672737751.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}