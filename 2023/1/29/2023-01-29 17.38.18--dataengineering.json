{"kind": "Listing", "data": {"after": null, "dist": 9, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to find ways to optimize pipelines. We currently have 10-12 large datasets that are used on a front end application and I\u2019m looking for ways to refactor the code base and improve performance. Is there any common design patterns you are aware of?\n\nSome of mine are very basic because I inherited a very poorly maintained project \n\n- MapReduce pattern \n- use reduce function call for multiple dataframes \n- repartitioning, coalescing \n- reducing rate of data updates (mine is biweekly at 5am before big reporting meeting for the front end users)\n- removing any for loops you can find\n- removing any array type columns to save on memory\n- Incremental updates to only update fresh data \n- Pruning old branches, deleting any test datasets \n- use as much AWS codeless architecture \n- Putting in a CI/CD pipeline via GitHub Actions Jenkins \n- Updating documentation on confluence", "author_fullname": "t2_nutp89h4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the most common patterns or efficiency enablers you encounter in your job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10o3ytb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674985144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to find ways to optimize pipelines. We currently have 10-12 large datasets that are used on a front end application and I\u2019m looking for ways to refactor the code base and improve performance. Is there any common design patterns you are aware of?&lt;/p&gt;\n\n&lt;p&gt;Some of mine are very basic because I inherited a very poorly maintained project &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;MapReduce pattern &lt;/li&gt;\n&lt;li&gt;use reduce function call for multiple dataframes &lt;/li&gt;\n&lt;li&gt;repartitioning, coalescing &lt;/li&gt;\n&lt;li&gt;reducing rate of data updates (mine is biweekly at 5am before big reporting meeting for the front end users)&lt;/li&gt;\n&lt;li&gt;removing any for loops you can find&lt;/li&gt;\n&lt;li&gt;removing any array type columns to save on memory&lt;/li&gt;\n&lt;li&gt;Incremental updates to only update fresh data &lt;/li&gt;\n&lt;li&gt;Pruning old branches, deleting any test datasets &lt;/li&gt;\n&lt;li&gt;use as much AWS codeless architecture &lt;/li&gt;\n&lt;li&gt;Putting in a CI/CD pipeline via GitHub Actions Jenkins &lt;/li&gt;\n&lt;li&gt;Updating documentation on confluence&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10o3ytb", "is_robot_indexable": true, "report_reasons": null, "author": "hositir", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10o3ytb/what_are_the_most_common_patterns_or_efficiency/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10o3ytb/what_are_the_most_common_patterns_or_efficiency/", "subreddit_subscribers": 87838, "created_utc": 1674985144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wonder if knowing only Spark will help me get a job, or I need to be an expert on both?", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does it make sense to learn Apache Spark without Apache Hadoop (for DE jobs/career)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10nkjen", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674927181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wonder if knowing only Spark will help me get a job, or I need to be an expert on both?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10nkjen", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10nkjen/does_it_make_sense_to_learn_apache_spark_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10nkjen/does_it_make_sense_to_learn_apache_spark_without/", "subreddit_subscribers": 87838, "created_utc": 1674927181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im ingesting a data with the api calls and would like to use `widgets` to parameterize. In azure I have the following set up:\n\nhttps://preview.redd.it/v4qkge2tfuea1.png?width=945&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=18eb6675515ca19be2711da514657d124d39558b\n\nI have the list of `attribute_codes`, reading them with `lookup` activtiy and passing these parameters inside the databricks notebook code. Code inside the databricks:\n\n&amp;#x200B;\n\n    data, response = get_data_url(url=f\"https://p.cloud.com/api/rest/v1/attributes/{attribute_code}/options\",access_token=access_token)\n    #Removing the folder in Data Lake\n    dbutils.fs.rm(f'/mnt/bronze/attribute_code/{day}',True)\n    #Creating the folder in the Data Lake\n    dbutils.fs.mkdirs(f'/mnt/bronze/attribute_code/{day}')\n    \n    count = 0\n    #Putting the response inside of the Data Lake folder\n    dbutils.fs.put(f'/mnt/bronze/attribute_code/{day}/data_{count}.json', response.text)\n\nMy problem is that, since its in the `ForEach` loop, eveytime new parameter is passed, it deletes the entire folder with previosly, loaded data. Now someone can come and say to remove line where I drop and create the spacific daily folder but pipeline should run multiple times a day and I need to drop previously loaded data on that day and load new one.  \n\n&amp;#x200B;\n\nMy **goal** is to iterte over the entire list of the `attribute_code` and load them all in one folder with the name \"data\\_{count}.json.", "author_fullname": "t2_56g5f4cg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory - passing parameters to Azure Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 60, "top_awarded_type": null, "hide_score": false, "media_metadata": {"v4qkge2tfuea1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 46, "x": 108, "u": "https://preview.redd.it/v4qkge2tfuea1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=feed411f6f3af64de9ef542436581ca3243ec0b5"}, {"y": 93, "x": 216, "u": "https://preview.redd.it/v4qkge2tfuea1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e8d9741c2ad635e64258a6f0c5ba52fa6233df1"}, {"y": 137, "x": 320, "u": "https://preview.redd.it/v4qkge2tfuea1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b737a6fccdf6ff3466360b7696344ed791f7c3a9"}, {"y": 275, "x": 640, "u": "https://preview.redd.it/v4qkge2tfuea1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06dc271c4c3a9feec4ba77957b900a953ff0974b"}], "s": {"y": 407, "x": 945, "u": "https://preview.redd.it/v4qkge2tfuea1.png?width=945&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=18eb6675515ca19be2711da514657d124d39558b"}, "id": "v4qkge2tfuea1"}}, "name": "t3_10noo11", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jiu-iAYlkd_BZScqa-DiPcOOGgdLLLb7QSaOG_wxqBI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674937715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im ingesting a data with the api calls and would like to use &lt;code&gt;widgets&lt;/code&gt; to parameterize. In azure I have the following set up:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v4qkge2tfuea1.png?width=945&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=18eb6675515ca19be2711da514657d124d39558b\"&gt;https://preview.redd.it/v4qkge2tfuea1.png?width=945&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=18eb6675515ca19be2711da514657d124d39558b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I have the list of &lt;code&gt;attribute_codes&lt;/code&gt;, reading them with &lt;code&gt;lookup&lt;/code&gt; activtiy and passing these parameters inside the databricks notebook code. Code inside the databricks:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;data, response = get_data_url(url=f&amp;quot;https://p.cloud.com/api/rest/v1/attributes/{attribute_code}/options&amp;quot;,access_token=access_token)\n#Removing the folder in Data Lake\ndbutils.fs.rm(f&amp;#39;/mnt/bronze/attribute_code/{day}&amp;#39;,True)\n#Creating the folder in the Data Lake\ndbutils.fs.mkdirs(f&amp;#39;/mnt/bronze/attribute_code/{day}&amp;#39;)\n\ncount = 0\n#Putting the response inside of the Data Lake folder\ndbutils.fs.put(f&amp;#39;/mnt/bronze/attribute_code/{day}/data_{count}.json&amp;#39;, response.text)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;My problem is that, since its in the &lt;code&gt;ForEach&lt;/code&gt; loop, eveytime new parameter is passed, it deletes the entire folder with previosly, loaded data. Now someone can come and say to remove line where I drop and create the spacific daily folder but pipeline should run multiple times a day and I need to drop previously loaded data on that day and load new one.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My &lt;strong&gt;goal&lt;/strong&gt; is to iterte over the entire list of the &lt;code&gt;attribute_code&lt;/code&gt; and load them all in one folder with the name &amp;quot;data_{count}.json.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10noo11", "is_robot_indexable": true, "report_reasons": null, "author": "9gg6", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10noo11/azure_data_factory_passing_parameters_to_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10noo11/azure_data_factory_passing_parameters_to_azure/", "subreddit_subscribers": 87838, "created_utc": 1674937715.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, has anyone enroled in this course by Udacity? I would like some opinions.\nI think the syllabus is quite interesting and I like that It focuses on AWS, but I don't know what quality of learning material it has.\nWould you recommend Datacamp or Coursera over this one?\n\nhttps://www.udacity.com/course/data-engineer-nanodegree--nd027", "author_fullname": "t2_69uxhxiu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Aws Data Engineering Nanodegree", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10o5gei", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.56, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QyegggTQqfRdRoKyLvd0ruuUBGx7viZ3BqJw9MsuM6s.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674991057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, has anyone enroled in this course by Udacity? I would like some opinions.\nI think the syllabus is quite interesting and I like that It focuses on AWS, but I don&amp;#39;t know what quality of learning material it has.\nWould you recommend Datacamp or Coursera over this one?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.udacity.com/course/data-engineer-nanodegree--nd027\"&gt;https://www.udacity.com/course/data-engineer-nanodegree--nd027&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/2atd4audc0fa1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/2atd4audc0fa1.jpg?auto=webp&amp;v=enabled&amp;s=15e0831fe1aa53901ce041365b3bc0dce736670b", "width": 1080, "height": 2340}, "resolutions": [{"url": "https://preview.redd.it/2atd4audc0fa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f2c4e6609efb5f2400150d2d126840ef6ee3cc5", "width": 108, "height": 216}, {"url": "https://preview.redd.it/2atd4audc0fa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e54bc63757a8062954e12672d2422b590bdd7445", "width": 216, "height": 432}, {"url": "https://preview.redd.it/2atd4audc0fa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63c0ea03180e4a7339c5802f7316d3936efd4db1", "width": 320, "height": 640}, {"url": "https://preview.redd.it/2atd4audc0fa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0aff1d5c0e37a5f1496d197f22ea7f104cb615d0", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/2atd4audc0fa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd3b93a8bc9b96085ca883b5649a2d09656df1d1", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/2atd4audc0fa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f16ec28a27db48467239eef4c9ba69c5ef3c7ac", "width": 1080, "height": 2160}], "variants": {}, "id": "Rh33R0DDQS5mAIN3XEqoo-1Hj_6y_dKHDh3sQbQMMJQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10o5gei", "is_robot_indexable": true, "report_reasons": null, "author": "rzgzLuis", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10o5gei/aws_data_engineering_nanodegree/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/2atd4audc0fa1.jpg", "subreddit_subscribers": 87838, "created_utc": 1674991057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I have a question about what's happening behind the scenes when I do a direct write / update in place. Say my ETL will do a batch load weekly to refresh my SCD1 dimension table. Each time, my ETL will compare the new data from the source with the current target table, and if my ETL sees that, for the same natural key, one field has a new value, my ETL will keep the old surrogate key of that row, but overwrite that one field with the new value. It's a SCD1 since my target table only keeps the latest value. But, one drawback of SCD1 is that you can't go back to the previous state if you want to abort the write. Since this seems quite risky, I'm wondering if certain databases could still keep track of history, even if my table seems to be a SCD1 table? \n\n&amp;#x200B;\n\nTo illustrate my descriptions above, here's an example: \n\nMy target table looks like below. Let's say this morning, my weekly ETL notices that for ID 12345, the \"City\" field gets a new value \"Toronto\". It will directly update that first row in place, and replace \"Ottawa\" with \"Toronto\". In this case, would the database still track that history of \"Ottawa\" behind the scenes?  Thanks everyone.\n\n|PK|ID#|Name |City|\n|:-|:-|:-|:-|\n|1|12345|Jenn|Ottawa|\n|2|23456|John|Toronto|", "author_fullname": "t2_szomhuik", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question - Keep History for SCD1?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10nuzdr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674954479.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I have a question about what&amp;#39;s happening behind the scenes when I do a direct write / update in place. Say my ETL will do a batch load weekly to refresh my SCD1 dimension table. Each time, my ETL will compare the new data from the source with the current target table, and if my ETL sees that, for the same natural key, one field has a new value, my ETL will keep the old surrogate key of that row, but overwrite that one field with the new value. It&amp;#39;s a SCD1 since my target table only keeps the latest value. But, one drawback of SCD1 is that you can&amp;#39;t go back to the previous state if you want to abort the write. Since this seems quite risky, I&amp;#39;m wondering if certain databases could still keep track of history, even if my table seems to be a SCD1 table? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;To illustrate my descriptions above, here&amp;#39;s an example: &lt;/p&gt;\n\n&lt;p&gt;My target table looks like below. Let&amp;#39;s say this morning, my weekly ETL notices that for ID 12345, the &amp;quot;City&amp;quot; field gets a new value &amp;quot;Toronto&amp;quot;. It will directly update that first row in place, and replace &amp;quot;Ottawa&amp;quot; with &amp;quot;Toronto&amp;quot;. In this case, would the database still track that history of &amp;quot;Ottawa&amp;quot; behind the scenes?  Thanks everyone.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;PK&lt;/th&gt;\n&lt;th align=\"left\"&gt;ID#&lt;/th&gt;\n&lt;th align=\"left\"&gt;Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;City&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;12345&lt;/td&gt;\n&lt;td align=\"left\"&gt;Jenn&lt;/td&gt;\n&lt;td align=\"left\"&gt;Ottawa&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;23456&lt;/td&gt;\n&lt;td align=\"left\"&gt;John&lt;/td&gt;\n&lt;td align=\"left\"&gt;Toronto&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10nuzdr", "is_robot_indexable": true, "report_reasons": null, "author": "TendMyOwnGarden", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10nuzdr/question_keep_history_for_scd1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10nuzdr/question_keep_history_for_scd1/", "subreddit_subscribers": 87838, "created_utc": 1674954479.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been speaking with my manager and we are thinking of having a data warehouse instance for each customer we have\n\nThey all use the same data except instead of RLS we segregate the data by giving them their own DB\n\nSeems overkill if we can have it all in one place and just use multi tenant approach?\n\nImagine having 100 Redshift instances for each customer! Has anyone done something like that?\n\nThe only benefit I can see is no data leaks between customers that\u2019s basically guaranteed", "author_fullname": "t2_htptc13q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reference architecture for serving many customers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10npejj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674939590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been speaking with my manager and we are thinking of having a data warehouse instance for each customer we have&lt;/p&gt;\n\n&lt;p&gt;They all use the same data except instead of RLS we segregate the data by giving them their own DB&lt;/p&gt;\n\n&lt;p&gt;Seems overkill if we can have it all in one place and just use multi tenant approach?&lt;/p&gt;\n\n&lt;p&gt;Imagine having 100 Redshift instances for each customer! Has anyone done something like that?&lt;/p&gt;\n\n&lt;p&gt;The only benefit I can see is no data leaks between customers that\u2019s basically guaranteed&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10npejj", "is_robot_indexable": true, "report_reasons": null, "author": "Main_Tap_1256", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10npejj/reference_architecture_for_serving_many_customers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10npejj/reference_architecture_for_serving_many_customers/", "subreddit_subscribers": 87838, "created_utc": 1674939590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,   \nin my company we have clear distinction between the reporting team and DWH team. I am looking into how to optimise the flow of requests between the two teams, mainly how should the guys from reporting form their request when requesting for a new data asset to be created on the DWH side. If you have any resources, templates or advices on this topic please share.   \nThanks!", "author_fullname": "t2_11jvjx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flow of requests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10obh87", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675009249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;br/&gt;\nin my company we have clear distinction between the reporting team and DWH team. I am looking into how to optimise the flow of requests between the two teams, mainly how should the guys from reporting form their request when requesting for a new data asset to be created on the DWH side. If you have any resources, templates or advices on this topic please share.&lt;br/&gt;\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10obh87", "is_robot_indexable": true, "report_reasons": null, "author": "isak000", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10obh87/flow_of_requests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10obh87/flow_of_requests/", "subreddit_subscribers": 87838, "created_utc": 1675009249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'll keep it brief. 21M, Spring or Fall 2024 Grad Compsci\n\n* Pursuing DE, longterm DS/ML\n* Comfortable with python and javascript\n* Learning how to use Docker and pipelines, currently taking the 2023 DE [bootcamp](https://www.reddit.com/r/dataengineering/comments/104xlft/free_data_engineering_bootcamp_data_engineering/), struggling but understanding slowly\n* Enjoy coding and learning, very beginner\n* Southern California located\n\nI was accepted into Galvanize's March full-time full-stack bootcamp program. Even after hours of research on Reddit, I'm still unsure about how I should follow my career path. I believe that improving my abilities and developing my personal portfolio/resume will increase my chances of obtaining a good job here. While focusing on my fundamental DE skills, my question is if I should study much fullstack or backend dev. Is this bootcamp even worth it at this point? Given that I won't graduate until next year. It's pricey, and I'm more interested in qualifying myself as soon as possible to enter this job market. I constantly hearing about how the job market is getting worse, and I completely understand. I just don't want to stay this end of the stick (unemployed). Courses in school are very basic and I'm learning way more off campus. I have more than enough time outside of class, as I am remote, and any advice for what I should do with my time going into this year would be GREATLY appreciated.", "author_fullname": "t2_pwk2f3iz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I delay this bootcamp?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10o2ial", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674979443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll keep it brief. 21M, Spring or Fall 2024 Grad Compsci&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pursuing DE, longterm DS/ML&lt;/li&gt;\n&lt;li&gt;Comfortable with python and javascript&lt;/li&gt;\n&lt;li&gt;Learning how to use Docker and pipelines, currently taking the 2023 DE &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/104xlft/free_data_engineering_bootcamp_data_engineering/\"&gt;bootcamp&lt;/a&gt;, struggling but understanding slowly&lt;/li&gt;\n&lt;li&gt;Enjoy coding and learning, very beginner&lt;/li&gt;\n&lt;li&gt;Southern California located&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I was accepted into Galvanize&amp;#39;s March full-time full-stack bootcamp program. Even after hours of research on Reddit, I&amp;#39;m still unsure about how I should follow my career path. I believe that improving my abilities and developing my personal portfolio/resume will increase my chances of obtaining a good job here. While focusing on my fundamental DE skills, my question is if I should study much fullstack or backend dev. Is this bootcamp even worth it at this point? Given that I won&amp;#39;t graduate until next year. It&amp;#39;s pricey, and I&amp;#39;m more interested in qualifying myself as soon as possible to enter this job market. I constantly hearing about how the job market is getting worse, and I completely understand. I just don&amp;#39;t want to stay this end of the stick (unemployed). Courses in school are very basic and I&amp;#39;m learning way more off campus. I have more than enough time outside of class, as I am remote, and any advice for what I should do with my time going into this year would be GREATLY appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10o2ial", "is_robot_indexable": true, "report_reasons": null, "author": "CowUnfair4318", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10o2ial/should_i_delay_this_bootcamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10o2ial/should_i_delay_this_bootcamp/", "subreddit_subscribers": 87838, "created_utc": 1674979443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have the following task at hand:\n\nI need to syncronize a list of products gathered from a SOAP API to a WooCommerce based webshop. I have written a Python script that does this seemingly well, but it is messy as f and I kinda feel that I am reinventing the wheel at certain points (the only external libraries that it uses are *zeep* (for SOAP requests), *lxml* and *woocommerce*).\n\nCurrently my Python script does the following:\n\n* extract all products from the SOAP API and store them as dictionaries\n* go through each of them and add each new category that is found onto a list\n* compare this list of categories to the list from the previous snapshot and sync the changes to woocommerce\n* compare the current list of products to the list from the previous snapshot\n  * if a product is not present in the previous snapshot upload it into woocommerce\n  * if an attribute of a product (stock status, price, etc.) has changed since the last snapshot update it in woocommerce\n  * if a product that was present in the previous snapshot is no longer present in the current list remove it from woocommerce\n* save the snapshot of the current state of woocommerce into a JSON file\n\n\n\nFrom what I was able to gather this task of mine is basically an ETL operation: I need to extract the data from the SOAP API, transform it into the format that is accepted by the woocommerce rest api and load it into woocommerce.\n\nAre there any data engineering libraries or technologies that you guys would recommend me to incorporate into this project. Could Apache Airflow or the Bonobo ETL library simplify this task? Should I invest time into learning those technologies? Right now my plan is to simply run this aforementioned python script once a day using a cronjob.", "author_fullname": "t2_3wultj0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Syncing products from a SOAP API to Woocommerce via it's REST API. Any libraries or technologies that could simplify this task?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10o8dm6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675001112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have the following task at hand:&lt;/p&gt;\n\n&lt;p&gt;I need to syncronize a list of products gathered from a SOAP API to a WooCommerce based webshop. I have written a Python script that does this seemingly well, but it is messy as f and I kinda feel that I am reinventing the wheel at certain points (the only external libraries that it uses are &lt;em&gt;zeep&lt;/em&gt; (for SOAP requests), &lt;em&gt;lxml&lt;/em&gt; and &lt;em&gt;woocommerce&lt;/em&gt;).&lt;/p&gt;\n\n&lt;p&gt;Currently my Python script does the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;extract all products from the SOAP API and store them as dictionaries&lt;/li&gt;\n&lt;li&gt;go through each of them and add each new category that is found onto a list&lt;/li&gt;\n&lt;li&gt;compare this list of categories to the list from the previous snapshot and sync the changes to woocommerce&lt;/li&gt;\n&lt;li&gt;compare the current list of products to the list from the previous snapshot\n\n&lt;ul&gt;\n&lt;li&gt;if a product is not present in the previous snapshot upload it into woocommerce&lt;/li&gt;\n&lt;li&gt;if an attribute of a product (stock status, price, etc.) has changed since the last snapshot update it in woocommerce&lt;/li&gt;\n&lt;li&gt;if a product that was present in the previous snapshot is no longer present in the current list remove it from woocommerce&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;save the snapshot of the current state of woocommerce into a JSON file&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;From what I was able to gather this task of mine is basically an ETL operation: I need to extract the data from the SOAP API, transform it into the format that is accepted by the woocommerce rest api and load it into woocommerce.&lt;/p&gt;\n\n&lt;p&gt;Are there any data engineering libraries or technologies that you guys would recommend me to incorporate into this project. Could Apache Airflow or the Bonobo ETL library simplify this task? Should I invest time into learning those technologies? Right now my plan is to simply run this aforementioned python script once a day using a cronjob.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10o8dm6", "is_robot_indexable": true, "report_reasons": null, "author": "Clock_Wise_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10o8dm6/syncing_products_from_a_soap_api_to_woocommerce/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10o8dm6/syncing_products_from_a_soap_api_to_woocommerce/", "subreddit_subscribers": 87838, "created_utc": 1675001112.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}