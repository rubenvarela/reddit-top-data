{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've received 2 job offers that originally was an easy choice but was made more difficult by a counter offer by company A.\n\nCompany A: $110k Data Analyst mid (will change to DE title), hybrid, large consulting firm who's client is the federal government. Would get a secret clearance. Original job description/title was for a Mid level Data Analyst. Responsibilities include: some ETL work, EDA, presenting, consulting, data visualization, GCP, SQL, R / Python (based on interview it seemed it was R focused which I don't like), opportunity to use Flask and Docker but they said it was something they are planning on using. I rejected their offer after receiving the offer from Company B but they countered with $110k and more data engineering work which I will confirm what that means in a meeting tomorrow.\n\nCompany B: $88k Data Engineer, 5 days in office, large supply chain company. Mature legacy data system but need help with migrating to the cloud as well as maintain current pipelines. Job description includes data modeling, build test deploy and monitor pipelines, validation testing. Tools include SQL, python, pyspark, kafka, databricks, sql server (legacy), Azure Synapse and Azure Data Factory, dbt and airflow, great expectations (used for legacy system)\n\nClearly the tools and opportunity at Company B is far better from a data engineering standpoint which is what I ultimately want to do and is more valuable long term as I grow. But if company A offers more DE work in my meeting tomorrow this will be tricky. I also have the opportunity to renegotiate with Company B for more pay but I have never done that before and really don't want to push it because I really want to be a DE now and get out of my current job ASAP.\n\nThanks!\n\n(Edit): My goal is to take the job that gives me the best experience and leverage for better roles down the line (tech companies etc.)", "author_fullname": "t2_ww0mp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help choosing offer - $110k DA vs $88k DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108jbv8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673392615.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673380915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve received 2 job offers that originally was an easy choice but was made more difficult by a counter offer by company A.&lt;/p&gt;\n\n&lt;p&gt;Company A: $110k Data Analyst mid (will change to DE title), hybrid, large consulting firm who&amp;#39;s client is the federal government. Would get a secret clearance. Original job description/title was for a Mid level Data Analyst. Responsibilities include: some ETL work, EDA, presenting, consulting, data visualization, GCP, SQL, R / Python (based on interview it seemed it was R focused which I don&amp;#39;t like), opportunity to use Flask and Docker but they said it was something they are planning on using. I rejected their offer after receiving the offer from Company B but they countered with $110k and more data engineering work which I will confirm what that means in a meeting tomorrow.&lt;/p&gt;\n\n&lt;p&gt;Company B: $88k Data Engineer, 5 days in office, large supply chain company. Mature legacy data system but need help with migrating to the cloud as well as maintain current pipelines. Job description includes data modeling, build test deploy and monitor pipelines, validation testing. Tools include SQL, python, pyspark, kafka, databricks, sql server (legacy), Azure Synapse and Azure Data Factory, dbt and airflow, great expectations (used for legacy system)&lt;/p&gt;\n\n&lt;p&gt;Clearly the tools and opportunity at Company B is far better from a data engineering standpoint which is what I ultimately want to do and is more valuable long term as I grow. But if company A offers more DE work in my meeting tomorrow this will be tricky. I also have the opportunity to renegotiate with Company B for more pay but I have never done that before and really don&amp;#39;t want to push it because I really want to be a DE now and get out of my current job ASAP.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;(Edit): My goal is to take the job that gives me the best experience and leverage for better roles down the line (tech companies etc.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "108jbv8", "is_robot_indexable": true, "report_reasons": null, "author": "SilentSturm", "discussion_type": null, "num_comments": 76, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108jbv8/need_help_choosing_offer_110k_da_vs_88k_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108jbv8/need_help_choosing_offer_110k_da_vs_88k_de/", "subreddit_subscribers": 85948, "created_utc": 1673380915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://vast.io/blog/parquet-and-feather-data-engineering-woes](https://vast.io/blog/parquet-and-feather-data-engineering-woes)\n\nNice article discussing the experience of integrating Parquet and Feather.", "author_fullname": "t2_4g2a7kl3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parquet &amp; Feather: Data Engineering Woes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108a9rt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673358558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://vast.io/blog/parquet-and-feather-data-engineering-woes\"&gt;https://vast.io/blog/parquet-and-feather-data-engineering-woes&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Nice article discussing the experience of integrating Parquet and Feather.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "108a9rt", "is_robot_indexable": true, "report_reasons": null, "author": "PsychologicalLoss829", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108a9rt/parquet_feather_data_engineering_woes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108a9rt/parquet_feather_data_engineering_woes/", "subreddit_subscribers": 85948, "created_utc": 1673358558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title states my manager wants to push me into a data architect/SME role. Most of my current responsibilities align with the ones an architect would have, but I have concerns about narrowing my potential job pool and losing technical skills in the long run. Im guessing much of my hesitation is due to some personal biases against nontechnical folks in our industry. Does anyone have any advice/thoughts regarding this next step?", "author_fullname": "t2_reqvlo0l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting encouraged to move into an architect role from being a DE. Looking for pros/cons and advice on this transition.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108ksz0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673384361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title states my manager wants to push me into a data architect/SME role. Most of my current responsibilities align with the ones an architect would have, but I have concerns about narrowing my potential job pool and losing technical skills in the long run. Im guessing much of my hesitation is due to some personal biases against nontechnical folks in our industry. Does anyone have any advice/thoughts regarding this next step?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "108ksz0", "is_robot_indexable": true, "report_reasons": null, "author": "Holiday_Lab_6766", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108ksz0/getting_encouraged_to_move_into_an_architect_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108ksz0/getting_encouraged_to_move_into_an_architect_role/", "subreddit_subscribers": 85948, "created_utc": 1673384361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_14i1sz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Faster PostgreSQL To BigQuery Transfers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108fdj7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1673371433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tech.marksblogg.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tech.marksblogg.com/postgresql-to-bigquery.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "108fdj7", "is_robot_indexable": true, "report_reasons": null, "author": "pescennius", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108fdj7/faster_postgresql_to_bigquery_transfers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tech.marksblogg.com/postgresql-to-bigquery.html", "subreddit_subscribers": 85948, "created_utc": 1673371433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_s63bu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Github Actions CI/CD workflow for feature engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_108mq97", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/95r5QYWjV_zG9e6cpYigLJwryouvkgweVo2LsPSeQ7E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673388810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hopsworks.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.hopsworks.ai/post/optimize-your-mlops-workflow-with-a-feature-store-ci-cd-and-github-actions", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ar8JQxTu7-nFejxZNVm0ZO9cGj9IhrUuXqd6OX1P5S0.jpg?auto=webp&amp;v=enabled&amp;s=53a7d23ada4faeefa432e0c495dac8dba70f816a", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ar8JQxTu7-nFejxZNVm0ZO9cGj9IhrUuXqd6OX1P5S0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e09f6cbd6c5277e5ad5be408bb0837fd46ed1507", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/ar8JQxTu7-nFejxZNVm0ZO9cGj9IhrUuXqd6OX1P5S0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d57c1bff9a4fc2767f44a6f6172172aba81a792", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/ar8JQxTu7-nFejxZNVm0ZO9cGj9IhrUuXqd6OX1P5S0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4197cf09fd98239ce650a52ee5ef486f62a7cf3c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/ar8JQxTu7-nFejxZNVm0ZO9cGj9IhrUuXqd6OX1P5S0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=710f94ef36a6e44756ebf5a770e3672cbe6305c0", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/ar8JQxTu7-nFejxZNVm0ZO9cGj9IhrUuXqd6OX1P5S0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=750f8b60a4ee93c20769f5c81e8870b0d1be6fb3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/ar8JQxTu7-nFejxZNVm0ZO9cGj9IhrUuXqd6OX1P5S0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ab3b0405535be8ec6052854ff60a8eeba187911", "width": 1080, "height": 567}], "variants": {}, "id": "v4XZV9eOnkvVD6dFIAsZlFSbhlSKyI5axqqq2vG2Lbo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "108mq97", "is_robot_indexable": true, "report_reasons": null, "author": "SirOibaf", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108mq97/github_actions_cicd_workflow_for_feature/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.hopsworks.ai/post/optimize-your-mlops-workflow-with-a-feature-store-ci-cd-and-github-actions", "subreddit_subscribers": 85948, "created_utc": 1673388810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have good side gig ideas that I can use my DE skills for lol ?", "author_fullname": "t2_eoqc2m0b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any side gigs for data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108pil0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673395263.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have good side gig ideas that I can use my DE skills for lol ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "108pil0", "is_robot_indexable": true, "report_reasons": null, "author": "__post_init__", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108pil0/any_side_gigs_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108pil0/any_side_gigs_for_data_engineers/", "subreddit_subscribers": 85948, "created_utc": 1673395263.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, I wanted to ask for advice on how get a job with more challenging problems/cool tech in Data Engineering. I would be psyched about FAANG or a startup or really anything - I just want to work in a sharp team and continue expanding an in-demand skillset as I feel like I'm stagnating a bit right now. Whether that's 'Software Engineer, Data' or a DE at NFLX/AMZN working with Spark, etc. and doing some DevOps stuff doesn't matter so much - I just want to keep learning.\n\nMy background: I'm a former PhD student (stem, but not CS) with \\~1 YOE at my current job (title is Data Engineer) at a medium-sized banking company. My current job responsibilities are occasional Airflow ingestion jobs to BigQuery and building dimensional models (we don't do it well, but we do it) with dbt, and some ad hoc analytics requests. It's really more of what would be called Analytics Engineer or BI engineer elsewhere. It's a great tech stack and I'll take it over Talend or Matillion or Informatica any day, but I'm pretty restricted to 'business analytics' related problems and nothing else.\n\nI feel underpaid and a bit bored - I want to work on challenging problems and keep learning new things. I would love to move towards a 'Software Engineer, Data' type role but am feeling stuck. I feel like I don't have the toolkit (Spark, streaming tech, more complicated infra projects) to make the jump. I know the best thing to do would be to work with the Software Engineering team and get more experience with things like Docker/k8s, streaming, CRUD and other backend, but I don't see that happening at my current company. I've talked to my manager about these goals as we have a pretty mature dedicated Software Engineering team but he hasn't been receptive or helpful.\n\nWhen I started out with the job hunt last spring I was getting a few interviews here and there for these more SWE-oriented DE jobs (made it to onsites, but no offers - painfully close). I took this job because I didn't want to have a big gap in my resume and figured it was best to get SOME engineering experience somewhere. Since starting to apply again I'm getting a lower response rate and worry that I've taken a 'BI Engineer' job called Data Engineer my resume bullet points suck and I've pigeonholed myself a bit. I know the job market has cooled off significantly too.\n\nI feel like there are a number of things I could do like work on personal projects to show I can use Spark/Pyspark, or learn a backend language and expose a database with Java or Go - I'm just keen to hear what any hiring managers would want to see for someone making the jump if expanding my skillset at my current company isn't a possibility.\n\nThanks in advance!", "author_fullname": "t2_qn7rymsr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning from Analytics Engineer/BI Engineer to Software Engineer, Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108un1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673408987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I wanted to ask for advice on how get a job with more challenging problems/cool tech in Data Engineering. I would be psyched about FAANG or a startup or really anything - I just want to work in a sharp team and continue expanding an in-demand skillset as I feel like I&amp;#39;m stagnating a bit right now. Whether that&amp;#39;s &amp;#39;Software Engineer, Data&amp;#39; or a DE at NFLX/AMZN working with Spark, etc. and doing some DevOps stuff doesn&amp;#39;t matter so much - I just want to keep learning.&lt;/p&gt;\n\n&lt;p&gt;My background: I&amp;#39;m a former PhD student (stem, but not CS) with ~1 YOE at my current job (title is Data Engineer) at a medium-sized banking company. My current job responsibilities are occasional Airflow ingestion jobs to BigQuery and building dimensional models (we don&amp;#39;t do it well, but we do it) with dbt, and some ad hoc analytics requests. It&amp;#39;s really more of what would be called Analytics Engineer or BI engineer elsewhere. It&amp;#39;s a great tech stack and I&amp;#39;ll take it over Talend or Matillion or Informatica any day, but I&amp;#39;m pretty restricted to &amp;#39;business analytics&amp;#39; related problems and nothing else.&lt;/p&gt;\n\n&lt;p&gt;I feel underpaid and a bit bored - I want to work on challenging problems and keep learning new things. I would love to move towards a &amp;#39;Software Engineer, Data&amp;#39; type role but am feeling stuck. I feel like I don&amp;#39;t have the toolkit (Spark, streaming tech, more complicated infra projects) to make the jump. I know the best thing to do would be to work with the Software Engineering team and get more experience with things like Docker/k8s, streaming, CRUD and other backend, but I don&amp;#39;t see that happening at my current company. I&amp;#39;ve talked to my manager about these goals as we have a pretty mature dedicated Software Engineering team but he hasn&amp;#39;t been receptive or helpful.&lt;/p&gt;\n\n&lt;p&gt;When I started out with the job hunt last spring I was getting a few interviews here and there for these more SWE-oriented DE jobs (made it to onsites, but no offers - painfully close). I took this job because I didn&amp;#39;t want to have a big gap in my resume and figured it was best to get SOME engineering experience somewhere. Since starting to apply again I&amp;#39;m getting a lower response rate and worry that I&amp;#39;ve taken a &amp;#39;BI Engineer&amp;#39; job called Data Engineer my resume bullet points suck and I&amp;#39;ve pigeonholed myself a bit. I know the job market has cooled off significantly too.&lt;/p&gt;\n\n&lt;p&gt;I feel like there are a number of things I could do like work on personal projects to show I can use Spark/Pyspark, or learn a backend language and expose a database with Java or Go - I&amp;#39;m just keen to hear what any hiring managers would want to see for someone making the jump if expanding my skillset at my current company isn&amp;#39;t a possibility.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "108un1j", "is_robot_indexable": true, "report_reasons": null, "author": "Proof_Elk_2281", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108un1j/transitioning_from_analytics_engineerbi_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108un1j/transitioning_from_analytics_engineerbi_engineer/", "subreddit_subscribers": 85948, "created_utc": 1673408987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9dz8x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The easiest way to move data from MongoDB to a PostgreSQL DB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 94, "top_awarded_type": null, "hide_score": false, "name": "t3_108ef4b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/h_Pz_EKkGS-n_BWKGba3ZZ4yMKg9I3Gddlm5QzqiHLE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673369177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@henryivesjones/the-easiest-way-to-move-data-from-mongodb-to-a-postgresql-db-bc30846ac1b0", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/l3FjJ4b75QWKftB5myxTSEH7PUZYZ-TVWgVLJofbIDo.jpg?auto=webp&amp;v=enabled&amp;s=a0d1614a4bf46a7775639f63e6bd75d5510c957e", "width": 920, "height": 621}, "resolutions": [{"url": "https://external-preview.redd.it/l3FjJ4b75QWKftB5myxTSEH7PUZYZ-TVWgVLJofbIDo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eabb0302312d47b995fb33585f371d82db2d83c4", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/l3FjJ4b75QWKftB5myxTSEH7PUZYZ-TVWgVLJofbIDo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70cd943d2dd57e1a466af5d730671d21f6022b47", "width": 216, "height": 145}, {"url": "https://external-preview.redd.it/l3FjJ4b75QWKftB5myxTSEH7PUZYZ-TVWgVLJofbIDo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=047749125cc9fd9326429a95a874584e912db611", "width": 320, "height": 216}, {"url": "https://external-preview.redd.it/l3FjJ4b75QWKftB5myxTSEH7PUZYZ-TVWgVLJofbIDo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a3e514f2cfcf52944634fc1a498dd0778285cd6", "width": 640, "height": 432}], "variants": {}, "id": "O-WP4-c1TYayjTlfFwLLSbhh00h6ZAz8zGCDwlJiEH4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "108ef4b", "is_robot_indexable": true, "report_reasons": null, "author": "MidgetDufus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108ef4b/the_easiest_way_to_move_data_from_mongodb_to_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@henryivesjones/the-easiest-way-to-move-data-from-mongodb-to-a-postgresql-db-bc30846ac1b0", "subreddit_subscribers": 85948, "created_utc": 1673369177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to data engineering and I have learnt about the Parquet file. I have been able to take a csv file and using pandas and pyarrow, create a Parquet file.\n\nMy question now is, in real life/production/work environment how are Parquet files handled and stored? I can imagine in these environments, data will be retrieved from various sources: sensors, apis, databases etc and needs to be converted to Parquet files to enable analysis. But how will these files be managed?\n\nDo I need to create and manage directories filled with Parquet files? Or are there software systems that abstracts this away from you?", "author_fullname": "t2_3euic3tq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are Parquet files managed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1083os1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673335522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to data engineering and I have learnt about the Parquet file. I have been able to take a csv file and using pandas and pyarrow, create a Parquet file.&lt;/p&gt;\n\n&lt;p&gt;My question now is, in real life/production/work environment how are Parquet files handled and stored? I can imagine in these environments, data will be retrieved from various sources: sensors, apis, databases etc and needs to be converted to Parquet files to enable analysis. But how will these files be managed?&lt;/p&gt;\n\n&lt;p&gt;Do I need to create and manage directories filled with Parquet files? Or are there software systems that abstracts this away from you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1083os1", "is_robot_indexable": true, "report_reasons": null, "author": "finlaydotweber", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1083os1/how_are_parquet_files_managed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1083os1/how_are_parquet_files_managed/", "subreddit_subscribers": 85948, "created_utc": 1673335522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ever doing a task or fixing something thinking you should have asked the DBA to do?\n\nOr do you ever feel like you're being a pest to the DBA because you ask them to do something you probably could've done yourself?", "author_fullname": "t2_8jq30m4x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How often do the lines get blurred between your responsibilities and the database administrator's responsibilities?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108weep", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673414247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever doing a task or fixing something thinking you should have asked the DBA to do?&lt;/p&gt;\n\n&lt;p&gt;Or do you ever feel like you&amp;#39;re being a pest to the DBA because you ask them to do something you probably could&amp;#39;ve done yourself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "108weep", "is_robot_indexable": true, "report_reasons": null, "author": "patheticadam", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108weep/how_often_do_the_lines_get_blurred_between_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108weep/how_often_do_the_lines_get_blurred_between_your/", "subreddit_subscribers": 85948, "created_utc": 1673414247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my previous job we used SSIS to do some minor transformations to raw data and upload to our 'staging zones' in SQL Server, whereupon sprocs would run periodically and create DWH objects for analysts. Boring tech stack maybe but we had ample room to munge the data at any point before/after it hit our DBs if we wanted.\n\nAt my new job, they use Segment to directly load ('sync') data to Snowflake, and although I can see the queries that Segment is executing in Snowflake's query history, I seem unable to actually tinker with the queries in Segment -- am I missing something here? Or is this because Segment is a low-code 'plug and play' kind of deal? \n\nIf we wanted greater low-level access (i.e. ability to code transformations) to the data Segment is handling before it hits our DBs, do you think looking into Kafka/Kinesis and Snowpipe as an intermediary stage would be a good thing to explore? I'm entirely new to Snowflake and Segment here so I'd appreciate any information.", "author_fullname": "t2_418hrmde", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone used Segment with Snowflake before? What are the benefits/challenges?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108ecbt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673368990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my previous job we used SSIS to do some minor transformations to raw data and upload to our &amp;#39;staging zones&amp;#39; in SQL Server, whereupon sprocs would run periodically and create DWH objects for analysts. Boring tech stack maybe but we had ample room to munge the data at any point before/after it hit our DBs if we wanted.&lt;/p&gt;\n\n&lt;p&gt;At my new job, they use Segment to directly load (&amp;#39;sync&amp;#39;) data to Snowflake, and although I can see the queries that Segment is executing in Snowflake&amp;#39;s query history, I seem unable to actually tinker with the queries in Segment -- am I missing something here? Or is this because Segment is a low-code &amp;#39;plug and play&amp;#39; kind of deal? &lt;/p&gt;\n\n&lt;p&gt;If we wanted greater low-level access (i.e. ability to code transformations) to the data Segment is handling before it hits our DBs, do you think looking into Kafka/Kinesis and Snowpipe as an intermediary stage would be a good thing to explore? I&amp;#39;m entirely new to Snowflake and Segment here so I&amp;#39;d appreciate any information.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "108ecbt", "is_robot_indexable": true, "report_reasons": null, "author": "Glaukosss", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108ecbt/anyone_used_segment_with_snowflake_before_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108ecbt/anyone_used_segment_with_snowflake_before_what/", "subreddit_subscribers": 85948, "created_utc": 1673368990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve seen many posts on the subreddit about ELT, dbt and data warehousing, but I haven\u2019t seen a whole load of posts discussing orchestrators in an ML event-driven context.\n\nI\u2019ve had a lot of experience using Airflow and Dagster, but I know there are many more orchestrators out there I haven\u2019t had a chance to use, so wanted to get advice on which orchestrator would be appropriate for my current use case.\n\nUse case:\n\n1.\tFile arrives in an S3 bucket which kicks off various scripts\n2.\tAmong these scripts is sending the file to a ML model for inference\n3.\tAll scripts write the data extracted and inferred from file into a document in Elasticsearch\n\nWhich Orchestrator would be best for this? \n\n-\tIt needs to be event driven, so have solid support for sensors - instead of running periodically\n-\tIt needs to be able to initiate the scripts in Kubernetes or as Docker containers, or even initiate Lambda functions\n-\tIdeally it would support retrying singular tasks in the event of a task failure\n-\tIt needs lineage and task dependencies as there are currently race conditions between the scripts. This is the main reason I want an orchestrator\n-\tAnd finally it needs solid logging, monitoring and performance metrics\n\nRight now I\u2019m leaning towards Dagster over Airflow, but I can\u2019t speak to orchestrators like Prefect, Mage or Luigi because I\u2019ve never used them.", "author_fullname": "t2_9usjyayk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best orchestrator for event based ETL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108bg7z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673361711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve seen many posts on the subreddit about ELT, dbt and data warehousing, but I haven\u2019t seen a whole load of posts discussing orchestrators in an ML event-driven context.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve had a lot of experience using Airflow and Dagster, but I know there are many more orchestrators out there I haven\u2019t had a chance to use, so wanted to get advice on which orchestrator would be appropriate for my current use case.&lt;/p&gt;\n\n&lt;p&gt;Use case:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; File arrives in an S3 bucket which kicks off various scripts&lt;/li&gt;\n&lt;li&gt; Among these scripts is sending the file to a ML model for inference&lt;/li&gt;\n&lt;li&gt; All scripts write the data extracted and inferred from file into a document in Elasticsearch&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Which Orchestrator would be best for this? &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;  It needs to be event driven, so have solid support for sensors - instead of running periodically&lt;/li&gt;\n&lt;li&gt;  It needs to be able to initiate the scripts in Kubernetes or as Docker containers, or even initiate Lambda functions&lt;/li&gt;\n&lt;li&gt;  Ideally it would support retrying singular tasks in the event of a task failure&lt;/li&gt;\n&lt;li&gt;  It needs lineage and task dependencies as there are currently race conditions between the scripts. This is the main reason I want an orchestrator&lt;/li&gt;\n&lt;li&gt;  And finally it needs solid logging, monitoring and performance metrics&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Right now I\u2019m leaning towards Dagster over Airflow, but I can\u2019t speak to orchestrators like Prefect, Mage or Luigi because I\u2019ve never used them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "108bg7z", "is_robot_indexable": true, "report_reasons": null, "author": "feathersurf", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108bg7z/best_orchestrator_for_event_based_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108bg7z/best_orchestrator_for_event_based_etl/", "subreddit_subscribers": 85948, "created_utc": 1673361711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I'm using the sFTP source in Airbyte to fetch a csv file nightly from a server of a supplier, and load the data into BigQuery. On a selection of the fetched enteties I want to enrich the data using the same supplier's HTTP API which returns more metadata on a single entity. What's a good way to do this in batch in a GCP environment? \nMy first idea was BQ -&gt; PubSub (one message per entity id) -&gt; CloudFunction (to do the HTTP request / save result) but it seems a bit clunky.", "author_fullname": "t2_82s0a64", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Enrich data from HTTP API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108ilyd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673379215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m using the sFTP source in Airbyte to fetch a csv file nightly from a server of a supplier, and load the data into BigQuery. On a selection of the fetched enteties I want to enrich the data using the same supplier&amp;#39;s HTTP API which returns more metadata on a single entity. What&amp;#39;s a good way to do this in batch in a GCP environment? \nMy first idea was BQ -&amp;gt; PubSub (one message per entity id) -&amp;gt; CloudFunction (to do the HTTP request / save result) but it seems a bit clunky.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "108ilyd", "is_robot_indexable": true, "report_reasons": null, "author": "Ootoootooo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108ilyd/enrich_data_from_http_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108ilyd/enrich_data_from_http_api/", "subreddit_subscribers": 85948, "created_utc": 1673379215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI am implementing an ETL streaming job using Spark Structured Streaming. This job takes data from a kafka topic and stores it into separate Delta tables according to the value of a specific field.\n\nI keep the latest version for each row, and right now I am using the DeltaTable's merge function to upsert data into the destination table. I am already optimizing by using partition columns in the merge condition, and by doing so I am getting a latency in the order of 10 minutes.\n\nI was wondering if an \"append-only\" approach to the tables would decrease the latency or not.\n\nThe steps would be as follows:\n\nIn the batch, for each Delta table\n\n1. Calculate the latest versions for each event in the batch and keep them\n2. Append the latest event versions to the delta table\n3. Read the table partitions containing changes\n4. Filter the data to keep only the latest event versions\n5. Overwrite the involved table partitions using `replaceWhere`\n\nThe idea here is to avoid any joins that delta's merge [does](https://www.databricks.com/wp-content/uploads/2020/09/blog-diving-delta-4.png).\n\n&amp;#x200B;\n\nHas anyone tried this approach? Do you think it will give some performance gains?\n\nI'm wondering if I am reinventing the wheel and Delta is already optimized or not, or if there are some corner cases I am not considering.\n\n&amp;#x200B;\n\nEDIT1: The way I'd keep the latest event versions would be a window function:\n\nrow\\_number partitioning by key and ordering by event timestamp in descending order, then I'd keep only the rows where the value equals 1.", "author_fullname": "t2_do2bj7xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta's merge vs overwriting with replaceWhere", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10857r4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673343815.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673341273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am implementing an ETL streaming job using Spark Structured Streaming. This job takes data from a kafka topic and stores it into separate Delta tables according to the value of a specific field.&lt;/p&gt;\n\n&lt;p&gt;I keep the latest version for each row, and right now I am using the DeltaTable&amp;#39;s merge function to upsert data into the destination table. I am already optimizing by using partition columns in the merge condition, and by doing so I am getting a latency in the order of 10 minutes.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if an &amp;quot;append-only&amp;quot; approach to the tables would decrease the latency or not.&lt;/p&gt;\n\n&lt;p&gt;The steps would be as follows:&lt;/p&gt;\n\n&lt;p&gt;In the batch, for each Delta table&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Calculate the latest versions for each event in the batch and keep them&lt;/li&gt;\n&lt;li&gt;Append the latest event versions to the delta table&lt;/li&gt;\n&lt;li&gt;Read the table partitions containing changes&lt;/li&gt;\n&lt;li&gt;Filter the data to keep only the latest event versions&lt;/li&gt;\n&lt;li&gt;Overwrite the involved table partitions using &lt;code&gt;replaceWhere&lt;/code&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The idea here is to avoid any joins that delta&amp;#39;s merge &lt;a href=\"https://www.databricks.com/wp-content/uploads/2020/09/blog-diving-delta-4.png\"&gt;does&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anyone tried this approach? Do you think it will give some performance gains?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if I am reinventing the wheel and Delta is already optimized or not, or if there are some corner cases I am not considering.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EDIT1: The way I&amp;#39;d keep the latest event versions would be a window function:&lt;/p&gt;\n\n&lt;p&gt;row_number partitioning by key and ordering by event timestamp in descending order, then I&amp;#39;d keep only the rows where the value equals 1.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?auto=webp&amp;v=enabled&amp;s=d87b9e1343bbe01113f9a5de248fda2154f4219f", "width": 864, "height": 487}, "resolutions": [{"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e4e607e4058490f4ca840cb1f85a678abd05adc", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f7613e6e083214441d71b5fe6259231bdde8dc1", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b86406f74a2bcce3bd2e75d927cbbf74c0a66430", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4408ac85003d8739b6be4eea63d6b673463bb9a4", "width": 640, "height": 360}], "variants": {}, "id": "RHUM16d2JyL38mWZUyE_7-xAj3xsCEo-fboQVyXjq1M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10857r4", "is_robot_indexable": true, "report_reasons": null, "author": "somerandomdataeng", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10857r4/deltas_merge_vs_overwriting_with_replacewhere/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10857r4/deltas_merge_vs_overwriting_with_replacewhere/", "subreddit_subscribers": 85948, "created_utc": 1673341273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The last two years have been a boom for tech and Data Engineers in particular but in the last two months i've noticed a disquieting silence from recruiters calling /spamming on linkedin messages. It used to be quite comforting to receive constant emails from recruiters offering jobs wayyyy above my experience levels with stupid high day rates since it suggested they've got high demand and low supply of candidates now they've all gone quiet which suggests the opposite. I work in what I think is a relatively secure industry (fingers crossed) , insurance, so i'm not too worried but i dont think moving is an option anytime soon. For context i'm in the UK. Whats everyone elses experience so far ?", "author_fullname": "t2_x0wpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone else worried about job security in the current tech climate?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108svmg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673404003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The last two years have been a boom for tech and Data Engineers in particular but in the last two months i&amp;#39;ve noticed a disquieting silence from recruiters calling /spamming on linkedin messages. It used to be quite comforting to receive constant emails from recruiters offering jobs wayyyy above my experience levels with stupid high day rates since it suggested they&amp;#39;ve got high demand and low supply of candidates now they&amp;#39;ve all gone quiet which suggests the opposite. I work in what I think is a relatively secure industry (fingers crossed) , insurance, so i&amp;#39;m not too worried but i dont think moving is an option anytime soon. For context i&amp;#39;m in the UK. Whats everyone elses experience so far ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "108svmg", "is_robot_indexable": true, "report_reasons": null, "author": "CingKan", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/108svmg/anyone_else_worried_about_job_security_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108svmg/anyone_else_worried_about_job_security_in_the/", "subreddit_subscribers": 85948, "created_utc": 1673404003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was considering using them but having never used them, I was unsure about its use cases and whereabouts its reasonable and where it becomes unreasonable. \n\nI was wondering if anyone who has used them can share how it's been used at their job and at what type of scale or speed and if you considered it successful or a headache.\n\nThanks for any insight or experiences you are willing to share.", "author_fullname": "t2_t5lw9zm8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you guys use cron or systemd timers at your job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108s6xy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673402202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was considering using them but having never used them, I was unsure about its use cases and whereabouts its reasonable and where it becomes unreasonable. &lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone who has used them can share how it&amp;#39;s been used at their job and at what type of scale or speed and if you considered it successful or a headache.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any insight or experiences you are willing to share.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "108s6xy", "is_robot_indexable": true, "report_reasons": null, "author": "Heavy_Bread_5919", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108s6xy/do_you_guys_use_cron_or_systemd_timers_at_your_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108s6xy/do_you_guys_use_cron_or_systemd_timers_at_your_job/", "subreddit_subscribers": 85948, "created_utc": 1673402202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What's the interview process for data engineer/forward deployed data engineer at the Boston Consulting Group ?", "author_fullname": "t2_aed1ixws", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the DE interview at BCG (Boston Consulting Group", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108pkbv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673395376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the interview process for data engineer/forward deployed data engineer at the Boston Consulting Group ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "108pkbv", "is_robot_indexable": true, "report_reasons": null, "author": "ameyricano", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108pkbv/whats_the_de_interview_at_bcg_boston_consulting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108pkbv/whats_the_de_interview_at_bcg_boston_consulting/", "subreddit_subscribers": 85948, "created_utc": 1673395376.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A bit of background, I studied chemical engineering before moving into an environmental engineering background for the past 2-3 years. I have enjoyed working with data and some simpler coding but you can class me as a full beginner when it comes to DE. \nI have an offer to start a DE training and placement scheme starting later this year, are there any good free courses or sites where I can try DE before I leave my current job?", "author_fullname": "t2_496l49vx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to know I will like DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108nn5o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673390878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A bit of background, I studied chemical engineering before moving into an environmental engineering background for the past 2-3 years. I have enjoyed working with data and some simpler coding but you can class me as a full beginner when it comes to DE. \nI have an offer to start a DE training and placement scheme starting later this year, are there any good free courses or sites where I can try DE before I leave my current job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "108nn5o", "is_robot_indexable": true, "report_reasons": null, "author": "barneythunder", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108nn5o/how_to_know_i_will_like_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108nn5o/how_to_know_i_will_like_de/", "subreddit_subscribers": 85948, "created_utc": 1673390878.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm starting a new project where I want to build data pipelines using Python to extract and load some CSV data on the web. (I'm planning to use Airflow). I'll be collaborating with another dev who would build custom functionality that depends on the data I will collect, in a Spring Boot API. \n\nFrom a project setup perspective have others worked with a similar split between data pipeline code and API code? Do you have any recommendations or advice, especially for how to both test the schema effectively and avoid duplication of SQL/schema definition to the extent possible? \n\nI'd like to support database migrations, should I add something like flyway to the API project? Or manage that on the pipeline side of things?", "author_fullname": "t2_319xs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipelines and API project setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108mvn0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673389158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m starting a new project where I want to build data pipelines using Python to extract and load some CSV data on the web. (I&amp;#39;m planning to use Airflow). I&amp;#39;ll be collaborating with another dev who would build custom functionality that depends on the data I will collect, in a Spring Boot API. &lt;/p&gt;\n\n&lt;p&gt;From a project setup perspective have others worked with a similar split between data pipeline code and API code? Do you have any recommendations or advice, especially for how to both test the schema effectively and avoid duplication of SQL/schema definition to the extent possible? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to support database migrations, should I add something like flyway to the API project? Or manage that on the pipeline side of things?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "108mvn0", "is_robot_indexable": true, "report_reasons": null, "author": "jaydub", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108mvn0/data_pipelines_and_api_project_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108mvn0/data_pipelines_and_api_project_setup/", "subreddit_subscribers": 85948, "created_utc": 1673389158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nWe have a very simple data pipeline used to create self-service dashboards and feed reports. We need to put a diagram of the data architecture in a PowerPoint slide, but I'm not sure what term to use for the database layer. Here are the pieces:\n\nOrchestration: Apache Airflow\n\nRaw data ingest &amp; staging: AWS S3\n\nETL: Python (pandas) scripts which get refactored into Airflow DAGs\n\nRelational database: AWS RDS Postgres instance\n\nDashboards: Apache Superset\n\nI'm wondering what to call the database in this architecture. It's not really a warehouse...is it a single data mart? Do we just skip the jargon and call it a database?", "author_fullname": "t2_cqvp4nt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the correct term for the relational data storage in my team's data architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108hegl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673376274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;We have a very simple data pipeline used to create self-service dashboards and feed reports. We need to put a diagram of the data architecture in a PowerPoint slide, but I&amp;#39;m not sure what term to use for the database layer. Here are the pieces:&lt;/p&gt;\n\n&lt;p&gt;Orchestration: Apache Airflow&lt;/p&gt;\n\n&lt;p&gt;Raw data ingest &amp;amp; staging: AWS S3&lt;/p&gt;\n\n&lt;p&gt;ETL: Python (pandas) scripts which get refactored into Airflow DAGs&lt;/p&gt;\n\n&lt;p&gt;Relational database: AWS RDS Postgres instance&lt;/p&gt;\n\n&lt;p&gt;Dashboards: Apache Superset&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering what to call the database in this architecture. It&amp;#39;s not really a warehouse...is it a single data mart? Do we just skip the jargon and call it a database?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "108hegl", "is_robot_indexable": true, "report_reasons": null, "author": "udonthave2call", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108hegl/whats_the_correct_term_for_the_relational_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108hegl/whats_the_correct_term_for_the_relational_data/", "subreddit_subscribers": 85948, "created_utc": 1673376274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My Airflow task is uploading JSON files to BigQuery every 2 hours. It takes longer than an hour to upload them all so it times out.\n\nI was thinking of using the async library but not sure - Because it has to read each file into memory and convert it to a dictionary so wondering if I should use multiprocessing instead?\n\nThe reason for reading the file is because we add some extra fields to each file", "author_fullname": "t2_ggg0wfmt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Time Outs - BigQuery Upload", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1084axv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673337795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My Airflow task is uploading JSON files to BigQuery every 2 hours. It takes longer than an hour to upload them all so it times out.&lt;/p&gt;\n\n&lt;p&gt;I was thinking of using the async library but not sure - Because it has to read each file into memory and convert it to a dictionary so wondering if I should use multiprocessing instead?&lt;/p&gt;\n\n&lt;p&gt;The reason for reading the file is because we add some extra fields to each file&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1084axv", "is_robot_indexable": true, "report_reasons": null, "author": "camelCaseInsensitive", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1084axv/airflow_time_outs_bigquery_upload/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1084axv/airflow_time_outs_bigquery_upload/", "subreddit_subscribers": 85948, "created_utc": 1673337795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello community,\n\nI am in the delicate situation that in two days I have to teach a course about MDM, and in a broader sense, how to build master/reference data hubs. It is a 7-days long course part of a Data Engineering course given to 14 employees working in the French public postal service.\nThe description is \"A module to set up a coherent data repository with the different stakeholders of the company\napplying agile methods to guarantee the success of the project.\" Data repository here is originally \"r\u00e9f\u00e9rentiel de donn\u00e9es\" which can be translated as Master Data Management.\n\n\nAnd the problem I'm facing is how to make them practice on that without a company setting and the databases associated (I don't have access to their company data at all, I was contracted by a 3rd party).\nThe listed tasks they should practice are things that require exactly that, such as \"Identify data sources with potential use according to the company's data strategy\" or \"assess the volume and nature of the target data\" or \"question the potential users of the data to accompany the expression of the need. \nThe document is in French so these are rough translations but I'd be happy to provide more.\n\nMy plan was to make them work on fake database like the Adventureworks database, but I obviously don't have access to costly MDM solutions either, so I was looking for ways to build a custom, very simple MDM from this database. I know it doesn't even really make sense but those are the requirements. I need to make them work on building something akin to an MDM at least.\n\nI am truly at my wits' end here. How would you go about this?\n\nThank you", "author_fullname": "t2_ep8da", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple approach to teach Master Data Management?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108rbkr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673399898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello community,&lt;/p&gt;\n\n&lt;p&gt;I am in the delicate situation that in two days I have to teach a course about MDM, and in a broader sense, how to build master/reference data hubs. It is a 7-days long course part of a Data Engineering course given to 14 employees working in the French public postal service.\nThe description is &amp;quot;A module to set up a coherent data repository with the different stakeholders of the company\napplying agile methods to guarantee the success of the project.&amp;quot; Data repository here is originally &amp;quot;r\u00e9f\u00e9rentiel de donn\u00e9es&amp;quot; which can be translated as Master Data Management.&lt;/p&gt;\n\n&lt;p&gt;And the problem I&amp;#39;m facing is how to make them practice on that without a company setting and the databases associated (I don&amp;#39;t have access to their company data at all, I was contracted by a 3rd party).\nThe listed tasks they should practice are things that require exactly that, such as &amp;quot;Identify data sources with potential use according to the company&amp;#39;s data strategy&amp;quot; or &amp;quot;assess the volume and nature of the target data&amp;quot; or &amp;quot;question the potential users of the data to accompany the expression of the need. \nThe document is in French so these are rough translations but I&amp;#39;d be happy to provide more.&lt;/p&gt;\n\n&lt;p&gt;My plan was to make them work on fake database like the Adventureworks database, but I obviously don&amp;#39;t have access to costly MDM solutions either, so I was looking for ways to build a custom, very simple MDM from this database. I know it doesn&amp;#39;t even really make sense but those are the requirements. I need to make them work on building something akin to an MDM at least.&lt;/p&gt;\n\n&lt;p&gt;I am truly at my wits&amp;#39; end here. How would you go about this?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "108rbkr", "is_robot_indexable": true, "report_reasons": null, "author": "Daemoniss", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108rbkr/simple_approach_to_teach_master_data_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108rbkr/simple_approach_to_teach_master_data_management/", "subreddit_subscribers": 85948, "created_utc": 1673399898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to data engineering and one of the popular software you hear used in this field is Apache Spark, so I am spending some time to understand what exactly it does.\n\nThe Spark websites describes it as _Unified engine for large-scale data analytics_. So okay, I interpret that to mean it enables you to crunch and analyze data very fast.\n\nThe next question I now had was, okay, if it helps in data analytics, where does it get the data it analyzes from?\n\nSo I discover it could be various sources even including relational databases! I found this video [here](https://www.youtube.com/watch?v=rAeQ2k-C00o) that shows how to connect it to a Postgres database in Scala.\n\nBut I can connect to Postgress with something like [Quill](https://getquill.io/) and run sophisticated queries to fetch data. Which then got me thinking, what is the difference between using Spark to connect to the database and using something like Quill or your normal pure JDBC driver?\n\nSo in this particular case of using Spark to fetch data from a relational database, what is the advantage? Does Spark magically enable me to perform queries and analytics faster?", "author_fullname": "t2_3euic3tq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why use Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108mihu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673388304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to data engineering and one of the popular software you hear used in this field is Apache Spark, so I am spending some time to understand what exactly it does.&lt;/p&gt;\n\n&lt;p&gt;The Spark websites describes it as &lt;em&gt;Unified engine for large-scale data analytics&lt;/em&gt;. So okay, I interpret that to mean it enables you to crunch and analyze data very fast.&lt;/p&gt;\n\n&lt;p&gt;The next question I now had was, okay, if it helps in data analytics, where does it get the data it analyzes from?&lt;/p&gt;\n\n&lt;p&gt;So I discover it could be various sources even including relational databases! I found this video &lt;a href=\"https://www.youtube.com/watch?v=rAeQ2k-C00o\"&gt;here&lt;/a&gt; that shows how to connect it to a Postgres database in Scala.&lt;/p&gt;\n\n&lt;p&gt;But I can connect to Postgress with something like &lt;a href=\"https://getquill.io/\"&gt;Quill&lt;/a&gt; and run sophisticated queries to fetch data. Which then got me thinking, what is the difference between using Spark to connect to the database and using something like Quill or your normal pure JDBC driver?&lt;/p&gt;\n\n&lt;p&gt;So in this particular case of using Spark to fetch data from a relational database, what is the advantage? Does Spark magically enable me to perform queries and analytics faster?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qu0xbbx0XLBqbTvaSpRfBHk53OxS8WeGeyrOO4Xkb-s.jpg?auto=webp&amp;v=enabled&amp;s=b1b9a39e3a780e322eef9691ce2555372bcdcc74", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/qu0xbbx0XLBqbTvaSpRfBHk53OxS8WeGeyrOO4Xkb-s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a6c591ad4e1bf894f721c6529dd829f800082545", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/qu0xbbx0XLBqbTvaSpRfBHk53OxS8WeGeyrOO4Xkb-s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7fd90e8d7c3c0cff5b0bf55061362db6f0448108", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/qu0xbbx0XLBqbTvaSpRfBHk53OxS8WeGeyrOO4Xkb-s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c74ad5e0c31731ecc4e3e778315b7b40e23323f5", "width": 320, "height": 240}], "variants": {}, "id": "SdBHJCFRf-f5J_pheUrRA_o6dYlpxHHJXdz8tHdXLMc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "108mihu", "is_robot_indexable": true, "report_reasons": null, "author": "finlaydotweber", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108mihu/why_use_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108mihu/why_use_spark/", "subreddit_subscribers": 85948, "created_utc": 1673388304.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}