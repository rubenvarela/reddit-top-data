{"kind": "Listing", "data": {"after": "t3_10ki9mr", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_2hjf6lgy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Didn\u2019t really know which group to post this. Why is this so cheap. Is there a catch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10jyt6c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 285, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 285, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Oq9W0omgV9EbkVCH0r8Sdl8wxXUKPrRSCq66HXfQY68.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674540477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9yrysgok4zda1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9yrysgok4zda1.jpg?auto=webp&amp;v=enabled&amp;s=c5497fdb2666de6211696d85660a7ddb729c989a", "width": 1125, "height": 2436}, "resolutions": [{"url": "https://preview.redd.it/9yrysgok4zda1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f07b6c554be5155ec4847d2986f6a645ab7450d", "width": 108, "height": 216}, {"url": "https://preview.redd.it/9yrysgok4zda1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1df338125bf48348ec94f2d0976743ea3a5d1830", "width": 216, "height": 432}, {"url": "https://preview.redd.it/9yrysgok4zda1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86b33639840ef9b24c6c5dbaca4a4fe2e3cae1c0", "width": 320, "height": 640}, {"url": "https://preview.redd.it/9yrysgok4zda1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09490b060f7920bb9a2fe49171d050bce7662d22", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/9yrysgok4zda1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8e547d8918a1e1a8031a194f757dda4bc7dcca4", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/9yrysgok4zda1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a6d8f89afb8e55b2d56b7e7ed771abe47c985c3", "width": 1080, "height": 2160}], "variants": {}, "id": "-eiJfedWdVhQansmCF2XprviU7lHCw_XzMCXpIWcO34"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jyt6c", "is_robot_indexable": true, "report_reasons": null, "author": "AngelOfGod3", "discussion_type": null, "num_comments": 113, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jyt6c/didnt_really_know_which_group_to_post_this_why_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9yrysgok4zda1.jpg", "subreddit_subscribers": 667125, "created_utc": 1674540477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Pokemon Fossil Museum](https://my.matterport.com/show/?m=P9WCbyCBGBM)\n\n&amp;#x200B;\n\nThere is a wonderful virtual tour of the Pokemon Fossial Museum but I'm confident this tour won't last much longer as I believe the real event is now over. How would I best preserve its contents?", "author_fullname": "t2_a803a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would I best archive the Pokemon Fossil Museum ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k5p3p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 162, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 162, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674567551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://my.matterport.com/show/?m=P9WCbyCBGBM\"&gt;Pokemon Fossil Museum&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;There is a wonderful virtual tour of the Pokemon Fossial Museum but I&amp;#39;m confident this tour won&amp;#39;t last much longer as I believe the real event is now over. How would I best preserve its contents?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?auto=webp&amp;v=enabled&amp;s=31a49400b27cc8e895ed64fc1f661843df1702bd", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07e93e0352eaec3c9b7e195b45e46fc6bc95bfa7", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46ad8f09aaa117b0ea4f1ce4d6ae239af341aa5b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98878cf2f9655172cdf32cc0a9edc00a25e4ca4e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e0ee1036ad2c931eb6aa78eca4547f946465003", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=557dd207ca5a81d20dfd562b3da884c367cef3de", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e75e7d52fe47a8c38dce79416de2f6ceba8d3a7e", "width": 1080, "height": 607}], "variants": {}, "id": "fGnKi5YB-vxBlAhbeSiM_drqVykiurxKM0A3j_rn0ds"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k5p3p", "is_robot_indexable": true, "report_reasons": null, "author": "shadowfax1007", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k5p3p/how_would_i_best_archive_the_pokemon_fossil_museum/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k5p3p/how_would_i_best_archive_the_pokemon_fossil_museum/", "subreddit_subscribers": 667125, "created_utc": 1674567551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all! Is there a cost-efficient way of scraping all Tiktok profiles with at least 1000 followers? Looking to scrape publicly available info such as location, followers count, bio description, etc.\n\nProxy rotations are too expensive for crawling all of Tiktok. Wondering how did [Modash.io](https://modash.io/) do it.. Thanks.", "author_fullname": "t2_7kzwkpfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TikTok Scraper Circumventing Captcha?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kbwhj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674583881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! Is there a cost-efficient way of scraping all Tiktok profiles with at least 1000 followers? Looking to scrape publicly available info such as location, followers count, bio description, etc.&lt;/p&gt;\n\n&lt;p&gt;Proxy rotations are too expensive for crawling all of Tiktok. Wondering how did &lt;a href=\"https://modash.io/\"&gt;Modash.io&lt;/a&gt; do it.. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?auto=webp&amp;v=enabled&amp;s=ed954da66f4723760dc2c89ce1545147412f968b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f06ef51e31a8656f91bff7663c06fbe3bcaa7967", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07ac42222bcc6b7ffe596ee8553dab8f02dc6cfe", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74eaf7134130675ede79fcaa6edeb4c38bab74d0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=afe22e6e2ddef5f5344ff00640ec8bbbd209a1a2", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc3df65675ee1fa61f14c41740ac1c3b884d86f2", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0288dba92b80cd65c221517a3d9cab170af34c14", "width": 1080, "height": 567}], "variants": {}, "id": "-l5_PZcs8KMe4kCpXmzXyiV8mH9uxR7V1pVa6LLZ2N8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kbwhj", "is_robot_indexable": true, "report_reasons": null, "author": "jkay18", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kbwhj/tiktok_scraper_circumventing_captcha/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kbwhj/tiktok_scraper_circumventing_captcha/", "subreddit_subscribers": 667125, "created_utc": 1674583881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not planning on using them constantly, I'd just grab what I need and copy it over to my main drive whenever I feel like it. Wouldn't be read from the drives either. Looking for pure storage.", "author_fullname": "t2_dsagmsmp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External HDDs or internals better for ROM and ISO hoarding?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k2mh2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674556760.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not planning on using them constantly, I&amp;#39;d just grab what I need and copy it over to my main drive whenever I feel like it. Wouldn&amp;#39;t be read from the drives either. Looking for pure storage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k2mh2", "is_robot_indexable": true, "report_reasons": null, "author": "metatron_ebooks", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k2mh2/external_hdds_or_internals_better_for_rom_and_iso/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k2mh2/external_hdds_or_internals_better_for_rom_and_iso/", "subreddit_subscribers": 667125, "created_utc": 1674556760.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I ran a scan on my nas and cloud drives after pulling it all together into one ball and trying to sift it out. I got A LOT of files in multiple places and need to clean it up. I was wondering if there is a good tool for dealing with like 40-50tbs worth of data that needs a heavy dedupping?", "author_fullname": "t2_dqoru", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Massive Dedup Job, Which Tool Is Best?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10keg0a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674590049.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I ran a scan on my nas and cloud drives after pulling it all together into one ball and trying to sift it out. I got A LOT of files in multiple places and need to clean it up. I was wondering if there is a good tool for dealing with like 40-50tbs worth of data that needs a heavy dedupping?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "64TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10keg0a", "is_robot_indexable": true, "report_reasons": null, "author": "RiffyDivine2", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10keg0a/massive_dedup_job_which_tool_is_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10keg0a/massive_dedup_job_which_tool_is_best/", "subreddit_subscribers": 667125, "created_utc": 1674590049.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Claimed 8, 6TB SAS harddrives from a retired SAN from work. Not interested in creating a new NAS, Server, or Individual drive storage with them. I just want a simple RAID 1 or 5 for a bunch of data at home for a simple backup solution that can plug-in to USB or similar to my Macbook Pro.\n\nMy first research brought me to something like this [Highpoint Rocketstor](https://www.highpoint-tech.com/rs6628a-catalog), but I'm not prepared to spend $1300 and I don't need the speed. I already have an SSD RAID setup for my video editing.\n\nAnyone have a simple backup solution or advice? I also don't need to use all 8 drives if there's something else out there like a 4 or 5 bay enclosure. There are so many low-cost Enclosure options for SATA drives but these SAS drives make it tricky.", "author_fullname": "t2_b1slz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on 8, 6TB SAS drives I just acquired", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ke5py", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674589344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Claimed 8, 6TB SAS harddrives from a retired SAN from work. Not interested in creating a new NAS, Server, or Individual drive storage with them. I just want a simple RAID 1 or 5 for a bunch of data at home for a simple backup solution that can plug-in to USB or similar to my Macbook Pro.&lt;/p&gt;\n\n&lt;p&gt;My first research brought me to something like this &lt;a href=\"https://www.highpoint-tech.com/rs6628a-catalog\"&gt;Highpoint Rocketstor&lt;/a&gt;, but I&amp;#39;m not prepared to spend $1300 and I don&amp;#39;t need the speed. I already have an SSD RAID setup for my video editing.&lt;/p&gt;\n\n&lt;p&gt;Anyone have a simple backup solution or advice? I also don&amp;#39;t need to use all 8 drives if there&amp;#39;s something else out there like a 4 or 5 bay enclosure. There are so many low-cost Enclosure options for SATA drives but these SAS drives make it tricky.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ke5py", "is_robot_indexable": true, "report_reasons": null, "author": "vanbeezy", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ke5py/advice_on_8_6tb_sas_drives_i_just_acquired/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ke5py/advice_on_8_6tb_sas_drives_i_just_acquired/", "subreddit_subscribers": 667125, "created_utc": 1674589344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am putting a TrueNAS system in my garage. Its a 4 bay system and I am putting in quite old used 8TB SAS disk in it. It will be hot, it will be cold, it will be dusty, and the system will only have 8GB of RAM total. Performance is not a concern, this is only for replicated backups\n\nRAIDZ2 or Striped Mirrors? I'm thinking RAIDZ2 so ANY second drive can fail. What do you think? Networking will just be dual 1Gb.", "author_fullname": "t2_6xkyp933", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Striped Mirror vs RAIDZ2 in 4 Disk harsh enviroment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kc7mz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674584621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am putting a TrueNAS system in my garage. Its a 4 bay system and I am putting in quite old used 8TB SAS disk in it. It will be hot, it will be cold, it will be dusty, and the system will only have 8GB of RAM total. Performance is not a concern, this is only for replicated backups&lt;/p&gt;\n\n&lt;p&gt;RAIDZ2 or Striped Mirrors? I&amp;#39;m thinking RAIDZ2 so ANY second drive can fail. What do you think? Networking will just be dual 1Gb.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kc7mz", "is_robot_indexable": true, "report_reasons": null, "author": "VviFMCgY", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kc7mz/striped_mirror_vs_raidz2_in_4_disk_harsh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kc7mz/striped_mirror_vs_raidz2_in_4_disk_harsh/", "subreddit_subscribers": 667125, "created_utc": 1674584621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Pulled this out of a server at work thats no longer in use. Never used a server drive before and thought that was a standard SATA connection. Plan was to use the drives with just a SATA to USB adapter. But thats not a standard SATA connector.\n\nAnyone know if there is an adapter for this so I can use it not in a SAS setup. As I type I discover its a SAS connector. Are there SAS to USB connecters by chance?", "author_fullname": "t2_3xqpnc72", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Dell Constellation 2 drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ytw16vxbm0ea1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 192, "x": 108, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db80e404c0dc170a7897dbd976f46d0bb86feb50"}, {"y": 384, "x": 216, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e0a625d366d5c56e674bb78c05a7adcee9fd2264"}, {"y": 568, "x": 320, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de94409716bbeaff0763ddd43c32d92f56f4817d"}, {"y": 1137, "x": 640, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=05c0d0259864dc24d4c318a030423e1f48ff34b5"}, {"y": 1706, "x": 960, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9258a28edc98e039af5d8735afad76878d96db0"}, {"y": 1920, "x": 1080, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f500819e128c89deb8525f4ad125010ce8583b7a"}], "s": {"y": 4032, "x": 2268, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=2268&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=47c208bd3767bde51d903251b1567e138af24335"}, "id": "ytw16vxbm0ea1"}, "mza1pxubm0ea1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 192, "x": 108, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ae8a1c399b0c2094a16e821d9c8733c275ba499"}, {"y": 384, "x": 216, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=590e038af70f3a6e509bd8fc271357761206344b"}, {"y": 568, "x": 320, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e4436bee6b40bab1d82f40bd094875d23756ecd0"}, {"y": 1137, "x": 640, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c9a1076f875033569a2cd01dfce4c9d90fda3e8"}, {"y": 1706, "x": 960, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=688fea5562715b61633e27129443b665175c4a9e"}, {"y": 1920, "x": 1080, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75181ccac4bcc07e32a7fe915b688ffec1933e73"}], "s": {"y": 4032, "x": 2268, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=2268&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6896c9adceb2692fc29364b4994099b3532b9d46"}, "id": "mza1pxubm0ea1"}, "vmjswepbm0ea1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 192, "x": 108, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d40a1e11124ff2b74acdeb8b5be4efda1f25ec8"}, {"y": 384, "x": 216, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70b878b7c56a52ea5e690ebdcb9a44768b4e52bb"}, {"y": 568, "x": 320, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f5ee0dd8485c179e3957a0d438c657ea66c70b3d"}, {"y": 1137, "x": 640, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72dc86e042dffb3b8b291dcdaea41088a11832e8"}, {"y": 1706, "x": 960, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd6c3a089e72d1e2d06867817eea08feaf95edd1"}, {"y": 1920, "x": 1080, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97203872590d6e255b0236e470eb98b469ffe1fe"}], "s": {"y": 4032, "x": 2268, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=2268&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=f002c7aaf1fafac2fc6e20996d32480f6dc4688a"}, "id": "vmjswepbm0ea1"}}, "name": "t3_10k8yg8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "ups": 8, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "vmjswepbm0ea1", "id": 233135198}, {"media_id": "mza1pxubm0ea1", "id": 233135199}, {"media_id": "ytw16vxbm0ea1", "id": 233135200}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/heLTMo0y_IhpGy50tmgOsbx076tmHrq7VAp_o4ENebg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674576555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pulled this out of a server at work thats no longer in use. Never used a server drive before and thought that was a standard SATA connection. Plan was to use the drives with just a SATA to USB adapter. But thats not a standard SATA connector.&lt;/p&gt;\n\n&lt;p&gt;Anyone know if there is an adapter for this so I can use it not in a SAS setup. As I type I discover its a SAS connector. Are there SAS to USB connecters by chance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/10k8yg8", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k8yg8", "is_robot_indexable": true, "report_reasons": null, "author": "steviefaux", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k8yg8/dell_constellation_2_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/10k8yg8", "subreddit_subscribers": 667125, "created_utc": 1674576555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've read this subreddit and there's very few posts related to this, and without info.\n\nIs it the equivalent to \"bad sectors\" from sata drives? If so, how much is \"bad\" for SAS drives? Is this the only parameter to look at when checking SMART status of SAS drives?\n\nrecently I got a 4000hs used Ultrastar drive from 2012 showing \"grown defect list: 2\", should i worry about this? Should I return it again and ask for another one?", "author_fullname": "t2_3gdxkeob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Grown defect list, what is it exactly? About SAS drives smart data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kapoo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674581283.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674581029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve read this subreddit and there&amp;#39;s very few posts related to this, and without info.&lt;/p&gt;\n\n&lt;p&gt;Is it the equivalent to &amp;quot;bad sectors&amp;quot; from sata drives? If so, how much is &amp;quot;bad&amp;quot; for SAS drives? Is this the only parameter to look at when checking SMART status of SAS drives?&lt;/p&gt;\n\n&lt;p&gt;recently I got a 4000hs used Ultrastar drive from 2012 showing &amp;quot;grown defect list: 2&amp;quot;, should i worry about this? Should I return it again and ask for another one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kapoo", "is_robot_indexable": true, "report_reasons": null, "author": "JoaGamo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kapoo/grown_defect_list_what_is_it_exactly_about_sas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kapoo/grown_defect_list_what_is_it_exactly_about_sas/", "subreddit_subscribers": 667125, "created_utc": 1674581029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I would like to buy some cheap unpacked clening tapes that spend time as spared never used. I have HP LTO5 drive and cleaning tapes are IBM 35L2086 but on front cover is written \"for use with ultrium 1 and 2 drives\". Are they going fit to my drive/ is it back compatible from LTO5 to LTO2?", "author_fullname": "t2_4mfclctu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTO cleaning tapes compatibility", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k1n97", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674552371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to buy some cheap unpacked clening tapes that spend time as spared never used. I have HP LTO5 drive and cleaning tapes are IBM 35L2086 but on front cover is written &amp;quot;for use with ultrium 1 and 2 drives&amp;quot;. Are they going fit to my drive/ is it back compatible from LTO5 to LTO2?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k1n97", "is_robot_indexable": true, "report_reasons": null, "author": "ruffpl", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k1n97/lto_cleaning_tapes_compatibility/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k1n97/lto_cleaning_tapes_compatibility/", "subreddit_subscribers": 667125, "created_utc": 1674552371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello.\n\nIn 2003, a process was made for storing various documents, including media files and records. As a base, a server with Windows XP, NTFS and a local copy of the system was used as a backup in case the main one had a problem.\n\nTime went by and by May, I have about 51 TB of files, including PDFs, media files, documents, records and log files that need to be stored for exactly 30 years (due to legislation in the country where I live and work).\n\nI am unsure which file system to use and which operating system to backup. I have been a Linux user for a little over 2 years, I know that btrfs and ext4 are excellent for this, but what about in the long run? Would I have any problems? Could anyone with experience in \"cold\" storage give me some insight?", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "51 TB file storage - Where can you start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jzqmw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": "", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674544062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.&lt;/p&gt;\n\n&lt;p&gt;In 2003, a process was made for storing various documents, including media files and records. As a base, a server with Windows XP, NTFS and a local copy of the system was used as a backup in case the main one had a problem.&lt;/p&gt;\n\n&lt;p&gt;Time went by and by May, I have about 51 TB of files, including PDFs, media files, documents, records and log files that need to be stored for exactly 30 years (due to legislation in the country where I live and work).&lt;/p&gt;\n\n&lt;p&gt;I am unsure which file system to use and which operating system to backup. I have been a Linux user for a little over 2 years, I know that btrfs and ext4 are excellent for this, but what about in the long run? Would I have any problems? Could anyone with experience in &amp;quot;cold&amp;quot; storage give me some insight?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jzqmw", "is_robot_indexable": true, "report_reasons": null, "author": "[deleted]", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10jzqmw/51_tb_file_storage_where_can_you_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jzqmw/51_tb_file_storage_where_can_you_start/", "subreddit_subscribers": 667125, "created_utc": 1674544062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I'm trying to get ESXI to recognize the drives attached to my LSI HBA. Was playing around in the config utility and naively thought maybe they needed to be formatted before being fulyl configured/setup. Kicked off the format through the LSI BIOS and only then thought about investigating how long it will take..... \n\n&amp;#x200B;\n\nNow that I see it will be days, can I escape this? Because of how basic this BIOS is, it seems the only way to stop it is to power cycle... is that safe?", "author_fullname": "t2_55fv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Formatting 6TB drive from LSI card, can I quit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10kmzgf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674611817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m trying to get ESXI to recognize the drives attached to my LSI HBA. Was playing around in the config utility and naively thought maybe they needed to be formatted before being fulyl configured/setup. Kicked off the format through the LSI BIOS and only then thought about investigating how long it will take..... &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Now that I see it will be days, can I escape this? Because of how basic this BIOS is, it seems the only way to stop it is to power cycle... is that safe?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kmzgf", "is_robot_indexable": true, "report_reasons": null, "author": "citrus_based_arson", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kmzgf/formatting_6tb_drive_from_lsi_card_can_i_quit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kmzgf/formatting_6tb_drive_from_lsi_card_can_i_quit/", "subreddit_subscribers": 667125, "created_utc": 1674611817.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi\n\nI am trying to wipe a drive to resell it. I tried to wipe the drive first with following command in Windows:  `format D: /fs:NTFS /p:1`. However, I received an error.\n\nAfter multiple tries to 0 the drive in Windows, I tried to give it a shot with dban. Nontheless, I still get errors (only when also verifying the last pass).\n\nI'm unsure now what that errors and warning really mean in the dban logs. Is the drive failing? I wiped an SSD before with he same dban USB-drive and it worked perfectly. Would be nice if anyone could point me in the right direction on what I should do now. :)\n\nThe log:\n\n    [ 8713.513712] WARNING: CPU: 3 PID: 495 at fs/block_dev.c:67 bdev_inode_switch_bdi+0x57/0x60()\n    [ 8713.513713] Modules linked in: joydev usb_storage ppdev microcode psmouse pcspkr serio_raw i2c_piix4 r8169 e1000e mii ptp pps_core xhci_pci ahci xhci_hcd libahci shpchp tpm_tis tpm parport_pc parport\n    [ 8713.513723] CPU: 3 PID: 495 Comm: dwipe Tainted: G        W      3.18.6-dban #2\n    [ 8713.513724] Hardware name: Micro-Star International Co., Ltd MS-7B86/B450-A PRO MAX (MS-7B86), BIOS M.50 11/07/2019\n    [ 8713.513725]  00000000 00000000 c1f3fe7c c137ec6c 00000000 c1f3feac c103bb64 c1439bcc\n    [ 8713.513726]  00000003 000001ef c1441de0 00000043 c10fe1a7 c10fe1a7 f5a106dc f5a1068c\n    [ 8713.513728]  c14c5440 c1f3febc c103bbfd 00000009 00000000 c1f3fed0 c10fe1a7 f5a10600\n    [ 8713.513729] Call Trace:\n    [ 8713.513732]  [&lt;c137ec6c&gt;] dump_stack+0x41/0x52\n    [ 8713.513734]  [&lt;c103bb64&gt;] warn_slowpath_common+0x74/0x90\n    [ 8713.513735]  [&lt;c10fe1a7&gt;] ? bdev_inode_switch_bdi+0x57/0x60\n    [ 8713.513736]  [&lt;c10fe1a7&gt;] ? bdev_inode_switch_bdi+0x57/0x60\n    [ 8713.513737]  [&lt;c103bbfd&gt;] warn_slowpath_null+0x1d/0x20\n    [ 8713.513738]  [&lt;c10fe1a7&gt;] bdev_inode_switch_bdi+0x57/0x60\n    [ 8713.513739]  [&lt;c10febc9&gt;] __blkdev_put+0x59/0x130\n    [ 8713.513740]  [&lt;c10ff33b&gt;] blkdev_put+0x3b/0xf0\n    [ 8713.513741]  [&lt;c10ff48a&gt;] blkdev_close+0x1a/0x20\n    [ 8713.513742]  [&lt;c10d4e8a&gt;] __fput+0xba/0x1a0\n    [ 8713.513743]  [&lt;c10d4fa8&gt;] ____fput+0x8/0x10\n    [ 8713.513745]  [&lt;c104f519&gt;] task_work_run+0x89/0xa0\n    [ 8713.513746]  [&lt;c103cb4a&gt;] do_exit+0x22a/0x880\n    [ 8713.513747]  [&lt;c10d4fa8&gt;] ? ____fput+0x8/0x10\n    [ 8713.513748]  [&lt;c104f506&gt;] ? task_work_run+0x76/0xa0\n    [ 8713.513749]  [&lt;c103dd11&gt;] SyS_exit+0x11/0x20\n    [ 8713.513750]  [&lt;c1382f9e&gt;] syscall_call+0x7/0x7\n    [ 8713.513751] ---[ end trace e8927cac9cbec423 ]---\n    [ 8713.513795] sd 1:0:0:0: [sdb] READ CAPACITY(16) failed\n    [ 8713.513796] sd 1:0:0:0: [sdb]  \n    [ 8713.513796] Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK\n    [ 8713.513797] sd 1:0:0:0: [sdb] Sense not available.\n    [ 8713.513803] sd 1:0:0:0: [sdb] READ CAPACITY failed\n    [ 8713.513804] sd 1:0:0:0: [sdb]  \n    [ 8713.513804] Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK\n    [ 8713.513805] sd 1:0:0:0: [sdb] Sense not available.\n    [ 8713.513814] sdb: detected capacity change from 1000204886016 to 0\n    DBAN logs:\n    [2023/01/22 20:18:23] dwipe: notice: Program loaded.\n    [2023/01/22 20:18:23] dwipe: notice: Opened entropy source '/dev/urandom'.\n    [2023/01/22 20:18:23] dwipe: info: Device '/dev/sdc' has sector size 512.\n    [2023/01/22 20:18:23] dwipe: warning: Changing '/dev/sdc' block size from 4096 to 512.\n    [2023/01/22 20:18:23] dwipe: info: Device '/dev/sdc' is size 16071852032.\n    [2023/01/22 20:18:23] dwipe: info: Device '/dev/sdb' has sector size 512.\n    [2023/01/22 20:18:23] dwipe: warning: Changing '/dev/sdb' block size from 4096 to 512.\n    [2023/01/22 20:18:23] dwipe: info: Device '/dev/sdb' is size 1000204886016.\n    [2023/01/22 20:18:23] dwipe: info: Device '/dev/sda' has sector size 512.\n    [2023/01/22 20:18:23] dwipe: warning: Changing '/dev/sda' block size from 4096 to 512.\n    [2023/01/22 20:18:23] dwipe: info: Device '/dev/sda' is size 250059350016.\n    [2023/01/22 20:20:01] dwipe: notice: Invoking method 'Quick Erase' on device '/dev/sdb'.\n    [2023/01/22 20:20:01] dwipe: notice: Starting round 1 of 1 on device '/dev/sdb'.\n    [2023/01/22 20:20:01] dwipe: notice: Finished round 1 of 1 on device '/dev/sdb'.\n    [2023/01/22 20:20:01] dwipe: notice: Blanking device '/dev/sdb'.\n    [2023/01/22 22:42:13] dwipe: warning: dwipe_static_pass: The size of '/dev/sdb' is not a multiple of its block size 4096.\n    [2023/01/22 22:42:13] dwipe: error: dwipe_static_pass: fdatasync: Input/output error\n    [2023/01/22 22:42:13] dwipe: warning: Buffer flush failure on '/dev/sdb'.\n    [2023/01/22 22:42:13] dwipe: notice: Verifying that '/dev/sdb' is empty.\n    [2023/01/22 22:42:13] dwipe: error: dwipe_static_verify: fdatasync: Input/output error\n    [2023/01/22 22:42:13] dwipe: warning: Buffer flush failure on '/dev/sdb'.\n    [2023/01/22 22:42:13] dwipe: error: dwipe_static_verify: read: Input/output error\n    [2023/01/22 22:42:13] dwipe: error: Unable to read from '/dev/sdb'.\n    [2023/01/22 22:42:15] dwipe: notice: Wipe finished.\n    [2023/01/22 22:42:15] dwipe: notice: Wipe of device '/dev/sdb' incomplete.", "author_fullname": "t2_1ek0on2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wiping drive errors", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10keu8j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674591012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I am trying to wipe a drive to resell it. I tried to wipe the drive first with following command in Windows:  &lt;code&gt;format D: /fs:NTFS /p:1&lt;/code&gt;. However, I received an error.&lt;/p&gt;\n\n&lt;p&gt;After multiple tries to 0 the drive in Windows, I tried to give it a shot with dban. Nontheless, I still get errors (only when also verifying the last pass).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m unsure now what that errors and warning really mean in the dban logs. Is the drive failing? I wiped an SSD before with he same dban USB-drive and it worked perfectly. Would be nice if anyone could point me in the right direction on what I should do now. :)&lt;/p&gt;\n\n&lt;p&gt;The log:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[ 8713.513712] WARNING: CPU: 3 PID: 495 at fs/block_dev.c:67 bdev_inode_switch_bdi+0x57/0x60()\n[ 8713.513713] Modules linked in: joydev usb_storage ppdev microcode psmouse pcspkr serio_raw i2c_piix4 r8169 e1000e mii ptp pps_core xhci_pci ahci xhci_hcd libahci shpchp tpm_tis tpm parport_pc parport\n[ 8713.513723] CPU: 3 PID: 495 Comm: dwipe Tainted: G        W      3.18.6-dban #2\n[ 8713.513724] Hardware name: Micro-Star International Co., Ltd MS-7B86/B450-A PRO MAX (MS-7B86), BIOS M.50 11/07/2019\n[ 8713.513725]  00000000 00000000 c1f3fe7c c137ec6c 00000000 c1f3feac c103bb64 c1439bcc\n[ 8713.513726]  00000003 000001ef c1441de0 00000043 c10fe1a7 c10fe1a7 f5a106dc f5a1068c\n[ 8713.513728]  c14c5440 c1f3febc c103bbfd 00000009 00000000 c1f3fed0 c10fe1a7 f5a10600\n[ 8713.513729] Call Trace:\n[ 8713.513732]  [&amp;lt;c137ec6c&amp;gt;] dump_stack+0x41/0x52\n[ 8713.513734]  [&amp;lt;c103bb64&amp;gt;] warn_slowpath_common+0x74/0x90\n[ 8713.513735]  [&amp;lt;c10fe1a7&amp;gt;] ? bdev_inode_switch_bdi+0x57/0x60\n[ 8713.513736]  [&amp;lt;c10fe1a7&amp;gt;] ? bdev_inode_switch_bdi+0x57/0x60\n[ 8713.513737]  [&amp;lt;c103bbfd&amp;gt;] warn_slowpath_null+0x1d/0x20\n[ 8713.513738]  [&amp;lt;c10fe1a7&amp;gt;] bdev_inode_switch_bdi+0x57/0x60\n[ 8713.513739]  [&amp;lt;c10febc9&amp;gt;] __blkdev_put+0x59/0x130\n[ 8713.513740]  [&amp;lt;c10ff33b&amp;gt;] blkdev_put+0x3b/0xf0\n[ 8713.513741]  [&amp;lt;c10ff48a&amp;gt;] blkdev_close+0x1a/0x20\n[ 8713.513742]  [&amp;lt;c10d4e8a&amp;gt;] __fput+0xba/0x1a0\n[ 8713.513743]  [&amp;lt;c10d4fa8&amp;gt;] ____fput+0x8/0x10\n[ 8713.513745]  [&amp;lt;c104f519&amp;gt;] task_work_run+0x89/0xa0\n[ 8713.513746]  [&amp;lt;c103cb4a&amp;gt;] do_exit+0x22a/0x880\n[ 8713.513747]  [&amp;lt;c10d4fa8&amp;gt;] ? ____fput+0x8/0x10\n[ 8713.513748]  [&amp;lt;c104f506&amp;gt;] ? task_work_run+0x76/0xa0\n[ 8713.513749]  [&amp;lt;c103dd11&amp;gt;] SyS_exit+0x11/0x20\n[ 8713.513750]  [&amp;lt;c1382f9e&amp;gt;] syscall_call+0x7/0x7\n[ 8713.513751] ---[ end trace e8927cac9cbec423 ]---\n[ 8713.513795] sd 1:0:0:0: [sdb] READ CAPACITY(16) failed\n[ 8713.513796] sd 1:0:0:0: [sdb]  \n[ 8713.513796] Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK\n[ 8713.513797] sd 1:0:0:0: [sdb] Sense not available.\n[ 8713.513803] sd 1:0:0:0: [sdb] READ CAPACITY failed\n[ 8713.513804] sd 1:0:0:0: [sdb]  \n[ 8713.513804] Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK\n[ 8713.513805] sd 1:0:0:0: [sdb] Sense not available.\n[ 8713.513814] sdb: detected capacity change from 1000204886016 to 0\nDBAN logs:\n[2023/01/22 20:18:23] dwipe: notice: Program loaded.\n[2023/01/22 20:18:23] dwipe: notice: Opened entropy source &amp;#39;/dev/urandom&amp;#39;.\n[2023/01/22 20:18:23] dwipe: info: Device &amp;#39;/dev/sdc&amp;#39; has sector size 512.\n[2023/01/22 20:18:23] dwipe: warning: Changing &amp;#39;/dev/sdc&amp;#39; block size from 4096 to 512.\n[2023/01/22 20:18:23] dwipe: info: Device &amp;#39;/dev/sdc&amp;#39; is size 16071852032.\n[2023/01/22 20:18:23] dwipe: info: Device &amp;#39;/dev/sdb&amp;#39; has sector size 512.\n[2023/01/22 20:18:23] dwipe: warning: Changing &amp;#39;/dev/sdb&amp;#39; block size from 4096 to 512.\n[2023/01/22 20:18:23] dwipe: info: Device &amp;#39;/dev/sdb&amp;#39; is size 1000204886016.\n[2023/01/22 20:18:23] dwipe: info: Device &amp;#39;/dev/sda&amp;#39; has sector size 512.\n[2023/01/22 20:18:23] dwipe: warning: Changing &amp;#39;/dev/sda&amp;#39; block size from 4096 to 512.\n[2023/01/22 20:18:23] dwipe: info: Device &amp;#39;/dev/sda&amp;#39; is size 250059350016.\n[2023/01/22 20:20:01] dwipe: notice: Invoking method &amp;#39;Quick Erase&amp;#39; on device &amp;#39;/dev/sdb&amp;#39;.\n[2023/01/22 20:20:01] dwipe: notice: Starting round 1 of 1 on device &amp;#39;/dev/sdb&amp;#39;.\n[2023/01/22 20:20:01] dwipe: notice: Finished round 1 of 1 on device &amp;#39;/dev/sdb&amp;#39;.\n[2023/01/22 20:20:01] dwipe: notice: Blanking device &amp;#39;/dev/sdb&amp;#39;.\n[2023/01/22 22:42:13] dwipe: warning: dwipe_static_pass: The size of &amp;#39;/dev/sdb&amp;#39; is not a multiple of its block size 4096.\n[2023/01/22 22:42:13] dwipe: error: dwipe_static_pass: fdatasync: Input/output error\n[2023/01/22 22:42:13] dwipe: warning: Buffer flush failure on &amp;#39;/dev/sdb&amp;#39;.\n[2023/01/22 22:42:13] dwipe: notice: Verifying that &amp;#39;/dev/sdb&amp;#39; is empty.\n[2023/01/22 22:42:13] dwipe: error: dwipe_static_verify: fdatasync: Input/output error\n[2023/01/22 22:42:13] dwipe: warning: Buffer flush failure on &amp;#39;/dev/sdb&amp;#39;.\n[2023/01/22 22:42:13] dwipe: error: dwipe_static_verify: read: Input/output error\n[2023/01/22 22:42:13] dwipe: error: Unable to read from &amp;#39;/dev/sdb&amp;#39;.\n[2023/01/22 22:42:15] dwipe: notice: Wipe finished.\n[2023/01/22 22:42:15] dwipe: notice: Wipe of device &amp;#39;/dev/sdb&amp;#39; incomplete.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10keu8j", "is_robot_indexable": true, "report_reasons": null, "author": "gripfly", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10keu8j/wiping_drive_errors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10keu8j/wiping_drive_errors/", "subreddit_subscribers": 667125, "created_utc": 1674591012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So im trying to locally archive a website that focuses on archiving everything on the Ultra64 (codename for nintendo 64 before release) I have all offline functionality complete except one part, the Gallery\n\nThe website is divided by Home, Blog, Resources, Gallery and Getting Started and im having trouble with the gallery. The gallery has a collection of images on the Developement Kits and Parts and is subdivided by the name of the company that made the dev kit with a drop down menu when you hover the mouse over gallery (I can navigate to all the pages listed on the dropdown except 1). \n\nNow my problem is wget grabbed all the files and images in the Gallery dropdown menu except 1, Looking at the command prompt (Using Windows wget) I found that one page gave out a 404 error and was skipped so only 6 of the 7 companies have working offline links to their respective pages on the website im trying to archive\n\nI already tried pasting the actual link for the page im missing in wget and I still get error 404 which I dont get why considering that \n\n1. I can navigate the page when browsing online so its obviously not down and to verify this when i reload the page in my browser with dev tools enabled I get status 200 not 404.\n\n2. Doubt its wordpress and js as its used through the whole site which I successfully archived and all images are uploaded to the same folder according to the dev tools on the actual website I also checked all the APIs being used between the page that gave 404 and one that didnt and all the names and versions matched up\n\nMy theory is that the page uses a non standard character in the url but i dont see why that would be an issue but who am i to say since i dont know how wget works it, the character is \u00b5 and ive tried both with the actual character which is the decoded and the orignal which has this  \"%c2%b5\" substituted for \u00b5\n\nive been using wget -m -p -k --follow-ftp --random-wait -w 7 --tries=4 -e robots=off -U Mozilla/5.0 url which worked with absolutely everything on the website except this specific page and have tried putting the url with quotes as well\n\nif anyone wants to see the page im having difficulty with, the domain is the codename used for the nintendo 64 with ca as the ccTop Level Domain and its the kyoto microcomputer page under the gallery, all other links under the gallery worked fine for wget (Not actually linking the site in my post in case its impolite or against rules especially since its a hobby site)\n\nAlso if anyone knows of a better subreddit that would be able to help me out pls link it", "author_fullname": "t2_ejb61oh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting 404 on a specific page on website when using wget", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k8mmg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674575719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So im trying to locally archive a website that focuses on archiving everything on the Ultra64 (codename for nintendo 64 before release) I have all offline functionality complete except one part, the Gallery&lt;/p&gt;\n\n&lt;p&gt;The website is divided by Home, Blog, Resources, Gallery and Getting Started and im having trouble with the gallery. The gallery has a collection of images on the Developement Kits and Parts and is subdivided by the name of the company that made the dev kit with a drop down menu when you hover the mouse over gallery (I can navigate to all the pages listed on the dropdown except 1). &lt;/p&gt;\n\n&lt;p&gt;Now my problem is wget grabbed all the files and images in the Gallery dropdown menu except 1, Looking at the command prompt (Using Windows wget) I found that one page gave out a 404 error and was skipped so only 6 of the 7 companies have working offline links to their respective pages on the website im trying to archive&lt;/p&gt;\n\n&lt;p&gt;I already tried pasting the actual link for the page im missing in wget and I still get error 404 which I dont get why considering that &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I can navigate the page when browsing online so its obviously not down and to verify this when i reload the page in my browser with dev tools enabled I get status 200 not 404.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Doubt its wordpress and js as its used through the whole site which I successfully archived and all images are uploaded to the same folder according to the dev tools on the actual website I also checked all the APIs being used between the page that gave 404 and one that didnt and all the names and versions matched up&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My theory is that the page uses a non standard character in the url but i dont see why that would be an issue but who am i to say since i dont know how wget works it, the character is \u00b5 and ive tried both with the actual character which is the decoded and the orignal which has this  &amp;quot;%c2%b5&amp;quot; substituted for \u00b5&lt;/p&gt;\n\n&lt;p&gt;ive been using wget -m -p -k --follow-ftp --random-wait -w 7 --tries=4 -e robots=off -U Mozilla/5.0 url which worked with absolutely everything on the website except this specific page and have tried putting the url with quotes as well&lt;/p&gt;\n\n&lt;p&gt;if anyone wants to see the page im having difficulty with, the domain is the codename used for the nintendo 64 with ca as the ccTop Level Domain and its the kyoto microcomputer page under the gallery, all other links under the gallery worked fine for wget (Not actually linking the site in my post in case its impolite or against rules especially since its a hobby site)&lt;/p&gt;\n\n&lt;p&gt;Also if anyone knows of a better subreddit that would be able to help me out pls link it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k8mmg", "is_robot_indexable": true, "report_reasons": null, "author": "Bark_bark-im-a-doggo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k8mmg/getting_404_on_a_specific_page_on_website_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k8mmg/getting_404_on_a_specific_page_on_website_when/", "subreddit_subscribers": 667125, "created_utc": 1674575719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Man, this sub is a veritable cornucopia of knowledge.  Thanks to all of you for your help in my past couple of threads here.  Sorry for such a long post.\n\nI'm moving my business and expanding our operations.  I've decided it's time to upgrade all of my existing workflows and part of that is network infrastructure.  Larger, faster Synology unit, faster NIC's, faster switches, etc.\n\nWe do a ton of drive imaging using Macrium Reflect.  We used to just drag/copy clients' home folders to a backup folder on the NAS but there was an incident once(luckily we were able to resolve) where the copy didn't complete and the file system became corrupt in the process - scary.  So, despite the extra time it takes, when we are having anything to do with important data, EVERY OS PARTITION gets imaged first.  The way we've been doing this is every tech has their own workstation and dock which they do the imaging from.  I don't love the idea of doing other things on the machine that's imaging so I've decided I'd like to build a machine which is dedicated to imaging and can perform more than one image task at a time across multiple docks.  Does that sound good or bad?\n\nI don't know if I can run multiple instances of the same software be it Macrium or whatever, so I thought I would just run Proxmox and a bunch of VM's to do it?  I have a tendency to wildly overcomplicate things, so please...  your thoughts?", "author_fullname": "t2_6eiisul2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice on a data collection machine(basically a mass drive imaging rig)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k7q88", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674573306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Man, this sub is a veritable cornucopia of knowledge.  Thanks to all of you for your help in my past couple of threads here.  Sorry for such a long post.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m moving my business and expanding our operations.  I&amp;#39;ve decided it&amp;#39;s time to upgrade all of my existing workflows and part of that is network infrastructure.  Larger, faster Synology unit, faster NIC&amp;#39;s, faster switches, etc.&lt;/p&gt;\n\n&lt;p&gt;We do a ton of drive imaging using Macrium Reflect.  We used to just drag/copy clients&amp;#39; home folders to a backup folder on the NAS but there was an incident once(luckily we were able to resolve) where the copy didn&amp;#39;t complete and the file system became corrupt in the process - scary.  So, despite the extra time it takes, when we are having anything to do with important data, EVERY OS PARTITION gets imaged first.  The way we&amp;#39;ve been doing this is every tech has their own workstation and dock which they do the imaging from.  I don&amp;#39;t love the idea of doing other things on the machine that&amp;#39;s imaging so I&amp;#39;ve decided I&amp;#39;d like to build a machine which is dedicated to imaging and can perform more than one image task at a time across multiple docks.  Does that sound good or bad?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know if I can run multiple instances of the same software be it Macrium or whatever, so I thought I would just run Proxmox and a bunch of VM&amp;#39;s to do it?  I have a tendency to wildly overcomplicate things, so please...  your thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k7q88", "is_robot_indexable": true, "report_reasons": null, "author": "Pleaseclap4", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k7q88/looking_for_advice_on_a_data_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k7q88/looking_for_advice_on_a_data_collection/", "subreddit_subscribers": 667125, "created_utc": 1674573306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys, first post here looking for some help!I need to copy files to a disk, but I need the program to have \"resume\" function like downloaders like Jdownloader2 for example.   \nI want to copy files to an external USB drive but for some reason the disk sometimes fails when I'm copying big files to it.  \nI tested the disk with several disk checking tools and it shows no error at all. Even did surface test with no problem.  \nDo you guys know any copy tool for windows that has something like \"resume\" function but also needs to work if the USB external disk disconnects for any reason?  \nAlready tried with Teracopy and FileCopy but they don't have such function.  \n\n\nThanks!", "author_fullname": "t2_62608i1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copy file by parts - Windows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k7l1f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674572916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, first post here looking for some help!I need to copy files to a disk, but I need the program to have &amp;quot;resume&amp;quot; function like downloaders like Jdownloader2 for example.&lt;br/&gt;\nI want to copy files to an external USB drive but for some reason the disk sometimes fails when I&amp;#39;m copying big files to it.&lt;br/&gt;\nI tested the disk with several disk checking tools and it shows no error at all. Even did surface test with no problem.&lt;br/&gt;\nDo you guys know any copy tool for windows that has something like &amp;quot;resume&amp;quot; function but also needs to work if the USB external disk disconnects for any reason?&lt;br/&gt;\nAlready tried with Teracopy and FileCopy but they don&amp;#39;t have such function.  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k7l1f", "is_robot_indexable": true, "report_reasons": null, "author": "hikarusniper", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k7l1f/copy_file_by_parts_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k7l1f/copy_file_by_parts_windows/", "subreddit_subscribers": 667125, "created_utc": 1674572916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have some data drives and all are 8tb. I' got a deal on a 14tb drive and so it will become the parity drive. I was wondering if I could run the 14tb with a 2tb as parity and have the ability to recover two drives. Obviously if the 14tb goes I have nothing.\n\nI did multi drive parity when I first started with 4x2tb to cover 1x8tb failure. This is similar but a bit different.", "author_fullname": "t2_f8az5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does this is with snapraid make sense.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k63nk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674568761.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some data drives and all are 8tb. I&amp;#39; got a deal on a 14tb drive and so it will become the parity drive. I was wondering if I could run the 14tb with a 2tb as parity and have the ability to recover two drives. Obviously if the 14tb goes I have nothing.&lt;/p&gt;\n\n&lt;p&gt;I did multi drive parity when I first started with 4x2tb to cover 1x8tb failure. This is similar but a bit different.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k63nk", "is_robot_indexable": true, "report_reasons": null, "author": "light5out", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k63nk/does_this_is_with_snapraid_make_sense/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k63nk/does_this_is_with_snapraid_make_sense/", "subreddit_subscribers": 667125, "created_utc": 1674568761.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I need to backup my OS and also a storage drive but I'm running into issues with robocopy \n\nI've tried using \"Robocopy Sorce Destination /S /Copyall /xn /xo /xc \" But this doesn't update files if they have been changed, so now I started using \"Robocopy P: G: /MIR /b /copyall\" which runs into some system files and gets stuck in 30s loops \n\nAnd when I try backup normal folders on my OS, like videos, it doesn't seem to get everything, video folder is missing 8gb and 3 files, not sure where \n\nwhat do you guys use?", "author_fullname": "t2_lgib4i1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using robocopy to create rolling backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k5xbj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674568241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I need to backup my OS and also a storage drive but I&amp;#39;m running into issues with robocopy &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried using &amp;quot;Robocopy Sorce Destination /S /Copyall /xn /xo /xc &amp;quot; But this doesn&amp;#39;t update files if they have been changed, so now I started using &amp;quot;Robocopy P: G: /MIR /b /copyall&amp;quot; which runs into some system files and gets stuck in 30s loops &lt;/p&gt;\n\n&lt;p&gt;And when I try backup normal folders on my OS, like videos, it doesn&amp;#39;t seem to get everything, video folder is missing 8gb and 3 files, not sure where &lt;/p&gt;\n\n&lt;p&gt;what do you guys use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k5xbj", "is_robot_indexable": true, "report_reasons": null, "author": "Mockbubbles2628", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k5xbj/using_robocopy_to_create_rolling_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k5xbj/using_robocopy_to_create_rolling_backups/", "subreddit_subscribers": 667125, "created_utc": 1674568241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Quick query regarding whether this scenario is possible :\n\nI have 4 3tb discs in the EX4, running in RAID 5 currently.\n\nI need to switch the drives at some point to the DS420+ and thought to ask if i could do following :\n\n* Pull one drive from EX4 and format\n* Insert to DS420+ and setup as SHR (Synology Hybrid Raid) in readiness for other 3 drives\n* Copy data from EX4 to DS420+ (will fit)\n* Remove other 3 drives, format and insert into DS420+ SHR unit\n\nI'm currently backing up to a USB3 HDD through PC but it's taking a while - jus thinking out loud as to whether this would be a suitable solution also.\n\nThanks in advance.", "author_fullname": "t2_9gciy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching from WD EX4 to Synology DS420+", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k1mwd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674552329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Quick query regarding whether this scenario is possible :&lt;/p&gt;\n\n&lt;p&gt;I have 4 3tb discs in the EX4, running in RAID 5 currently.&lt;/p&gt;\n\n&lt;p&gt;I need to switch the drives at some point to the DS420+ and thought to ask if i could do following :&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pull one drive from EX4 and format&lt;/li&gt;\n&lt;li&gt;Insert to DS420+ and setup as SHR (Synology Hybrid Raid) in readiness for other 3 drives&lt;/li&gt;\n&lt;li&gt;Copy data from EX4 to DS420+ (will fit)&lt;/li&gt;\n&lt;li&gt;Remove other 3 drives, format and insert into DS420+ SHR unit&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m currently backing up to a USB3 HDD through PC but it&amp;#39;s taking a while - jus thinking out loud as to whether this would be a suitable solution also.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k1mwd", "is_robot_indexable": true, "report_reasons": null, "author": "manguish", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k1mwd/switching_from_wd_ex4_to_synology_ds420/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k1mwd/switching_from_wd_ex4_to_synology_ds420/", "subreddit_subscribers": 667125, "created_utc": 1674552329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I bought a Synology DS-720+ with two 6TB HDD, but I'm running low on storage. So I would like to increase its capacity. I heard about the Synology DX517 5-bay expansion unit, but it's way too expensive. Is there a cheaper way to increase my storage capacity ?\n\nThanks", "author_fullname": "t2_3r6m4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to increase the capacity of a Synology DS-720+ 2-bay NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k1f7q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674551395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I bought a Synology DS-720+ with two 6TB HDD, but I&amp;#39;m running low on storage. So I would like to increase its capacity. I heard about the Synology DX517 5-bay expansion unit, but it&amp;#39;s way too expensive. Is there a cheaper way to increase my storage capacity ?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k1f7q", "is_robot_indexable": true, "report_reasons": null, "author": "Harkonnen", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k1f7q/what_is_the_best_way_to_increase_the_capacity_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k1f7q/what_is_the_best_way_to_increase_the_capacity_of/", "subreddit_subscribers": 667125, "created_utc": 1674551395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently began backing up all of my decade-old CDs and DVDs as they may become unreadable or damaged. However, I have noticed that my DVD drive is no longer detecting these DVDs. I have tried using my dad's PC to check some of the DVDs and some are working while others are not. I am unsure if the issue is with both of my DVD writers. Can you please suggest a solution for getting them to work again? I have also tried using ISO buster and it shows the DVDs as blank.", "author_fullname": "t2_7s5s7atf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DVD not detecting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jxg0s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674535681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently began backing up all of my decade-old CDs and DVDs as they may become unreadable or damaged. However, I have noticed that my DVD drive is no longer detecting these DVDs. I have tried using my dad&amp;#39;s PC to check some of the DVDs and some are working while others are not. I am unsure if the issue is with both of my DVD writers. Can you please suggest a solution for getting them to work again? I have also tried using ISO buster and it shows the DVDs as blank.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jxg0s", "is_robot_indexable": true, "report_reasons": null, "author": "nagarajtg", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jxg0s/dvd_not_detecting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jxg0s/dvd_not_detecting/", "subreddit_subscribers": 667125, "created_utc": 1674535681.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It seems that if you grab videos from youtube and pornhub just after they've been released (im not talking of immediately released when only 360p may be available, but once 1080p is available), you get one file. but if you try and download the same files (i use jdownloader 2) after some time (days?), you get another file with a slightly different file size.\n\nJust to clarify i'm not talking of grabbing lives that have just finished streaming, and I'm not talking of jd grabbing sometimes both 'regular video' and '\\_hls video' that usually end up having very similar file sizes and quality.\n\nThe file size difference is small, however it's a shame because it messes up the easiest duplicate detection by file size (and file name may also end up being slightly different depending on your settings, and it's possible that sometimes even the autograbbed 'date' is different), and can also mess up jd 'already downloaded' detection.\n\nAnyone else encounter this and know why?", "author_fullname": "t2_111o3ncd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Youtube and PH seem to sometimes replace video files with very similar ones?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jxf4b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674535599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems that if you grab videos from youtube and pornhub just after they&amp;#39;ve been released (im not talking of immediately released when only 360p may be available, but once 1080p is available), you get one file. but if you try and download the same files (i use jdownloader 2) after some time (days?), you get another file with a slightly different file size.&lt;/p&gt;\n\n&lt;p&gt;Just to clarify i&amp;#39;m not talking of grabbing lives that have just finished streaming, and I&amp;#39;m not talking of jd grabbing sometimes both &amp;#39;regular video&amp;#39; and &amp;#39;_hls video&amp;#39; that usually end up having very similar file sizes and quality.&lt;/p&gt;\n\n&lt;p&gt;The file size difference is small, however it&amp;#39;s a shame because it messes up the easiest duplicate detection by file size (and file name may also end up being slightly different depending on your settings, and it&amp;#39;s possible that sometimes even the autograbbed &amp;#39;date&amp;#39; is different), and can also mess up jd &amp;#39;already downloaded&amp;#39; detection.&lt;/p&gt;\n\n&lt;p&gt;Anyone else encounter this and know why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jxf4b", "is_robot_indexable": true, "report_reasons": null, "author": "BitsAndBobs304", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jxf4b/youtube_and_ph_seem_to_sometimes_replace_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jxf4b/youtube_and_ph_seem_to_sometimes_replace_video/", "subreddit_subscribers": 667125, "created_utc": 1674535599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\nI've recently transferred my entire Google Drive storage (about 6 TB) to an external drive. I'm wondering if there is a way to effectively have a similar 'Google Drive' experience with my external drive. Is there software available (ideally free) that would essentially turn my external drive/computer into a server where I could read/write my files remotely? I figured if anyone would have an answer, it would be this group!\n\n&amp;#x200B;\n\nThank you for your time!", "author_fullname": "t2_n5y35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to allow remote access to my External Hard Drive/NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kixlp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674600911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently transferred my entire Google Drive storage (about 6 TB) to an external drive. I&amp;#39;m wondering if there is a way to effectively have a similar &amp;#39;Google Drive&amp;#39; experience with my external drive. Is there software available (ideally free) that would essentially turn my external drive/computer into a server where I could read/write my files remotely? I figured if anyone would have an answer, it would be this group!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kixlp", "is_robot_indexable": true, "report_reasons": null, "author": "JGoldz75", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kixlp/best_way_to_allow_remote_access_to_my_external/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kixlp/best_way_to_allow_remote_access_to_my_external/", "subreddit_subscribers": 667125, "created_utc": 1674600911.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got a very strange issue and would like to tap the hive mind for ideas.\n\n&amp;#x200B;\n\nI've got a 24-Bay 4U chassis I picked up from Alibaba ( [https://www.alibaba.com/product-detail/ZhenLoong-4u-24-bay-rackmount-hot\\_1600372384846.html](https://www.alibaba.com/product-detail/ZhenLoong-4u-24-bay-rackmount-hot_1600372384846.html) ) and the version I purchased has the 12Gb/s SAS/SATA backplane with LSI 3X36 expander.\n\n&amp;#x200B;\n\nI've got the backplane connected to a Supermicro H12SSL-CT Motherboard (with onboard Broadcom 3008 SAS3 controller flashed to IT mode).\n\n&amp;#x200B;\n\nThe whole shebang is powered by a 1000W SuperMicro PWS-1K25P-PQ PSU.\n\n&amp;#x200B;\n\nInstalled in the backplane are 18 drives:\n\n&amp;#x200B;\n\n\u2981 2x Intel S3510 120GB (SSDSC2BB120G6) SSD\n\n\u2981 4x SanDisk CloudSpeed Eco Gen II 2TB (SDLF1CRR-019T-1HA1) SSD\n\n\u2981 12x Seagate Exos X18 18TB (ST18000NM000J) HDD\n\n&amp;#x200B;\n\nNow for the issue...\n\n&amp;#x200B;\n\nWhen I hot-plug an additional spinning rust HDD into an open slot on the backplane the 4x SanDisk SSDs disconnect and then reconnect causing data corruption.  I've tried multiple different HDDs (as the new hot-plug device), different combinations of drive placement on the backplane, and two different HBAs and the behavior is the same.  The other 14 drives on the backplane don't ever seem to have any issues at all during the hot-plug procedure.\n\n&amp;#x200B;\n\nIf I hot-plug an additional SSD into an open slot on the backplane (as apposed to a spinning disk) everything seems to be fine and the SanDisk drives don't disconnect/reconnect.\n\n&amp;#x200B;\n\nFinally, if I move two of the SanDisk SSDs directly to the motherboard (leaving two on the backplane) and perform the hot-plug procedure with a spinning HDD only the two SanDisk drives connected to the backplane disconnect/reconnect while the two connected directly to the motherboard stay connected.\n\n&amp;#x200B;\n\nWhen the SanDisk SSDs disconnect I see the following entries in the syslog for each SanDisk SSD:\n\n    sd 8:0:12:0: device_block, handle(0x0016)\n    sd 8:0:12:0: device_unblock and setting to running, handle(0x0016)\n    sd 8:0:12:0: [sdm] Synchronizing SCSI cache\n    sd 8:0:12:0: [sdm] Synchronize Cache(10) failed: Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK\n    mpt3sas_cm0: mpt3sas_transport_port_remove: removed: sas_addr(0x500605b00001788e)\n    mpt3sas_cm0: removing handle(0x0016), sas_addr(0x500605b00001788e)\n    mpt3sas_cm0: enclosure logical id(0x500605b0000178bf), slot(14)\n    mpt3sas_cm0: enclosure level(0x0000), connector name(     )\n    mpt3sas_cm0: handle(0x16) sas_address(0x500605b00001788e) port_type(0x1)\n    scsi 8:0:18:0: Direct-Access     ATA      SDLF1CRR-019T-1H RPA1 PQ: 0 ANSI: 6\n    scsi 8:0:18:0: SATA: handle(0x0016), sas_addr(0x500605b00001788e), phy(14), device_name(0x0000000000000000)\n    scsi 8:0:18:0: enclosure logical id (0x500605b0000178bf), slot(14) \n    scsi 8:0:18:0: enclosure level(0x0000), connector name(     )\n    scsi 8:0:18:0: atapi(n), ncq(y), asyn_notify(n), smart(y), fua(y), sw_preserve(y)\n    scsi 8:0:18:0: qdepth(32), tagged(1), scsi_level(7), cmd_que(1)\n    sd 8:0:18:0: Attached scsi generic sg13 type 0\n    sd 8:0:18:0: Power-on or device reset occurred\n    end_device-8:0:18: add: handle(0x0016), sas_addr(0x500605b00001788e)\n    sd 8:0:18:0: [sdn] 3750748848 512-byte logical blocks: (1.92 TB/1.75 TiB)\n    sd 8:0:18:0: [sdn] 4096-byte physical blocks\n    sd 8:0:18:0: [sdn] Write Protect is off\n    sd 8:0:18:0: [sdn] Mode Sense: 9b 00 10 08\n    sd 8:0:18:0: [sdn] Write cache: enabled, read cache: enabled, supports DPO and FUA\n    sdn: sdn1 sdn2\n    sd 8:0:18:0: [sdn] Attached SCSI disk\n    sdn: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/sdn' failed with exit code 1.\n    sdn1: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/sdn1' failed with exit code 1.\n\n\n Is it possible there is an incompatibility between the backplane and the SanDisk drives?  Perhaps they're more sensitive to some power fluctuation or a command delay that takes place during the spinning HDD hot-plug?  Any ideas on how I could narrow this down further or log more useful data?", "author_fullname": "t2_xeiex", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strange backplane behavior, need help diagnosing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kis9j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674600534.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a very strange issue and would like to tap the hive mind for ideas.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a 24-Bay 4U chassis I picked up from Alibaba ( &lt;a href=\"https://www.alibaba.com/product-detail/ZhenLoong-4u-24-bay-rackmount-hot_1600372384846.html\"&gt;https://www.alibaba.com/product-detail/ZhenLoong-4u-24-bay-rackmount-hot_1600372384846.html&lt;/a&gt; ) and the version I purchased has the 12Gb/s SAS/SATA backplane with LSI 3X36 expander.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got the backplane connected to a Supermicro H12SSL-CT Motherboard (with onboard Broadcom 3008 SAS3 controller flashed to IT mode).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The whole shebang is powered by a 1000W SuperMicro PWS-1K25P-PQ PSU.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Installed in the backplane are 18 drives:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;\u2981 2x Intel S3510 120GB (SSDSC2BB120G6) SSD&lt;/p&gt;\n\n&lt;p&gt;\u2981 4x SanDisk CloudSpeed Eco Gen II 2TB (SDLF1CRR-019T-1HA1) SSD&lt;/p&gt;\n\n&lt;p&gt;\u2981 12x Seagate Exos X18 18TB (ST18000NM000J) HDD&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Now for the issue...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;When I hot-plug an additional spinning rust HDD into an open slot on the backplane the 4x SanDisk SSDs disconnect and then reconnect causing data corruption.  I&amp;#39;ve tried multiple different HDDs (as the new hot-plug device), different combinations of drive placement on the backplane, and two different HBAs and the behavior is the same.  The other 14 drives on the backplane don&amp;#39;t ever seem to have any issues at all during the hot-plug procedure.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If I hot-plug an additional SSD into an open slot on the backplane (as apposed to a spinning disk) everything seems to be fine and the SanDisk drives don&amp;#39;t disconnect/reconnect.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Finally, if I move two of the SanDisk SSDs directly to the motherboard (leaving two on the backplane) and perform the hot-plug procedure with a spinning HDD only the two SanDisk drives connected to the backplane disconnect/reconnect while the two connected directly to the motherboard stay connected.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;When the SanDisk SSDs disconnect I see the following entries in the syslog for each SanDisk SSD:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sd 8:0:12:0: device_block, handle(0x0016)\nsd 8:0:12:0: device_unblock and setting to running, handle(0x0016)\nsd 8:0:12:0: [sdm] Synchronizing SCSI cache\nsd 8:0:12:0: [sdm] Synchronize Cache(10) failed: Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK\nmpt3sas_cm0: mpt3sas_transport_port_remove: removed: sas_addr(0x500605b00001788e)\nmpt3sas_cm0: removing handle(0x0016), sas_addr(0x500605b00001788e)\nmpt3sas_cm0: enclosure logical id(0x500605b0000178bf), slot(14)\nmpt3sas_cm0: enclosure level(0x0000), connector name(     )\nmpt3sas_cm0: handle(0x16) sas_address(0x500605b00001788e) port_type(0x1)\nscsi 8:0:18:0: Direct-Access     ATA      SDLF1CRR-019T-1H RPA1 PQ: 0 ANSI: 6\nscsi 8:0:18:0: SATA: handle(0x0016), sas_addr(0x500605b00001788e), phy(14), device_name(0x0000000000000000)\nscsi 8:0:18:0: enclosure logical id (0x500605b0000178bf), slot(14) \nscsi 8:0:18:0: enclosure level(0x0000), connector name(     )\nscsi 8:0:18:0: atapi(n), ncq(y), asyn_notify(n), smart(y), fua(y), sw_preserve(y)\nscsi 8:0:18:0: qdepth(32), tagged(1), scsi_level(7), cmd_que(1)\nsd 8:0:18:0: Attached scsi generic sg13 type 0\nsd 8:0:18:0: Power-on or device reset occurred\nend_device-8:0:18: add: handle(0x0016), sas_addr(0x500605b00001788e)\nsd 8:0:18:0: [sdn] 3750748848 512-byte logical blocks: (1.92 TB/1.75 TiB)\nsd 8:0:18:0: [sdn] 4096-byte physical blocks\nsd 8:0:18:0: [sdn] Write Protect is off\nsd 8:0:18:0: [sdn] Mode Sense: 9b 00 10 08\nsd 8:0:18:0: [sdn] Write cache: enabled, read cache: enabled, supports DPO and FUA\nsdn: sdn1 sdn2\nsd 8:0:18:0: [sdn] Attached SCSI disk\nsdn: Process &amp;#39;/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/sdn&amp;#39; failed with exit code 1.\nsdn1: Process &amp;#39;/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/sdn1&amp;#39; failed with exit code 1.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Is it possible there is an incompatibility between the backplane and the SanDisk drives?  Perhaps they&amp;#39;re more sensitive to some power fluctuation or a command delay that takes place during the spinning HDD hot-plug?  Any ideas on how I could narrow this down further or log more useful data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kis9j", "is_robot_indexable": true, "report_reasons": null, "author": "yutlin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kis9j/strange_backplane_behavior_need_help_diagnosing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kis9j/strange_backplane_behavior_need_help_diagnosing/", "subreddit_subscribers": 667125, "created_utc": 1674600534.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a mix of old SATA SSD that range from 500MB to 4TB, all Samsungs, one QVO 4TB and I'd like to use them as backup.\n\nFrom what I read, SSD rarely fail, like mechanical drives used to do. I've never had problems with them but one of them already has sectors marked by the controller as unusable.\n\nIs there a form of RAID that'll add some parity check and self correction on such a heterogenous ensemble? Oh and it's on Windows Server.", "author_fullname": "t2_54hox", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to do harden a bunch of old SSD to use as backup? (On windows server)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ki9mr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674599251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a mix of old SATA SSD that range from 500MB to 4TB, all Samsungs, one QVO 4TB and I&amp;#39;d like to use them as backup.&lt;/p&gt;\n\n&lt;p&gt;From what I read, SSD rarely fail, like mechanical drives used to do. I&amp;#39;ve never had problems with them but one of them already has sectors marked by the controller as unusable.&lt;/p&gt;\n\n&lt;p&gt;Is there a form of RAID that&amp;#39;ll add some parity check and self correction on such a heterogenous ensemble? Oh and it&amp;#39;s on Windows Server.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ki9mr", "is_robot_indexable": true, "report_reasons": null, "author": "mcdroid", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ki9mr/is_there_a_way_to_do_harden_a_bunch_of_old_ssd_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ki9mr/is_there_a_way_to_do_harden_a_bunch_of_old_ssd_to/", "subreddit_subscribers": 667125, "created_utc": 1674599251.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}