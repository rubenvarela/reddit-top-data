{"kind": "Listing", "data": {"after": "t3_10kpav7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve read the about page for this sub, I\u2019m not asking for myself but for the entire English speaking sumo community (20k strong on reddit alone) and the youtubers who put in a lot of work to livestream and edit videos.\n\nJust now, [Jason\u2019s all sumo\u2019s](https://twitter.com/Sumo_Jason?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor) channel was terminated. He has been uploading for over 10 years.\n\n4 days ago [Nattosumo](https://www.reddit.com/r/Sumo/comments/10gnxxg/natto_offline/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf), another huge YouTuber was also shut down for the 4th time. They\u2019ve started a new channel again but have lost their old videos.\n\nNHK (the Japanese Broadcasting Corporation) only posts a handful of **new** fights on their app. The JSA (Japanese sumo association) cherry picks 4-5 out of about 30 **new** fights to upload everyday and they\u2019ve only been doing that for the last year.\n\nOne member of the community u/lalalalandlalala has [already downloaded](https://www.reddit.com/r/Sumo/comments/10kodhp/jasons_allsumo_channel_has_been_terminated/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf) a good portion of Jason\u2019s content but other channels are already gone and more will soon follow.\n\nPewdiepie also recently uploaded a vlog where he went to see sumo live and many people have already joined the subreddit and subscribed to other channels because of it. These videos are very important resources for those who are just getting into the sport as well as those who have been following for a while and want to rewatch a retired wrestlers\u2019 career or the first tournament they ever watched.\n\nIf anyone can help out until the sumo community can figure out a permanent solution it is greatly appreciated.", "author_fullname": "t2_7tee1o5n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "10+ years of Sumo GONE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kqg9n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 195, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 195, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674628618.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674622336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve read the about page for this sub, I\u2019m not asking for myself but for the entire English speaking sumo community (20k strong on reddit alone) and the youtubers who put in a lot of work to livestream and edit videos.&lt;/p&gt;\n\n&lt;p&gt;Just now, &lt;a href=\"https://twitter.com/Sumo_Jason?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor\"&gt;Jason\u2019s all sumo\u2019s&lt;/a&gt; channel was terminated. He has been uploading for over 10 years.&lt;/p&gt;\n\n&lt;p&gt;4 days ago &lt;a href=\"https://www.reddit.com/r/Sumo/comments/10gnxxg/natto_offline/?utm_source=share&amp;amp;utm_medium=ios_app&amp;amp;utm_name=iossmf\"&gt;Nattosumo&lt;/a&gt;, another huge YouTuber was also shut down for the 4th time. They\u2019ve started a new channel again but have lost their old videos.&lt;/p&gt;\n\n&lt;p&gt;NHK (the Japanese Broadcasting Corporation) only posts a handful of &lt;strong&gt;new&lt;/strong&gt; fights on their app. The JSA (Japanese sumo association) cherry picks 4-5 out of about 30 &lt;strong&gt;new&lt;/strong&gt; fights to upload everyday and they\u2019ve only been doing that for the last year.&lt;/p&gt;\n\n&lt;p&gt;One member of the community &lt;a href=\"/u/lalalalandlalala\"&gt;u/lalalalandlalala&lt;/a&gt; has &lt;a href=\"https://www.reddit.com/r/Sumo/comments/10kodhp/jasons_allsumo_channel_has_been_terminated/?utm_source=share&amp;amp;utm_medium=ios_app&amp;amp;utm_name=iossmf\"&gt;already downloaded&lt;/a&gt; a good portion of Jason\u2019s content but other channels are already gone and more will soon follow.&lt;/p&gt;\n\n&lt;p&gt;Pewdiepie also recently uploaded a vlog where he went to see sumo live and many people have already joined the subreddit and subscribed to other channels because of it. These videos are very important resources for those who are just getting into the sport as well as those who have been following for a while and want to rewatch a retired wrestlers\u2019 career or the first tournament they ever watched.&lt;/p&gt;\n\n&lt;p&gt;If anyone can help out until the sumo community can figure out a permanent solution it is greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4X-a_ceia2SxJkWK2b_c9yvJI3Qm6qEe_LeDRI1tN4U.jpg?auto=webp&amp;v=enabled&amp;s=7cdd4b594d137260c066353877848fda0eb2d337", "width": 48, "height": 48}, "resolutions": [], "variants": {}, "id": "oDSesybUCGkQm6J5_VcMIq0eT8DqlQ4_0ys60lJeS1w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10kqg9n", "is_robot_indexable": true, "report_reasons": null, "author": "Maddy_km", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kqg9n/10_years_of_sumo_gone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kqg9n/10_years_of_sumo_gone/", "subreddit_subscribers": 667183, "created_utc": 1674622336.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all! Is there a cost-efficient way of scraping all Tiktok profiles with at least 1000 followers? Looking to scrape publicly available info such as location, followers count, bio description, etc.\n\nProxy rotations are too expensive for crawling all of Tiktok. Wondering how did [Modash.io](https://modash.io/) do it.. Thanks.", "author_fullname": "t2_7kzwkpfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TikTok Scraper Circumventing Captcha?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kbwhj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674583881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! Is there a cost-efficient way of scraping all Tiktok profiles with at least 1000 followers? Looking to scrape publicly available info such as location, followers count, bio description, etc.&lt;/p&gt;\n\n&lt;p&gt;Proxy rotations are too expensive for crawling all of Tiktok. Wondering how did &lt;a href=\"https://modash.io/\"&gt;Modash.io&lt;/a&gt; do it.. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?auto=webp&amp;v=enabled&amp;s=ed954da66f4723760dc2c89ce1545147412f968b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f06ef51e31a8656f91bff7663c06fbe3bcaa7967", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07ac42222bcc6b7ffe596ee8553dab8f02dc6cfe", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74eaf7134130675ede79fcaa6edeb4c38bab74d0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=afe22e6e2ddef5f5344ff00640ec8bbbd209a1a2", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc3df65675ee1fa61f14c41740ac1c3b884d86f2", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/RFFzvi7xGF-R_DboGBcFPaz_DSnYTvC4irjRslmRwKM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0288dba92b80cd65c221517a3d9cab170af34c14", "width": 1080, "height": 567}], "variants": {}, "id": "-l5_PZcs8KMe4kCpXmzXyiV8mH9uxR7V1pVa6LLZ2N8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kbwhj", "is_robot_indexable": true, "report_reasons": null, "author": "jkay18", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kbwhj/tiktok_scraper_circumventing_captcha/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kbwhj/tiktok_scraper_circumventing_captcha/", "subreddit_subscribers": 667183, "created_utc": 1674583881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_dskne", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scored a 10TB Seagate for $6.16/TB!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 136, "top_awarded_type": null, "hide_score": false, "name": "t3_10kquv2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "ups": 23, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2F9RWo6PD%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D900&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2F9RWo6PD&amp;image=https%3A%2F%2Fi.imgur.com%2FKsddFnr.jpg%3Ffb&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"600\" height=\"627\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 600, "scrolling": false, "height": 627}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": {"type": "imgur.com", "oembed": {"provider_url": "http://imgur.com", "description": "Post with 0 views. Seagate Backup Plus Shuck", "title": "Seagate Backup Plus Shuck", "url": "https://imgur.com/a/9RWo6PD", "type": "rich", "thumbnail_width": 600, "height": 627, "width": 600, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2F9RWo6PD%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D900&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2F9RWo6PD&amp;image=https%3A%2F%2Fi.imgur.com%2FKsddFnr.jpg%3Ffb&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"600\" height=\"627\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Imgur", "thumbnail_url": "https://i.imgur.com/KsddFnr.jpg?fb", "thumbnail_height": 315}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2F9RWo6PD%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D900&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2F9RWo6PD&amp;image=https%3A%2F%2Fi.imgur.com%2FKsddFnr.jpg%3Ffb&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"600\" height=\"627\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 600, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/10kquv2", "height": 627}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/H29ivFWhpGme-5ZxPg-ZT8tEVTuE3UjKGLk2XUWJ1cU.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674623661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgur.com/a/9RWo6PD", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5DIeCh-adMYWGp9jtnU6rbd0tiqxn4FWg-imCp1bAUM.jpg?auto=webp&amp;v=enabled&amp;s=5b552cdf7fa366a6faefb5fef2f904e8a13e4cbc", "width": 2092, "height": 2045}, "resolutions": [{"url": "https://external-preview.redd.it/5DIeCh-adMYWGp9jtnU6rbd0tiqxn4FWg-imCp1bAUM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b84fe023fd2ee8797a6f983426ef54cec6155e", "width": 108, "height": 105}, {"url": "https://external-preview.redd.it/5DIeCh-adMYWGp9jtnU6rbd0tiqxn4FWg-imCp1bAUM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45da174f219f51757b58afeed002b1890ebc6e1d", "width": 216, "height": 211}, {"url": "https://external-preview.redd.it/5DIeCh-adMYWGp9jtnU6rbd0tiqxn4FWg-imCp1bAUM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd0a43de3d3696c410cbec84efb47dd9b3bed045", "width": 320, "height": 312}, {"url": "https://external-preview.redd.it/5DIeCh-adMYWGp9jtnU6rbd0tiqxn4FWg-imCp1bAUM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c9a4b8b18f551b0a1be48969ac64bd7b73e77e0", "width": 640, "height": 625}, {"url": "https://external-preview.redd.it/5DIeCh-adMYWGp9jtnU6rbd0tiqxn4FWg-imCp1bAUM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8c339a287dacfd33348e968456df06201cd51d5", "width": 960, "height": 938}, {"url": "https://external-preview.redd.it/5DIeCh-adMYWGp9jtnU6rbd0tiqxn4FWg-imCp1bAUM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0674aac82e1c9b91c0977e0b049cc149991004da", "width": 1080, "height": 1055}], "variants": {}, "id": "FYS_hKPlwz6SZKSgfm_Uxto8AEdfgO8SGrjLsD1U8zw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "72TB usable, Supermicro 847, TrueNAS Core", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10kquv2", "is_robot_indexable": true, "report_reasons": null, "author": "TechGeek01", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10kquv2/scored_a_10tb_seagate_for_616tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgur.com/a/9RWo6PD", "subreddit_subscribers": 667183, "created_utc": 1674623661.0, "num_crossposts": 0, "media": {"type": "imgur.com", "oembed": {"provider_url": "http://imgur.com", "description": "Post with 0 views. Seagate Backup Plus Shuck", "title": "Seagate Backup Plus Shuck", "url": "https://imgur.com/a/9RWo6PD", "type": "rich", "thumbnail_width": 600, "height": 627, "width": 600, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2F9RWo6PD%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D900&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2F9RWo6PD&amp;image=https%3A%2F%2Fi.imgur.com%2FKsddFnr.jpg%3Ffb&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"600\" height=\"627\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Imgur", "thumbnail_url": "https://i.imgur.com/KsddFnr.jpg?fb", "thumbnail_height": 315}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is something that has had me stumped for a few years now. I know there have been a lot of threads on this topic in general, I know because I've just been looking through them; but nothing really seems to address this issue with storage and properly implementing a 3-2-1 backup strategy with large amounts of data, without making some big compromises, like cost, time between backups, time for recovery or not having a proper 3rd copy. None of which I'm happy with.\n\nRight now my total data storage is around 5.5TB and growing on a NAS (4+4+6+6 raid5). I should be able to keep it under 10TB for the foreseeable future.\n\nSecond copy is a daily differential backup is to a second NAS, with a 12TB HDD, daily, and scrubbing &amp; full backups weekly.\n\nThird is where is gets complicated. For the past 5 or so years, I have selectively picked out around 1TB of the most important data (photos, videos, documents, etc) I want to backup, and I do a differential backup to Backblaze B2. \n\nEverything else does not have a third copy, or is within the same house. \n\n* Just under 1TB is physical media I own, and while I do own it, recovering from original media would be tedious. \n* 2TB I can put into cold storage without issue (pair of archive / NAS HDDs?).\n* Last roughly 1.5TB is \"backups\". Part home / user folders that have files that exist nowhere else, or files that have a lot of duplicates; plus time machine, etc...\n\nThings that are worth noting:\n\n* Today I have a gigabit internet connection, but I am only renting and move every few years; and I could end up with a 50 megabit connection, like I had 5 years ago.\n* I live in a sub-tropical hot and humid environment.\n* I do see safety deposit boxes are available in my city for about $20/month. At that price not wide enough for a 3.5\" drive, and having to go there regularly to rotate HDDs would be a PITA.", "author_fullname": "t2_5g468zv7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost effective 3-2-1 backup strategy with 5TB or more?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10knqsx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674613997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is something that has had me stumped for a few years now. I know there have been a lot of threads on this topic in general, I know because I&amp;#39;ve just been looking through them; but nothing really seems to address this issue with storage and properly implementing a 3-2-1 backup strategy with large amounts of data, without making some big compromises, like cost, time between backups, time for recovery or not having a proper 3rd copy. None of which I&amp;#39;m happy with.&lt;/p&gt;\n\n&lt;p&gt;Right now my total data storage is around 5.5TB and growing on a NAS (4+4+6+6 raid5). I should be able to keep it under 10TB for the foreseeable future.&lt;/p&gt;\n\n&lt;p&gt;Second copy is a daily differential backup is to a second NAS, with a 12TB HDD, daily, and scrubbing &amp;amp; full backups weekly.&lt;/p&gt;\n\n&lt;p&gt;Third is where is gets complicated. For the past 5 or so years, I have selectively picked out around 1TB of the most important data (photos, videos, documents, etc) I want to backup, and I do a differential backup to Backblaze B2. &lt;/p&gt;\n\n&lt;p&gt;Everything else does not have a third copy, or is within the same house. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Just under 1TB is physical media I own, and while I do own it, recovering from original media would be tedious. &lt;/li&gt;\n&lt;li&gt;2TB I can put into cold storage without issue (pair of archive / NAS HDDs?).&lt;/li&gt;\n&lt;li&gt;Last roughly 1.5TB is &amp;quot;backups&amp;quot;. Part home / user folders that have files that exist nowhere else, or files that have a lot of duplicates; plus time machine, etc...&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Things that are worth noting:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Today I have a gigabit internet connection, but I am only renting and move every few years; and I could end up with a 50 megabit connection, like I had 5 years ago.&lt;/li&gt;\n&lt;li&gt;I live in a sub-tropical hot and humid environment.&lt;/li&gt;\n&lt;li&gt;I do see safety deposit boxes are available in my city for about $20/month. At that price not wide enough for a 3.5&amp;quot; drive, and having to go there regularly to rotate HDDs would be a PITA.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10knqsx", "is_robot_indexable": true, "report_reasons": null, "author": "iwashackedlastweek", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10knqsx/cost_effective_321_backup_strategy_with_5tb_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10knqsx/cost_effective_321_backup_strategy_with_5tb_or/", "subreddit_subscribers": 667183, "created_utc": 1674613997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Bought a used scanner thinking I\u2019d buy a replacement tray\u2026 but they are over $200 ea. \ud83e\udd26\ud83c\udffb\u200d\u2640\ufe0f", "author_fullname": "t2_9b531", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "This is a long shot, but does anyone have an Epson FF-640 or DS-860 and feel like 3d scanning the input (top) tray for me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kppca", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674619882.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bought a used scanner thinking I\u2019d buy a replacement tray\u2026 but they are over $200 ea. \ud83e\udd26\ud83c\udffb\u200d\u2640\ufe0f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kppca", "is_robot_indexable": true, "report_reasons": null, "author": "deaflemon", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kppca/this_is_a_long_shot_but_does_anyone_have_an_epson/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kppca/this_is_a_long_shot_but_does_anyone_have_an_epson/", "subreddit_subscribers": 667183, "created_utc": 1674619882.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Pulled this out of a server at work thats no longer in use. Never used a server drive before and thought that was a standard SATA connection. Plan was to use the drives with just a SATA to USB adapter. But thats not a standard SATA connector.\n\nAnyone know if there is an adapter for this so I can use it not in a SAS setup. As I type I discover its a SAS connector. Are there SAS to USB connecters by chance?", "author_fullname": "t2_3xqpnc72", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Dell Constellation 2 drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ytw16vxbm0ea1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 192, "x": 108, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db80e404c0dc170a7897dbd976f46d0bb86feb50"}, {"y": 384, "x": 216, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e0a625d366d5c56e674bb78c05a7adcee9fd2264"}, {"y": 568, "x": 320, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de94409716bbeaff0763ddd43c32d92f56f4817d"}, {"y": 1137, "x": 640, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=05c0d0259864dc24d4c318a030423e1f48ff34b5"}, {"y": 1706, "x": 960, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9258a28edc98e039af5d8735afad76878d96db0"}, {"y": 1920, "x": 1080, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f500819e128c89deb8525f4ad125010ce8583b7a"}], "s": {"y": 4032, "x": 2268, "u": "https://preview.redd.it/ytw16vxbm0ea1.jpg?width=2268&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=47c208bd3767bde51d903251b1567e138af24335"}, "id": "ytw16vxbm0ea1"}, "mza1pxubm0ea1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 192, "x": 108, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ae8a1c399b0c2094a16e821d9c8733c275ba499"}, {"y": 384, "x": 216, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=590e038af70f3a6e509bd8fc271357761206344b"}, {"y": 568, "x": 320, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e4436bee6b40bab1d82f40bd094875d23756ecd0"}, {"y": 1137, "x": 640, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c9a1076f875033569a2cd01dfce4c9d90fda3e8"}, {"y": 1706, "x": 960, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=688fea5562715b61633e27129443b665175c4a9e"}, {"y": 1920, "x": 1080, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75181ccac4bcc07e32a7fe915b688ffec1933e73"}], "s": {"y": 4032, "x": 2268, "u": "https://preview.redd.it/mza1pxubm0ea1.jpg?width=2268&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6896c9adceb2692fc29364b4994099b3532b9d46"}, "id": "mza1pxubm0ea1"}, "vmjswepbm0ea1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 192, "x": 108, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d40a1e11124ff2b74acdeb8b5be4efda1f25ec8"}, {"y": 384, "x": 216, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70b878b7c56a52ea5e690ebdcb9a44768b4e52bb"}, {"y": 568, "x": 320, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f5ee0dd8485c179e3957a0d438c657ea66c70b3d"}, {"y": 1137, "x": 640, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72dc86e042dffb3b8b291dcdaea41088a11832e8"}, {"y": 1706, "x": 960, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd6c3a089e72d1e2d06867817eea08feaf95edd1"}, {"y": 1920, "x": 1080, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97203872590d6e255b0236e470eb98b469ffe1fe"}], "s": {"y": 4032, "x": 2268, "u": "https://preview.redd.it/vmjswepbm0ea1.jpg?width=2268&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=f002c7aaf1fafac2fc6e20996d32480f6dc4688a"}, "id": "vmjswepbm0ea1"}}, "name": "t3_10k8yg8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 11, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "vmjswepbm0ea1", "id": 233135198}, {"media_id": "mza1pxubm0ea1", "id": 233135199}, {"media_id": "ytw16vxbm0ea1", "id": 233135200}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/heLTMo0y_IhpGy50tmgOsbx076tmHrq7VAp_o4ENebg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674576555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pulled this out of a server at work thats no longer in use. Never used a server drive before and thought that was a standard SATA connection. Plan was to use the drives with just a SATA to USB adapter. But thats not a standard SATA connector.&lt;/p&gt;\n\n&lt;p&gt;Anyone know if there is an adapter for this so I can use it not in a SAS setup. As I type I discover its a SAS connector. Are there SAS to USB connecters by chance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/10k8yg8", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k8yg8", "is_robot_indexable": true, "report_reasons": null, "author": "steviefaux", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k8yg8/dell_constellation_2_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/10k8yg8", "subreddit_subscribers": 667183, "created_utc": 1674576555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had a question. I have a very old Netgear NAS that I need to replace. I have been looking at what makes sense for my space and requirements.\n\nI have access to some HP MiniPC's and I was wondering if it would be possible to setup a small NAS using it and some large USB3 external drives? I don't need anything fancy, my storage is basically for light streaming duties internally on my network to my Plex or long term storage of pictures and STLs. \n\nI would also like to be able to share via NFS for a Linux box or two. Is that feasible to do with FreeNAS or something similar running on that kind of PC?\n\nThanks for any input/suggestions!", "author_fullname": "t2_5m0lb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS Setup on MiniPC with USB Drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ko6pa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674615321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had a question. I have a very old Netgear NAS that I need to replace. I have been looking at what makes sense for my space and requirements.&lt;/p&gt;\n\n&lt;p&gt;I have access to some HP MiniPC&amp;#39;s and I was wondering if it would be possible to setup a small NAS using it and some large USB3 external drives? I don&amp;#39;t need anything fancy, my storage is basically for light streaming duties internally on my network to my Plex or long term storage of pictures and STLs. &lt;/p&gt;\n\n&lt;p&gt;I would also like to be able to share via NFS for a Linux box or two. Is that feasible to do with FreeNAS or something similar running on that kind of PC?&lt;/p&gt;\n\n&lt;p&gt;Thanks for any input/suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ko6pa", "is_robot_indexable": true, "report_reasons": null, "author": "Undeadlord", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ko6pa/nas_setup_on_minipc_with_usb_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ko6pa/nas_setup_on_minipc_with_usb_drives/", "subreddit_subscribers": 667183, "created_utc": 1674615321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I ran a scan on my nas and cloud drives after pulling it all together into one ball and trying to sift it out. I got A LOT of files in multiple places and need to clean it up. I was wondering if there is a good tool for dealing with like 40-50tbs worth of data that needs a heavy dedupping?", "author_fullname": "t2_dqoru", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Massive Dedup Job, Which Tool Is Best?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10keg0a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674590049.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I ran a scan on my nas and cloud drives after pulling it all together into one ball and trying to sift it out. I got A LOT of files in multiple places and need to clean it up. I was wondering if there is a good tool for dealing with like 40-50tbs worth of data that needs a heavy dedupping?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "64TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10keg0a", "is_robot_indexable": true, "report_reasons": null, "author": "RiffyDivine2", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10keg0a/massive_dedup_job_which_tool_is_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10keg0a/massive_dedup_job_which_tool_is_best/", "subreddit_subscribers": 667183, "created_utc": 1674590049.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Claimed 8, 6TB SAS harddrives from a retired SAN from work. Not interested in creating a new NAS, Server, or Individual drive storage with them. I just want a simple RAID 1 or 5 for a bunch of data at home for a simple backup solution that can plug-in to USB or similar to my Macbook Pro.\n\nMy first research brought me to something like this [Highpoint Rocketstor](https://www.highpoint-tech.com/rs6628a-catalog), but I'm not prepared to spend $1300 and I don't need the speed. I already have an SSD RAID setup for my video editing.\n\nAnyone have a simple backup solution or advice? I also don't need to use all 8 drives if there's something else out there like a 4 or 5 bay enclosure. There are so many low-cost Enclosure options for SATA drives but these SAS drives make it tricky.", "author_fullname": "t2_b1slz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on 8, 6TB SAS drives I just acquired", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ke5py", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674589344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Claimed 8, 6TB SAS harddrives from a retired SAN from work. Not interested in creating a new NAS, Server, or Individual drive storage with them. I just want a simple RAID 1 or 5 for a bunch of data at home for a simple backup solution that can plug-in to USB or similar to my Macbook Pro.&lt;/p&gt;\n\n&lt;p&gt;My first research brought me to something like this &lt;a href=\"https://www.highpoint-tech.com/rs6628a-catalog\"&gt;Highpoint Rocketstor&lt;/a&gt;, but I&amp;#39;m not prepared to spend $1300 and I don&amp;#39;t need the speed. I already have an SSD RAID setup for my video editing.&lt;/p&gt;\n\n&lt;p&gt;Anyone have a simple backup solution or advice? I also don&amp;#39;t need to use all 8 drives if there&amp;#39;s something else out there like a 4 or 5 bay enclosure. There are so many low-cost Enclosure options for SATA drives but these SAS drives make it tricky.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ke5py", "is_robot_indexable": true, "report_reasons": null, "author": "vanbeezy", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ke5py/advice_on_8_6tb_sas_drives_i_just_acquired/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ke5py/advice_on_8_6tb_sas_drives_i_just_acquired/", "subreddit_subscribers": 667183, "created_utc": 1674589344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am putting a TrueNAS system in my garage. Its a 4 bay system and I am putting in quite old used 8TB SAS disk in it. It will be hot, it will be cold, it will be dusty, and the system will only have 8GB of RAM total. Performance is not a concern, this is only for replicated backups\n\nRAIDZ2 or Striped Mirrors? I'm thinking RAIDZ2 so ANY second drive can fail. What do you think? Networking will just be dual 1Gb.", "author_fullname": "t2_6xkyp933", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Striped Mirror vs RAIDZ2 in 4 Disk harsh enviroment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kc7mz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674584621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am putting a TrueNAS system in my garage. Its a 4 bay system and I am putting in quite old used 8TB SAS disk in it. It will be hot, it will be cold, it will be dusty, and the system will only have 8GB of RAM total. Performance is not a concern, this is only for replicated backups&lt;/p&gt;\n\n&lt;p&gt;RAIDZ2 or Striped Mirrors? I&amp;#39;m thinking RAIDZ2 so ANY second drive can fail. What do you think? Networking will just be dual 1Gb.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kc7mz", "is_robot_indexable": true, "report_reasons": null, "author": "VviFMCgY", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kc7mz/striped_mirror_vs_raidz2_in_4_disk_harsh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kc7mz/striped_mirror_vs_raidz2_in_4_disk_harsh/", "subreddit_subscribers": 667183, "created_utc": 1674584621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Last week I noticed one of my HDDs (Seagate ST2000VN004-2E4164) is shown as failing in Scrutiny. The problem seems to be with the \"High Fly Writes\" - here's a [screenshot](https://ibb.co/qxz2Mbc). Also, here's the S.M.A.R.T. [full log](https://haste.benjaminhollon.com/ayihesaceh.yaml) - OMV reports it as 'good' - and also, it showed no errors in my zfs pool (I detached it since I saw the report in scrutiny).\n\nSo, is it bad? I found some discussions on this, but I can't draw a definitive conclusion. So I'm looking for better knowledge.", "author_fullname": "t2_1q8ajrrd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Problems with a Seagate HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kvics", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674641987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last week I noticed one of my HDDs (Seagate ST2000VN004-2E4164) is shown as failing in Scrutiny. The problem seems to be with the &amp;quot;High Fly Writes&amp;quot; - here&amp;#39;s a &lt;a href=\"https://ibb.co/qxz2Mbc\"&gt;screenshot&lt;/a&gt;. Also, here&amp;#39;s the S.M.A.R.T. &lt;a href=\"https://haste.benjaminhollon.com/ayihesaceh.yaml\"&gt;full log&lt;/a&gt; - OMV reports it as &amp;#39;good&amp;#39; - and also, it showed no errors in my zfs pool (I detached it since I saw the report in scrutiny).&lt;/p&gt;\n\n&lt;p&gt;So, is it bad? I found some discussions on this, but I can&amp;#39;t draw a definitive conclusion. So I&amp;#39;m looking for better knowledge.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/x_akGUx57VbSj1sDWYvzLt-05WhZYGWn6sGniDMg9UE.jpg?auto=webp&amp;v=enabled&amp;s=2fd9edcaaa8ea2b9f2469940e7c3e9f5a10f21eb", "width": 1488, "height": 2076}, "resolutions": [{"url": "https://external-preview.redd.it/x_akGUx57VbSj1sDWYvzLt-05WhZYGWn6sGniDMg9UE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b650961773f29ae95c3970fd8bb64ef7272b7854", "width": 108, "height": 150}, {"url": "https://external-preview.redd.it/x_akGUx57VbSj1sDWYvzLt-05WhZYGWn6sGniDMg9UE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8db45ac40fe0018613cf0027cb4acc11db44c1fc", "width": 216, "height": 301}, {"url": "https://external-preview.redd.it/x_akGUx57VbSj1sDWYvzLt-05WhZYGWn6sGniDMg9UE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97aa97f75b9d0463ef3baf2716f0fadfd292c52c", "width": 320, "height": 446}, {"url": "https://external-preview.redd.it/x_akGUx57VbSj1sDWYvzLt-05WhZYGWn6sGniDMg9UE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43d1a1e40c9bdb99effcf84e4d0102fb8da711e2", "width": 640, "height": 892}, {"url": "https://external-preview.redd.it/x_akGUx57VbSj1sDWYvzLt-05WhZYGWn6sGniDMg9UE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c2c8364d243fe3c34cc26a943e5323151402fb23", "width": 960, "height": 1339}, {"url": "https://external-preview.redd.it/x_akGUx57VbSj1sDWYvzLt-05WhZYGWn6sGniDMg9UE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0dc55ed815024fe042d4b63f283eb2f9f3111cb7", "width": 1080, "height": 1506}], "variants": {}, "id": "ZKqa3fvHEs8EHpjQxqPk9eeuLHhT3NyOQIP5pIr9UVc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kvics", "is_robot_indexable": true, "report_reasons": null, "author": "_calm_bomb_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kvics/problems_with_a_seagate_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kvics/problems_with_a_seagate_hdd/", "subreddit_subscribers": 667183, "created_utc": 1674641987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Edit: It was pointed out that the 10x change in power actually means two times the perceived loudness. Sorry for the title, no clickbait, but honest mistake.\n\n&amp;#x200B;\n\nI wanted to expand my storage with the same model of drive that I bought a few years ago, specifically because they used to be advertised as very quiet, only to notice that the specs changed.\n\n&amp;#x200B;\n\nOld product manual from a price comparison website:\n\nIdle: 1.8 bels (typical) to 2.0 bels (max), Performance seek 2.6 bels (typical) to 2.8 bels (max).\n\n[https://gzhls.at/blob/ldb/8/3/0/0/4758b5d02c674e13c53b2d3255133667ac80.pdf](https://gzhls.at/blob/ldb/8/3/0/0/4758b5d02c674e13c53b2d3255133667ac80.pdf)\n\n&amp;#x200B;\n\nCurrent product manual from the Seagate website. Those are the values, which are shown as of now on the price comparison site as well:\n\nIdle: 2.8 bels (typical) to 3.0 bels (max), Performance seek 3.2 bels (typical) to 3.4 bels (max).\n\n[https://www.seagate.com/content/dam/seagate/migrated-assets/www-content/product-content/ironwolf/en-us/docs/202717100a.pdf](https://www.seagate.com/content/dam/seagate/migrated-assets/www-content/product-content/ironwolf/en-us/docs/202717100a.pdf)\n\n&amp;#x200B;\n\nSince Seagate marketing seems to lurk here for the givaways and such and there is no direct way to contact you (I tried): What gives? Why would the specs change within the exact same model number like that?\n\n&amp;#x200B;\n\nDo others have experience with the newer ST12000VN0008 drives and can tell, whether they actually got louder? 28dB idle seems outrageously noisy for a drive with helium filling.", "author_fullname": "t2_vgunau05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IronWolf 12TB (ST12000VN0008) idle now ten times louder?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kvs61", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674655230.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674643177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edit: It was pointed out that the 10x change in power actually means two times the perceived loudness. Sorry for the title, no clickbait, but honest mistake.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I wanted to expand my storage with the same model of drive that I bought a few years ago, specifically because they used to be advertised as very quiet, only to notice that the specs changed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Old product manual from a price comparison website:&lt;/p&gt;\n\n&lt;p&gt;Idle: 1.8 bels (typical) to 2.0 bels (max), Performance seek 2.6 bels (typical) to 2.8 bels (max).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://gzhls.at/blob/ldb/8/3/0/0/4758b5d02c674e13c53b2d3255133667ac80.pdf\"&gt;https://gzhls.at/blob/ldb/8/3/0/0/4758b5d02c674e13c53b2d3255133667ac80.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Current product manual from the Seagate website. Those are the values, which are shown as of now on the price comparison site as well:&lt;/p&gt;\n\n&lt;p&gt;Idle: 2.8 bels (typical) to 3.0 bels (max), Performance seek 3.2 bels (typical) to 3.4 bels (max).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.seagate.com/content/dam/seagate/migrated-assets/www-content/product-content/ironwolf/en-us/docs/202717100a.pdf\"&gt;https://www.seagate.com/content/dam/seagate/migrated-assets/www-content/product-content/ironwolf/en-us/docs/202717100a.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Since Seagate marketing seems to lurk here for the givaways and such and there is no direct way to contact you (I tried): What gives? Why would the specs change within the exact same model number like that?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do others have experience with the newer ST12000VN0008 drives and can tell, whether they actually got louder? 28dB idle seems outrageously noisy for a drive with helium filling.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kvs61", "is_robot_indexable": true, "report_reasons": null, "author": "Huge-Function4796", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kvs61/ironwolf_12tb_st12000vn0008_idle_now_ten_times/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kvs61/ironwolf_12tb_st12000vn0008_idle_now_ten_times/", "subreddit_subscribers": 667183, "created_utc": 1674643177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As maybe you know 6 years ago BitTorrent v2 \\[BEP\\]([https://www.bittorrent.org/beps/bep\\_0052.html](https://www.bittorrent.org/beps/bep_0052.html)) was created (they started working on it in 2008).\n\n&amp;#x200B;\n\nThe outstanding feature of this proposal was including sha2 hash of every file in torrent file (info dictionary).\n\n&amp;#x200B;\n\nSince then people were spamming in GitHub for developers to get a working feature where every file could be announced in DHT network to finally get rid of the different swarm problem.\n\n&amp;#x200B;\n\nWhere users could finally download files that were present in other swarms just by quering its individual hash. This would decentralize network more, swarms with valuable files wouldn't die fast, even when updating a torrent, where info hash differs, say tv series, adding new episode, all the previous seeds of swarm could contribute bandwidth and resistance to new swarms, there were \\[nice\\]([https://github.com/arvidn/libtorrent/issues/6289](https://github.com/arvidn/libtorrent/issues/6289)) proposals at libtorrent project (library which many clients we know use), as \\[f.e.\\]([https://github.com/arvidn/libtorrent/issues/6289#issuecomment-941760884](https://github.com/arvidn/libtorrent/issues/6289#issuecomment-941760884)) not announcing that a peer \\*has files\\* to not overwhelm network, but announcing that \\*he wants\\* them.\n\n&amp;#x200B;\n\n**What would data hoarders gain by this?**\n\nI wrote the post just after watching the DHExchange subreddit, people are looking for games, movies, materials.\n\nIf someone has saved a file, part of the resources, say an xml file from a lost game (for example [Neva Shut](https://www.reddit.com/r/flash/comments/jfvuen/nevashut/)), or say a movie booklet from a cd, or whatever, then you can search the network by the hash of these files, and find the missing resources or all the material. No additional resources, no indexers, and if you implement the reading/caching of file hashes inside archives, the chances of finding the file you need increases several times.\n\nBut since BitTorrent is community driven open-source project, as with most open source projects there is more interest in  having features be implemented than there is man-power to implement it.\n\nSo I'm, humbly from the face of BitTorrent community, ask developers to contribute with anything they're capable of.\n\n\\*\\*Share if possible, you can show your support to the feature by \\_\\_commenting\\_\\_ newly opened Github \\[\\*\\*issue\\*\\*\\]([https://github.com/arvidn/libtorrent/issues/7252#issue-1525596239](https://github.com/arvidn/libtorrent/issues/7252#issue-1525596239))!", "author_fullname": "t2_7w743fwz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do we have any developers willing to greatly help BitTorrent/Data hoarders community? [Interesting challenge]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10kzgxz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674655809.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As maybe you know 6 years ago BitTorrent v2 [BEP](&lt;a href=\"https://www.bittorrent.org/beps/bep_0052.html\"&gt;https://www.bittorrent.org/beps/bep_0052.html&lt;/a&gt;) was created (they started working on it in 2008).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The outstanding feature of this proposal was including sha2 hash of every file in torrent file (info dictionary).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Since then people were spamming in GitHub for developers to get a working feature where every file could be announced in DHT network to finally get rid of the different swarm problem.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Where users could finally download files that were present in other swarms just by quering its individual hash. This would decentralize network more, swarms with valuable files wouldn&amp;#39;t die fast, even when updating a torrent, where info hash differs, say tv series, adding new episode, all the previous seeds of swarm could contribute bandwidth and resistance to new swarms, there were [nice](&lt;a href=\"https://github.com/arvidn/libtorrent/issues/6289\"&gt;https://github.com/arvidn/libtorrent/issues/6289&lt;/a&gt;) proposals at libtorrent project (library which many clients we know use), as [f.e.](&lt;a href=\"https://github.com/arvidn/libtorrent/issues/6289#issuecomment-941760884\"&gt;https://github.com/arvidn/libtorrent/issues/6289#issuecomment-941760884&lt;/a&gt;) not announcing that a peer *has files* to not overwhelm network, but announcing that *he wants* them.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What would data hoarders gain by this?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I wrote the post just after watching the DHExchange subreddit, people are looking for games, movies, materials.&lt;/p&gt;\n\n&lt;p&gt;If someone has saved a file, part of the resources, say an xml file from a lost game (for example &lt;a href=\"https://www.reddit.com/r/flash/comments/jfvuen/nevashut/\"&gt;Neva Shut&lt;/a&gt;), or say a movie booklet from a cd, or whatever, then you can search the network by the hash of these files, and find the missing resources or all the material. No additional resources, no indexers, and if you implement the reading/caching of file hashes inside archives, the chances of finding the file you need increases several times.&lt;/p&gt;\n\n&lt;p&gt;But since BitTorrent is community driven open-source project, as with most open source projects there is more interest in  having features be implemented than there is man-power to implement it.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m, humbly from the face of BitTorrent community, ask developers to contribute with anything they&amp;#39;re capable of.&lt;/p&gt;\n\n&lt;p&gt;**Share if possible, you can show your support to the feature by __commenting__ newly opened Github [**issue**](&lt;a href=\"https://github.com/arvidn/libtorrent/issues/7252#issue-1525596239\"&gt;https://github.com/arvidn/libtorrent/issues/7252#issue-1525596239&lt;/a&gt;)!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "40TB Y.Disk", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10kzgxz", "is_robot_indexable": true, "report_reasons": null, "author": "Bear_with_a_hammer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10kzgxz/do_we_have_any_developers_willing_to_greatly_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kzgxz/do_we_have_any_developers_willing_to_greatly_help/", "subreddit_subscribers": 667183, "created_utc": 1674655809.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone. I would like to encrypt my whole (4 TB) hard drive without needing to decrypt it every time to access its contents. I only want to encrypt it against theft or unauthorized access and be able to add/remove/access content easily.  \n\n\nI did some research on Google, but couldn't find what I was looking for (Don't have the technical skill, or I don't know how to correct terms in order to find a better solution). The only interesting thing was locking the hard disk in bios, but I don't see that option. \n\nMy Specs are like this: Windows 10 Pro 64bit - 22H2  \nDesktop PC - 10700K, 32 GB RAM", "author_fullname": "t2_c3iorpk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for suggestions to encrypt the whole hard drive in case of theft or unauthorized access other than my own Windows OS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10knz75", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674614697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I would like to encrypt my whole (4 TB) hard drive without needing to decrypt it every time to access its contents. I only want to encrypt it against theft or unauthorized access and be able to add/remove/access content easily.  &lt;/p&gt;\n\n&lt;p&gt;I did some research on Google, but couldn&amp;#39;t find what I was looking for (Don&amp;#39;t have the technical skill, or I don&amp;#39;t know how to correct terms in order to find a better solution). The only interesting thing was locking the hard disk in bios, but I don&amp;#39;t see that option. &lt;/p&gt;\n\n&lt;p&gt;My Specs are like this: Windows 10 Pro 64bit - 22H2&lt;br/&gt;\nDesktop PC - 10700K, 32 GB RAM&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10knz75", "is_robot_indexable": true, "report_reasons": null, "author": "quetzakoatlus", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10knz75/looking_for_suggestions_to_encrypt_the_whole_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10knz75/looking_for_suggestions_to_encrypt_the_whole_hard/", "subreddit_subscribers": 667183, "created_utc": 1674614697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyone who downloads music from SoundCloud what do you use to download entire playlists?", "author_fullname": "t2_v8czybru", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SoundCloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kke5q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674604646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone who downloads music from SoundCloud what do you use to download entire playlists?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kke5q", "is_robot_indexable": true, "report_reasons": null, "author": "Fraudadvocate", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kke5q/soundcloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kke5q/soundcloud/", "subreddit_subscribers": 667183, "created_utc": 1674604646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am a patron of a 3d modeler that lists all of his models on a phpbb forum that is sorted by month, category, etc. Instead of manually downloading multiple hundreds of files, is there a way to scrape it or mass download the files and related info from the threads?", "author_fullname": "t2_3oosbomy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scraping phpBB site for downloads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10keo5l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674590594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a patron of a 3d modeler that lists all of his models on a phpbb forum that is sorted by month, category, etc. Instead of manually downloading multiple hundreds of files, is there a way to scrape it or mass download the files and related info from the threads?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10keo5l", "is_robot_indexable": true, "report_reasons": null, "author": "itsmiak", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10keo5l/scraping_phpbb_site_for_downloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10keo5l/scraping_phpbb_site_for_downloads/", "subreddit_subscribers": 667183, "created_utc": 1674590594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've read this subreddit and there's very few posts related to this, and without info.\n\nIs it the equivalent to \"bad sectors\" from sata drives? If so, how much is \"bad\" for SAS drives? Is this the only parameter to look at when checking SMART status of SAS drives?\n\nrecently I got a 4000hs used Ultrastar drive from 2012 showing \"grown defect list: 2\", should i worry about this? Should I return it again and ask for another one?", "author_fullname": "t2_3gdxkeob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Grown defect list, what is it exactly? About SAS drives smart data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kapoo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674581283.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674581029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve read this subreddit and there&amp;#39;s very few posts related to this, and without info.&lt;/p&gt;\n\n&lt;p&gt;Is it the equivalent to &amp;quot;bad sectors&amp;quot; from sata drives? If so, how much is &amp;quot;bad&amp;quot; for SAS drives? Is this the only parameter to look at when checking SMART status of SAS drives?&lt;/p&gt;\n\n&lt;p&gt;recently I got a 4000hs used Ultrastar drive from 2012 showing &amp;quot;grown defect list: 2&amp;quot;, should i worry about this? Should I return it again and ask for another one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kapoo", "is_robot_indexable": true, "report_reasons": null, "author": "JoaGamo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kapoo/grown_defect_list_what_is_it_exactly_about_sas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kapoo/grown_defect_list_what_is_it_exactly_about_sas/", "subreddit_subscribers": 667183, "created_utc": 1674581029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve been using the Synology DS918+ for several years and have loved it.  I\u2019m looking to upgrade. I\u2019ve looked at their DS3622xs+, but I\u2019ve read that Synology is now mandating that users purchase synology hard drives. I don\u2019t like it it so now I\u2019m trying to find other options. \n\nDo you guys and gals have any recommendations for other brands/companies that have products comparable to the previously mention synology disk stations?", "author_fullname": "t2_nwi6x26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which NAS to buy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ka9yd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674579938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been using the Synology DS918+ for several years and have loved it.  I\u2019m looking to upgrade. I\u2019ve looked at their DS3622xs+, but I\u2019ve read that Synology is now mandating that users purchase synology hard drives. I don\u2019t like it it so now I\u2019m trying to find other options. &lt;/p&gt;\n\n&lt;p&gt;Do you guys and gals have any recommendations for other brands/companies that have products comparable to the previously mention synology disk stations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ka9yd", "is_robot_indexable": true, "report_reasons": null, "author": "asdfredditusername", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ka9yd/which_nas_to_buy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ka9yd/which_nas_to_buy/", "subreddit_subscribers": 667183, "created_utc": 1674579938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ro5284id", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download complete file streaming into segments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": 21, "top_awarded_type": null, "hide_score": false, "name": "t3_10kw1ma", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/R39W2VoMNUtNJauX4gSmULqClg9jB_W7IsGbUfwJPJM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674644288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/oaktzv6k76ea1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/oaktzv6k76ea1.jpg?auto=webp&amp;v=enabled&amp;s=70dddcac0fd1204c66e63ef5e4a66ba287b9edc1", "width": 1134, "height": 176}, "resolutions": [{"url": "https://preview.redd.it/oaktzv6k76ea1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=177c3649ef4586ca120a78e65bdf0488aca030b0", "width": 108, "height": 16}, {"url": "https://preview.redd.it/oaktzv6k76ea1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e23804ac9594e9a7fea3927374b96d6edf9acc2a", "width": 216, "height": 33}, {"url": "https://preview.redd.it/oaktzv6k76ea1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8f9f89c5241a27f2c94508b7e6786bbb83140e7", "width": 320, "height": 49}, {"url": "https://preview.redd.it/oaktzv6k76ea1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8cdd71cfcc326911bc582f800a86ab9470c3aa66", "width": 640, "height": 99}, {"url": "https://preview.redd.it/oaktzv6k76ea1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ec050428de7f6859ac18dd59436d6ea2b0bfda4", "width": 960, "height": 148}, {"url": "https://preview.redd.it/oaktzv6k76ea1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d2cfd59431fac9fb3e1a6f93d7cd74c5417c746", "width": 1080, "height": 167}], "variants": {}, "id": "d1CLSPvuJXJV2EY77ir-D8haQ4Qv_pbvx7nUG-a4qUw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kw1ma", "is_robot_indexable": true, "report_reasons": null, "author": "OmnisPrime", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kw1ma/how_to_download_complete_file_streaming_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/oaktzv6k76ea1.jpg", "subreddit_subscribers": 667183, "created_utc": 1674644288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I've got two external HDD's plugged in via USB 3.0 ports, I'm trying to transfer about 1.7tb of data, what seems to be happening is when my transfer gets to like 5% or 11% (not specifically these two percentages it just does it at random really) both my drives disconnect from the PC then automatically reconnect causing an error and asking me to try again with the transfer. Upon pressing \"try again\" it does actually carry on and continue to work. But what's causing my drives to disconnect, I don't want to have any corrupt data or my drive to disconnect after a certain percentage.\n\nI have a feeling it's to do with the transfer rate being too high and the drives can't handle such large file transfers on USB 3.0... I've since put them both in a 2.0 port however I'm still awaiting something to happen, will update if I get disconnected again.", "author_fullname": "t2_evrgtr0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Problem transferring large quantity of files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kth1t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674633429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve got two external HDD&amp;#39;s plugged in via USB 3.0 ports, I&amp;#39;m trying to transfer about 1.7tb of data, what seems to be happening is when my transfer gets to like 5% or 11% (not specifically these two percentages it just does it at random really) both my drives disconnect from the PC then automatically reconnect causing an error and asking me to try again with the transfer. Upon pressing &amp;quot;try again&amp;quot; it does actually carry on and continue to work. But what&amp;#39;s causing my drives to disconnect, I don&amp;#39;t want to have any corrupt data or my drive to disconnect after a certain percentage.&lt;/p&gt;\n\n&lt;p&gt;I have a feeling it&amp;#39;s to do with the transfer rate being too high and the drives can&amp;#39;t handle such large file transfers on USB 3.0... I&amp;#39;ve since put them both in a 2.0 port however I&amp;#39;m still awaiting something to happen, will update if I get disconnected again.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kth1t", "is_robot_indexable": true, "report_reasons": null, "author": "BadFixMate", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kth1t/problem_transferring_large_quantity_of_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kth1t/problem_transferring_large_quantity_of_files/", "subreddit_subscribers": 667183, "created_utc": 1674633429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For one of my RAID10 arrays (that I have an offsite backup of!) what `mdadm` and `df` are reporting for \"used\" is very different. As far as I can tell, all the data (as reported by `df`) is still intact and accessible. \n\n\\[FWIW, I understand that both tools use different methods for calculating size, so they won't ever be identical, but they should be close (using 1024 vs 1000 for bytes/kb)\\].\n\nOutput of `df`:\n\n    Filesystem       1K-blocks        Used   Available Use% Mounted on\n    /dev/md0       15503488420 12965751368  1756331256  89% /mnt/md0\n\nOutput of `mdadm`:\n\n    /dev/md0:\n               Version : 1.2\n         Creation Time : Sat Nov  7 07:43:04 2020\n            Raid Level : raid10\n            Array Size : 15627788288 (14903.82 GiB 16002.86 GB)\n         Used Dev Size : 7813894144 (7451.91 GiB 8001.43 GB)\n          Raid Devices : 4\n         Total Devices : 4\n           Persistence : Superblock is persistent\n    \n         Intent Bitmap : Internal\n    \n           Update Time : Tue Jan 24 22:57:33 2023\n                 State : clean\n        Active Devices : 4\n       Working Devices : 4\n        Failed Devices : 0\n         Spare Devices : 0\n    \n                Layout : near=2\n            Chunk Size : 512K\n    \n    Consistency Policy : bitmap\n    \n                  Name : &lt;snip&gt;\n                  UUID : &lt;snip&gt;\n                Events : 332699\n    \n        Number   Major   Minor   RaidDevice State\n           0       8       64        0      active sync set-A   /dev/sde\n           1       8       80        1      active sync set-B   /dev/sdf\n           2       8       96        2      active sync set-A   /dev/sdg\n           3       8      112        3      active sync set-B   /dev/sdh\n\nNotice that the used count is: `df`: 12965751368 vs `mdm:` 7813894144.\n\nThere is a difference for my other RAID10 array, but it's within the difference of how both utilities calculate size. `df`: 14407941652 vs `mdm`: 15625747456\n\nDoes anyone know what's going on here?", "author_fullname": "t2_8zyu4htp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mismatch in used size between mdadm and df", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ksxcn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674631140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For one of my RAID10 arrays (that I have an offsite backup of!) what &lt;code&gt;mdadm&lt;/code&gt; and &lt;code&gt;df&lt;/code&gt; are reporting for &amp;quot;used&amp;quot; is very different. As far as I can tell, all the data (as reported by &lt;code&gt;df&lt;/code&gt;) is still intact and accessible. &lt;/p&gt;\n\n&lt;p&gt;[FWIW, I understand that both tools use different methods for calculating size, so they won&amp;#39;t ever be identical, but they should be close (using 1024 vs 1000 for bytes/kb)].&lt;/p&gt;\n\n&lt;p&gt;Output of &lt;code&gt;df&lt;/code&gt;:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Filesystem       1K-blocks        Used   Available Use% Mounted on\n/dev/md0       15503488420 12965751368  1756331256  89% /mnt/md0\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Output of &lt;code&gt;mdadm&lt;/code&gt;:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;/dev/md0:\n           Version : 1.2\n     Creation Time : Sat Nov  7 07:43:04 2020\n        Raid Level : raid10\n        Array Size : 15627788288 (14903.82 GiB 16002.86 GB)\n     Used Dev Size : 7813894144 (7451.91 GiB 8001.43 GB)\n      Raid Devices : 4\n     Total Devices : 4\n       Persistence : Superblock is persistent\n\n     Intent Bitmap : Internal\n\n       Update Time : Tue Jan 24 22:57:33 2023\n             State : clean\n    Active Devices : 4\n   Working Devices : 4\n    Failed Devices : 0\n     Spare Devices : 0\n\n            Layout : near=2\n        Chunk Size : 512K\n\nConsistency Policy : bitmap\n\n              Name : &amp;lt;snip&amp;gt;\n              UUID : &amp;lt;snip&amp;gt;\n            Events : 332699\n\n    Number   Major   Minor   RaidDevice State\n       0       8       64        0      active sync set-A   /dev/sde\n       1       8       80        1      active sync set-B   /dev/sdf\n       2       8       96        2      active sync set-A   /dev/sdg\n       3       8      112        3      active sync set-B   /dev/sdh\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Notice that the used count is: &lt;code&gt;df&lt;/code&gt;: 12965751368 vs &lt;code&gt;mdm:&lt;/code&gt; 7813894144.&lt;/p&gt;\n\n&lt;p&gt;There is a difference for my other RAID10 array, but it&amp;#39;s within the difference of how both utilities calculate size. &lt;code&gt;df&lt;/code&gt;: 14407941652 vs &lt;code&gt;mdm&lt;/code&gt;: 15625747456&lt;/p&gt;\n\n&lt;p&gt;Does anyone know what&amp;#39;s going on here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "70TB usable, 48TB backup, 70TB cloud backup", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ksxcn", "is_robot_indexable": true, "report_reasons": null, "author": "ComputingElephant", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10ksxcn/mismatch_in_used_size_between_mdadm_and_df/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ksxcn/mismatch_in_used_size_between_mdadm_and_df/", "subreddit_subscribers": 667183, "created_utc": 1674631140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for something to carry around with me. What would you recommend, a Samsung T7 or a StarTech/ Icy Box/ another brand? I'm a bit nervous of the reliability of the connectors on a standard external SSD. If it's the Samsung, should I carry it always with the cable attached ? Maybe I'm just worried for no reason, I don't know.", "author_fullname": "t2_lzd1r1x2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External SSD vs. enclosured SSD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kslm2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674631296.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674629910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for something to carry around with me. What would you recommend, a Samsung T7 or a StarTech/ Icy Box/ another brand? I&amp;#39;m a bit nervous of the reliability of the connectors on a standard external SSD. If it&amp;#39;s the Samsung, should I carry it always with the cable attached ? Maybe I&amp;#39;m just worried for no reason, I don&amp;#39;t know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kslm2", "is_robot_indexable": true, "report_reasons": null, "author": "ManicMambo", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kslm2/external_ssd_vs_enclosured_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kslm2/external_ssd_vs_enclosured_ssd/", "subreddit_subscribers": 667183, "created_utc": 1674629910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey looking for any feedback.\n\nI have multiple external drives and I want to merge the data to a single 6tb-8tb external drive and then I want to clone that drive so I have 2 copies.\n\nHere are some questions I have:\n\n1) how should I encrypt the external drives? (software, hardware, etc)\n2) should the external drive be ssd or spinning disks?\n3) how would a NAS help? how would cloud help?\n4) is there anything I should incorporate to make the backup strategy robust?\n5) can I use an Intel nuc for data transfer so I don't put a load on my main PC?\n6) what brands are recommended for HD backup?\n7) how can I ensure fast data transfers? any mediums I should consider?\n8) any software or harddrive tools recommended to make the process smooth and efficient?\n9) should there be a 3rd or 4th copy?\n\nThanks for all your help!", "author_fullname": "t2_1f2oms3y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "brainstorming backup strategy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10krpc3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674626609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey looking for any feedback.&lt;/p&gt;\n\n&lt;p&gt;I have multiple external drives and I want to merge the data to a single 6tb-8tb external drive and then I want to clone that drive so I have 2 copies.&lt;/p&gt;\n\n&lt;p&gt;Here are some questions I have:&lt;/p&gt;\n\n&lt;p&gt;1) how should I encrypt the external drives? (software, hardware, etc)\n2) should the external drive be ssd or spinning disks?\n3) how would a NAS help? how would cloud help?\n4) is there anything I should incorporate to make the backup strategy robust?\n5) can I use an Intel nuc for data transfer so I don&amp;#39;t put a load on my main PC?\n6) what brands are recommended for HD backup?\n7) how can I ensure fast data transfers? any mediums I should consider?\n8) any software or harddrive tools recommended to make the process smooth and efficient?\n9) should there be a 3rd or 4th copy?&lt;/p&gt;\n\n&lt;p&gt;Thanks for all your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10krpc3", "is_robot_indexable": true, "report_reasons": null, "author": "tstarx10304", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10krpc3/brainstorming_backup_strategy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10krpc3/brainstorming_backup_strategy/", "subreddit_subscribers": 667183, "created_utc": 1674626609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been trying to archive a certain [Invision forum](https://invisioncommunity.com/) on and off for months with little success. I have tried using both Wget and HTTrack. \n\nHTTrack gets flagged immediately whereas Wget can download the site's folder structure and some files before getting flagged. On Wget, I am using the same user agent that I usually browse the forum with, have limited the download rate, and have randomized download intervals within 30secs.\n\nOh, and it is crucial that Wget logs into my account, so I have been loading it cookies. When Wget is detected, the forum logs my session out.\n\nAny ideas on how I can resolve this issue?", "author_fullname": "t2_nu5tsavd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would be the best way to archive an Invision forum?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kpepq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674618978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been trying to archive a certain &lt;a href=\"https://invisioncommunity.com/\"&gt;Invision forum&lt;/a&gt; on and off for months with little success. I have tried using both Wget and HTTrack. &lt;/p&gt;\n\n&lt;p&gt;HTTrack gets flagged immediately whereas Wget can download the site&amp;#39;s folder structure and some files before getting flagged. On Wget, I am using the same user agent that I usually browse the forum with, have limited the download rate, and have randomized download intervals within 30secs.&lt;/p&gt;\n\n&lt;p&gt;Oh, and it is crucial that Wget logs into my account, so I have been loading it cookies. When Wget is detected, the forum logs my session out.&lt;/p&gt;\n\n&lt;p&gt;Any ideas on how I can resolve this issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/v1tA3V0b8amjc7hvVup5qFQzMkIiBJJKKqcQ_S6Bpqs.jpg?auto=webp&amp;v=enabled&amp;s=d44e3381d8cebec09918311838e0200342fdb309", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/v1tA3V0b8amjc7hvVup5qFQzMkIiBJJKKqcQ_S6Bpqs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e821fc2ea3342b747aa0ca0dacba289bae28db8", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/v1tA3V0b8amjc7hvVup5qFQzMkIiBJJKKqcQ_S6Bpqs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8625aad7c2474cbabe4b849131ba0c51dd171b6f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/v1tA3V0b8amjc7hvVup5qFQzMkIiBJJKKqcQ_S6Bpqs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd0013caf1340a236b584beb4fdb3ae1c6d5900d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/v1tA3V0b8amjc7hvVup5qFQzMkIiBJJKKqcQ_S6Bpqs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a9ad94a17b78aa8816def1d54d30633c50965da", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/v1tA3V0b8amjc7hvVup5qFQzMkIiBJJKKqcQ_S6Bpqs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aba3cc3af934a30940df2cda2a914050ae92eda9", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/v1tA3V0b8amjc7hvVup5qFQzMkIiBJJKKqcQ_S6Bpqs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35075f7fcd77ee131d5db9921cf76d98eda46366", "width": 1080, "height": 567}], "variants": {}, "id": "Xz9aFesV7iuHBTXdGaEpaejEIROwaObOy0MN59KvQuI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kpepq", "is_robot_indexable": true, "report_reasons": null, "author": "ValleyEliminator", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kpepq/what_would_be_the_best_way_to_archive_an_invision/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kpepq/what_would_be_the_best_way_to_archive_an_invision/", "subreddit_subscribers": 667183, "created_utc": 1674618978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to rename a lot of images based on content. \n\nthe images are currently named by numbers from the scanner, \"img 001.jpg\", \"img 002.jpg\", etc. I'd like to keep the prefix and add the descriptoin. ex: \"img 001 - Bob, Fred.jpg\", \"img 001 - Suzy - Las Vegas - March 1989.jpg\", etc.\n\nIdeally i'd like a program where the extension can not be edited, the image can be previewed, and it is easy to continue to the next image (tab or enter rather than using the mouse).\n\nthis seems like it would be a common thing, but I'm having trouble finding a program to do this. ideally free of course.", "author_fullname": "t2_5p8igwd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "windows software for renaming individual images with preview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kpav7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674618640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to rename a lot of images based on content. &lt;/p&gt;\n\n&lt;p&gt;the images are currently named by numbers from the scanner, &amp;quot;img 001.jpg&amp;quot;, &amp;quot;img 002.jpg&amp;quot;, etc. I&amp;#39;d like to keep the prefix and add the descriptoin. ex: &amp;quot;img 001 - Bob, Fred.jpg&amp;quot;, &amp;quot;img 001 - Suzy - Las Vegas - March 1989.jpg&amp;quot;, etc.&lt;/p&gt;\n\n&lt;p&gt;Ideally i&amp;#39;d like a program where the extension can not be edited, the image can be previewed, and it is easy to continue to the next image (tab or enter rather than using the mouse).&lt;/p&gt;\n\n&lt;p&gt;this seems like it would be a common thing, but I&amp;#39;m having trouble finding a program to do this. ideally free of course.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10kpav7", "is_robot_indexable": true, "report_reasons": null, "author": "joey123z", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10kpav7/windows_software_for_renaming_individual_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10kpav7/windows_software_for_renaming_individual_images/", "subreddit_subscribers": 667183, "created_utc": 1674618640.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}