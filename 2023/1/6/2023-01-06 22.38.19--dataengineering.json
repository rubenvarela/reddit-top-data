{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Do you want to learn Data Engineering? In 2023, we start another iteration of Data Engineering Zoomcamp. It's a free, practical, 10-week long course about the main concepts in Data Engineering.\u00a0  \n\n\nJoin us to learn about:\n\n* Docker, Terraform and GCP\n* Orchestration with Prefect\n* Creating a data warehouse with BigQuery\n* Analytics engineering and dbt\n* Batch processing and Spark\n* Stream processing and Kafka\n\nIt starts on the 16th of January, 2023.\u00a0 \n\nSign up here: [https://github.com/DataTalksClub/data-engineering-zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)", "author_fullname": "t2_ipugc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Data engineering bootcamp - Data Engineering Zoomcamp - starts in 10 days", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104xlft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 117, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 117, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673020464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you want to learn Data Engineering? In 2023, we start another iteration of Data Engineering Zoomcamp. It&amp;#39;s a free, practical, 10-week long course about the main concepts in Data Engineering.\u00a0  &lt;/p&gt;\n\n&lt;p&gt;Join us to learn about:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Docker, Terraform and GCP&lt;/li&gt;\n&lt;li&gt;Orchestration with Prefect&lt;/li&gt;\n&lt;li&gt;Creating a data warehouse with BigQuery&lt;/li&gt;\n&lt;li&gt;Analytics engineering and dbt&lt;/li&gt;\n&lt;li&gt;Batch processing and Spark&lt;/li&gt;\n&lt;li&gt;Stream processing and Kafka&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It starts on the 16th of January, 2023.\u00a0 &lt;/p&gt;\n\n&lt;p&gt;Sign up here: &lt;a href=\"https://github.com/DataTalksClub/data-engineering-zoomcamp\"&gt;https://github.com/DataTalksClub/data-engineering-zoomcamp&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?auto=webp&amp;s=816cc84d06fc71a798a41729c8e59b67057a7a40", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e6c2088f704523b4111a08f9c68f79d1887dce1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=95bdc1082cd1b4f1284442c0ab1040df5fb7abee", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=166df298958f127071afac0a0bdce49711d6fe49", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=480667fc30b7edfb18a8b621ffcb16b3a90021f1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b2bc45b5c081e5724f5c73326b5ab510230aa36a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=99f073ffe961cbf90656031cd111d9f3ecafad3a", "width": 1080, "height": 540}], "variants": {}, "id": "b6oRDf9UWdoqcJ_iqebxKYsWb30ncp2wq3cvwnbvclo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "104xlft", "is_robot_indexable": true, "report_reasons": null, "author": "stolzen", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104xlft/free_data_engineering_bootcamp_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104xlft/free_data_engineering_bootcamp_data_engineering/", "subreddit_subscribers": 85377, "created_utc": 1673020464.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My organization is a Microsoft partner and we usually do our ETL/ELT projects using Azure products like ADF or Databricks. Sometimes, we have request from customers for data solutions that could be deployed on both cloud and on-premise. As an intern, I was checking on Talend Open Studio and Apache Hop and preparing a comparison matrix between. From an initial observation, I could see both these tools connect to all major sources and data pipeline could be implemented on both of these tools using a drag and drop UI. \n\nWhat are the other features for comparing these tools ?. In your opinion, which tool is better.", "author_fullname": "t2_o78u2p44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Criterias to compare ETL tools in the market: Talend, Apache Hop, Azure Data Factory", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104qyuu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673001254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My organization is a Microsoft partner and we usually do our ETL/ELT projects using Azure products like ADF or Databricks. Sometimes, we have request from customers for data solutions that could be deployed on both cloud and on-premise. As an intern, I was checking on Talend Open Studio and Apache Hop and preparing a comparison matrix between. From an initial observation, I could see both these tools connect to all major sources and data pipeline could be implemented on both of these tools using a drag and drop UI. &lt;/p&gt;\n\n&lt;p&gt;What are the other features for comparing these tools ?. In your opinion, which tool is better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "104qyuu", "is_robot_indexable": true, "report_reasons": null, "author": "SignificanceNo136", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104qyuu/criterias_to_compare_etl_tools_in_the_market/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104qyuu/criterias_to_compare_etl_tools_in_the_market/", "subreddit_subscribers": 85377, "created_utc": 1673001254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi redditors!\n\nI'm glad to announce a new major release of our [UI for Apache Kafka](https://github.com/provectus/kafka-ui).\n\nAlso we've made a new launch/update on [ProductHunt](https://www.producthunt.com/posts/ui-for-apache-kafka-2), please support us by upvoting!\n\nFeel free to ask any questions in the comments section, I'll be answering them soon. Or feel free to join us on [discord](https://discord.gg/4DWzD7pGE5) for a quicker reply!\n\n&amp;#x200B;\n\nThis release comes with new feature like role-based access control, pluggable serde plugins and data masking.\n\n# New Features\n\n\\- Role-based Access Control \u2014 Manage permissions for your groups and users via various authentication methods, including GitHub, Google, Cognito, etc.\n\n\\- Pluggable SerDe API \u2014 Use UI for Apache Kafka\u2019s built-in plugins, or code your own, to serialize and deserialize custom data types in your topic messages. We also implemented plugins for AWS Glue and Smile!\n\n\\- Data Masking \u2014 Easily mask any sensitive data in topic messages with predefined patterns and data types.\n\n\\- Preview of Message Fields \u2014 Quickly view fields of nested topic messages in list format via JSON paths.\n\n\\- Topic Analysis \u2014 Analyze your topic messages, distribution of messages, message size, and more.\n\n\\- Per-cluster Polling Rate Limits \u2014 Set up polling limits to avoid I/O spikes on your Apache Kafka, and high loads and flood in live mode.\n\n\\- Managing broker configs \u2014 Edit broker configs values within UI\n\n# Enhancements\n\n\\- Improved support for AWS MSK Serverless, some tweaks to support MSK and other serverless platforms .\n\n\\- Support prometheus as metrics interface, can be used for retrieving kafka metrics if you prefer it over JMX (e.g. you use MSK)\n\n\\- mTLS support for KSQL, SR, KC\n\n\\- Support for multiple protobuf files\n\n\\- Active Directory support for LDAP authentication\n\n\\- Support basic auth for KSQL server\n\n\\- A view to display broker metrics\n\n\\- A view to display broker log dirs\n\n&amp;#x200B;\n\nThanks everyone who contributed in any way :)", "author_fullname": "t2_ttr4u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A major release of UI for Apache Kafka with RBAC and SerDe API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104tr7z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673010404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi redditors!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m glad to announce a new major release of our &lt;a href=\"https://github.com/provectus/kafka-ui\"&gt;UI for Apache Kafka&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Also we&amp;#39;ve made a new launch/update on &lt;a href=\"https://www.producthunt.com/posts/ui-for-apache-kafka-2\"&gt;ProductHunt&lt;/a&gt;, please support us by upvoting!&lt;/p&gt;\n\n&lt;p&gt;Feel free to ask any questions in the comments section, I&amp;#39;ll be answering them soon. Or feel free to join us on &lt;a href=\"https://discord.gg/4DWzD7pGE5\"&gt;discord&lt;/a&gt; for a quicker reply!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This release comes with new feature like role-based access control, pluggable serde plugins and data masking.&lt;/p&gt;\n\n&lt;h1&gt;New Features&lt;/h1&gt;\n\n&lt;p&gt;- Role-based Access Control \u2014 Manage permissions for your groups and users via various authentication methods, including GitHub, Google, Cognito, etc.&lt;/p&gt;\n\n&lt;p&gt;- Pluggable SerDe API \u2014 Use UI for Apache Kafka\u2019s built-in plugins, or code your own, to serialize and deserialize custom data types in your topic messages. We also implemented plugins for AWS Glue and Smile!&lt;/p&gt;\n\n&lt;p&gt;- Data Masking \u2014 Easily mask any sensitive data in topic messages with predefined patterns and data types.&lt;/p&gt;\n\n&lt;p&gt;- Preview of Message Fields \u2014 Quickly view fields of nested topic messages in list format via JSON paths.&lt;/p&gt;\n\n&lt;p&gt;- Topic Analysis \u2014 Analyze your topic messages, distribution of messages, message size, and more.&lt;/p&gt;\n\n&lt;p&gt;- Per-cluster Polling Rate Limits \u2014 Set up polling limits to avoid I/O spikes on your Apache Kafka, and high loads and flood in live mode.&lt;/p&gt;\n\n&lt;p&gt;- Managing broker configs \u2014 Edit broker configs values within UI&lt;/p&gt;\n\n&lt;h1&gt;Enhancements&lt;/h1&gt;\n\n&lt;p&gt;- Improved support for AWS MSK Serverless, some tweaks to support MSK and other serverless platforms .&lt;/p&gt;\n\n&lt;p&gt;- Support prometheus as metrics interface, can be used for retrieving kafka metrics if you prefer it over JMX (e.g. you use MSK)&lt;/p&gt;\n\n&lt;p&gt;- mTLS support for KSQL, SR, KC&lt;/p&gt;\n\n&lt;p&gt;- Support for multiple protobuf files&lt;/p&gt;\n\n&lt;p&gt;- Active Directory support for LDAP authentication&lt;/p&gt;\n\n&lt;p&gt;- Support basic auth for KSQL server&lt;/p&gt;\n\n&lt;p&gt;- A view to display broker metrics&lt;/p&gt;\n\n&lt;p&gt;- A view to display broker log dirs&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks everyone who contributed in any way :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?auto=webp&amp;s=20227d19ad3d0d65ec5dbca0de7a4e70a8d89d58", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2bf878ad8bcbe0475ad4b47560ff8fbec03a39fc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f228ac9a1a215cd3ad51cf9ee351eaa9ee36826", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b0ff0b955167495ca6639fbb5db1a1a48f9bf81", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9f2d60fcfc5f47b1e4518e936abb27b94c950f90", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b4c76f29b8991610e97e46be2d172f09c8a39c53", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=faad6b047facc25325d5534ef8e82b66831a8d5c", "width": 1080, "height": 540}], "variants": {}, "id": "RcuEPg0I8fPEiPOV6vLeiVtieDOLcxOX0OWhC4xG2Ro"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "104tr7z", "is_robot_indexable": true, "report_reasons": null, "author": "Haarolean", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104tr7z/a_major_release_of_ui_for_apache_kafka_with_rbac/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104tr7z/a_major_release_of_ui_for_apache_kafka_with_rbac/", "subreddit_subscribers": 85377, "created_utc": 1673010404.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, which you all an amazing new year!\n\nLong story short, I'm a system administrator professional who landed a big data platform engineer job last year, even thought I enjoy it, it was not what I wanted since I wanted to start developing myself towards data engineering. This year I have a lot of things going on in life and one of my goals is to move to Netherlands and land a junior/entry level job in data engineering. Since I already work with Cloudera, astronomer and gitlab and pretty much understand all concepts and what not's from a platform perspective, I'm a little lost on what to develop in order to land in data engineering world, is there anything recommendable to start working on some personal portfolio?\n\n[Also is there something else from a move perspective that would make sense to add here or remove?](https://preview.redd.it/f0eex7mgeeaa1.png?width=1262&amp;format=png&amp;auto=webp&amp;s=ae30fe409a0928732506cc022a2119c26a41c69a)\n\nAppreciate any kind of feedback and sorry for one more of this posts.", "author_fullname": "t2_c0gho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Move from big data platform engineer to data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"f0eex7mgeeaa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 140, "x": 108, "u": "https://preview.redd.it/f0eex7mgeeaa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=566ddcaec540d458c0accc180a17963d1cbc3d6a"}, {"y": 281, "x": 216, "u": "https://preview.redd.it/f0eex7mgeeaa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=61369afb79e6db97f51ca59313f16f97ea5b83c0"}, {"y": 416, "x": 320, "u": "https://preview.redd.it/f0eex7mgeeaa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0030b7ea31af407fe3d4faeb6764d94b89993aba"}, {"y": 832, "x": 640, "u": "https://preview.redd.it/f0eex7mgeeaa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4aa4e0bcf8be09d89730dfc3f070148c1fc7fd30"}, {"y": 1249, "x": 960, "u": "https://preview.redd.it/f0eex7mgeeaa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fc02704e7c143ed1816163095d928eefbfccea07"}, {"y": 1405, "x": 1080, "u": "https://preview.redd.it/f0eex7mgeeaa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4272b937e4ab9feaccb09ae218d64b8fbf35bdd0"}], "s": {"y": 1642, "x": 1262, "u": "https://preview.redd.it/f0eex7mgeeaa1.png?width=1262&amp;format=png&amp;auto=webp&amp;s=ae30fe409a0928732506cc022a2119c26a41c69a"}, "id": "f0eex7mgeeaa1"}}, "name": "t3_104qu41", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vlNVQCQMK077NYydskMIULrmmSW9mwxxIKr-Js4OKQE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673000776.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, which you all an amazing new year!&lt;/p&gt;\n\n&lt;p&gt;Long story short, I&amp;#39;m a system administrator professional who landed a big data platform engineer job last year, even thought I enjoy it, it was not what I wanted since I wanted to start developing myself towards data engineering. This year I have a lot of things going on in life and one of my goals is to move to Netherlands and land a junior/entry level job in data engineering. Since I already work with Cloudera, astronomer and gitlab and pretty much understand all concepts and what not&amp;#39;s from a platform perspective, I&amp;#39;m a little lost on what to develop in order to land in data engineering world, is there anything recommendable to start working on some personal portfolio?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/f0eex7mgeeaa1.png?width=1262&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ae30fe409a0928732506cc022a2119c26a41c69a\"&gt;Also is there something else from a move perspective that would make sense to add here or remove?&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Appreciate any kind of feedback and sorry for one more of this posts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "104qu41", "is_robot_indexable": true, "report_reasons": null, "author": "dAwiener", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104qu41/move_from_big_data_platform_engineer_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104qu41/move_from_big_data_platform_engineer_to_data/", "subreddit_subscribers": 85377, "created_utc": 1673000776.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Need feedback from fellow data engineers on this blog:** \n\n# ETL testing \u2014 How to test your data pipelines the right way?\n\nIt is 2023! New data paradigms (or buzz words) like ELT, reverse ETL, EtLT, Data mesh, Data contracts, FinOps and modern data stack found their way into mainstream data conversations. Our data teams are still figuring out what is hype and what is not.\n\nThere may be 10 new paradigms tomorrow but some of the fundamental challenges in data engineering \u2014 like data quality \u2014 are still relevant and not solved completely (I don\u2019t think we ever will). The first step in improving data quality is to test changes to our data pipelines vigorously.\n\nLet us review the challenges involved in testing data pipelines effectively and how to build a well-rounded testing strategy for your organization.\n\n# Why is achieving data quality hard?\n\nIn software application development world, improving the quality of software meant rigorous testing. Similarly in data engineering, we need a comprehensive testing strategy to achieve high quality data in production.\n\n&gt;Most data teams are running against hard deadlines. So, data engineering culture is such that we end up building pipelines that serve data by the end of the week instead of incorporating all the best practices that are valuable in the long run.\n\n* In ETL testing, we compare huge volumes of data (say millions of records) often from different source systems. We are comparing transformed data that are a result of complex SQL queries or Spark jobs.\n* Not all data engineers (and the data leaders) are from software engineering background and are strong in SWE development principles and best practices.\n* Running automated suite of tests and automated deployment/release of data products is still not mainstream.\n\n&gt;ETL testing is a data-centric testing process. To effectively test our pipelines, we need production like data (in terms of volume, variety, and velocity).\n\n&amp;#x200B;\n\nGetting access to production like data is hard. Here is how data teams in different companies tackle the problem of getting the right data to test the data pipelines.\n\n# 1. Mock Data:\n\nPros: This approach is prevalently used by all of us data engineers because of ease of mock data creation and availability of synthetic data generation tools (such as Faker).\n\nCons: Mock data does not reflect the production data in terms of volume, variety or velocity.\n\n# 2. Sample Prod data to Test/Dev Env:\n\nPros: Easy to copy fraction of production data than to copy huge swathes of prod data.\n\nCons: Should use the right sampling strategy to ensure the sample reflects real world prod data. Tests that run successfully on sample prod data might fail on actual prod data because volume and variety is not guaranteed.\n\n# 3. Copy all of Prod data to Test Env:\n\nPros: Availability of real world production data for testing.\n\nCons: If prod contains PII data, it might lead to data privacy violations. If the prod data is constantly changing, then the copy of prod data in test/dev environment will become stale and needs to be constantly updated. Volume and variety guaranteed, but not velocity.\n\n# 4. Copy anonymized prod data to Test Env:\n\nPros: Availability of real world production data for testing. Compliance to all data privacy regulations.\n\nCons: Again, a constantly changing prod data means the data in test env becomes stale and needs to be refreshed often. PII anonymization needs to be run every time you copy data out of prod. Manually running anonymization steps every time and maintaining a long-running test data environment is error-prone and resource intensive.\n\n# 5. Using a data versioning tool to mirror prod data to Dev/Test Env:\n\nPros: Availability of real-world production data. Automated short-lived test environments that are available through git-like API.\n\nCons: Add a new tool to your existing data stack.\n\n&amp;#x200B;\n\n[Here](https://vinodhini-sd.medium.com/forget-about-the-new-data-trends-in-2023-d2756add3317) is the full blog and appreciate your feedback!", "author_fullname": "t2_msviuy1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forget about the new data trends in 2023! This fundamental data engineering challenge is still not solved.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10525tw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673031207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Need feedback from fellow data engineers on this blog:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;h1&gt;ETL testing \u2014 How to test your data pipelines the right way?&lt;/h1&gt;\n\n&lt;p&gt;It is 2023! New data paradigms (or buzz words) like ELT, reverse ETL, EtLT, Data mesh, Data contracts, FinOps and modern data stack found their way into mainstream data conversations. Our data teams are still figuring out what is hype and what is not.&lt;/p&gt;\n\n&lt;p&gt;There may be 10 new paradigms tomorrow but some of the fundamental challenges in data engineering \u2014 like data quality \u2014 are still relevant and not solved completely (I don\u2019t think we ever will). The first step in improving data quality is to test changes to our data pipelines vigorously.&lt;/p&gt;\n\n&lt;p&gt;Let us review the challenges involved in testing data pipelines effectively and how to build a well-rounded testing strategy for your organization.&lt;/p&gt;\n\n&lt;h1&gt;Why is achieving data quality hard?&lt;/h1&gt;\n\n&lt;p&gt;In software application development world, improving the quality of software meant rigorous testing. Similarly in data engineering, we need a comprehensive testing strategy to achieve high quality data in production.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Most data teams are running against hard deadlines. So, data engineering culture is such that we end up building pipelines that serve data by the end of the week instead of incorporating all the best practices that are valuable in the long run.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In ETL testing, we compare huge volumes of data (say millions of records) often from different source systems. We are comparing transformed data that are a result of complex SQL queries or Spark jobs.&lt;/li&gt;\n&lt;li&gt;Not all data engineers (and the data leaders) are from software engineering background and are strong in SWE development principles and best practices.&lt;/li&gt;\n&lt;li&gt;Running automated suite of tests and automated deployment/release of data products is still not mainstream.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;ETL testing is a data-centric testing process. To effectively test our pipelines, we need production like data (in terms of volume, variety, and velocity).&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Getting access to production like data is hard. Here is how data teams in different companies tackle the problem of getting the right data to test the data pipelines.&lt;/p&gt;\n\n&lt;h1&gt;1. Mock Data:&lt;/h1&gt;\n\n&lt;p&gt;Pros: This approach is prevalently used by all of us data engineers because of ease of mock data creation and availability of synthetic data generation tools (such as Faker).&lt;/p&gt;\n\n&lt;p&gt;Cons: Mock data does not reflect the production data in terms of volume, variety or velocity.&lt;/p&gt;\n\n&lt;h1&gt;2. Sample Prod data to Test/Dev Env:&lt;/h1&gt;\n\n&lt;p&gt;Pros: Easy to copy fraction of production data than to copy huge swathes of prod data.&lt;/p&gt;\n\n&lt;p&gt;Cons: Should use the right sampling strategy to ensure the sample reflects real world prod data. Tests that run successfully on sample prod data might fail on actual prod data because volume and variety is not guaranteed.&lt;/p&gt;\n\n&lt;h1&gt;3. Copy all of Prod data to Test Env:&lt;/h1&gt;\n\n&lt;p&gt;Pros: Availability of real world production data for testing.&lt;/p&gt;\n\n&lt;p&gt;Cons: If prod contains PII data, it might lead to data privacy violations. If the prod data is constantly changing, then the copy of prod data in test/dev environment will become stale and needs to be constantly updated. Volume and variety guaranteed, but not velocity.&lt;/p&gt;\n\n&lt;h1&gt;4. Copy anonymized prod data to Test Env:&lt;/h1&gt;\n\n&lt;p&gt;Pros: Availability of real world production data for testing. Compliance to all data privacy regulations.&lt;/p&gt;\n\n&lt;p&gt;Cons: Again, a constantly changing prod data means the data in test env becomes stale and needs to be refreshed often. PII anonymization needs to be run every time you copy data out of prod. Manually running anonymization steps every time and maintaining a long-running test data environment is error-prone and resource intensive.&lt;/p&gt;\n\n&lt;h1&gt;5. Using a data versioning tool to mirror prod data to Dev/Test Env:&lt;/h1&gt;\n\n&lt;p&gt;Pros: Availability of real-world production data. Automated short-lived test environments that are available through git-like API.&lt;/p&gt;\n\n&lt;p&gt;Cons: Add a new tool to your existing data stack.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://vinodhini-sd.medium.com/forget-about-the-new-data-trends-in-2023-d2756add3317\"&gt;Here&lt;/a&gt; is the full blog and appreciate your feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?auto=webp&amp;s=e5cbcb7cfa87dac95dba6b8ca6fe9316e6743aaa", "width": 1200, "height": 457}, "resolutions": [{"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a97a00584776b1c8d3857040195bc99d972c8e9c", "width": 108, "height": 41}, {"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db1c7466b344a3c6f90f8619927913ac7bfd49d9", "width": 216, "height": 82}, {"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=43ff0968a785930b735f52a684727234f0da8b9f", "width": 320, "height": 121}, {"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5a44bd5a70fefd17d7af0a6b7d94ff0ec4d51e0", "width": 640, "height": 243}, {"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a0b31b3c62c016354ce15cf8e754fce15787e482", "width": 960, "height": 365}, {"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc1b0446e9dcea6a2d74bdc0a16c6c5e85d2a7f2", "width": 1080, "height": 411}], "variants": {}, "id": "OO3_ymKJyfHeZYENRf5HYl6tOP--MMtRe2gGCKBfBgw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Applied Data &amp; ML Engineer | Developer Advocate", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10525tw", "is_robot_indexable": true, "report_reasons": null, "author": "vino_and_data", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10525tw/forget_about_the_new_data_trends_in_2023_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10525tw/forget_about_the_new_data_trends_in_2023_this/", "subreddit_subscribers": 85377, "created_utc": 1673031207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4qx35pk2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any thoughts or advice would be greatly appreciated!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_104jt6m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/C_1xpNuJuHfZZTAVmbH6Wbsaf2pYcMOLdmRw5mVSMkg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1672976808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/lyw349zehcaa1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/lyw349zehcaa1.jpg?auto=webp&amp;s=31ab97c57be62d78b1b0df98fb7ea0dea9d07e4b", "width": 791, "height": 1024}, "resolutions": [{"url": "https://preview.redd.it/lyw349zehcaa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=345191d06c5de716d480ee82a3d89d786260d585", "width": 108, "height": 139}, {"url": "https://preview.redd.it/lyw349zehcaa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4660a8ad2714dbf79e0c39304e55ce560ed4b7cf", "width": 216, "height": 279}, {"url": "https://preview.redd.it/lyw349zehcaa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ddb0bd94ac49ab72f752154bc5bdfbbeb64c23de", "width": 320, "height": 414}, {"url": "https://preview.redd.it/lyw349zehcaa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5ab533e922a4e16c6e3a458a4df2439628985315", "width": 640, "height": 828}], "variants": {}, "id": "CZnHMWge9rPxtqB0qxVWATunbKATH-CMwP7nxJz7wE0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "104jt6m", "is_robot_indexable": true, "report_reasons": null, "author": "waldo_92", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104jt6m/any_thoughts_or_advice_would_be_greatly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/lyw349zehcaa1.jpg", "subreddit_subscribers": 85377, "created_utc": 1672976808.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI'm learning data engineering and i know python and SQL .\nSo out of curiosity i got an idea to make a project.\n\nI want to collect the temperature of 10 cities for each hour every day for 30 days.\n\nSo, my project involves the below steps\n1. Request open weathermap api for temperature of 10 cities.\n2. Extract only temperatures,and city names from the received json data\n3. Write this data into a table in SQL server.\n\nI have developed a python code and I'm running it manually every hour.\n\nI just learnt some airflow basics and trying to schedule it.\n\nHow to run this whole project on cloud?\n\nIm kind of stuck.\nShould I create a database instance on cloud?\nI'm beginner to this field.", "author_fullname": "t2_apg4thop", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help in my personal project.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105162f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673028876.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m learning data engineering and i know python and SQL .\nSo out of curiosity i got an idea to make a project.&lt;/p&gt;\n\n&lt;p&gt;I want to collect the temperature of 10 cities for each hour every day for 30 days.&lt;/p&gt;\n\n&lt;p&gt;So, my project involves the below steps\n1. Request open weathermap api for temperature of 10 cities.\n2. Extract only temperatures,and city names from the received json data\n3. Write this data into a table in SQL server.&lt;/p&gt;\n\n&lt;p&gt;I have developed a python code and I&amp;#39;m running it manually every hour.&lt;/p&gt;\n\n&lt;p&gt;I just learnt some airflow basics and trying to schedule it.&lt;/p&gt;\n\n&lt;p&gt;How to run this whole project on cloud?&lt;/p&gt;\n\n&lt;p&gt;Im kind of stuck.\nShould I create a database instance on cloud?\nI&amp;#39;m beginner to this field.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105162f", "is_robot_indexable": true, "report_reasons": null, "author": "Tombraider2598", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105162f/need_help_in_my_personal_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105162f/need_help_in_my_personal_project/", "subreddit_subscribers": 85377, "created_utc": 1673028876.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is looking at Palantir Foundry, and the consultants told us the storage is this Atlas DB thing: [https://palantir.github.io/atlasdb/html/index.html](https://palantir.github.io/atlasdb/html/index.html).\n\nI honestly haven't seen k/v databases used outside of things like s3/buckets and website backend type stuff. Lots of read/write but not exactly analytical queries. \n\nAnyone use Cassandra or K/V databases in an analytical setting?", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cassandra or K/V Databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104mncm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672985445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is looking at Palantir Foundry, and the consultants told us the storage is this Atlas DB thing: &lt;a href=\"https://palantir.github.io/atlasdb/html/index.html\"&gt;https://palantir.github.io/atlasdb/html/index.html&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I honestly haven&amp;#39;t seen k/v databases used outside of things like s3/buckets and website backend type stuff. Lots of read/write but not exactly analytical queries. &lt;/p&gt;\n\n&lt;p&gt;Anyone use Cassandra or K/V databases in an analytical setting?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "104mncm", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104mncm/cassandra_or_kv_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104mncm/cassandra_or_kv_databases/", "subreddit_subscribers": 85377, "created_utc": 1672985445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\n[Big Spatial Data Visualization using DeckGL - In this tutorial, we will explain how to process UK-Accidents\\(1.5 Million Points\\) spatial data using python and visualize it using DeckGL](https://preview.redd.it/l2atljbwucaa1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=e8ed893773f9038a5dd9225099dc8262d98ca1a8)\n\n[Big Spatial Data Visualization using DeckGL - In this tutorial, we will explain how to process UK-Accidents(1.5 Million Points) spatial data using python and visualize it using DeckGL](https://spatial-dev.guru/2023/01/05/big-data-visualization-using-deckgl/)", "author_fullname": "t2_avt84u4i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Big Spatial Data Visualization using DeckGL - In this tutorial, we will explain how to process UK-Accidents(1.5 Million Points) spatial data using python and visualize it using DeckGL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"l2atljbwucaa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/l2atljbwucaa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=17b99ecaddce05973184dda444fd4f91e29a1dab"}, {"y": 109, "x": 216, "u": "https://preview.redd.it/l2atljbwucaa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b95914d2dbfd6f6b1a27d886efa7564aca7fc3d2"}, {"y": 161, "x": 320, "u": "https://preview.redd.it/l2atljbwucaa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d15af3e79aafb02c1e44e36dd242ca0a4b5297f5"}, {"y": 323, "x": 640, "u": "https://preview.redd.it/l2atljbwucaa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b60712dd46ca1fb6757455004e0b8cb4b89ec53a"}, {"y": 485, "x": 960, "u": "https://preview.redd.it/l2atljbwucaa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=277704986778738d0669a2e963858fced4179ead"}], "s": {"y": 518, "x": 1024, "u": "https://preview.redd.it/l2atljbwucaa1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=e8ed893773f9038a5dd9225099dc8262d98ca1a8"}, "id": "l2atljbwucaa1"}}, "name": "t3_104lcmw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DO1nvu2_cqOhz0tr7pNro1ifAjlsQrOYOCY9Tigh_vQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672981341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/l2atljbwucaa1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e8ed893773f9038a5dd9225099dc8262d98ca1a8\"&gt;Big Spatial Data Visualization using DeckGL - In this tutorial, we will explain how to process UK-Accidents(1.5 Million Points) spatial data using python and visualize it using DeckGL&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://spatial-dev.guru/2023/01/05/big-data-visualization-using-deckgl/\"&gt;Big Spatial Data Visualization using DeckGL - In this tutorial, we will explain how to process UK-Accidents(1.5 Million Points) spatial data using python and visualize it using DeckGL&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "104lcmw", "is_robot_indexable": true, "report_reasons": null, "author": "iamgeoknight", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104lcmw/big_spatial_data_visualization_using_deckgl_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104lcmw/big_spatial_data_visualization_using_deckgl_in/", "subreddit_subscribers": 85377, "created_utc": 1672981341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jp6tbwl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modern Polars: an extensive side-by-side comparison of Polars and Pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1050b8o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1673026851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kevinheavey.github.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://kevinheavey.github.io/modern-polars/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1050b8o", "is_robot_indexable": true, "report_reasons": null, "author": "caoimhin_o_h", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1050b8o/modern_polars_an_extensive_sidebyside_comparison/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://kevinheavey.github.io/modern-polars/", "subreddit_subscribers": 85377, "created_utc": 1673026851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI currently work for a startup (\\~50 people, 10s engineers) focusing on scientific research.\n\n**Context**:   \nDue to the nature of our work, and the fact that most coders weren't engineers, after our few years of existence we ended up in a state where we have built several services (mostly batch computations with a config file for the context) which mostly read and write in unreferenced parquet files all across our S3. Due to the lack of unified storage framework, some other teams use postgre, while others use custom solutions.  \nThe thing is, our data now tends to grow fast (TBs) and it becomes cumbersome (impossible ?) to aggregate several parquets to make joins from different services in our code. Moreover, as the company expands, we would like to build transformations and analytics (you can think of it as datamarts) on top of all the raw data we generate.\n\n**The problem**:  \nSo my goal is to create a unique (or at least by default) datalake to unify our datasets, make joins between them without having to know the physical layout of the files, etc... I'm sure you get the idea.\n\nOne of the candidates solution is to use Iceberg (or equivalent like delta lake) for both raw and future transformed data.   \nI have read a bunch of articles on how it works and how to use it and I have come to this conclusion :   \nSince we are fully on AWS, EMR (spark) seems very good to handle both transformations and explorations of dataset with studio.  \nHowever, how would one handle simple transactions (reads, writes, updates) in our services' code ? To make it clearer, we currently do `df.to_parquet(\"s3://xxx\")` or `df.write_in_postgre(table)` to produce our raw data, would it be possible/a good pattern to translate to something like `df.write_in_iceberg` and use EMR cluster capabilities for that ? Is it possible to spawn cluster when a query is done or do we need to manually ask for the start of one ? (It is out of the question to integrate spark code in our services)  \nI have also seen that Athena could be a candidate but two things scares me :\n\n1. People are complaining a lot about random bugs/failures/wait queues\n2. Paying on scanned data frightens me a bit\n\nWhat do you think of such an architecture and choice of services ? Maybe they can co-exist ?\n\nI hope that where I struggle is clear and that the problem is correctly explained.  \nApologies in advance if this isn't the right subreddit to ask and if I should go to r/aws please tell me :)  \nThank you", "author_fullname": "t2_7wiej82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to compute queries aiming at Iceberg tables ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1050aur", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673026825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I currently work for a startup (~50 people, 10s engineers) focusing on scientific research.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;:&lt;br/&gt;\nDue to the nature of our work, and the fact that most coders weren&amp;#39;t engineers, after our few years of existence we ended up in a state where we have built several services (mostly batch computations with a config file for the context) which mostly read and write in unreferenced parquet files all across our S3. Due to the lack of unified storage framework, some other teams use postgre, while others use custom solutions.&lt;br/&gt;\nThe thing is, our data now tends to grow fast (TBs) and it becomes cumbersome (impossible ?) to aggregate several parquets to make joins from different services in our code. Moreover, as the company expands, we would like to build transformations and analytics (you can think of it as datamarts) on top of all the raw data we generate.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;:&lt;br/&gt;\nSo my goal is to create a unique (or at least by default) datalake to unify our datasets, make joins between them without having to know the physical layout of the files, etc... I&amp;#39;m sure you get the idea.&lt;/p&gt;\n\n&lt;p&gt;One of the candidates solution is to use Iceberg (or equivalent like delta lake) for both raw and future transformed data.&lt;br/&gt;\nI have read a bunch of articles on how it works and how to use it and I have come to this conclusion :&lt;br/&gt;\nSince we are fully on AWS, EMR (spark) seems very good to handle both transformations and explorations of dataset with studio.&lt;br/&gt;\nHowever, how would one handle simple transactions (reads, writes, updates) in our services&amp;#39; code ? To make it clearer, we currently do &lt;code&gt;df.to_parquet(&amp;quot;s3://xxx&amp;quot;)&lt;/code&gt; or &lt;code&gt;df.write_in_postgre(table)&lt;/code&gt; to produce our raw data, would it be possible/a good pattern to translate to something like &lt;code&gt;df.write_in_iceberg&lt;/code&gt; and use EMR cluster capabilities for that ? Is it possible to spawn cluster when a query is done or do we need to manually ask for the start of one ? (It is out of the question to integrate spark code in our services)&lt;br/&gt;\nI have also seen that Athena could be a candidate but two things scares me :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;People are complaining a lot about random bugs/failures/wait queues&lt;/li&gt;\n&lt;li&gt;Paying on scanned data frightens me a bit&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What do you think of such an architecture and choice of services ? Maybe they can co-exist ?&lt;/p&gt;\n\n&lt;p&gt;I hope that where I struggle is clear and that the problem is correctly explained.&lt;br/&gt;\nApologies in advance if this isn&amp;#39;t the right subreddit to ask and if I should go to &lt;a href=\"/r/aws\"&gt;r/aws&lt;/a&gt; please tell me :)&lt;br/&gt;\nThank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1050aur", "is_robot_indexable": true, "report_reasons": null, "author": "otineb_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1050aur/where_to_compute_queries_aiming_at_iceberg_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1050aur/where_to_compute_queries_aiming_at_iceberg_tables/", "subreddit_subscribers": 85377, "created_utc": 1673026825.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm curious about data security as it relates to source control.\n\nCurrently we are running databricks on a dedicated VM for data security reasons and want to integrate it with git (Azure DevOps) but there are concerns around having data checked in via notebooks or inappropriate commits which could lead to data leaking out of the system.\n\nShould this be a concern at all or have any of you done anything to mitigate this risk?", "author_fullname": "t2_lmtu1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Security around Databricks and Azure DevOps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104vmb7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673015518.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious about data security as it relates to source control.&lt;/p&gt;\n\n&lt;p&gt;Currently we are running databricks on a dedicated VM for data security reasons and want to integrate it with git (Azure DevOps) but there are concerns around having data checked in via notebooks or inappropriate commits which could lead to data leaking out of the system.&lt;/p&gt;\n\n&lt;p&gt;Should this be a concern at all or have any of you done anything to mitigate this risk?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "104vmb7", "is_robot_indexable": true, "report_reasons": null, "author": "IMisterDogI", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104vmb7/data_security_around_databricks_and_azure_devops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104vmb7/data_security_around_databricks_and_azure_devops/", "subreddit_subscribers": 85377, "created_utc": 1673015518.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am applying internally for a Data Engineer role that recently opened up at my company. \n\nThe hiring manager for the role, a Sr. DE, is encouraging me to apply. I am currently a Sr. BA with a couple years of experience with SQL (on the Analytics side) with very little professional experience with Python. \n\nI expect to be afforded at minimum a courtesy interview (since that\u2019s typically standard for internal applicants).\n\nAll things above considered, what in your advice should be the 5-10 things/concepts/skills I should focus on (cram) to better prepare for the interview within a month?", "author_fullname": "t2_ry76f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview prep", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104ff8w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672965097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am applying internally for a Data Engineer role that recently opened up at my company. &lt;/p&gt;\n\n&lt;p&gt;The hiring manager for the role, a Sr. DE, is encouraging me to apply. I am currently a Sr. BA with a couple years of experience with SQL (on the Analytics side) with very little professional experience with Python. &lt;/p&gt;\n\n&lt;p&gt;I expect to be afforded at minimum a courtesy interview (since that\u2019s typically standard for internal applicants).&lt;/p&gt;\n\n&lt;p&gt;All things above considered, what in your advice should be the 5-10 things/concepts/skills I should focus on (cram) to better prepare for the interview within a month?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "104ff8w", "is_robot_indexable": true, "report_reasons": null, "author": "CosmicNightmare", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104ff8w/interview_prep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104ff8w/interview_prep/", "subreddit_subscribers": 85377, "created_utc": 1672965097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking at D365 and Dataverse, it seems Microsoft is pushing for people to use the Azure Synapse Link, which synchronises data in the data lake using the common data model format.\n\nIs there any actual benefit to using this format compared to other data formats, eg. Flat files or parquet.", "author_fullname": "t2_6o5du", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the benefits (if any) of the common data model format?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104v8au", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673014502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking at D365 and Dataverse, it seems Microsoft is pushing for people to use the Azure Synapse Link, which synchronises data in the data lake using the common data model format.&lt;/p&gt;\n\n&lt;p&gt;Is there any actual benefit to using this format compared to other data formats, eg. Flat files or parquet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "104v8au", "is_robot_indexable": true, "report_reasons": null, "author": "Cypher211", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104v8au/what_are_the_benefits_if_any_of_the_common_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104v8au/what_are_the_benefits_if_any_of_the_common_data/", "subreddit_subscribers": 85377, "created_utc": 1673014502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I stumbled upon [https://github.com/pola-rs/polars/pull/5175](https://github.com/pola-rs/polars/pull/5175) which is merged but I can't find any documentation or examples to invoke `polars-cli`. Does anyone know if this feature is live?\n\n(If it is, this might completely replace any use case I was planning to use `duckdb` for...)", "author_fullname": "t2_3i0xn3gy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone know if polars has a cli?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104pak3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1672995080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I stumbled upon &lt;a href=\"https://github.com/pola-rs/polars/pull/5175\"&gt;https://github.com/pola-rs/polars/pull/5175&lt;/a&gt; which is merged but I can&amp;#39;t find any documentation or examples to invoke &lt;code&gt;polars-cli&lt;/code&gt;. Does anyone know if this feature is live?&lt;/p&gt;\n\n&lt;p&gt;(If it is, this might completely replace any use case I was planning to use &lt;code&gt;duckdb&lt;/code&gt; for...)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nePsGIPEROTJbaCgk4pT9P-oY-VDiVNPrnGbOC8FRGE.jpg?auto=webp&amp;s=255cdb87c03eb466662355e9577645b57ac80681", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/nePsGIPEROTJbaCgk4pT9P-oY-VDiVNPrnGbOC8FRGE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d47595ca117f8c2ce488e57dd0e6b761f6f0dca1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/nePsGIPEROTJbaCgk4pT9P-oY-VDiVNPrnGbOC8FRGE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2349680cc0caefee19942a9eb4ffa2876e4fdee7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/nePsGIPEROTJbaCgk4pT9P-oY-VDiVNPrnGbOC8FRGE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4bd9c3e64d11001e57bd11cac5bfbd4e3d0569e4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/nePsGIPEROTJbaCgk4pT9P-oY-VDiVNPrnGbOC8FRGE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc32be5262a84523304d78f479d5d3fc309dc782", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/nePsGIPEROTJbaCgk4pT9P-oY-VDiVNPrnGbOC8FRGE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=da29e6cba5c95fa95805b35d1d00b1107f071091", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/nePsGIPEROTJbaCgk4pT9P-oY-VDiVNPrnGbOC8FRGE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a9b3bd69e2b956d85bdec7f1b87df38e9ad2ee60", "width": 1080, "height": 540}], "variants": {}, "id": "7TR_RjUZEuKFQfA9f2YpxmtFu-Q7Q2_89v5MzGyVfSg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "104pak3", "is_robot_indexable": true, "report_reasons": null, "author": "ddanieltan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104pak3/does_anyone_know_if_polars_has_a_cli/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104pak3/does_anyone_know_if_polars_has_a_cli/", "subreddit_subscribers": 85377, "created_utc": 1672995080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is general approach when it comes to data that can directly be determined by other data:\n\nX' = f(X)  with X being the fixed data and X' the derived data while f an arbitrary function.\n\nIn particular my questions are:\n\n1. Calculate X' once and store it vs. only store X and apply f when required later on?\n2. How to handle a scenario where X' can either be derived from X or provided directly as \"fixed\" data depending on different data sources? Telemetry data as example: device #1 provides only location data over time (-&gt; velocity (X') can be derived) while device #2 provides both location and velocity data over time (-&gt; velocity (X') can optionally be derived from the location data).\n\n&amp;#x200B;", "author_fullname": "t2_vvk7h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best approach to handle data that is based on other data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1054m6v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673039935.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673037034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is general approach when it comes to data that can directly be determined by other data:&lt;/p&gt;\n\n&lt;p&gt;X&amp;#39; = f(X)  with X being the fixed data and X&amp;#39; the derived data while f an arbitrary function.&lt;/p&gt;\n\n&lt;p&gt;In particular my questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Calculate X&amp;#39; once and store it vs. only store X and apply f when required later on?&lt;/li&gt;\n&lt;li&gt;How to handle a scenario where X&amp;#39; can either be derived from X or provided directly as &amp;quot;fixed&amp;quot; data depending on different data sources? Telemetry data as example: device #1 provides only location data over time (-&amp;gt; velocity (X&amp;#39;) can be derived) while device #2 provides both location and velocity data over time (-&amp;gt; velocity (X&amp;#39;) can optionally be derived from the location data).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1054m6v", "is_robot_indexable": true, "report_reasons": null, "author": "22Maxx", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1054m6v/best_approach_to_handle_data_that_is_based_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1054m6v/best_approach_to_handle_data_that_is_based_on/", "subreddit_subscribers": 85377, "created_utc": 1673037034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nI just wanted to share an article I wrote about deploying machine learning models in production. If you're working with machine learning, you know that building and training models is just the first step. To get the full value from your models, you need to deploy them in production, where they can be used to make real-time predictions and decisions.\n\nBut deploying machine learning models in production can be a complex and challenging task. That's why I wrote this article, \"From Development to Production: A Comprehensive Guide to Deploying Machine Learning Models.\" It covers all the key considerations for deploying machine learning models in production, and provides a framework for doing so successfully.\n\nI hope you find it helpful! Let me know if you have any questions or feedback.\n\n[https://medium.com/p/91aea854437e](https://medium.com/p/91aea854437e)", "author_fullname": "t2_7obzgfpa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From Development to Production: A Comprehensive Guide to Deploying Machine Learning Models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1051lon", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673029904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I just wanted to share an article I wrote about deploying machine learning models in production. If you&amp;#39;re working with machine learning, you know that building and training models is just the first step. To get the full value from your models, you need to deploy them in production, where they can be used to make real-time predictions and decisions.&lt;/p&gt;\n\n&lt;p&gt;But deploying machine learning models in production can be a complex and challenging task. That&amp;#39;s why I wrote this article, &amp;quot;From Development to Production: A Comprehensive Guide to Deploying Machine Learning Models.&amp;quot; It covers all the key considerations for deploying machine learning models in production, and provides a framework for doing so successfully.&lt;/p&gt;\n\n&lt;p&gt;I hope you find it helpful! Let me know if you have any questions or feedback.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/p/91aea854437e\"&gt;https://medium.com/p/91aea854437e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1051lon", "is_robot_indexable": true, "report_reasons": null, "author": "Ill-Psychology-2407", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1051lon/from_development_to_production_a_comprehensive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1051lon/from_development_to_production_a_comprehensive/", "subreddit_subscribers": 85377, "created_utc": 1673029904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Preface:  I'm more of a data engineer and less of a cloud architect.\n\nWe have a Data Factory that is using a self hosted integration runtime on a VM.  This works fine, but now we are standing up a Synapse (serverless) instance and I need it to either connect to the existing self hosted runtime (the Data Factory one) or create a new one.  You cannot install more than one Integration Runtime on the same VM, and it does not seem that you can connect other services to it.\n\nIs there any way to connect Synapse to a SHIR that is already allocated to ADF?", "author_fullname": "t2_4270ek0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Synapse Analytics AND Data Factory users here? I have a question.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104ycyg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673022265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Preface:  I&amp;#39;m more of a data engineer and less of a cloud architect.&lt;/p&gt;\n\n&lt;p&gt;We have a Data Factory that is using a self hosted integration runtime on a VM.  This works fine, but now we are standing up a Synapse (serverless) instance and I need it to either connect to the existing self hosted runtime (the Data Factory one) or create a new one.  You cannot install more than one Integration Runtime on the same VM, and it does not seem that you can connect other services to it.&lt;/p&gt;\n\n&lt;p&gt;Is there any way to connect Synapse to a SHIR that is already allocated to ADF?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "104ycyg", "is_robot_indexable": true, "report_reasons": null, "author": "mr_electric_wizard", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104ycyg/any_synapse_analytics_and_data_factory_users_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104ycyg/any_synapse_analytics_and_data_factory_users_here/", "subreddit_subscribers": 85377, "created_utc": 1673022265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The article [\u2018Parse, Don\u2019t Validate\u2019](https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/) was written by Alexis King in the context of Haskell, but it\u2019s also been mentioned in [this article](https://stianlagstad.no/2022/05/parse-dont-validate-python-edition/) in the context of Python.\n\nI mainly work with **PySpark** and, apart from validation checks, there isn\u2019t really any other way to vouch for the correctness of a table\u2019s data. I was wondering if anyone might know about the notion of \u2018parse, don\u2019t validate\u2019 as applied to a dataframe context.\n\nFor me at least, one application of having a Spark dataframe take on a specific custom type is in functions that become a lot narrower in their usage space, which should hopefully make it more difficult for people to misuse them.\n\nFor instance, maybe there is a function that groups by and aggregates:\n\n    from typing import List\n    \n    def group_by_and_count(\n        input_df: DataFrame,\n        grouping_cols: List[str]\n    ) -&gt; int:\n        return (\n            input_df\n            .groupBy(*grouping_cols)\n            .count()\n        )\n\nThis is a generic function, but perhaps the problem is that it is too generic and allows people to dump whatever they want in there, with no real checks on what goes in the `grouping_cols` parameter.\n\nWhat happens if I, for example, need to return a specific aggregated dataframe that is fed into Tableau and used by data analysts for reporting, where the grouping columns are fixed? I wouldn\u2019t want *any* column to be used (e.g. both country and ice cream flavour are strings, but one would likely be more relevant to stakeholders than the other in Tableau).\n\nIs it even possible to write `group_by_and_count` such that it accepts a set of grouping columns which are of a certain type (and here, something more specific than `str` or `int` \u2013 something like `Country` or `HttpErrorStatus` respectively)? (And, again, I work with PySpark and not Scala Spark.)\n\nI reckon this is a tough problem, since this isn\u2019t usually how we think of dataframes, and validations that we apply involve running queries against a dataframe\u2019s data. But, given that we often struggle with data quality and generally discover poor data after the fact (i.e. through a query), perhaps this helps us while we\u2019re building our workflows instead of after they\u2019re put in production.\n\nOr I could just be overthinking it and there\u2019s already a simpler solution out there. (In which case I\u2019m happy to learn more about it!)", "author_fullname": "t2_a8joqf7z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any way to parse and not validate in PySpark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104w6hi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673027413.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673016974.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The article &lt;a href=\"https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/\"&gt;\u2018Parse, Don\u2019t Validate\u2019&lt;/a&gt; was written by Alexis King in the context of Haskell, but it\u2019s also been mentioned in &lt;a href=\"https://stianlagstad.no/2022/05/parse-dont-validate-python-edition/\"&gt;this article&lt;/a&gt; in the context of Python.&lt;/p&gt;\n\n&lt;p&gt;I mainly work with &lt;strong&gt;PySpark&lt;/strong&gt; and, apart from validation checks, there isn\u2019t really any other way to vouch for the correctness of a table\u2019s data. I was wondering if anyone might know about the notion of \u2018parse, don\u2019t validate\u2019 as applied to a dataframe context.&lt;/p&gt;\n\n&lt;p&gt;For me at least, one application of having a Spark dataframe take on a specific custom type is in functions that become a lot narrower in their usage space, which should hopefully make it more difficult for people to misuse them.&lt;/p&gt;\n\n&lt;p&gt;For instance, maybe there is a function that groups by and aggregates:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from typing import List\n\ndef group_by_and_count(\n    input_df: DataFrame,\n    grouping_cols: List[str]\n) -&amp;gt; int:\n    return (\n        input_df\n        .groupBy(*grouping_cols)\n        .count()\n    )\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This is a generic function, but perhaps the problem is that it is too generic and allows people to dump whatever they want in there, with no real checks on what goes in the &lt;code&gt;grouping_cols&lt;/code&gt; parameter.&lt;/p&gt;\n\n&lt;p&gt;What happens if I, for example, need to return a specific aggregated dataframe that is fed into Tableau and used by data analysts for reporting, where the grouping columns are fixed? I wouldn\u2019t want &lt;em&gt;any&lt;/em&gt; column to be used (e.g. both country and ice cream flavour are strings, but one would likely be more relevant to stakeholders than the other in Tableau).&lt;/p&gt;\n\n&lt;p&gt;Is it even possible to write &lt;code&gt;group_by_and_count&lt;/code&gt; such that it accepts a set of grouping columns which are of a certain type (and here, something more specific than &lt;code&gt;str&lt;/code&gt; or &lt;code&gt;int&lt;/code&gt; \u2013 something like &lt;code&gt;Country&lt;/code&gt; or &lt;code&gt;HttpErrorStatus&lt;/code&gt; respectively)? (And, again, I work with PySpark and not Scala Spark.)&lt;/p&gt;\n\n&lt;p&gt;I reckon this is a tough problem, since this isn\u2019t usually how we think of dataframes, and validations that we apply involve running queries against a dataframe\u2019s data. But, given that we often struggle with data quality and generally discover poor data after the fact (i.e. through a query), perhaps this helps us while we\u2019re building our workflows instead of after they\u2019re put in production.&lt;/p&gt;\n\n&lt;p&gt;Or I could just be overthinking it and there\u2019s already a simpler solution out there. (In which case I\u2019m happy to learn more about it!)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "104w6hi", "is_robot_indexable": true, "report_reasons": null, "author": "haskathon", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104w6hi/is_there_any_way_to_parse_and_not_validate_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104w6hi/is_there_any_way_to_parse_and_not_validate_in/", "subreddit_subscribers": 85377, "created_utc": 1673016974.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hello, I believe we are experiencing an infrastructure issue when trying to extract data from SAP using COPY DATA in ADF. Large tables are timing out and we are unable to get a preview of some specific tables. Has anyone else encountered this problem and found a solution? We have tried breaking the data down by document and date, but it has not helped. Most of us think this is an infrastructure issue, but it's always worth double checking. Do you have any suggestions for efficiently extracting data from SAP on a daily basis? Thank you for your help.", "author_fullname": "t2_ed8nwd3n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Infrastructure Issues with Extracting Data from SAP using COPY DATA in ADF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104jcsa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672975527.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I believe we are experiencing an infrastructure issue when trying to extract data from SAP using COPY DATA in ADF. Large tables are timing out and we are unable to get a preview of some specific tables. Has anyone else encountered this problem and found a solution? We have tried breaking the data down by document and date, but it has not helped. Most of us think this is an infrastructure issue, but it&amp;#39;s always worth double checking. Do you have any suggestions for efficiently extracting data from SAP on a daily basis? Thank you for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "104jcsa", "is_robot_indexable": true, "report_reasons": null, "author": "Similar-Public-8486", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104jcsa/infrastructure_issues_with_extracting_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104jcsa/infrastructure_issues_with_extracting_data_from/", "subreddit_subscribers": 85377, "created_utc": 1672975527.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you manage around tech debt and the increased LOE that exists in a heavily indebted environment?\n\nObviously this is most environments except for new development, but most customers have a hard time understanding LOE, or even how tech debt exacerbates the challenge of an environment.", "author_fullname": "t2_eblvtun2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tech Debt and LOE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104hlyo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672970762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you manage around tech debt and the increased LOE that exists in a heavily indebted environment?&lt;/p&gt;\n\n&lt;p&gt;Obviously this is most environments except for new development, but most customers have a hard time understanding LOE, or even how tech debt exacerbates the challenge of an environment.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "104hlyo", "is_robot_indexable": true, "report_reasons": null, "author": "Glotto_Gold", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104hlyo/tech_debt_and_loe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104hlyo/tech_debt_and_loe/", "subreddit_subscribers": 85377, "created_utc": 1672970762.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}