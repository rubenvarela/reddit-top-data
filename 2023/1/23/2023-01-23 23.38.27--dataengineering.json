{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ggio6wps", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Is Databricks's autoscaling cost efficient?\" - We take a deep dive with the TPC-DS benchmark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_10jfgxm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b_KdZkJwiQj1rTbtg4P_c8pMtKT9rMq5YMn0KV5apTY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674488625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@jeffchousync/is-databrickss-autoscaling-cost-efficient-610e6ece4831", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aYFTorRbi51aUmx2F5kFBnIkjNkCwdfNltJuXdKWvAs.jpg?auto=webp&amp;v=enabled&amp;s=704027bca845b6d95149fadbce17d0d317ac7686", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/aYFTorRbi51aUmx2F5kFBnIkjNkCwdfNltJuXdKWvAs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=784a1e2ed076a4f03aea5223529b76e9981496b6", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/aYFTorRbi51aUmx2F5kFBnIkjNkCwdfNltJuXdKWvAs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04fb76b375b06bd72fe8336444568f14dd5308a5", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/aYFTorRbi51aUmx2F5kFBnIkjNkCwdfNltJuXdKWvAs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e57d7113012a7b5cb6a4f13dde3852e8765662f5", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/aYFTorRbi51aUmx2F5kFBnIkjNkCwdfNltJuXdKWvAs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6e86017fc8279c8dc5f0f164538604e931d029c", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/aYFTorRbi51aUmx2F5kFBnIkjNkCwdfNltJuXdKWvAs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fbf349392eed31e6c6c0f506689ec431f1bdca6", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/aYFTorRbi51aUmx2F5kFBnIkjNkCwdfNltJuXdKWvAs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e08f8c70f8e3ee514cf62bd39ff951a4e7c4a54", "width": 1080, "height": 720}], "variants": {}, "id": "jk_zmYBFc4Ic_fBVeBsjIZA9avp8JovVuVkJ7pxqlXk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10jfgxm", "is_robot_indexable": true, "report_reasons": null, "author": "gobstopper_chicken", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10jfgxm/is_databrickss_autoscaling_cost_efficient_we_take/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@jeffchousync/is-databrickss-autoscaling-cost-efficient-610e6ece4831", "subreddit_subscribers": 87209, "created_utc": 1674488625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI\u2019ve made a few ETLs using pandas, for personal projects as well as for work and it has come very handy.\n\nPandas is very memory intensive so it\u2019s not a good solution for large datasets, which is something I\u2019m facing currently.\n\nI\u2019ve heard that using pure python is a good step before using Spark, but I can\u2019t find an example of ETL using just pure python.\n\nHow would that work? Is there an example script for an ETL without using pandas? \n\nAny help would be appreciated!", "author_fullname": "t2_pcb7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL using pure python (no Pandas)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jgwfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674492186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve made a few ETLs using pandas, for personal projects as well as for work and it has come very handy.&lt;/p&gt;\n\n&lt;p&gt;Pandas is very memory intensive so it\u2019s not a good solution for large datasets, which is something I\u2019m facing currently.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve heard that using pure python is a good step before using Spark, but I can\u2019t find an example of ETL using just pure python.&lt;/p&gt;\n\n&lt;p&gt;How would that work? Is there an example script for an ETL without using pandas? &lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10jgwfs", "is_robot_indexable": true, "report_reasons": null, "author": "IceStallion", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10jgwfs/etl_using_pure_python_no_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10jgwfs/etl_using_pure_python_no_pandas/", "subreddit_subscribers": 87209, "created_utc": 1674492186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is my second data project. I wanted to build an automated dashboard that refreshed daily with data/statistics from the current season of the Premier League. After a couple of months of building, it's now fully automated.\n\nI used Python to extract data from [API-FOOTBALL](https://rapidapi.com/api-sports/api/API-FOOTBALL) which is hosted on RapidAPI (very easy to work with), clean up the data and build dataframes, then load in BigQuery. \n\nThe API didn't have data on stadium locations (lat and lon coordinates) so I took the opportunity to build one with Go and Gin. This API endpoint is hosted on [Cloud Run](https://cloud.google.com/run/docs/overview/what-is-cloud-run). I used [this guide](https://go.dev/doc/tutorial/web-service-gin) to build it.\n\nAll of the Python files are in a Docker container which is hosted on [Artifact Registry](https://cloud.google.com/artifact-registry).\n\nThe infrastructure takes places on Google Cloud. I use [Cloud Scheduler](https://cloud.google.com/scheduler) to trigger the execution of a Cloud Run Job which in turn runs `main.py` which runs the classes from the other Python files. (a Job is different than a Service. Jobs are still in preview). The Job uses the latest Docker digest (image) that is in Artifact Registry.\n\nI was going to stop the project there but decided that learning/implementing CI/CD would only benefit the project and myself so I use [GitHub Actions](https://docs.github.com/en/actions) to build a new Docker image, upload it to Artifact Registry, then deploy to Cloud Run as a Job when a commit is made to the `main` branch.\n\nOne caveat with the workflow is that it only supports deploying as a Service which didn't work for this project. Luckily, I found this [pull request](https://github.com/google-github-actions/deploy-cloudrun/pull/422) where a user modified the code to allow deployment as a Job. This was a godsend and was the final piece of the puzzle.\n\nHere is the [Streamlit dashboard](https://premierleague.streamlit.app/). It\u2019s not great but will continue to improve it now that the backbone is in place. \n\nHere is the [GitHub repo](https://github.com/digitalghost-dev/football-data-pipeline).\n\nHere is a more [detailed document](https://storage.googleapis.com/website-storage-bucket/docs/football-data-pipeline-doc.html) on what's needed to build it.\n\nFlowchart:\n\n(Sorry if it's a mess. It's the best design I could think of.\n\nhttps://preview.redd.it/k3oz0nmm7uda1.png?width=2711&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=26a41a431999b032e52f7529da2cb9493a02b92e", "author_fullname": "t2_bix7v2w5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Another data project, this time with Python, Go, (some SQL), Docker, Google Cloud Services, Streamlit, and GitHub Actions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "media_metadata": {"k3oz0nmm7uda1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 58, "x": 108, "u": "https://preview.redd.it/k3oz0nmm7uda1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9e507b23b0641f354fbf0147580c807885a5bdb"}, {"y": 117, "x": 216, "u": "https://preview.redd.it/k3oz0nmm7uda1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9219e21b1aa10a9a19de1f17f3374c7c327768bf"}, {"y": 174, "x": 320, "u": "https://preview.redd.it/k3oz0nmm7uda1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4972b66ec4566b85953f8488dc15678c2b560e15"}, {"y": 348, "x": 640, "u": "https://preview.redd.it/k3oz0nmm7uda1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=23ca2789cd5e1046bbbff6283df78055a2668238"}, {"y": 522, "x": 960, "u": "https://preview.redd.it/k3oz0nmm7uda1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adddab3cfceae9c54d00a7cbf61f3b76872571df"}, {"y": 587, "x": 1080, "u": "https://preview.redd.it/k3oz0nmm7uda1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=79b011fe0354b7ebc0b043144bd38b7f6736cf2a"}], "s": {"y": 1475, "x": 2711, "u": "https://preview.redd.it/k3oz0nmm7uda1.png?width=2711&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=26a41a431999b032e52f7529da2cb9493a02b92e"}, "id": "k3oz0nmm7uda1"}}, "name": "t3_10jjsfp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/S_VhRppmfdheeScHxGjesOU_4jt49_t2WyC3BWhIhT8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1674498997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is my second data project. I wanted to build an automated dashboard that refreshed daily with data/statistics from the current season of the Premier League. After a couple of months of building, it&amp;#39;s now fully automated.&lt;/p&gt;\n\n&lt;p&gt;I used Python to extract data from &lt;a href=\"https://rapidapi.com/api-sports/api/API-FOOTBALL\"&gt;API-FOOTBALL&lt;/a&gt; which is hosted on RapidAPI (very easy to work with), clean up the data and build dataframes, then load in BigQuery. &lt;/p&gt;\n\n&lt;p&gt;The API didn&amp;#39;t have data on stadium locations (lat and lon coordinates) so I took the opportunity to build one with Go and Gin. This API endpoint is hosted on &lt;a href=\"https://cloud.google.com/run/docs/overview/what-is-cloud-run\"&gt;Cloud Run&lt;/a&gt;. I used &lt;a href=\"https://go.dev/doc/tutorial/web-service-gin\"&gt;this guide&lt;/a&gt; to build it.&lt;/p&gt;\n\n&lt;p&gt;All of the Python files are in a Docker container which is hosted on &lt;a href=\"https://cloud.google.com/artifact-registry\"&gt;Artifact Registry&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;The infrastructure takes places on Google Cloud. I use &lt;a href=\"https://cloud.google.com/scheduler\"&gt;Cloud Scheduler&lt;/a&gt; to trigger the execution of a Cloud Run Job which in turn runs &lt;code&gt;main.py&lt;/code&gt; which runs the classes from the other Python files. (a Job is different than a Service. Jobs are still in preview). The Job uses the latest Docker digest (image) that is in Artifact Registry.&lt;/p&gt;\n\n&lt;p&gt;I was going to stop the project there but decided that learning/implementing CI/CD would only benefit the project and myself so I use &lt;a href=\"https://docs.github.com/en/actions\"&gt;GitHub Actions&lt;/a&gt; to build a new Docker image, upload it to Artifact Registry, then deploy to Cloud Run as a Job when a commit is made to the &lt;code&gt;main&lt;/code&gt; branch.&lt;/p&gt;\n\n&lt;p&gt;One caveat with the workflow is that it only supports deploying as a Service which didn&amp;#39;t work for this project. Luckily, I found this &lt;a href=\"https://github.com/google-github-actions/deploy-cloudrun/pull/422\"&gt;pull request&lt;/a&gt; where a user modified the code to allow deployment as a Job. This was a godsend and was the final piece of the puzzle.&lt;/p&gt;\n\n&lt;p&gt;Here is the &lt;a href=\"https://premierleague.streamlit.app/\"&gt;Streamlit dashboard&lt;/a&gt;. It\u2019s not great but will continue to improve it now that the backbone is in place. &lt;/p&gt;\n\n&lt;p&gt;Here is the &lt;a href=\"https://github.com/digitalghost-dev/football-data-pipeline\"&gt;GitHub repo&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Here is a more &lt;a href=\"https://storage.googleapis.com/website-storage-bucket/docs/football-data-pipeline-doc.html\"&gt;detailed document&lt;/a&gt; on what&amp;#39;s needed to build it.&lt;/p&gt;\n\n&lt;p&gt;Flowchart:&lt;/p&gt;\n\n&lt;p&gt;(Sorry if it&amp;#39;s a mess. It&amp;#39;s the best design I could think of.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/k3oz0nmm7uda1.png?width=2711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=26a41a431999b032e52f7529da2cb9493a02b92e\"&gt;https://preview.redd.it/k3oz0nmm7uda1.png?width=2711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=26a41a431999b032e52f7529da2cb9493a02b92e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xBHA4ELPSDav5ZVxE456KBARw8gkohmcMGvQfmlCDQ8.png?auto=webp&amp;v=enabled&amp;s=6598074286d59906d5c44247a4a696ed5b419537", "width": 2711, "height": 1475}, "resolutions": [{"url": "https://external-preview.redd.it/xBHA4ELPSDav5ZVxE456KBARw8gkohmcMGvQfmlCDQ8.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c1d3e1093d3e56d69e91a37697b7f5f240dcadb", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/xBHA4ELPSDav5ZVxE456KBARw8gkohmcMGvQfmlCDQ8.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d7da8a2b5d6bb21afe4bd7f410416a08b2c233d", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/xBHA4ELPSDav5ZVxE456KBARw8gkohmcMGvQfmlCDQ8.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e38ec4ed8e956c4cac666b17df282e74a488dc7", "width": 320, "height": 174}, {"url": "https://external-preview.redd.it/xBHA4ELPSDav5ZVxE456KBARw8gkohmcMGvQfmlCDQ8.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9f0558b459b606ad122e78d3512a5710025a62c", "width": 640, "height": 348}, {"url": "https://external-preview.redd.it/xBHA4ELPSDav5ZVxE456KBARw8gkohmcMGvQfmlCDQ8.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51f6dc574858c1fbf5d532628844b2a7f3d25add", "width": 960, "height": 522}, {"url": "https://external-preview.redd.it/xBHA4ELPSDav5ZVxE456KBARw8gkohmcMGvQfmlCDQ8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=547334d5b61f6d6ea203bd177c974c9f6069b393", "width": 1080, "height": 587}], "variants": {}, "id": "3uckV975S_JuS7Qnrom3RWIKq9VdiWq91CsXQwedgh8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "10jjsfp", "is_robot_indexable": true, "report_reasons": null, "author": "digitalghost-dev", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10jjsfp/another_data_project_this_time_with_python_go/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10jjsfp/another_data_project_this_time_with_python_go/", "subreddit_subscribers": 87209, "created_utc": 1674498997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, hope you are doing great. \n\nI've been doing Data engineer Jobs for 2 years, tomorrow is my first day as a Data Analyst in a big retail company in my country. As far as i understood in the technical interview, they don't have a modern Data driven architecture, wich I'm sure I'd like to migrate in some near future, by now, i want to be focused in data analytics. So, I've seen quite a bit of videos and posts talking about the impact of BI in analytics, i'm getting into a place where they want me to find KPI, make reports and everything a Data Amalyst has to do.\n\n- How can i make an impact in the business using data analytics/BI/data science?\n\n- is there a standard for retail analytics? What are some areas that can get benefits from analytics?\n\n- what are some best practices for a data analyst with solid Data engineering background? I feel quite anxious because i feel that i can make a real impact but must be patient and make thing right in the right time.\n\n\nThanks for reading me!!! I hope this thread can help anyone going through a work transition.", "author_fullname": "t2_39jk39ho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "switching from DE to DA.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10j2daq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674443775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, hope you are doing great. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been doing Data engineer Jobs for 2 years, tomorrow is my first day as a Data Analyst in a big retail company in my country. As far as i understood in the technical interview, they don&amp;#39;t have a modern Data driven architecture, wich I&amp;#39;m sure I&amp;#39;d like to migrate in some near future, by now, i want to be focused in data analytics. So, I&amp;#39;ve seen quite a bit of videos and posts talking about the impact of BI in analytics, i&amp;#39;m getting into a place where they want me to find KPI, make reports and everything a Data Amalyst has to do.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;How can i make an impact in the business using data analytics/BI/data science?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;is there a standard for retail analytics? What are some areas that can get benefits from analytics?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;what are some best practices for a data analyst with solid Data engineering background? I feel quite anxious because i feel that i can make a real impact but must be patient and make thing right in the right time.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks for reading me!!! I hope this thread can help anyone going through a work transition.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10j2daq", "is_robot_indexable": true, "report_reasons": null, "author": "Rokuzov", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10j2daq/switching_from_de_to_da/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10j2daq/switching_from_de_to_da/", "subreddit_subscribers": 87209, "created_utc": 1674443775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So now that the holidays are over and the implications of dbt Cloud\u2019s abrupt pricing model change have sunk in, what are people doing about dbt Cloud\u2019s last-minute pricing extortion? We vastly underestimated how much it was going to cost, and we learned pretty late that they want annual contracts for Enterprise. Until we figure out the long-term solution, we\u2019re removing our developer seats to just 8 and going month-by-month, as well as reconfiguring our jobs to only run 2 massive ones. There\u2019s zero chance we pay so much money to dbt, especially after how big of a trust breaker this abrupt news was.\n\nSo what is everybody doing? Moving to self hosting and dbt Core, or are there comparable dbt Cloud-esque products out there?\n\nEdit: here is the news for those who didn\u2019t see it: https://www.getdbt.com/blog/dbt-cloud-package-update/\n\nTLDR is price changes all around, and many accounts being forced to move to Enterprise Edition", "author_fullname": "t2_8rod9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Cloud Alternatives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jn7dw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674507842.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674507262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So now that the holidays are over and the implications of dbt Cloud\u2019s abrupt pricing model change have sunk in, what are people doing about dbt Cloud\u2019s last-minute pricing extortion? We vastly underestimated how much it was going to cost, and we learned pretty late that they want annual contracts for Enterprise. Until we figure out the long-term solution, we\u2019re removing our developer seats to just 8 and going month-by-month, as well as reconfiguring our jobs to only run 2 massive ones. There\u2019s zero chance we pay so much money to dbt, especially after how big of a trust breaker this abrupt news was.&lt;/p&gt;\n\n&lt;p&gt;So what is everybody doing? Moving to self hosting and dbt Core, or are there comparable dbt Cloud-esque products out there?&lt;/p&gt;\n\n&lt;p&gt;Edit: here is the news for those who didn\u2019t see it: &lt;a href=\"https://www.getdbt.com/blog/dbt-cloud-package-update/\"&gt;https://www.getdbt.com/blog/dbt-cloud-package-update/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TLDR is price changes all around, and many accounts being forced to move to Enterprise Edition&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?auto=webp&amp;v=enabled&amp;s=d79b538f83c1d1d85f67027cb9fe8d40104143ee", "width": 2560, "height": 1440}, "resolutions": [{"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=406b0dbdfc36080ece4f08c5a16330a51da98e61", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8716eb8504848d6ced17131a496a366a342268d", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b685cf520249fa03a382aed24a5a2b8318953c09", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=946db5724bca2d711e45f05182d67957f980b4e2", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0a6168fcabfdc081ebef4ce5308a59ba70a5db3", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4995118b4ef77d74de7589c0d45bee01db934b2c", "width": 1080, "height": 607}], "variants": {}, "id": "1atsLEhqeX9kFaissrSt8acZk9ienTNBpjUXMcidAMU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10jn7dw", "is_robot_indexable": true, "report_reasons": null, "author": "FecesOfAtheism", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10jn7dw/dbt_cloud_alternatives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10jn7dw/dbt_cloud_alternatives/", "subreddit_subscribers": 87209, "created_utc": 1674507262.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently studying python / sql in order to try my chances into changing careers And I have the opportunity via Microsoft Enterprise Skills Initiative to participate in a free 4 day training session for DP-203. I might also have the opportunity to get a discount in the exam after the training.\n\nGiven the fact that I don't have relevant work experience and I have seen Azure's capabilities only in theory in similar sessions, do you think that it is worth devoting time and effort for the training + exam?", "author_fullname": "t2_sye55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft DP-203 for a newbie.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10j6alm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674457001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently studying python / sql in order to try my chances into changing careers And I have the opportunity via Microsoft Enterprise Skills Initiative to participate in a free 4 day training session for DP-203. I might also have the opportunity to get a discount in the exam after the training.&lt;/p&gt;\n\n&lt;p&gt;Given the fact that I don&amp;#39;t have relevant work experience and I have seen Azure&amp;#39;s capabilities only in theory in similar sessions, do you think that it is worth devoting time and effort for the training + exam?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10j6alm", "is_robot_indexable": true, "report_reasons": null, "author": "Vandertroll89", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10j6alm/microsoft_dp203_for_a_newbie/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10j6alm/microsoft_dp203_for_a_newbie/", "subreddit_subscribers": 87209, "created_utc": 1674457001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everybody !\n\nI have a spark job that is composed as fellows:\n\n1. read static dataFrame from Delta Lake.\n2. read a stream of dataFrame from Delta Lake.\n3. join the stream with the static.\n4. do a flatMapGroupsWithState.\n5. write output.\n\nThe problem is I have a different output from what I expected, like I lost events on `flatMapGroupsWithState`. Not only that, but the output is random. When I re-run with the same input, I get different output.\n\nBut when I added `.coalesce(1)` in the writing operation I always got the desired output in LocalMode but not in ClusterMode.\n\nI also asked in [StackOverflow](https://stackoverflow.com/questions/75209721/spark-flatmapgroupswithstate-random-lost-events).\n\nThank you in advance !", "author_fullname": "t2_m0ecu4ky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "spark flatMapGroupsWithState random lost events", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jc2is", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674487100.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674479227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everybody !&lt;/p&gt;\n\n&lt;p&gt;I have a spark job that is composed as fellows:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;read static dataFrame from Delta Lake.&lt;/li&gt;\n&lt;li&gt;read a stream of dataFrame from Delta Lake.&lt;/li&gt;\n&lt;li&gt;join the stream with the static.&lt;/li&gt;\n&lt;li&gt;do a flatMapGroupsWithState.&lt;/li&gt;\n&lt;li&gt;write output.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The problem is I have a different output from what I expected, like I lost events on &lt;code&gt;flatMapGroupsWithState&lt;/code&gt;. Not only that, but the output is random. When I re-run with the same input, I get different output.&lt;/p&gt;\n\n&lt;p&gt;But when I added &lt;code&gt;.coalesce(1)&lt;/code&gt; in the writing operation I always got the desired output in LocalMode but not in ClusterMode.&lt;/p&gt;\n\n&lt;p&gt;I also asked in &lt;a href=\"https://stackoverflow.com/questions/75209721/spark-flatmapgroupswithstate-random-lost-events\"&gt;StackOverflow&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10jc2is", "is_robot_indexable": true, "report_reasons": null, "author": "Dhia_lunar", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10jc2is/spark_flatmapgroupswithstate_random_lost_events/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10jc2is/spark_flatmapgroupswithstate_random_lost_events/", "subreddit_subscribers": 87209, "created_utc": 1674479227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am an analyst looking to transition over to the DE world. I have been casually looking at job postings and am having a hard time determining whether roles are analytical or platform oriented. From what I understand, DE roles these days are starting to fall into one of these categories. But I know it can vary greatly by company/specific role. I am definitely leaning towards the analytics side just based on my skills and interests; I would like to spend my time applying to roles that are in this space. \n\nAre there any telltale signs that a DE role falls more so into one of those two buckets? For instance are there tools a platform engineer uses that a analytics engineer would never use, or vice versa? Or certain responsibilities that a recruiter might list for one but not the other? Thanks in advance!", "author_fullname": "t2_fldpe8jx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is a job posting an Analytics Engineer or Platform Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10j3bvu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674446776.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an analyst looking to transition over to the DE world. I have been casually looking at job postings and am having a hard time determining whether roles are analytical or platform oriented. From what I understand, DE roles these days are starting to fall into one of these categories. But I know it can vary greatly by company/specific role. I am definitely leaning towards the analytics side just based on my skills and interests; I would like to spend my time applying to roles that are in this space. &lt;/p&gt;\n\n&lt;p&gt;Are there any telltale signs that a DE role falls more so into one of those two buckets? For instance are there tools a platform engineer uses that a analytics engineer would never use, or vice versa? Or certain responsibilities that a recruiter might list for one but not the other? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10j3bvu", "is_robot_indexable": true, "report_reasons": null, "author": "DefiniteKook", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10j3bvu/is_a_job_posting_an_analytics_engineer_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10j3bvu/is_a_job_posting_an_analytics_engineer_or/", "subreddit_subscribers": 87209, "created_utc": 1674446776.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Preface this with I'm a beginner here! Wondering what is the best way to validate data from S3 to Snowflake?\n\nI'm using COPY INTOs to load csv files from an S3 bucket into a set of raw tables.\n\nIdeally would like to do some basic data checks like verify files are fully loaded i.e source = target table\nand store them in a table in snowflake too  or check if certain columns are null\n\nI know you can use use the information schema but ideally like my own table to use if that makes sense\n\n\nAny help would he greatly appreciated", "author_fullname": "t2_u4449", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Checks / Verifications from S3 to Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ix5u5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674428829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Preface this with I&amp;#39;m a beginner here! Wondering what is the best way to validate data from S3 to Snowflake?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using COPY INTOs to load csv files from an S3 bucket into a set of raw tables.&lt;/p&gt;\n\n&lt;p&gt;Ideally would like to do some basic data checks like verify files are fully loaded i.e source = target table\nand store them in a table in snowflake too  or check if certain columns are null&lt;/p&gt;\n\n&lt;p&gt;I know you can use use the information schema but ideally like my own table to use if that makes sense&lt;/p&gt;\n\n&lt;p&gt;Any help would he greatly appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ix5u5", "is_robot_indexable": true, "report_reasons": null, "author": "bulbasaur2016", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ix5u5/data_checks_verifications_from_s3_to_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ix5u5/data_checks_verifications_from_s3_to_snowflake/", "subreddit_subscribers": 87209, "created_utc": 1674428829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Relataively new to DE so seeking advice:\n\nI have been in DE for a couple of months. Current organization uses IBM Datastage and will move to Azure Data Factory/Databricks in the future. 3 other people on the team with great knowledge and really helpful. It will possibly take a year or so to migrate. Things work slow here. \n\nI am in the middle of a process with another company (strong chances of a job offer by next week) that is currently using Pentaho Data Integration and migrating to ApacheHOP in the future. I will be the only DE and the Manager has been managing the workload by themselves until now. Smaller team and a lot more flexibility. \n\nAssuming salary is the same at both the places, does it make sense to take the new job? Which platform is more in demand in Canada/US? \n\nP.S. Salary is the same at both the places", "author_fullname": "t2_adckd9g5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career switch question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jo21k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674509312.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Relataively new to DE so seeking advice:&lt;/p&gt;\n\n&lt;p&gt;I have been in DE for a couple of months. Current organization uses IBM Datastage and will move to Azure Data Factory/Databricks in the future. 3 other people on the team with great knowledge and really helpful. It will possibly take a year or so to migrate. Things work slow here. &lt;/p&gt;\n\n&lt;p&gt;I am in the middle of a process with another company (strong chances of a job offer by next week) that is currently using Pentaho Data Integration and migrating to ApacheHOP in the future. I will be the only DE and the Manager has been managing the workload by themselves until now. Smaller team and a lot more flexibility. &lt;/p&gt;\n\n&lt;p&gt;Assuming salary is the same at both the places, does it make sense to take the new job? Which platform is more in demand in Canada/US? &lt;/p&gt;\n\n&lt;p&gt;P.S. Salary is the same at both the places&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10jo21k", "is_robot_indexable": true, "report_reasons": null, "author": "anonymously_666", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10jo21k/career_switch_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10jo21k/career_switch_question/", "subreddit_subscribers": 87209, "created_utc": 1674509312.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nLooking for an optimize/efficient way to extract data coming from an onprem mssql table to google bigquery. \nThe table schema type are all varchar which is being manage by other department and we only have a read only permission for the table. \n\nOur current setup is to use google dataflow to get the data coming from our on prem database. This is doable however, the data pull would be inefficient since there are no index and the timestamp column of the table has a schema of varchar.    \n\nI was thinking about Converting the timestamp column to timestamp/datetime  but i believe it would take too long as it would convert every row first before applying the filter. \n\nThe goal is to create a batch pipeline for the latest upload.   \n\nPlease advice,  \nThanks", "author_fullname": "t2_are11xb5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EL for mssql db with no index all schema type are varchar", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jlbyy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674502741.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Looking for an optimize/efficient way to extract data coming from an onprem mssql table to google bigquery. \nThe table schema type are all varchar which is being manage by other department and we only have a read only permission for the table. &lt;/p&gt;\n\n&lt;p&gt;Our current setup is to use google dataflow to get the data coming from our on prem database. This is doable however, the data pull would be inefficient since there are no index and the timestamp column of the table has a schema of varchar.    &lt;/p&gt;\n\n&lt;p&gt;I was thinking about Converting the timestamp column to timestamp/datetime  but i believe it would take too long as it would convert every row first before applying the filter. &lt;/p&gt;\n\n&lt;p&gt;The goal is to create a batch pipeline for the latest upload.   &lt;/p&gt;\n\n&lt;p&gt;Please advice,&lt;br/&gt;\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10jlbyy", "is_robot_indexable": true, "report_reasons": null, "author": "Sublime-01", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10jlbyy/el_for_mssql_db_with_no_index_all_schema_type_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10jlbyy/el_for_mssql_db_with_no_index_all_schema_type_are/", "subreddit_subscribers": 87209, "created_utc": 1674502741.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Data  Engineers - long time lurker but first time poster here looking for some recommendations / thoughts. I\u2019m currently a data analyst but have been more interested in getting into the DE side of things for a while. Recently I was given a great opportunity to start my transition by reassessing how my team handles incoming data and essentially start from scratch with a new solution that I can implement. \n\nSome important points to keep in mind: this process is not business critical and it only needs to run once a month. Data comes in from 3rd party company once a month in CSV file format. Typically between 150-200K rows each and 7/8 files. The files come in very messy and usually require some formatting, QC checks and minor cleaning before it makes any sense. \n\nThe current process:\n- Files stored in OneDrive and read into Alteryx where some transformations are done before writing to a SQL table. Each file has their own corresponding flattened table.\n- These tables are then used in another alteryx workflow which does quite a few transformations mainly involving column &amp; row wise calculations and doesn\u2019t require many joins to other tables. The result of this workflow is stored in another table in the same database which is then used for BI.\n- The main issue with this is the Alteryx workflow takes 15/16 hours to complete and I just believe it\u2019s becoming a bit obsolete. It also involves a lot of manual updates each month to point to the new files.\n\nWhat I\u2019ve brainstormed as a potential solution (please be kind, this is my first DE project).\n- Python script to run when the files come in which uses Pandas for some computations and cleansing such as adding columns, renaming, filling nulls and changing data types. \n- I dump the result of this Python script into SQL server using SQLAlchemy. I assume these tables function more like a data lake. Again each file has its own flat table. The files themselves don\u2019t have any relation to eachother so having fact and dim tables I\u2019m not sure makes sense here (?)\n- The plan is to then create Python scripts to read data out from these tables, do transformations on them and enhance the data mainly using lambdas and other custom functions. The output will be a table that the analysts can connect to and simply query for BI.\n\nThe tools available immediately that I\u2019d like to use mostly are obviously SQL (SQL Server) &amp; Python. I\u2019ve also read about Polars which may be an alternative to doing all my transformations in Pandas. We currently don\u2019t have any S3 instances or anything cloud based.\n\nI\u2019d appreciate any thoughts, recommendations or resources that you think may be helpful. I\u2019m trying to learn as much as possible for this project and make it as automated and efficient as can be.\n\nThank you in advance!", "author_fullname": "t2_az62upwd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pandas ETL - looking for thoughts on current process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10jpjev", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674512906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Data  Engineers - long time lurker but first time poster here looking for some recommendations / thoughts. I\u2019m currently a data analyst but have been more interested in getting into the DE side of things for a while. Recently I was given a great opportunity to start my transition by reassessing how my team handles incoming data and essentially start from scratch with a new solution that I can implement. &lt;/p&gt;\n\n&lt;p&gt;Some important points to keep in mind: this process is not business critical and it only needs to run once a month. Data comes in from 3rd party company once a month in CSV file format. Typically between 150-200K rows each and 7/8 files. The files come in very messy and usually require some formatting, QC checks and minor cleaning before it makes any sense. &lt;/p&gt;\n\n&lt;p&gt;The current process:\n- Files stored in OneDrive and read into Alteryx where some transformations are done before writing to a SQL table. Each file has their own corresponding flattened table.\n- These tables are then used in another alteryx workflow which does quite a few transformations mainly involving column &amp;amp; row wise calculations and doesn\u2019t require many joins to other tables. The result of this workflow is stored in another table in the same database which is then used for BI.\n- The main issue with this is the Alteryx workflow takes 15/16 hours to complete and I just believe it\u2019s becoming a bit obsolete. It also involves a lot of manual updates each month to point to the new files.&lt;/p&gt;\n\n&lt;p&gt;What I\u2019ve brainstormed as a potential solution (please be kind, this is my first DE project).\n- Python script to run when the files come in which uses Pandas for some computations and cleansing such as adding columns, renaming, filling nulls and changing data types. \n- I dump the result of this Python script into SQL server using SQLAlchemy. I assume these tables function more like a data lake. Again each file has its own flat table. The files themselves don\u2019t have any relation to eachother so having fact and dim tables I\u2019m not sure makes sense here (?)\n- The plan is to then create Python scripts to read data out from these tables, do transformations on them and enhance the data mainly using lambdas and other custom functions. The output will be a table that the analysts can connect to and simply query for BI.&lt;/p&gt;\n\n&lt;p&gt;The tools available immediately that I\u2019d like to use mostly are obviously SQL (SQL Server) &amp;amp; Python. I\u2019ve also read about Polars which may be an alternative to doing all my transformations in Pandas. We currently don\u2019t have any S3 instances or anything cloud based.&lt;/p&gt;\n\n&lt;p&gt;I\u2019d appreciate any thoughts, recommendations or resources that you think may be helpful. I\u2019m trying to learn as much as possible for this project and make it as automated and efficient as can be.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10jpjev", "is_robot_indexable": true, "report_reasons": null, "author": "hektar9987", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10jpjev/pandas_etl_looking_for_thoughts_on_current_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10jpjev/pandas_etl_looking_for_thoughts_on_current_process/", "subreddit_subscribers": 87209, "created_utc": 1674512906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started out as an analyst in a small company about a year ago with the keys to an AWS account. I've spent the year organising my team's data into an S3 bucket and using python scripts to create the necessary pipelines into and out of the bucket. The files are in parquet format and partitioned, with an Athena database sitting on top for querying. The metadata that manages what is in the bucket is sat in an SQL Server database. When I want to slice and process the files in the bucket, I query the metadata tables and use that to query the bucket.\n\nIt's all running nicely but I'm totally unaware of what I've built and how to contextualise it. Is it just an organised lake? I read about lakehouses but it doesn't seem to have a lot of what they seem to have. Currently, I only have SQL query ability into the data unless I pull it onto a server. Is there a way of doing more, i.e direct pandas sort of work? I've heard about Databricks and Spark and have a vague idea of distributing data and processing power across clusters, but don't know if that meshes with what I'm doing? As far as I know, S3 is a distributed file system but is a cluster somewhat different? I was asked why I didn't put my metadata in the bucket as well and I didn't have an answer. I just felt that the metadata seemed more warehousey than lakey; more CRUD. Is that me hanging on to a past I didn't know I had? The broader question is: how do I explain what I've done in the context of modern data architectures? Any book recommendations are also welcome.", "author_fullname": "t2_djdhkrg6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made a thing. What is it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jn42d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674507043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started out as an analyst in a small company about a year ago with the keys to an AWS account. I&amp;#39;ve spent the year organising my team&amp;#39;s data into an S3 bucket and using python scripts to create the necessary pipelines into and out of the bucket. The files are in parquet format and partitioned, with an Athena database sitting on top for querying. The metadata that manages what is in the bucket is sat in an SQL Server database. When I want to slice and process the files in the bucket, I query the metadata tables and use that to query the bucket.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s all running nicely but I&amp;#39;m totally unaware of what I&amp;#39;ve built and how to contextualise it. Is it just an organised lake? I read about lakehouses but it doesn&amp;#39;t seem to have a lot of what they seem to have. Currently, I only have SQL query ability into the data unless I pull it onto a server. Is there a way of doing more, i.e direct pandas sort of work? I&amp;#39;ve heard about Databricks and Spark and have a vague idea of distributing data and processing power across clusters, but don&amp;#39;t know if that meshes with what I&amp;#39;m doing? As far as I know, S3 is a distributed file system but is a cluster somewhat different? I was asked why I didn&amp;#39;t put my metadata in the bucket as well and I didn&amp;#39;t have an answer. I just felt that the metadata seemed more warehousey than lakey; more CRUD. Is that me hanging on to a past I didn&amp;#39;t know I had? The broader question is: how do I explain what I&amp;#39;ve done in the context of modern data architectures? Any book recommendations are also welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10jn42d", "is_robot_indexable": true, "report_reasons": null, "author": "user192034", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10jn42d/i_made_a_thing_what_is_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10jn42d/i_made_a_thing_what_is_it/", "subreddit_subscribers": 87209, "created_utc": 1674507043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nJust gathering feedback on Cloud Data Integration service on IDMC. Any thoughts/comments on this ?\n\nThanks!", "author_fullname": "t2_4mdsqonb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feedback on Cloud Data Integration service with Informatica's IDMC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jmyxr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674506701.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Just gathering feedback on Cloud Data Integration service on IDMC. Any thoughts/comments on this ?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10jmyxr", "is_robot_indexable": true, "report_reasons": null, "author": "vrakshith28", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10jmyxr/feedback_on_cloud_data_integration_service_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10jmyxr/feedback_on_cloud_data_integration_service_with/", "subreddit_subscribers": 87209, "created_utc": 1674506701.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wrote a post about creating a CICD pipeline using Github Actions for ETL jobs. I wanted to share with folks here for feedback and suggestions on how to improve/optimize my recommendations.  [https://www.upsolver.com/blog/change-data-capture-cdc-etl-pipelines-for-real-time-replication](https://www.upsolver.com/blog/change-data-capture-cdc-etl-pipelines-for-real-time-replication)", "author_fullname": "t2_vhiekvgo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CICD for data pipelines using Github Actions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jcc8l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.27, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674480046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote a post about creating a CICD pipeline using Github Actions for ETL jobs. I wanted to share with folks here for feedback and suggestions on how to improve/optimize my recommendations.  &lt;a href=\"https://www.upsolver.com/blog/change-data-capture-cdc-etl-pipelines-for-real-time-replication\"&gt;https://www.upsolver.com/blog/change-data-capture-cdc-etl-pipelines-for-real-time-replication&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A15wGK1xd9DrI4XAKXVOty9XP2QOi6yCUMeKrj3ySD0.jpg?auto=webp&amp;v=enabled&amp;s=61f05f5efdaab9d2aadb67b4f8445aa456d39b88", "width": 1856, "height": 765}, "resolutions": [{"url": "https://external-preview.redd.it/A15wGK1xd9DrI4XAKXVOty9XP2QOi6yCUMeKrj3ySD0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=015542d9fc1aaec0d3e31d33b54fcecf121c5789", "width": 108, "height": 44}, {"url": "https://external-preview.redd.it/A15wGK1xd9DrI4XAKXVOty9XP2QOi6yCUMeKrj3ySD0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e8b2ac637ca1fcb666232302c8041d6669c050c", "width": 216, "height": 89}, {"url": "https://external-preview.redd.it/A15wGK1xd9DrI4XAKXVOty9XP2QOi6yCUMeKrj3ySD0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7c752c33f853c46268d2c351eb99cd1bf8688ee", "width": 320, "height": 131}, {"url": "https://external-preview.redd.it/A15wGK1xd9DrI4XAKXVOty9XP2QOi6yCUMeKrj3ySD0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4616db014136813f2ec66d19a7725b794e6fe70", "width": 640, "height": 263}, {"url": "https://external-preview.redd.it/A15wGK1xd9DrI4XAKXVOty9XP2QOi6yCUMeKrj3ySD0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7c81d8aefcab4d7198ea1cff071cf1e2e11e6d81", "width": 960, "height": 395}, {"url": "https://external-preview.redd.it/A15wGK1xd9DrI4XAKXVOty9XP2QOi6yCUMeKrj3ySD0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3386602bfced0f3c4ff23aae85924db63e9a9c0", "width": 1080, "height": 445}], "variants": {}, "id": "a2TEN80IVj95i-oqeVGuwTnpm5OLg60lPCujEK133ss"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10jcc8l", "is_robot_indexable": true, "report_reasons": null, "author": "royondata", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10jcc8l/cicd_for_data_pipelines_using_github_actions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10jcc8l/cicd_for_data_pipelines_using_github_actions/", "subreddit_subscribers": 87209, "created_utc": 1674480046.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}