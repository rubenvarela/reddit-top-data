{"kind": "Listing", "data": {"after": "t3_103bciw", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The two books are regularly mentioned as must-reads for DE's ; but they're pretty long, and some parts are outdated. What parts are still important to read for someone who wants to get into the field, has a technical data background, but doesn't want to read 1000 odd pages?\n\n&amp;#x200B;\n\nAlso, would you recommend any other books? specifically any with practice problems?", "author_fullname": "t2_80ytrhvd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What parts of \"the data warehouse toolkit\" and \"designing data intensive applications\" are important to read?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103j5cq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 84, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 84, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672875912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The two books are regularly mentioned as must-reads for DE&amp;#39;s ; but they&amp;#39;re pretty long, and some parts are outdated. What parts are still important to read for someone who wants to get into the field, has a technical data background, but doesn&amp;#39;t want to read 1000 odd pages?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Also, would you recommend any other books? specifically any with practice problems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "103j5cq", "is_robot_indexable": true, "report_reasons": null, "author": "Particular-Bet-1828", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103j5cq/what_parts_of_the_data_warehouse_toolkit_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103j5cq/what_parts_of_the_data_warehouse_toolkit_and/", "subreddit_subscribers": 85240, "created_utc": 1672875912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a2zxqic6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Meltano can now run any Airbyte source connector thanks to a community contribution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1039njv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yanii5NteK25H5LV0gyfgeycmLXX6ot1nn0-48RvJsA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1672853981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "meltano.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://meltano.com/blog/meltano-community-contribution-enables-running-over-200-airbyte-connectors/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?auto=webp&amp;s=c281f7f1f173c9d757c14c22bd02904df6bbe926", "width": 2048, "height": 1365}, "resolutions": [{"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=542e31c66973c837670c355c9b39789836012881", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a0f1441845dca9f1f291d639b7b6bef2b15e3f7b", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f28b7d959f0f9abe9b78874b258d5d2db84e826e", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9dcf2528426908213926e342fd45708c44169b5", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=39a2699a04982e2cfa4c07fb50348c5629825b3f", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=354068f1be19deb6fe35dee8f01873f6817f072a", "width": 1080, "height": 719}], "variants": {}, "id": "UaWSMIAUKV85uXdc7bNp-jFBUuKP9UuqUorN-KPwqrg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1039njv", "is_robot_indexable": true, "report_reasons": null, "author": "tayloramurphy", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1039njv/meltano_can_now_run_any_airbyte_source_connector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://meltano.com/blog/meltano-community-contribution-enables-running-over-200-airbyte-connectors/", "subreddit_subscribers": 85240, "created_utc": 1672853981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7ec0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seems like astronomer quietly laid off 20%", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 44, "top_awarded_type": null, "hide_score": false, "name": "t3_1041369", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": "transparent", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Rq3tQTuon_x0D3gcWbR6BhAKt-fwmgL_vf0CpFaIGfQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1672931012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8cdbcg7t6aaa1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8cdbcg7t6aaa1.jpg?auto=webp&amp;s=9caccdfd4087c21d8f44722bb72e7b04aeee5833", "width": 826, "height": 262}, "resolutions": [{"url": "https://preview.redd.it/8cdbcg7t6aaa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c5fa4be7d0ca0e6d8eeabc05485afc81dc70cfa", "width": 108, "height": 34}, {"url": "https://preview.redd.it/8cdbcg7t6aaa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cae95eb3133108b12216ac2970525b36facfbef3", "width": 216, "height": 68}, {"url": "https://preview.redd.it/8cdbcg7t6aaa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bfa3262c678ec1fd5fd40bdb9a16e372785608b9", "width": 320, "height": 101}, {"url": "https://preview.redd.it/8cdbcg7t6aaa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c29b98e6d3915d6854111531090c2f424355b3ce", "width": 640, "height": 203}], "variants": {}, "id": "5kCs9draxkLO05Fz6K42BxwSnQEG2kMSmOF0AO9uXhE"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1041369", "is_robot_indexable": true, "report_reasons": null, "author": "mistanervous", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1041369/seems_like_astronomer_quietly_laid_off_20/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8cdbcg7t6aaa1.jpg", "subreddit_subscribers": 85240, "created_utc": 1672931012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was last fully employed as a DE two years ago but took off due to a surgery that was a very long recovery, had to get a re-surgery done and I'm just getting over the depression related to all of it.  Before this I had 8 years of experience.\n\nI am wondering if it's a hard job market to get back into right now due to the recession, or if hopefully it's the opposite and companies are still looking for employees in this area.  I've checked in with old colleagues and there doesn't seem to be much new technology that I would need to get up to speed on.", "author_fullname": "t2_cb9rpnop", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How's the job market right now?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103kw8y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672880326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was last fully employed as a DE two years ago but took off due to a surgery that was a very long recovery, had to get a re-surgery done and I&amp;#39;m just getting over the depression related to all of it.  Before this I had 8 years of experience.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if it&amp;#39;s a hard job market to get back into right now due to the recession, or if hopefully it&amp;#39;s the opposite and companies are still looking for employees in this area.  I&amp;#39;ve checked in with old colleagues and there doesn&amp;#39;t seem to be much new technology that I would need to get up to speed on.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "103kw8y", "is_robot_indexable": true, "report_reasons": null, "author": "sammessa789", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103kw8y/hows_the_job_market_right_now/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103kw8y/hows_the_job_market_right_now/", "subreddit_subscribers": 85240, "created_utc": 1672880326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I finally switched jobs. \nMy previous company was TCS, the management was toxic and I got burnt out trying to prove my worth to stay on a good project. I was moved from project to project without any feedback and pushed around at their will. I tried to get my hands on development work but they kept pulling me out of it, only to make me manage ops. As they dint have anyone else who would stick there and the senior who was the lead, and my mentor initially, ended up creating a huge dependency on himself and no one saw. Until he went on vacation and things blew up, I was pulled in and had to handle it. \n\nI finally put my papers down and started my new job this year. I\u2019m a full time DE from an data ops/dev role. \nIt\u2019s my second day on the job and I\u2019ve been assigned a project and I\u2019m expected to be the only person doing 100% of the ELT migration. The person who I\u2019m suppose to work with on the same project, will be at 50% availability. She even mentioned she hadn\u2019t even worked on the project for the past three months as there were budget cuts and they did not allocate hours. The principle lead mentioned they might be losing money too, but that\u2019s not something I should worry about. I\u2019m assigned to migrate the ETL and that\u2019s it. \nThe situation is, I\u2019ve never developed something from scratch as my old job was a very controlled environment and not enough room to explore. \n\nI have cold feet and I\u2019m freaking out that they might fire me. A little extreme I know. But yeah. \nAlso, I don\u2019t know why I\u2019m hesitant to mention I have just 3 years of experience and I might need more guidance. \nI\u2019ve been belittle/gaslit for asking questions/being honest in my previous role. I\u2019m just scared I might be over sharing and people might take advantage. \n\n\nCan I just tell them I need some handholding to quicken my pace of learning and how things are done there? \nThoughts?", "author_fullname": "t2_2b4iqu2b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New DE, third day on the job. Need advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103xjn5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1672922845.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672920959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I finally switched jobs. \nMy previous company was TCS, the management was toxic and I got burnt out trying to prove my worth to stay on a good project. I was moved from project to project without any feedback and pushed around at their will. I tried to get my hands on development work but they kept pulling me out of it, only to make me manage ops. As they dint have anyone else who would stick there and the senior who was the lead, and my mentor initially, ended up creating a huge dependency on himself and no one saw. Until he went on vacation and things blew up, I was pulled in and had to handle it. &lt;/p&gt;\n\n&lt;p&gt;I finally put my papers down and started my new job this year. I\u2019m a full time DE from an data ops/dev role. \nIt\u2019s my second day on the job and I\u2019ve been assigned a project and I\u2019m expected to be the only person doing 100% of the ELT migration. The person who I\u2019m suppose to work with on the same project, will be at 50% availability. She even mentioned she hadn\u2019t even worked on the project for the past three months as there were budget cuts and they did not allocate hours. The principle lead mentioned they might be losing money too, but that\u2019s not something I should worry about. I\u2019m assigned to migrate the ETL and that\u2019s it. \nThe situation is, I\u2019ve never developed something from scratch as my old job was a very controlled environment and not enough room to explore. &lt;/p&gt;\n\n&lt;p&gt;I have cold feet and I\u2019m freaking out that they might fire me. A little extreme I know. But yeah. \nAlso, I don\u2019t know why I\u2019m hesitant to mention I have just 3 years of experience and I might need more guidance. \nI\u2019ve been belittle/gaslit for asking questions/being honest in my previous role. I\u2019m just scared I might be over sharing and people might take advantage. &lt;/p&gt;\n\n&lt;p&gt;Can I just tell them I need some handholding to quicken my pace of learning and how things are done there? \nThoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "103xjn5", "is_robot_indexable": true, "report_reasons": null, "author": "Shrey_aa", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103xjn5/new_de_third_day_on_the_job_need_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103xjn5/new_de_third_day_on_the_job_need_advice/", "subreddit_subscribers": 85240, "created_utc": 1672920959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a DE, most of the times, we are struggling to keep up with the hard deadlines of the businesses and end up putting together bare-bones pipelines  that don\u2019t always follow the best practices. \n\nHow do you as a team tackle the tech debt, and also implement best practices? \n\nDo you have specific sprints quarterly or early to deal with these?", "author_fullname": "t2_msviuy1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you clear tech debt and implement software engineering best practices in your data workflows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103ezjm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672866242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a DE, most of the times, we are struggling to keep up with the hard deadlines of the businesses and end up putting together bare-bones pipelines  that don\u2019t always follow the best practices. &lt;/p&gt;\n\n&lt;p&gt;How do you as a team tackle the tech debt, and also implement best practices? &lt;/p&gt;\n\n&lt;p&gt;Do you have specific sprints quarterly or early to deal with these?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "103ezjm", "is_robot_indexable": true, "report_reasons": null, "author": "vino_and_data", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103ezjm/how_do_you_clear_tech_debt_and_implement_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103ezjm/how_do_you_clear_tech_debt_and_implement_software/", "subreddit_subscribers": 85240, "created_utc": 1672866242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One of the biggest pain points in our data organization is self-service. There is no documentation and we often field questions like \"Where does this data come from?\" and \"How is this derived?\"  \n\nI'm doing a dbt pilot and I'm wondering why I shouldn't just open up my dbt documentation to anyone in the org who wants it. Is there a security issue I'm not thinking of? I've heard that dbt docs are \"good for developers\" but I don't see why a SQL-savvy business user couldn't benefit too.", "author_fullname": "t2_bdzq3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any reason why I shouldn't give my dbt docs to everyone?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103szq0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672904255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of the biggest pain points in our data organization is self-service. There is no documentation and we often field questions like &amp;quot;Where does this data come from?&amp;quot; and &amp;quot;How is this derived?&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m doing a dbt pilot and I&amp;#39;m wondering why I shouldn&amp;#39;t just open up my dbt documentation to anyone in the org who wants it. Is there a security issue I&amp;#39;m not thinking of? I&amp;#39;ve heard that dbt docs are &amp;quot;good for developers&amp;quot; but I don&amp;#39;t see why a SQL-savvy business user couldn&amp;#39;t benefit too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "103szq0", "is_robot_indexable": true, "report_reasons": null, "author": "Acewox", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103szq0/any_reason_why_i_shouldnt_give_my_dbt_docs_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103szq0/any_reason_why_i_shouldnt_give_my_dbt_docs_to/", "subreddit_subscribers": 85240, "created_utc": 1672904255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is part 1 of an amazing episode we recorded with Arjun and Frank. Talking about Materialize, the technology on which low latency incremental materialization is based on and what it takes to build a product on top of a technology. \n\nOne of the best episodes we had so far, make sure to check it!\n\n[https://datastackshow.com/podcast/materialize-origins-a-timely-dataflow-story-with-arjun-narayan-and-frank-mcsherry/](https://datastackshow.com/podcast/materialize-origins-a-timely-dataflow-story-with-arjun-narayan-and-frank-mcsherry/)", "author_fullname": "t2_fb1s1pke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Arjun Narayan and Frank McSherry Talking about Materialize and Timely Dataflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103i38k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1672873404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is part 1 of an amazing episode we recorded with Arjun and Frank. Talking about Materialize, the technology on which low latency incremental materialization is based on and what it takes to build a product on top of a technology. &lt;/p&gt;\n\n&lt;p&gt;One of the best episodes we had so far, make sure to check it!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://datastackshow.com/podcast/materialize-origins-a-timely-dataflow-story-with-arjun-narayan-and-frank-mcsherry/\"&gt;https://datastackshow.com/podcast/materialize-origins-a-timely-dataflow-story-with-arjun-narayan-and-frank-mcsherry/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xRssCC1DrhB726t1EqWWgxRj0kh4tIX0mEwjBUUtIZY.jpg?auto=webp&amp;s=56ae6913b6f7de5c324bfdffc75f7f84afd44f39", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/xRssCC1DrhB726t1EqWWgxRj0kh4tIX0mEwjBUUtIZY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7023376789bc866810a8553914dfa3d297ebfc65", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/xRssCC1DrhB726t1EqWWgxRj0kh4tIX0mEwjBUUtIZY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=04ba6e4e9ab84659874eafb0e0075b6388f447ba", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/xRssCC1DrhB726t1EqWWgxRj0kh4tIX0mEwjBUUtIZY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6cecd9e330bd5544b9a5c997121e36d9846c4dfc", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/xRssCC1DrhB726t1EqWWgxRj0kh4tIX0mEwjBUUtIZY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=44ea216d54a28b3c8adce8d7c0f593be948edefc", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/xRssCC1DrhB726t1EqWWgxRj0kh4tIX0mEwjBUUtIZY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d721d5ff89f366bed6d972456a2d24922a118e61", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/xRssCC1DrhB726t1EqWWgxRj0kh4tIX0mEwjBUUtIZY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=409aa1809d3531309524842a75de1e67ff593095", "width": 1080, "height": 607}], "variants": {}, "id": "YOd5mOBpzsexj2GJIPxAWvPJ5gaa66EkKbvUVlXUfZY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "103i38k", "is_robot_indexable": true, "report_reasons": null, "author": "cpardl", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103i38k/arjun_narayan_and_frank_mcsherry_talking_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103i38k/arjun_narayan_and_frank_mcsherry_talking_about/", "subreddit_subscribers": 85240, "created_utc": 1672873404.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I want to share the Kubeflow tutorial (Machine Learning Operations on Kubernetes), and usage scenarios that I created as projects for myself. I know that Kubeflow is a detailed topic to learn in a short term, so I gathered useful information and create sample general usage scenarios of Kubeflow.\n\nThis repo covers Kubeflow Environment with LABs: Kubeflow GUI, Jupyter Notebooks running on Kubernetes Pod, Kubeflow Pipeline, KALE (Kubeflow Automated PipeLines Engine), KATIB (AutoML: Finding Best Hyperparameter Values), KFServe (Model Serving), Training Operators (Distributed Training), Projects, etc. Possible usage scenarios are aimed to update over time.\n\nKubeflow is powerful tool that runs on Kubernetes (K8s) with containers (process isolation, scaling, distributed and parallel training).\n\n**This repo makes easy to learn and apply projects on your local machine with MiniKF, Virtualbox and Vagrant without any FEE**.\n\n**Tutorial Link:** [**https://github.com/omerbsezer/Fast-Kubeflow**](https://github.com/omerbsezer/Fast-Kubeflow)\n\n**Extra Kubernetes-Tutorial Link:** [**https://github.com/omerbsezer/Fast-Kubernetes**](https://github.com/omerbsezer/Fast-Kubernetes)\n\n**Extra Docker-Tutorial Link:** [**https://github.com/omerbsezer/Fast-Docker**](https://github.com/omerbsezer/Fast-Docker)\n\nQuick Look (HowTo): Scenarios - Hands-on LABs\n\n* [LAB: Creating LAB Environment (WSL2), Installing Kubeflow with MicroK8s, Juju on Ubuntu 20.04](https://github.com/omerbsezer/Fast-Kubeflow/blob/main/Installing-Kubeflow.md)\n* [LAB: Creating LAB Environment, Installing MiniKF with Vagrant (Preffered for Easy Usage)](https://github.com/omerbsezer/Fast-Kubeflow/blob/main/Using-MiniKF.md)\n* [LAB/Project: Kubeflow Pipeline (From Scratch) with Kubeflow SDK (DSL Compiler) and Custom Docker Images (Decision Tree, Logistic Regression, SVM, Naive Bayes, Xg Boost)](https://github.com/omerbsezer/Fast-Kubeflow/blob/main/Kubeflow-Pipeline-Project.md)\n* [LAB/Project: KALE (Kubeflow Automated PipeLines Engine) and KATIB (AutoML: Finding Best Hyperparameter Values)](https://github.com/omerbsezer/Fast-Kubeflow/blob/main/Kale-Katib-Project.md)\n* [LAB/Project: KALE (Kubeflow Automated PipeLines Engine) and KServe (Model Serving) for Model Prediction](https://github.com/omerbsezer/Fast-Kubeflow/blob/main/KALE-KServe.md)\n* [LAB/Project: Distributed Training with Tensorflow (MNIST data)](https://github.com/omerbsezer/Fast-Kubeflow/blob/main/Distributed-Training-Tensorflow.md)\n\n**Table of Contents**\n\n* [Motivation](https://github.com/omerbsezer/Fast-Kubeflow#motivation)\n* [What is Kubelow?](https://github.com/omerbsezer/Fast-Kubeflow#whatIsKubeflow)\n* [How Kubeflow Works?](https://github.com/omerbsezer/Fast-Kubeflow#howKubeflowWorks)\n* [What is Container (Docker)?](https://github.com/omerbsezer/Fast-Kubeflow#whatareContainers)\n* [What is Kubernetes?](https://github.com/omerbsezer/Fast-Kubeflow#whatisKubeflow)\n* [Installing Kubeflow](https://github.com/omerbsezer/Fast-Kubeflow#labEnvironment)\n* [Kubeflow Basics](https://github.com/omerbsezer/Fast-Kubeflow#basics)\n* [Kubeflow Jupyter Notebook](https://github.com/omerbsezer/Fast-Kubeflow#notebook)\n* [Kubeflow Pipeline](https://github.com/omerbsezer/Fast-Kubeflow#pipeline)\n* [KALE (Kubeflow Automated PipeLines Engine)](https://github.com/omerbsezer/Fast-Kubeflow#kale)\n* [KATIB (AutoML: Finding Best Hyperparameter Values)](https://github.com/omerbsezer/Fast-Kubeflow#katib)\n* [KServe (Model Serving)](https://github.com/omerbsezer/Fast-Kubeflow#kserve)\n* [Training-Operators (Distributed Training)](https://github.com/omerbsezer/Fast-Kubeflow#operator)\n* [Minio (Object Storage) and ROK (Data Management Platform)](https://github.com/omerbsezer/Fast-Kubeflow#minio_rok)\n* [Project 1: Creating ML Pipeline with Custom Docker Images (Decision Tree, Logistic Regression, SVM, Naive Bayes, Xg Boost)](https://github.com/omerbsezer/Fast-Kubeflow#project1)\n* [Project 2: KALE (Kubeflow Automated PipeLines Engine) and KATIB (AutoML: Finding Best Hyperparameter Values)](https://github.com/omerbsezer/Fast-Kubeflow#project2)\n* [Project 3: KALE (Kubeflow Automated PipeLines Engine) and KServe (Model Serving) for Model Prediction](https://github.com/omerbsezer/Fast-Kubeflow#project3)\n* [Project 4: Distributed Training with Training Operator](https://github.com/omerbsezer/Fast-Kubeflow#project4)\n* [Other Useful Resources Related Kubeflow](https://github.com/omerbsezer/Fast-Kubeflow#resource)\n* [References](https://github.com/omerbsezer/Fast-Kubeflow#references)", "author_fullname": "t2_muiope", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fast-Kubeflow: Kubeflow Tutorial, Sample Usage Scenarios (Howto: Hands-on LAB)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103bk24", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1672858333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to share the Kubeflow tutorial (Machine Learning Operations on Kubernetes), and usage scenarios that I created as projects for myself. I know that Kubeflow is a detailed topic to learn in a short term, so I gathered useful information and create sample general usage scenarios of Kubeflow.&lt;/p&gt;\n\n&lt;p&gt;This repo covers Kubeflow Environment with LABs: Kubeflow GUI, Jupyter Notebooks running on Kubernetes Pod, Kubeflow Pipeline, KALE (Kubeflow Automated PipeLines Engine), KATIB (AutoML: Finding Best Hyperparameter Values), KFServe (Model Serving), Training Operators (Distributed Training), Projects, etc. Possible usage scenarios are aimed to update over time.&lt;/p&gt;\n\n&lt;p&gt;Kubeflow is powerful tool that runs on Kubernetes (K8s) with containers (process isolation, scaling, distributed and parallel training).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;This repo makes easy to learn and apply projects on your local machine with MiniKF, Virtualbox and Vagrant without any FEE&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Tutorial Link:&lt;/strong&gt; &lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow\"&gt;&lt;strong&gt;https://github.com/omerbsezer/Fast-Kubeflow&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Extra Kubernetes-Tutorial Link:&lt;/strong&gt; &lt;a href=\"https://github.com/omerbsezer/Fast-Kubernetes\"&gt;&lt;strong&gt;https://github.com/omerbsezer/Fast-Kubernetes&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Extra Docker-Tutorial Link:&lt;/strong&gt; &lt;a href=\"https://github.com/omerbsezer/Fast-Docker\"&gt;&lt;strong&gt;https://github.com/omerbsezer/Fast-Docker&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Quick Look (HowTo): Scenarios - Hands-on LABs&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow/blob/main/Installing-Kubeflow.md\"&gt;LAB: Creating LAB Environment (WSL2), Installing Kubeflow with MicroK8s, Juju on Ubuntu 20.04&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow/blob/main/Using-MiniKF.md\"&gt;LAB: Creating LAB Environment, Installing MiniKF with Vagrant (Preffered for Easy Usage)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow/blob/main/Kubeflow-Pipeline-Project.md\"&gt;LAB/Project: Kubeflow Pipeline (From Scratch) with Kubeflow SDK (DSL Compiler) and Custom Docker Images (Decision Tree, Logistic Regression, SVM, Naive Bayes, Xg Boost)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow/blob/main/Kale-Katib-Project.md\"&gt;LAB/Project: KALE (Kubeflow Automated PipeLines Engine) and KATIB (AutoML: Finding Best Hyperparameter Values)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow/blob/main/KALE-KServe.md\"&gt;LAB/Project: KALE (Kubeflow Automated PipeLines Engine) and KServe (Model Serving) for Model Prediction&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow/blob/main/Distributed-Training-Tensorflow.md\"&gt;LAB/Project: Distributed Training with Tensorflow (MNIST data)&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#motivation\"&gt;Motivation&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#whatIsKubeflow\"&gt;What is Kubelow?&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#howKubeflowWorks\"&gt;How Kubeflow Works?&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#whatareContainers\"&gt;What is Container (Docker)?&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#whatisKubeflow\"&gt;What is Kubernetes?&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#labEnvironment\"&gt;Installing Kubeflow&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#basics\"&gt;Kubeflow Basics&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#notebook\"&gt;Kubeflow Jupyter Notebook&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#pipeline\"&gt;Kubeflow Pipeline&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#kale\"&gt;KALE (Kubeflow Automated PipeLines Engine)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#katib\"&gt;KATIB (AutoML: Finding Best Hyperparameter Values)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#kserve\"&gt;KServe (Model Serving)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#operator\"&gt;Training-Operators (Distributed Training)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#minio_rok\"&gt;Minio (Object Storage) and ROK (Data Management Platform)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#project1\"&gt;Project 1: Creating ML Pipeline with Custom Docker Images (Decision Tree, Logistic Regression, SVM, Naive Bayes, Xg Boost)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#project2\"&gt;Project 2: KALE (Kubeflow Automated PipeLines Engine) and KATIB (AutoML: Finding Best Hyperparameter Values)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#project3\"&gt;Project 3: KALE (Kubeflow Automated PipeLines Engine) and KServe (Model Serving) for Model Prediction&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#project4\"&gt;Project 4: Distributed Training with Training Operator&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#resource\"&gt;Other Useful Resources Related Kubeflow&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Kubeflow#references\"&gt;References&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OgZTEcn6jPv9qhDq0bdea_xIzINxXytEo8_g2-1ZMNE.jpg?auto=webp&amp;s=8972239301c0c4d5affacbbd29b0f2e306811114", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/OgZTEcn6jPv9qhDq0bdea_xIzINxXytEo8_g2-1ZMNE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=16a94ca70f737f2370b40aac3f8e91906e0f957b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/OgZTEcn6jPv9qhDq0bdea_xIzINxXytEo8_g2-1ZMNE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=538ab272157f7fbbdc34990a022ebc76ef04a95d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/OgZTEcn6jPv9qhDq0bdea_xIzINxXytEo8_g2-1ZMNE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=18c191c7a365a3dcd7dd5453aa0d20081e4d1ce4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/OgZTEcn6jPv9qhDq0bdea_xIzINxXytEo8_g2-1ZMNE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e6839d5898e4ba4783cddafbc61cf720c03776b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/OgZTEcn6jPv9qhDq0bdea_xIzINxXytEo8_g2-1ZMNE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e82f3995a80d28748c000fc40912a475009bc8d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/OgZTEcn6jPv9qhDq0bdea_xIzINxXytEo8_g2-1ZMNE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=153d0307d71e80f4f61f9f274a4b1d4ba3c1ae20", "width": 1080, "height": 540}], "variants": {}, "id": "Up_qqgqv71AbRrRB6fD1yjG5QqUgCZnuuwZu0XqGuwk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "103bk24", "is_robot_indexable": true, "report_reasons": null, "author": "obsezer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103bk24/fastkubeflow_kubeflow_tutorial_sample_usage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103bk24/fastkubeflow_kubeflow_tutorial_sample_usage/", "subreddit_subscribers": 85240, "created_utc": 1672858333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hoping there are a few buding Spark/Delta/Hive experts on here. \n\nI've created a load of external Delta tables through Spark and registered them in a Hive metastore. So far so good, I can refer to them in my Jupyter/VS Code notebooks via {database_name}.{table_name} SQL queries. I can't, though, run queries against them through the Hive CLI or Dbeaver. I've installed the auxillary Hive connector jars from delta.io but it's still not working. The documentation for the connectors suggests it won't be possible? Hive can't read Delta tables created by Spark? This seems bonkers to me. I thought Delta was an open format. Same issue for JDBC connection through Dbeaver. So the only way to browse/query my Delta tables are through Spark?", "author_fullname": "t2_2bglroyh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta table question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103b21j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1672866915.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672857166.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hoping there are a few buding Spark/Delta/Hive experts on here. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve created a load of external Delta tables through Spark and registered them in a Hive metastore. So far so good, I can refer to them in my Jupyter/VS Code notebooks via {database_name}.{table_name} SQL queries. I can&amp;#39;t, though, run queries against them through the Hive CLI or Dbeaver. I&amp;#39;ve installed the auxillary Hive connector jars from delta.io but it&amp;#39;s still not working. The documentation for the connectors suggests it won&amp;#39;t be possible? Hive can&amp;#39;t read Delta tables created by Spark? This seems bonkers to me. I thought Delta was an open format. Same issue for JDBC connection through Dbeaver. So the only way to browse/query my Delta tables are through Spark?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "103b21j", "is_robot_indexable": true, "report_reasons": null, "author": "H0twax", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103b21j/delta_table_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103b21j/delta_table_question/", "subreddit_subscribers": 85240, "created_utc": 1672857166.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently got an invitation for an interview for a bank (quite large company). I'm just wondering if working in the banking sector is any good experience for DE. Thank you.", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE in bank, is it any good?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103sszx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672903603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently got an invitation for an interview for a bank (quite large company). I&amp;#39;m just wondering if working in the banking sector is any good experience for DE. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "103sszx", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103sszx/de_in_bank_is_it_any_good/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103sszx/de_in_bank_is_it_any_good/", "subreddit_subscribers": 85240, "created_utc": 1672903603.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I live in Pakistan and have been working as a Data Engineer for the past 3 years. I've been looking to switch positions due to lack of growth in my learning/knowledge recently (current job is a dead end now - I've learned all I could in the current scope). I have not had any luck for over 3 quarters since a lot of jobs require right to work in the host country (EU is next to impossible since they are bound to stick to GDPR regulations and not hire anybody who is not in EU for this position). Does anybody have any advice on how to make a breakthrough perhaps? Mass applying on LinkedIn can only get you so far and that has not been far enough in my case.", "author_fullname": "t2_9pynj741", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Remote jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103u1ut", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672908160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I live in Pakistan and have been working as a Data Engineer for the past 3 years. I&amp;#39;ve been looking to switch positions due to lack of growth in my learning/knowledge recently (current job is a dead end now - I&amp;#39;ve learned all I could in the current scope). I have not had any luck for over 3 quarters since a lot of jobs require right to work in the host country (EU is next to impossible since they are bound to stick to GDPR regulations and not hire anybody who is not in EU for this position). Does anybody have any advice on how to make a breakthrough perhaps? Mass applying on LinkedIn can only get you so far and that has not been far enough in my case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "103u1ut", "is_robot_indexable": true, "report_reasons": null, "author": "Temporary_Relation_3", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103u1ut/de_remote_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103u1ut/de_remote_jobs/", "subreddit_subscribers": 85240, "created_utc": 1672908160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI'm playing around with some snowflake ingestion stuff from S3 using snowpipe and I've hit a slight snag with handling the edge case of multiple updates to a given dimension arriving at the same time.\n\n**The Background**:\n\ni have a source dumping heaps of parquet files in an S3 Bucket. Ideally we want to process these files and load them into snowflake, using snowpipe. For simplicty sake lets say the data has the below shape:\n\n&amp;#x200B;\n\n|ID|FIRST\\_NAME|LAST\\_NAME|MODIFIED\\_ON|IS\\_DELETED|\n|:-|:-|:-|:-|:-|\n|1|John|doe|2022-11-01|False|\n|2|jane|doe|2022-11-01|False|\n\nI'm currently reading these files and their data raw into snowflake using snowpipe and COPY INTO from the S3 bucket. Storing them as a raw Variant with an INGEST\\_TIMESTAMP column in a table called RAW\\_PEOPLE.\n\n&amp;#x200B;\n\n|PARQUET\\_RAW|INGEST\\_TIMESTAMP|\n|:-|:-|\n|{data:stuff}|2022-11-01 22:50:00|\n\nI have a steam configured off this table, which is driving a snowflake Task every few minutes to flatten out the Variant Data into a table called STG\\_PEOPLE. So now i have a table with:\n\n&amp;#x200B;\n\n|ID|FIRST\\_NAME|LAST\\_NAME|MODIFIED\\_ON|IS\\_DELETED|INGEST\\_TIMESTAMP|\n|:-|:-|:-|:-|:-|:-|\n|1|John|doe|2022-11-01|False|2022-11-01 22:50:00|\n|2|jane|doe|2022-11-01|False|2022-11-01 22:50:00|\n\n&amp;#x200B;\n\nI have also configured a stream of this table to track CDC and would like to make a merge procedure to handle move this dataset into the final table outlined below:\n\nFinally I have a target DIM\\_PEOPLE table which is a type 2 SCD table with the following columns:\n\nID, FIRST\\_NAME, LAST\\_NAME, MODIFIED\\_ON, IS\\_DELETED, RECORD\\_CREATED\\_TS,  RECORD\\_ACTIVE\\_FROM\\_TS, RECORD\\_ACTIVE\\_TO\\_TS\n\n&amp;#x200B;\n\n|ID|FIRST\\_NAME|LAST\\_NAME|MODIFIED\\_ON|IS\\_DELETED|RECORD\\_ACTIVE\\_FROM\\_TS|RECORD\\_ACTIVE\\_TO\\_TS|RECORD\\_ACTIVE\\_FLAG|\n|:-|:-|:-|:-|:-|:-|:-|:-|\n|1|John|doe|2022-11-01|False|1900-01-01||Y|\n|2|jane|doe|2022-11-01|False|1900-01-01||Y|\n\n&amp;#x200B;\n\nI made up a very basic diagram to illustrate:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/3t31qkxas6aa1.png?width=1174&amp;format=png&amp;auto=webp&amp;s=786388cca454b6554086f1d6492d277f1a3fec42\n\n**The Problem**:\n\nI cannot gurantee that the source wont drop multiple updates to the same ID in the same file, or that we might have multiple files build up that contain the updates to the same ID.\n\nSo in a given ingest run i could get the below data turn up in STG\\_PEOPLE that i want to merge into DIM\\_PEOPLE:\n\n&amp;#x200B;\n\n|ID|FIRST\\_NAME|LAST\\_NAME|MODIFIED\\_ON|IS\\_DELETED|INGEST\\_TIMESTAMP|\n|:-|:-|:-|:-|:-|:-|\n|1|John-joe|doe|2022-11-02|False|2022-11-02 23:00|\n|2|jane|doe|2022-11-01|False|2022-11-02 23:00|\n|1|john-bill-joe|doe|2022-11-03|False|2022-12-03 23:00|\n\nI dont want to discard historical changes either. But I'm struggling on how to gracefully merge this STG\\_PEOPLE set into the DIM\\_PEOPLE set in a single merge statement?\n\nAs determining record active from periods for ID = 1, my plan is to set the latest ingest timestamp to active, and set the earlier duplicate record to have an active duration of 0, ie it has the same EFFECTIVE FROM and EFFECTIVE TO\n\nMy ideal dataset might be something like the below?\n\n&amp;#x200B;\n\n|ID|FIRST\\_NAME|LAST\\_NAME|MODIFIED\\_ON|IS\\_DELETED|RECORD\\_ACTIVE\\_FROM\\_TS|RECORD\\_ACTIVE\\_TO\\_TS|RECORD\\_ACTIVE\\_FLAG|\n|:-|:-|:-|:-|:-|:-|:-|:-|\n|1|john-bill-joe|doe|2022-11-03|False|2022-11-02 23:00||Y|\n|1|John-joe|doe|2022-11-02|False|2022-11-02 23:00|2022-11-02 23:00|N|\n|1|John|doe|2022-11-01|False|1900-01-01|2022-11-02 23:00|N|\n|2|jane|doe|2022-11-01|False|1900-01-01||Y|\n\n&amp;#x200B;\n\nSo my question is, within snowflake how would you execute a merge gracefully from a CDC stream if you capture multiple updates to the same record in the stream? Potentially i could union select both the target and source together and do some ranking in the merge, but it seems a bit heavyweight?\n\nI understand its a bit stackoverfloweque but seemed like a good starting point for discussion.\n\nThanks for taking the time to read my stream of consciousness, any help would be appreciated.", "author_fullname": "t2_5qbu2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Large ingestion of historical data into a type 2 SCD table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "media_metadata": {"3t31qkxas6aa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 45, "x": 108, "u": "https://preview.redd.it/3t31qkxas6aa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3ce37ec115c68a8d5ab3c1608f9e72f2aa82c669"}, {"y": 91, "x": 216, "u": "https://preview.redd.it/3t31qkxas6aa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7608faefe61f6fb1be0ddf54b1d9b2b37fd0b827"}, {"y": 136, "x": 320, "u": "https://preview.redd.it/3t31qkxas6aa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0fb5596afbd82b45c1f22c2c605217a6ed28e467"}, {"y": 272, "x": 640, "u": "https://preview.redd.it/3t31qkxas6aa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=90eb2453c9ba3c028055b745b7832f5c94faf8d0"}, {"y": 408, "x": 960, "u": "https://preview.redd.it/3t31qkxas6aa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1c1654a937c89763d887e108dbab89de1aff8e1f"}, {"y": 459, "x": 1080, "u": "https://preview.redd.it/3t31qkxas6aa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=189142c55b00e93fe36c2d05f9a946043b997153"}], "s": {"y": 500, "x": 1174, "u": "https://preview.redd.it/3t31qkxas6aa1.png?width=1174&amp;format=png&amp;auto=webp&amp;s=786388cca454b6554086f1d6492d277f1a3fec42"}, "id": "3t31qkxas6aa1"}}, "name": "t3_103trga", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/XerH1txSNzjZfPLK7dwAjhIc7Rnsmr9rNQly_uEm554.jpg", "edited": 1672907855.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672907070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m playing around with some snowflake ingestion stuff from S3 using snowpipe and I&amp;#39;ve hit a slight snag with handling the edge case of multiple updates to a given dimension arriving at the same time.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Background&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;i have a source dumping heaps of parquet files in an S3 Bucket. Ideally we want to process these files and load them into snowflake, using snowpipe. For simplicty sake lets say the data has the below shape:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;FIRST_NAME&lt;/th&gt;\n&lt;th align=\"left\"&gt;LAST_NAME&lt;/th&gt;\n&lt;th align=\"left\"&gt;MODIFIED_ON&lt;/th&gt;\n&lt;th align=\"left\"&gt;IS_DELETED&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;John&lt;/td&gt;\n&lt;td align=\"left\"&gt;doe&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;False&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;jane&lt;/td&gt;\n&lt;td align=\"left\"&gt;doe&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;False&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I&amp;#39;m currently reading these files and their data raw into snowflake using snowpipe and COPY INTO from the S3 bucket. Storing them as a raw Variant with an INGEST_TIMESTAMP column in a table called RAW_PEOPLE.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;PARQUET_RAW&lt;/th&gt;\n&lt;th align=\"left\"&gt;INGEST_TIMESTAMP&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;{data:stuff}&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-01 22:50:00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I have a steam configured off this table, which is driving a snowflake Task every few minutes to flatten out the Variant Data into a table called STG_PEOPLE. So now i have a table with:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;FIRST_NAME&lt;/th&gt;\n&lt;th align=\"left\"&gt;LAST_NAME&lt;/th&gt;\n&lt;th align=\"left\"&gt;MODIFIED_ON&lt;/th&gt;\n&lt;th align=\"left\"&gt;IS_DELETED&lt;/th&gt;\n&lt;th align=\"left\"&gt;INGEST_TIMESTAMP&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;John&lt;/td&gt;\n&lt;td align=\"left\"&gt;doe&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;False&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-01 22:50:00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;jane&lt;/td&gt;\n&lt;td align=\"left\"&gt;doe&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;False&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-01 22:50:00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have also configured a stream of this table to track CDC and would like to make a merge procedure to handle move this dataset into the final table outlined below:&lt;/p&gt;\n\n&lt;p&gt;Finally I have a target DIM_PEOPLE table which is a type 2 SCD table with the following columns:&lt;/p&gt;\n\n&lt;p&gt;ID, FIRST_NAME, LAST_NAME, MODIFIED_ON, IS_DELETED, RECORD_CREATED_TS,  RECORD_ACTIVE_FROM_TS, RECORD_ACTIVE_TO_TS&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;FIRST_NAME&lt;/th&gt;\n&lt;th align=\"left\"&gt;LAST_NAME&lt;/th&gt;\n&lt;th align=\"left\"&gt;MODIFIED_ON&lt;/th&gt;\n&lt;th align=\"left\"&gt;IS_DELETED&lt;/th&gt;\n&lt;th align=\"left\"&gt;RECORD_ACTIVE_FROM_TS&lt;/th&gt;\n&lt;th align=\"left\"&gt;RECORD_ACTIVE_TO_TS&lt;/th&gt;\n&lt;th align=\"left\"&gt;RECORD_ACTIVE_FLAG&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;John&lt;/td&gt;\n&lt;td align=\"left\"&gt;doe&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;False&lt;/td&gt;\n&lt;td align=\"left\"&gt;1900-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Y&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;jane&lt;/td&gt;\n&lt;td align=\"left\"&gt;doe&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;False&lt;/td&gt;\n&lt;td align=\"left\"&gt;1900-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Y&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I made up a very basic diagram to illustrate:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3t31qkxas6aa1.png?width=1174&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=786388cca454b6554086f1d6492d277f1a3fec42\"&gt;https://preview.redd.it/3t31qkxas6aa1.png?width=1174&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=786388cca454b6554086f1d6492d277f1a3fec42&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Problem&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;I cannot gurantee that the source wont drop multiple updates to the same ID in the same file, or that we might have multiple files build up that contain the updates to the same ID.&lt;/p&gt;\n\n&lt;p&gt;So in a given ingest run i could get the below data turn up in STG_PEOPLE that i want to merge into DIM_PEOPLE:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;FIRST_NAME&lt;/th&gt;\n&lt;th align=\"left\"&gt;LAST_NAME&lt;/th&gt;\n&lt;th align=\"left\"&gt;MODIFIED_ON&lt;/th&gt;\n&lt;th align=\"left\"&gt;IS_DELETED&lt;/th&gt;\n&lt;th align=\"left\"&gt;INGEST_TIMESTAMP&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;John-joe&lt;/td&gt;\n&lt;td align=\"left\"&gt;doe&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-02&lt;/td&gt;\n&lt;td align=\"left\"&gt;False&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-02 23:00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;jane&lt;/td&gt;\n&lt;td align=\"left\"&gt;doe&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;False&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-02 23:00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;john-bill-joe&lt;/td&gt;\n&lt;td align=\"left\"&gt;doe&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-03&lt;/td&gt;\n&lt;td align=\"left\"&gt;False&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-12-03 23:00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I dont want to discard historical changes either. But I&amp;#39;m struggling on how to gracefully merge this STG_PEOPLE set into the DIM_PEOPLE set in a single merge statement?&lt;/p&gt;\n\n&lt;p&gt;As determining record active from periods for ID = 1, my plan is to set the latest ingest timestamp to active, and set the earlier duplicate record to have an active duration of 0, ie it has the same EFFECTIVE FROM and EFFECTIVE TO&lt;/p&gt;\n\n&lt;p&gt;My ideal dataset might be something like the below?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;FIRST_NAME&lt;/th&gt;\n&lt;th align=\"left\"&gt;LAST_NAME&lt;/th&gt;\n&lt;th align=\"left\"&gt;MODIFIED_ON&lt;/th&gt;\n&lt;th align=\"left\"&gt;IS_DELETED&lt;/th&gt;\n&lt;th align=\"left\"&gt;RECORD_ACTIVE_FROM_TS&lt;/th&gt;\n&lt;th align=\"left\"&gt;RECORD_ACTIVE_TO_TS&lt;/th&gt;\n&lt;th align=\"left\"&gt;RECORD_ACTIVE_FLAG&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;john-bill-joe&lt;/td&gt;\n&lt;td align=\"left\"&gt;doe&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-03&lt;/td&gt;\n&lt;td align=\"left\"&gt;False&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-02 23:00&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Y&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;John-joe&lt;/td&gt;\n&lt;td align=\"left\"&gt;doe&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-02&lt;/td&gt;\n&lt;td align=\"left\"&gt;False&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-02 23:00&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-02 23:00&lt;/td&gt;\n&lt;td align=\"left\"&gt;N&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;John&lt;/td&gt;\n&lt;td align=\"left\"&gt;doe&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;False&lt;/td&gt;\n&lt;td align=\"left\"&gt;1900-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-02 23:00&lt;/td&gt;\n&lt;td align=\"left\"&gt;N&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;jane&lt;/td&gt;\n&lt;td align=\"left\"&gt;doe&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-11-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;False&lt;/td&gt;\n&lt;td align=\"left\"&gt;1900-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Y&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So my question is, within snowflake how would you execute a merge gracefully from a CDC stream if you capture multiple updates to the same record in the stream? Potentially i could union select both the target and source together and do some ranking in the merge, but it seems a bit heavyweight?&lt;/p&gt;\n\n&lt;p&gt;I understand its a bit stackoverfloweque but seemed like a good starting point for discussion.&lt;/p&gt;\n\n&lt;p&gt;Thanks for taking the time to read my stream of consciousness, any help would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "103trga", "is_robot_indexable": true, "report_reasons": null, "author": "crblasty", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103trga/large_ingestion_of_historical_data_into_a_type_2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103trga/large_ingestion_of_historical_data_into_a_type_2/", "subreddit_subscribers": 85240, "created_utc": 1672907070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I downloaded delta, spark and pyspark. Creating and updating delta tables do work fine on python or sql (spark-sql) base so far.\n\nNow I want to access my delta/parquet files via JDBC (here via dbeaver client). Some years ago, when working with good old Spark 2 it was just starting the thrift-server from spark package, configure the spark jdbc driver in beaver and connect.\n\nWith delta, something is weird or I am missing something completely. I can not get it working.\n\nThe online manuals from data bricks are only showing how to connect via jdbc to databricks hosted delta files on port 443 (HTTPS). \n\nTried also to access the delta files via trino but it did also not work. According to their delta connector documentation, you need to additionally install a dedicated hive metastore service (hms) in order to get it working (trino -&gt; hive -&gt; delta ?).  At the same time, in the delta GitHub issues, one of their main developer told that hive integration between spark &lt;-&gt; delta is not compatible.\n\nI am really confused..\n\nDid one of you have any experience / solution on this? Maybe I am overlooking something very obvious.\n\nAlso I wonder, which kind of delta lake tables could be accessed via jdbc driver (hive managed tables or external tables)?", "author_fullname": "t2_f4kq1cjq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to connect to local Delta Lake (OSS) via JDBC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103i9wv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672873834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded delta, spark and pyspark. Creating and updating delta tables do work fine on python or sql (spark-sql) base so far.&lt;/p&gt;\n\n&lt;p&gt;Now I want to access my delta/parquet files via JDBC (here via dbeaver client). Some years ago, when working with good old Spark 2 it was just starting the thrift-server from spark package, configure the spark jdbc driver in beaver and connect.&lt;/p&gt;\n\n&lt;p&gt;With delta, something is weird or I am missing something completely. I can not get it working.&lt;/p&gt;\n\n&lt;p&gt;The online manuals from data bricks are only showing how to connect via jdbc to databricks hosted delta files on port 443 (HTTPS). &lt;/p&gt;\n\n&lt;p&gt;Tried also to access the delta files via trino but it did also not work. According to their delta connector documentation, you need to additionally install a dedicated hive metastore service (hms) in order to get it working (trino -&amp;gt; hive -&amp;gt; delta ?).  At the same time, in the delta GitHub issues, one of their main developer told that hive integration between spark &amp;lt;-&amp;gt; delta is not compatible.&lt;/p&gt;\n\n&lt;p&gt;I am really confused..&lt;/p&gt;\n\n&lt;p&gt;Did one of you have any experience / solution on this? Maybe I am overlooking something very obvious.&lt;/p&gt;\n\n&lt;p&gt;Also I wonder, which kind of delta lake tables could be accessed via jdbc driver (hive managed tables or external tables)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "103i9wv", "is_robot_indexable": true, "report_reasons": null, "author": "consultant82", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103i9wv/how_to_connect_to_local_delta_lake_oss_via_jdbc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103i9wv/how_to_connect_to_local_delta_lake_oss_via_jdbc/", "subreddit_subscribers": 85240, "created_utc": 1672873834.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm making some experiments on a dataset where I have a float value associated with a DateTime object - Like a dict.\n\nI'm trying to downsample this dataset to optimize the algorithm - I'm currently extracting some elements by the date (removing elements with less than 15 days intervals).\n\nBut now, with that sample that I extract, I don't know how to ensure the quality of the data. How can I make sure that I don't lose any important info in the removed elements?\n\nTL;DR: How can I ensure the quality of info on downsampling compared with the original data?\n\n(I can't use correlation because of the different lengths of the original data and the filtered data.)\n\n(Language is not important, but I'm using Python)", "author_fullname": "t2_9s1yrp6d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ensure quality on data sampling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103hnww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672872417.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m making some experiments on a dataset where I have a float value associated with a DateTime object - Like a dict.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to downsample this dataset to optimize the algorithm - I&amp;#39;m currently extracting some elements by the date (removing elements with less than 15 days intervals).&lt;/p&gt;\n\n&lt;p&gt;But now, with that sample that I extract, I don&amp;#39;t know how to ensure the quality of the data. How can I make sure that I don&amp;#39;t lose any important info in the removed elements?&lt;/p&gt;\n\n&lt;p&gt;TL;DR: How can I ensure the quality of info on downsampling compared with the original data?&lt;/p&gt;\n\n&lt;p&gt;(I can&amp;#39;t use correlation because of the different lengths of the original data and the filtered data.)&lt;/p&gt;\n\n&lt;p&gt;(Language is not important, but I&amp;#39;m using Python)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "103hnww", "is_robot_indexable": true, "report_reasons": null, "author": "ddponwheels", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103hnww/ensure_quality_on_data_sampling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103hnww/ensure_quality_on_data_sampling/", "subreddit_subscribers": 85240, "created_utc": 1672872417.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. I'm running into some design choices which hinge on us either choosing the datatype for columns on their load into snowflake (for instance, bringing ints in as ints, and taking the time to define what each column should be coming in as in our config), or choosing to bring everything in as a string, and leave all transformations to be done with DBT in the database.\n\nDoes anyone have thoughts on what downsides I may be missing to the second approach? I expect that it would be more costly, though the nature of our work involves frequent new pipeline setups with shifting schemas, so it would be good if we can streamline things in this way.\n\nWe're using Pandas to pull in the data from CSV currently, and pushing to snowflake with the Snowflake \\`write\\_pandas\\` tool.", "author_fullname": "t2_7yr26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Direct loading to Snowflake\u2014to typecast or not?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103dp2x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1672863610.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672863269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I&amp;#39;m running into some design choices which hinge on us either choosing the datatype for columns on their load into snowflake (for instance, bringing ints in as ints, and taking the time to define what each column should be coming in as in our config), or choosing to bring everything in as a string, and leave all transformations to be done with DBT in the database.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have thoughts on what downsides I may be missing to the second approach? I expect that it would be more costly, though the nature of our work involves frequent new pipeline setups with shifting schemas, so it would be good if we can streamline things in this way.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re using Pandas to pull in the data from CSV currently, and pushing to snowflake with the Snowflake `write_pandas` tool.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "103dp2x", "is_robot_indexable": true, "report_reasons": null, "author": "nathanak21", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103dp2x/direct_loading_to_snowflaketo_typecast_or_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103dp2x/direct_loading_to_snowflaketo_typecast_or_not/", "subreddit_subscribers": 85240, "created_utc": 1672863269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've got a new data source containing thousands of multi GB json files (about 1,500 per calendar year) with [tens of] millions of rows of data each, and a bunch of data scientists and software engineers who are comfortable working in Spark. \n\nMost common analysis of these files will be done on a per year basis (e.g. looking at ~1500 at once) or on a file by file basis.\n\nI'm a relative novice here when it comes to data bigger than my standard relational DB can handle nicely. What are my best options for warehousing this data? If I were forbidden from going with a heavily proprietary set up (e.g. Databricks, Snowflake, etc.), are there alternatives this community would recommend?", "author_fullname": "t2_7b85k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice Needed: Data warehousing and Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103bamo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672857715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a new data source containing thousands of multi GB json files (about 1,500 per calendar year) with [tens of] millions of rows of data each, and a bunch of data scientists and software engineers who are comfortable working in Spark. &lt;/p&gt;\n\n&lt;p&gt;Most common analysis of these files will be done on a per year basis (e.g. looking at ~1500 at once) or on a file by file basis.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a relative novice here when it comes to data bigger than my standard relational DB can handle nicely. What are my best options for warehousing this data? If I were forbidden from going with a heavily proprietary set up (e.g. Databricks, Snowflake, etc.), are there alternatives this community would recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "103bamo", "is_robot_indexable": true, "report_reasons": null, "author": "elopeRstatS", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103bamo/advice_needed_data_warehousing_and_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103bamo/advice_needed_data_warehousing_and_spark/", "subreddit_subscribers": 85240, "created_utc": 1672857715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5r54hksr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get Your ML Data Back: Open-Source Tool to Export Your MLFlow Data to SQLite", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_103a1de", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/wXN1kwi7w4hRgvN57lDikPY8juBMJ0huUQAEaBSvho8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1672854862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ploomber.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ploomber.io/blog/mlflow2sql/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?auto=webp&amp;s=a01611b4c500985bec4eea7c81b9625f4fc84a8e", "width": 2402, "height": 1262}, "resolutions": [{"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d1d9ae27f0ab4cdf6194eaccce264d2df0ec7eb", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=59f9f616fd7113e5fd2b154d26c36fb0c6c92b19", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b6566be246f6de95ceb35ad40145567b4fee505", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=98b5bf2fa38b93b98c3d05573b811b24efd21bbb", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=49a1defd0a799cb42ab84d35032b27f24a29b5f9", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f206bfc6ef7a6b1ff19803d79315e77c5f727f05", "width": 1080, "height": 567}], "variants": {}, "id": "Objw1E7bxh1q2DHkIbVVfKXOpJIwmJvOhOKLMKIjEEs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "103a1de", "is_robot_indexable": true, "report_reasons": null, "author": "ploomber-io", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103a1de/get_your_ml_data_back_opensource_tool_to_export/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ploomber.io/blog/mlflow2sql/", "subreddit_subscribers": 85240, "created_utc": 1672854862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dear all,\n\nI'm in the process of studying Kimball's Data Warehouse Toolkit, trying to understand how to model my company's data, and I'm stuck at the following.\n\nOur product is ferry tickets &amp; related services: when booking from our website, you can plan your vacation and buy all of the items below in one single package:\n\n- passenger ferry tickets \n- vehicle ferry tickets \n- Travel insurance\n- Some other services that I ommit for simplicity\n\nFor example one can login and choose 2 passenger tickets, 1 vehicle ticket &amp; travel insurance all in one take, checkout and pay for all of them as a package.\n\nNow suppose that we want to model the process of selling the above goods and we decide to go with the finest granularity possible, i.e. each row in our fact table is one of the above items, along with purchase date, and amount for each item, etc.\n\nThe problem is that different categories of items have different dimensionality. For instance, a ferry ticket has the ferry operator as a dimension, but the travel insurance doesn't. Or, a passenger ticket has a dimension of \"ticket class\" (e.g. \"deck\", \"regular cabin\", \"lux cabin\") whereas the vehicle ticket doesn't.\n\nWhat are my options here? Different fact tables for each item category sounds like an overkill, right? My guess is that this would be similar to modelling retail of very different kind of goods, with different dimentionality.\n\nAny thoughts and advice would be highly appreciated!\n\nThank you in advance!", "author_fullname": "t2_dxh8y1bu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modelling: creating fact tables with different types of facts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1040as1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672929051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in the process of studying Kimball&amp;#39;s Data Warehouse Toolkit, trying to understand how to model my company&amp;#39;s data, and I&amp;#39;m stuck at the following.&lt;/p&gt;\n\n&lt;p&gt;Our product is ferry tickets &amp;amp; related services: when booking from our website, you can plan your vacation and buy all of the items below in one single package:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;passenger ferry tickets &lt;/li&gt;\n&lt;li&gt;vehicle ferry tickets &lt;/li&gt;\n&lt;li&gt;Travel insurance&lt;/li&gt;\n&lt;li&gt;Some other services that I ommit for simplicity&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For example one can login and choose 2 passenger tickets, 1 vehicle ticket &amp;amp; travel insurance all in one take, checkout and pay for all of them as a package.&lt;/p&gt;\n\n&lt;p&gt;Now suppose that we want to model the process of selling the above goods and we decide to go with the finest granularity possible, i.e. each row in our fact table is one of the above items, along with purchase date, and amount for each item, etc.&lt;/p&gt;\n\n&lt;p&gt;The problem is that different categories of items have different dimensionality. For instance, a ferry ticket has the ferry operator as a dimension, but the travel insurance doesn&amp;#39;t. Or, a passenger ticket has a dimension of &amp;quot;ticket class&amp;quot; (e.g. &amp;quot;deck&amp;quot;, &amp;quot;regular cabin&amp;quot;, &amp;quot;lux cabin&amp;quot;) whereas the vehicle ticket doesn&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;What are my options here? Different fact tables for each item category sounds like an overkill, right? My guess is that this would be similar to modelling retail of very different kind of goods, with different dimentionality.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts and advice would be highly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1040as1", "is_robot_indexable": true, "report_reasons": null, "author": "aWhaleNamedFreddie", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1040as1/data_modelling_creating_fact_tables_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1040as1/data_modelling_creating_fact_tables_with/", "subreddit_subscribers": 85240, "created_utc": 1672929051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're building pipelines for ingesting data from our SaaS application services to the data warehouse.\n\nEarly on we decided we don't want to do CDC as it couples the data warehouse to internal implementations, it is hard to enforce schema evolution and it requires the data warehouse team to have deep understanding of internal models of services.\n\nWe want to align the teams that work on the application on sending their data in kafka events. Maybe something similar to data products concept in data mesh (but not necessarily go all in on data mesh).\n\nWe tell them - every time you update an entity, send the full entity to the data warehouse. This way we can control the schema, we can decouple it from the internal representation of entities.\n\nWhen it works - it works great.\n\nBut the concept fails when:\n\n1. The team is doing a **partial update**. For example, it is using a document db to store user entities. It receives an \\`AddressChanged\\` event for a user and updates only the address field of the user. There is no point when the full user object is loaded in memory. The team does not want to read the entire document before the update because it will introduce race conditions and performance degradation. (this is maybe the easiest problem since we can maybe solve it by changing our pipelines to merge updates but we feel it will make the pipelines less robust and more complicated to maintain)\n2. The team is doing **bulk updates**. For example, the team is using a relational database and uses \\`UPDATE WHERE\\` to update a bulk of entities. The code might even not know which entities were updated without doing another expensive query and loading all the updated data.\n3. Troubleshooting and **manual updates** \\- let's say it is Saturday night and there's a P0 incident and to solve it someone had to manually update the database. How does the data warehouse gets notified? \n\nHas anyone here encountered similar problems? How did you solve them?", "author_fullname": "t2_20muts76", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle partial updates and bulk updates in the source systems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103zpjs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672927464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re building pipelines for ingesting data from our SaaS application services to the data warehouse.&lt;/p&gt;\n\n&lt;p&gt;Early on we decided we don&amp;#39;t want to do CDC as it couples the data warehouse to internal implementations, it is hard to enforce schema evolution and it requires the data warehouse team to have deep understanding of internal models of services.&lt;/p&gt;\n\n&lt;p&gt;We want to align the teams that work on the application on sending their data in kafka events. Maybe something similar to data products concept in data mesh (but not necessarily go all in on data mesh).&lt;/p&gt;\n\n&lt;p&gt;We tell them - every time you update an entity, send the full entity to the data warehouse. This way we can control the schema, we can decouple it from the internal representation of entities.&lt;/p&gt;\n\n&lt;p&gt;When it works - it works great.&lt;/p&gt;\n\n&lt;p&gt;But the concept fails when:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The team is doing a &lt;strong&gt;partial update&lt;/strong&gt;. For example, it is using a document db to store user entities. It receives an `AddressChanged` event for a user and updates only the address field of the user. There is no point when the full user object is loaded in memory. The team does not want to read the entire document before the update because it will introduce race conditions and performance degradation. (this is maybe the easiest problem since we can maybe solve it by changing our pipelines to merge updates but we feel it will make the pipelines less robust and more complicated to maintain)&lt;/li&gt;\n&lt;li&gt;The team is doing &lt;strong&gt;bulk updates&lt;/strong&gt;. For example, the team is using a relational database and uses `UPDATE WHERE` to update a bulk of entities. The code might even not know which entities were updated without doing another expensive query and loading all the updated data.&lt;/li&gt;\n&lt;li&gt;Troubleshooting and &lt;strong&gt;manual updates&lt;/strong&gt; - let&amp;#39;s say it is Saturday night and there&amp;#39;s a P0 incident and to solve it someone had to manually update the database. How does the data warehouse gets notified? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Has anyone here encountered similar problems? How did you solve them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "103zpjs", "is_robot_indexable": true, "report_reasons": null, "author": "daramasala", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103zpjs/how_to_handle_partial_updates_and_bulk_updates_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103zpjs/how_to_handle_partial_updates_and_bulk_updates_in/", "subreddit_subscribers": 85240, "created_utc": 1672927464.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI'm interested in pursuing a career in data engineering with a specialization in healthcare data.  I understand that I'd need to be proficient in Python and SQL to satisfy the minimum requirements, but I was wondering if someone could shed some light on data engineering  in the healthcare field.  I've provided some background information and a few questions I have below.  Sorry for any formatting issues,  I'm typing on my mobile.  TIA \n\n\nBackground:\n\nCurrent occupation: backend developer\n\nSkills: java (primary), some python, basic SQL, docker, k8s, starting to learn AWS tools, CI/CD\n\nSector:  finance\n\nYears of experience: 3\n\nLocation: US\n\nDegree:  BAAS (Bachelors of Applied Arts and Sciences)\n\n\nQuestions:\n\n1.  Is the healthcare field hard to get into?\n\n2.  Should I go back to school for a post-bacc degree or possibly a masters in health information systems?\n\n3.  What does a typical day look like?\n\n4.  Is the tech stack generally older?\n\n5.  How's the pay?\n\n6.  How's the job security?\n\n7.  Is remote work possible?\n\n8.  Any particular skills/topics I should learn?", "author_fullname": "t2_5sscegp1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE in healthcare", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103wpc1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1672920831.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672918045.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in pursuing a career in data engineering with a specialization in healthcare data.  I understand that I&amp;#39;d need to be proficient in Python and SQL to satisfy the minimum requirements, but I was wondering if someone could shed some light on data engineering  in the healthcare field.  I&amp;#39;ve provided some background information and a few questions I have below.  Sorry for any formatting issues,  I&amp;#39;m typing on my mobile.  TIA &lt;/p&gt;\n\n&lt;p&gt;Background:&lt;/p&gt;\n\n&lt;p&gt;Current occupation: backend developer&lt;/p&gt;\n\n&lt;p&gt;Skills: java (primary), some python, basic SQL, docker, k8s, starting to learn AWS tools, CI/CD&lt;/p&gt;\n\n&lt;p&gt;Sector:  finance&lt;/p&gt;\n\n&lt;p&gt;Years of experience: 3&lt;/p&gt;\n\n&lt;p&gt;Location: US&lt;/p&gt;\n\n&lt;p&gt;Degree:  BAAS (Bachelors of Applied Arts and Sciences)&lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Is the healthcare field hard to get into?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Should I go back to school for a post-bacc degree or possibly a masters in health information systems?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What does a typical day look like?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is the tech stack generally older?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How&amp;#39;s the pay?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How&amp;#39;s the job security?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is remote work possible?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Any particular skills/topics I should learn?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "103wpc1", "is_robot_indexable": true, "report_reasons": null, "author": "the_plants_are_here", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103wpc1/de_in_healthcare/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103wpc1/de_in_healthcare/", "subreddit_subscribers": 85240, "created_utc": 1672918045.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIm currently in the process of setting up a dataplatform for a small financial organisation.\n\nThe potential vendor proposed a databricks lakehouse setup + Powerbi as reporting tool. This due to our usecases of standard BI reporting, python models and in the future more datadriven demand.\n\nI understand the lakehouse concept and i see the strengths of it in the Azure Cloud. One question arised to me; What is the best practice of maintaining masterdata and model parameters on this architecture?", "author_fullname": "t2_mv3d8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks and manual masterdata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103u84d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672908824.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Im currently in the process of setting up a dataplatform for a small financial organisation.&lt;/p&gt;\n\n&lt;p&gt;The potential vendor proposed a databricks lakehouse setup + Powerbi as reporting tool. This due to our usecases of standard BI reporting, python models and in the future more datadriven demand.&lt;/p&gt;\n\n&lt;p&gt;I understand the lakehouse concept and i see the strengths of it in the Azure Cloud. One question arised to me; What is the best practice of maintaining masterdata and model parameters on this architecture?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "103u84d", "is_robot_indexable": true, "report_reasons": null, "author": "artopaper", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103u84d/databricks_and_manual_masterdata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103u84d/databricks_and_manual_masterdata/", "subreddit_subscribers": 85240, "created_utc": 1672908824.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I'm a college student right now and I'm officially declared as a double major in computer science and data science. I started college wanting to complete both, however I eventually realized the workload was just too much, and only took CS classes this past semester with no intention of completing my data science major requirements. However, I'm still officially declared as both and on my resume, linkedin, school recruitment portal, etc. it still says that I'm both. \n\nI should note that I have a \"relevant coursework\" section on my resume, so I'm being transparent about exactly what classes I've taken, and in all the interviews I've had so far, no employers have really even asked about my major(s), they've seemed to care way more about internship experience and specific coursework. \n\nAny thoughts would be appreciated. Thanks", "author_fullname": "t2_u0b7yebe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unethical to keep a double major on resume even if I intend to drop it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103kudn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672880188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I&amp;#39;m a college student right now and I&amp;#39;m officially declared as a double major in computer science and data science. I started college wanting to complete both, however I eventually realized the workload was just too much, and only took CS classes this past semester with no intention of completing my data science major requirements. However, I&amp;#39;m still officially declared as both and on my resume, linkedin, school recruitment portal, etc. it still says that I&amp;#39;m both. &lt;/p&gt;\n\n&lt;p&gt;I should note that I have a &amp;quot;relevant coursework&amp;quot; section on my resume, so I&amp;#39;m being transparent about exactly what classes I&amp;#39;ve taken, and in all the interviews I&amp;#39;ve had so far, no employers have really even asked about my major(s), they&amp;#39;ve seemed to care way more about internship experience and specific coursework. &lt;/p&gt;\n\n&lt;p&gt;Any thoughts would be appreciated. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "103kudn", "is_robot_indexable": true, "report_reasons": null, "author": "sheepbatman", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103kudn/unethical_to_keep_a_double_major_on_resume_even/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103kudn/unethical_to_keep_a_double_major_on_resume_even/", "subreddit_subscribers": 85240, "created_utc": 1672880188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Been hearing a lot about OLAP DB's with companies like ClickHouse and DuckDB - can someone who uses them in their org walk me through (in the simplest way) how this sits in your data stack, specific use case examples, and if I'm misunderstanding anything?\n\nI guess specific areas of confusion are what specific workflows OLAP DB's are being used for. At a high level I get that Clickhouse/DuckDB basically allow your team to run SQL queries on data that is not necessarily centralized (i.e., on an analyst's laptop, a random cloud endpoint, etc.) but how often does this come up? Was it difficult to gain adoption within your organization? How has this impacted your spend compared to using just Snowflake or whatever CDW?", "author_fullname": "t2_kve19uny", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELI5 OLAP databases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103kq8h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672879887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been hearing a lot about OLAP DB&amp;#39;s with companies like ClickHouse and DuckDB - can someone who uses them in their org walk me through (in the simplest way) how this sits in your data stack, specific use case examples, and if I&amp;#39;m misunderstanding anything?&lt;/p&gt;\n\n&lt;p&gt;I guess specific areas of confusion are what specific workflows OLAP DB&amp;#39;s are being used for. At a high level I get that Clickhouse/DuckDB basically allow your team to run SQL queries on data that is not necessarily centralized (i.e., on an analyst&amp;#39;s laptop, a random cloud endpoint, etc.) but how often does this come up? Was it difficult to gain adoption within your organization? How has this impacted your spend compared to using just Snowflake or whatever CDW?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "103kq8h", "is_robot_indexable": true, "report_reasons": null, "author": "y0urm0m82", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103kq8h/eli5_olap_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103kq8h/eli5_olap_databases/", "subreddit_subscribers": 85240, "created_utc": 1672879887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://memphis-dev.medium.com/memphis-dev-event-driven-2-0-9be884bab1e2](https://memphis-dev.medium.com/memphis-dev-event-driven-2-0-9be884bab1e2)", "author_fullname": "t2_bqjrmnud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Event Driven 2.0", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_103bciw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1672857846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://memphis-dev.medium.com/memphis-dev-event-driven-2-0-9be884bab1e2\"&gt;https://memphis-dev.medium.com/memphis-dev-event-driven-2-0-9be884bab1e2&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GVs2LeWhEigk_5F3_8UsDdPTa4DvZ1iR0gf4-qtaIxc.jpg?auto=webp&amp;s=b97954d73ffeb765174be750976191506bbf07f3", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/GVs2LeWhEigk_5F3_8UsDdPTa4DvZ1iR0gf4-qtaIxc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=166c1ee55952a69a81770edf907f25176bfec22f", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/GVs2LeWhEigk_5F3_8UsDdPTa4DvZ1iR0gf4-qtaIxc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=621c511b51433c0b69a6ee0a643c1f443d2dabff", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/GVs2LeWhEigk_5F3_8UsDdPTa4DvZ1iR0gf4-qtaIxc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8896fecb52c14c2562a678a34fd3c631a2ce86b3", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/GVs2LeWhEigk_5F3_8UsDdPTa4DvZ1iR0gf4-qtaIxc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8df222fa31b6f0e98d326dcbb65423706448004e", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/GVs2LeWhEigk_5F3_8UsDdPTa4DvZ1iR0gf4-qtaIxc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b59b1a43cf79e3fce46f46efe8fb20776f82f116", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/GVs2LeWhEigk_5F3_8UsDdPTa4DvZ1iR0gf4-qtaIxc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=03fbb61ca9c44d2324e97a724c8ba6d66318e325", "width": 1080, "height": 607}], "variants": {}, "id": "kFIoN7woyuD_BLelZb1ThQDWGropux-yn9HXyH1PbBc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "103bciw", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Bug105", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103bciw/event_driven_20/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/103bciw/event_driven_20/", "subreddit_subscribers": 85240, "created_utc": 1672857846.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}