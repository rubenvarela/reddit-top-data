{"kind": "Listing", "data": {"after": "t3_10jd32g", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I noticed my media contains almost no 4K content despite having a 4k monitor\n\nmost are 1080p and a few 2k\n\nI'm tempted to start hoarding 4K but the files are enormous in comparison to HD", "author_fullname": "t2_4e9da", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you hoard 1080p or 4k video content?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jdr01", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 261, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 261, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674484146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I noticed my media contains almost no 4K content despite having a 4k monitor&lt;/p&gt;\n\n&lt;p&gt;most are 1080p and a few 2k&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m tempted to start hoarding 4K but the files are enormous in comparison to HD&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10jdr01", "is_robot_indexable": true, "report_reasons": null, "author": "granmastern", "discussion_type": null, "num_comments": 286, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jdr01/do_you_hoard_1080p_or_4k_video_content/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jdr01/do_you_hoard_1080p_or_4k_video_content/", "subreddit_subscribers": 667018, "created_utc": 1674484146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_r76xkp0n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can someone please explain why Crystaldisk warns for Reallocated seconds count when none of the values are under 50?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10jgvxa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 90, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 90, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/URKUBF8YjkgM_rrBR95-3RwInVV5tl63rMDbEWYEPgc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674492152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/b478ivnbntda1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/b478ivnbntda1.jpg?auto=webp&amp;v=enabled&amp;s=79d7d6479a874874287be0f92b80617d0d9ae2d3", "width": 3024, "height": 3024}, "resolutions": [{"url": "https://preview.redd.it/b478ivnbntda1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c500b185a14adb324fd1977806a79f3e8cb7e86c", "width": 108, "height": 108}, {"url": "https://preview.redd.it/b478ivnbntda1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=703bb488b74bcad5a9560590e3c60dcbe70794ad", "width": 216, "height": 216}, {"url": "https://preview.redd.it/b478ivnbntda1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d6d5d6ade44a041c5d24be104e61bf732d4394b", "width": 320, "height": 320}, {"url": "https://preview.redd.it/b478ivnbntda1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e48ed468ba35c92cee455bbb109554762231d24", "width": 640, "height": 640}, {"url": "https://preview.redd.it/b478ivnbntda1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2607a9349b7b354146b63f4bdb4c99b2599cbf2", "width": 960, "height": 960}, {"url": "https://preview.redd.it/b478ivnbntda1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=943486f35177e2978c4ba5521f45c1ebeb01c722", "width": 1080, "height": 1080}], "variants": {}, "id": "T1l0-YEJiX8t8c68ilbo-P6S-ve9pcouTBkAqQi2NMQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jgvxa", "is_robot_indexable": true, "report_reasons": null, "author": "mediamystery", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jgvxa/can_someone_please_explain_why_crystaldisk_warns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/b478ivnbntda1.jpg", "subreddit_subscribers": 667018, "created_utc": 1674492152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently, I've decided to take on the project of digitizing my VHS home videos. My budget is up to $200 USD (though that's not set in stone). I'd just like to get a good bang-for-buck archive before the tapes degrade completely.\n\n&amp;#x200B;\n\nCurrently, I have a component VCR (which I'd like to replace with an S-video unit), and a desktop PC.\n\n&amp;#x200B;\n\nI've been reading LordSmurf's writings, where he recommends pricy TBCs and VCRs paired with VirtualDub. I don't have that budget, nor the need for utmost quality. However, I did find his recommendation of DMR-ES10/15 DVD players for \"TBC-ish\" performance helpful for my price constraints.\n\n&amp;#x200B;\n\nWith that being said, I was thinking of purchasing this setup:\n\n&amp;#x200B;\n\nS-video VCR&gt;DMR-ES15 DVD player&gt;ATI TV Wonder 600 USB Capture Card&gt;VirtualDub\n\n&amp;#x200B;\n\nWith that being said, are there any specific S-video VCRs which you recommend? Otherwise I'm just going to clean-up one from a thrift store or garage sale.\n\n&amp;#x200B;\n\nI've also heard of VHS\\_Decode, which seems to provide a great performance-to-price ratio. Then again, people like LordSmurf have said that this technique pales in-comparison to a TBC.\n\n&amp;#x200B;\n\nDoes this advice only apply to the upper end, or does it include my theoretical budget setup too?\n\n&amp;#x200B;\n\nFinally, if I were to go the VHS\\_Decode route, would it be worth expanding my budget and getting a Domesday Duplicator, or just going with the cheap, $30 card? I don't have any laserdiscs, but it would be nice to have the ability to archive them with this device, in the future.\n\n&amp;#x200B;\n\nTLDR;\n\n&amp;#x200B;\n\n\\- Budget: $200 USD (willing to go higher if necessary)\n\n&amp;#x200B;\n\n\\- Should I capture over S-Video and DMR-ES15 or VHS\\_Decode?\n\n&amp;#x200B;\n\n\\- Any recommendations for cheaper S-video VCRs?\n\n&amp;#x200B;\n\n\\- Is the Domesday Duplicator worth the extra money, if I use VHS\\_Decode?\n\n&amp;#x200B;\n\n\\- Do you have any other general pointers?\n\n&amp;#x200B;\n\nAny help would be appreciated. Thanks!", "author_fullname": "t2_kh76tepm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "VHS - S-Video Capture Or VHS_Decode?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10j7s3o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 85, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 85, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674463083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently, I&amp;#39;ve decided to take on the project of digitizing my VHS home videos. My budget is up to $200 USD (though that&amp;#39;s not set in stone). I&amp;#39;d just like to get a good bang-for-buck archive before the tapes degrade completely.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Currently, I have a component VCR (which I&amp;#39;d like to replace with an S-video unit), and a desktop PC.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been reading LordSmurf&amp;#39;s writings, where he recommends pricy TBCs and VCRs paired with VirtualDub. I don&amp;#39;t have that budget, nor the need for utmost quality. However, I did find his recommendation of DMR-ES10/15 DVD players for &amp;quot;TBC-ish&amp;quot; performance helpful for my price constraints.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;With that being said, I was thinking of purchasing this setup:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;S-video VCR&amp;gt;DMR-ES15 DVD player&amp;gt;ATI TV Wonder 600 USB Capture Card&amp;gt;VirtualDub&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;With that being said, are there any specific S-video VCRs which you recommend? Otherwise I&amp;#39;m just going to clean-up one from a thrift store or garage sale.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also heard of VHS_Decode, which seems to provide a great performance-to-price ratio. Then again, people like LordSmurf have said that this technique pales in-comparison to a TBC.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Does this advice only apply to the upper end, or does it include my theoretical budget setup too?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Finally, if I were to go the VHS_Decode route, would it be worth expanding my budget and getting a Domesday Duplicator, or just going with the cheap, $30 card? I don&amp;#39;t have any laserdiscs, but it would be nice to have the ability to archive them with this device, in the future.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;TLDR;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Budget: $200 USD (willing to go higher if necessary)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Should I capture over S-Video and DMR-ES15 or VHS_Decode?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Any recommendations for cheaper S-video VCRs?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Is the Domesday Duplicator worth the extra money, if I use VHS_Decode?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Do you have any other general pointers?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10j7s3o", "is_robot_indexable": true, "report_reasons": null, "author": "Bringback-T_D", "discussion_type": null, "num_comments": 67, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10j7s3o/vhs_svideo_capture_or_vhs_decode/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10j7s3o/vhs_svideo_capture_or_vhs_decode/", "subreddit_subscribers": 667018, "created_utc": 1674463083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys, I've got a 5TB hard drive and I've tried a few programmes to try get rid of dupes but often the size of the HDD causing them to crash, I'm weary rclone as I've had a bad experience due to me being ready horrible at anything with a screen. Any recommendations?  Mainly to identify duplicate music, pics and vids through tons of various files ?   Your help is appreciated", "author_fullname": "t2_sj2tce2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Powerful Deduplication software", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jcrlp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674481346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I&amp;#39;ve got a 5TB hard drive and I&amp;#39;ve tried a few programmes to try get rid of dupes but often the size of the HDD causing them to crash, I&amp;#39;m weary rclone as I&amp;#39;ve had a bad experience due to me being ready horrible at anything with a screen. Any recommendations?  Mainly to identify duplicate music, pics and vids through tons of various files ?   Your help is appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jcrlp", "is_robot_indexable": true, "report_reasons": null, "author": "Diligent_Object4546", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jcrlp/looking_for_powerful_deduplication_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jcrlp/looking_for_powerful_deduplication_software/", "subreddit_subscribers": 667018, "created_utc": 1674481346.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For this one, we are giving away an IronWolf Pro 125 960GB SSD to one lucky winner in this thread!\n\nHappy New Year! To kick 2023 off right, here's the terms for this one:\n\n**The prize is: one IronWolf Pro 125 960GB SSD**\n\n**How to enter:**\n\n**Just reply to this post once with a comment that includes the terms RunWithIronWolf and Seagate telling us how the prize would help further your data hoarding projects.**\n\n\nSelection process/rules\n\nOne entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. Entries are open until January 31st 2023, 23:59 UTC. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.\n\nGeographic restrictions:\n\nOur policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions (e.g. the obvious shipping restrictions to Russia and Belarus currently)\n\nUS\n\nCanada (exc. Quebec and will require a basic skills-based question if winner is chosen by law)\n\nBrazil\n\nSouth America\n\nUnited Kingdom\n\nGermany\n\nFrance\n\nIberia\n\nAustralia\n\nNew Zealand\n\nKorea\n\nIndia\n\nMalaysia\n\nSingapore\n\nChina", "author_fullname": "t2_16nn7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We're Back - Official January 2023 Seagate IronWolf Giveaway!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jjdfe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": "", "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "OFFICIAL", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674498007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For this one, we are giving away an IronWolf Pro 125 960GB SSD to one lucky winner in this thread!&lt;/p&gt;\n\n&lt;p&gt;Happy New Year! To kick 2023 off right, here&amp;#39;s the terms for this one:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The prize is: one IronWolf Pro 125 960GB SSD&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How to enter:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Just reply to this post once with a comment that includes the terms RunWithIronWolf and Seagate telling us how the prize would help further your data hoarding projects.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Selection process/rules&lt;/p&gt;\n\n&lt;p&gt;One entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. Entries are open until January 31st 2023, 23:59 UTC. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.&lt;/p&gt;\n\n&lt;p&gt;Geographic restrictions:&lt;/p&gt;\n\n&lt;p&gt;Our policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions (e.g. the obvious shipping restrictions to Russia and Belarus currently)&lt;/p&gt;\n\n&lt;p&gt;US&lt;/p&gt;\n\n&lt;p&gt;Canada (exc. Quebec and will require a basic skills-based question if winner is chosen by law)&lt;/p&gt;\n\n&lt;p&gt;Brazil&lt;/p&gt;\n\n&lt;p&gt;South America&lt;/p&gt;\n\n&lt;p&gt;United Kingdom&lt;/p&gt;\n\n&lt;p&gt;Germany&lt;/p&gt;\n\n&lt;p&gt;France&lt;/p&gt;\n\n&lt;p&gt;Iberia&lt;/p&gt;\n\n&lt;p&gt;Australia&lt;/p&gt;\n\n&lt;p&gt;New Zealand&lt;/p&gt;\n\n&lt;p&gt;Korea&lt;/p&gt;\n\n&lt;p&gt;India&lt;/p&gt;\n\n&lt;p&gt;Malaysia&lt;/p&gt;\n\n&lt;p&gt;Singapore&lt;/p&gt;\n\n&lt;p&gt;China&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "81f0a58e-b3f5-11ea-95d7-0e4db8ecc231", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "OFFICIAL SEAGATE", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "10jjdfe", "is_robot_indexable": true, "report_reasons": null, "author": "Seagate_Surfer", "discussion_type": null, "num_comments": 129, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10jjdfe/were_back_official_january_2023_seagate_ironwolf/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/10jjdfe/were_back_official_january_2023_seagate_ironwolf/", "subreddit_subscribers": 667018, "created_utc": 1674498007.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "*Nuke if not appropriate*\n\nI thought asking here would be the best place as it is likely something a Data Hoarder would have ran into and not just the average home-labber.\n\nHave any of you ran into an issue of drives randomly \"power cycling?\"  Disconnecting from the system and moments later being redetected by the system and working fine for a time frame.  Heavy data through-put on the drive will often be the trigger for this reset.\n\nIt will often be just one drive, randomly and rarely.  Then a second drive will start.  Maybe a third.  I pull the drive and test on another machine and it tests perfect.  \n\nIf I move the drive to a different power supply, the issue will disappear.  Other drives on the first power supply, may start to show the issue.  If I replace the power supply, the issue will almost always go away.\n\nThis has been going on for years now.  I would guess 6 years or so for me.  I have to replace the main power supply every 9 to 12 months.  \n\nI have moved to a setup that I have much smaller power supplies (1u style) power about 8 to 12 drives and little else.  These drives have never shown this issue (though this setup has only been in place for about 2 years.) But the drives that are in the main case with the actual system hardware and the system power supply are starting to show the issue again.\n\nThe last few cycles of this issue, I have been using Gold or Plat level PSUs usually around the 800w level.  (I did use a 1000w in one cycle but still did the same.)  Every power calculator I have used says it would be enough for the system and about 8 to 12 drives in the main box.\n\nI have used consumer grade HDDs (many shucked) mostly but I am slowly adding on Enterprise grade (though the most recent issue played against one of these Enterprise drives.)\n\nThis isn't a huge pressing issue as it seems I have a path of how to deal with it.  It is more an annoyance and I just am very curious if anyone else has seen this.  I am wondering if I am stressing one part of the PSU (maybe a 5v rail or something) without damaging the greater bit of the PSU and that leads to these little \"Electrical Gremlins.\"", "author_fullname": "t2_6ybts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you ever have power issues with a large number of hard drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jf6z1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674487910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Nuke if not appropriate&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I thought asking here would be the best place as it is likely something a Data Hoarder would have ran into and not just the average home-labber.&lt;/p&gt;\n\n&lt;p&gt;Have any of you ran into an issue of drives randomly &amp;quot;power cycling?&amp;quot;  Disconnecting from the system and moments later being redetected by the system and working fine for a time frame.  Heavy data through-put on the drive will often be the trigger for this reset.&lt;/p&gt;\n\n&lt;p&gt;It will often be just one drive, randomly and rarely.  Then a second drive will start.  Maybe a third.  I pull the drive and test on another machine and it tests perfect.  &lt;/p&gt;\n\n&lt;p&gt;If I move the drive to a different power supply, the issue will disappear.  Other drives on the first power supply, may start to show the issue.  If I replace the power supply, the issue will almost always go away.&lt;/p&gt;\n\n&lt;p&gt;This has been going on for years now.  I would guess 6 years or so for me.  I have to replace the main power supply every 9 to 12 months.  &lt;/p&gt;\n\n&lt;p&gt;I have moved to a setup that I have much smaller power supplies (1u style) power about 8 to 12 drives and little else.  These drives have never shown this issue (though this setup has only been in place for about 2 years.) But the drives that are in the main case with the actual system hardware and the system power supply are starting to show the issue again.&lt;/p&gt;\n\n&lt;p&gt;The last few cycles of this issue, I have been using Gold or Plat level PSUs usually around the 800w level.  (I did use a 1000w in one cycle but still did the same.)  Every power calculator I have used says it would be enough for the system and about 8 to 12 drives in the main box.&lt;/p&gt;\n\n&lt;p&gt;I have used consumer grade HDDs (many shucked) mostly but I am slowly adding on Enterprise grade (though the most recent issue played against one of these Enterprise drives.)&lt;/p&gt;\n\n&lt;p&gt;This isn&amp;#39;t a huge pressing issue as it seems I have a path of how to deal with it.  It is more an annoyance and I just am very curious if anyone else has seen this.  I am wondering if I am stressing one part of the PSU (maybe a 5v rail or something) without damaging the greater bit of the PSU and that leads to these little &amp;quot;Electrical Gremlins.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jf6z1", "is_robot_indexable": true, "report_reasons": null, "author": "CaptAngryPants", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jf6z1/do_you_ever_have_power_issues_with_a_large_number/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jf6z1/do_you_ever_have_power_issues_with_a_large_number/", "subreddit_subscribers": 667018, "created_utc": 1674487910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Please. Specifically letters P and N. I've had 1.0kB/s down for the past several months, 2 peers no seeders.", "author_fullname": "t2_8nbrguzr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shamelessly seeking seeders for the 2017 ihadp GW archives...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jpji4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674512912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please. Specifically letters P and N. I&amp;#39;ve had 1.0kB/s down for the past several months, 2 peers no seeders.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10jpji4", "is_robot_indexable": true, "report_reasons": null, "author": "parastro", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jpji4/shamelessly_seeking_seeders_for_the_2017_ihadp_gw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jpji4/shamelessly_seeking_seeders_for_the_2017_ihadp_gw/", "subreddit_subscribers": 667018, "created_utc": 1674512912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m sitting on many now-deleted YouTube videos by one of my favorite creators. I only kept them for myself because they mean a lot to me, but I\u2019ve been contemplating whether or not to upload them somewhere. Afaik, I might be the only person who has those files. I have some moral concerns for obvious reasons. Those videos were made unavailable because the creator is clearly not proud of them anymore (outdated beliefs, bad quality, etc.). What should I do?", "author_fullname": "t2_srv95nt7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it ethical to reupload YouTube videos that were taken down by the creator?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jms85", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674506242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m sitting on many now-deleted YouTube videos by one of my favorite creators. I only kept them for myself because they mean a lot to me, but I\u2019ve been contemplating whether or not to upload them somewhere. Afaik, I might be the only person who has those files. I have some moral concerns for obvious reasons. Those videos were made unavailable because the creator is clearly not proud of them anymore (outdated beliefs, bad quality, etc.). What should I do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jms85", "is_robot_indexable": true, "report_reasons": null, "author": "Jaded_Answer_2188", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jms85/is_it_ethical_to_reupload_youtube_videos_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jms85/is_it_ethical_to_reupload_youtube_videos_that/", "subreddit_subscribers": 667018, "created_utc": 1674506242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am expanding my business and I want to connect my data collection machine(does all the drive imaging) to our infrastructure via 10Gbe.  [This guy here](https://www.tp-link.com/us/business-networking/managed-switch/tl-sg3428x/v1/) has 10Gbe 'uplink' SFP+ ports.  Is that just a regular port?  Can I connect a desktop computer to that via SFP, as well as my NAS, and the two will talk over fiber?", "author_fullname": "t2_6eiisul2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does a 10Gbe SFP+ 'uplink' port mean?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jtp1b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674524007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am expanding my business and I want to connect my data collection machine(does all the drive imaging) to our infrastructure via 10Gbe.  &lt;a href=\"https://www.tp-link.com/us/business-networking/managed-switch/tl-sg3428x/v1/\"&gt;This guy here&lt;/a&gt; has 10Gbe &amp;#39;uplink&amp;#39; SFP+ ports.  Is that just a regular port?  Can I connect a desktop computer to that via SFP, as well as my NAS, and the two will talk over fiber?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AD4VaJQhVa0HoaI9VSnho334dCF3CKbVDA5TR1ZaI_A.jpg?auto=webp&amp;v=enabled&amp;s=da0441d55b300308428998a85a4fd8606597b7a6", "width": 1000, "height": 1000}, "resolutions": [{"url": "https://external-preview.redd.it/AD4VaJQhVa0HoaI9VSnho334dCF3CKbVDA5TR1ZaI_A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d828f8c49bce3a53ea132ca7fdd461cf5de956c4", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/AD4VaJQhVa0HoaI9VSnho334dCF3CKbVDA5TR1ZaI_A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8be492d83cf02aebcc2ea0cd0c810e3c55a06097", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/AD4VaJQhVa0HoaI9VSnho334dCF3CKbVDA5TR1ZaI_A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6050307cd4c828f50c443de225e9e984e0f04946", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/AD4VaJQhVa0HoaI9VSnho334dCF3CKbVDA5TR1ZaI_A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d342ed95bcc8a9adfb762c8ca2b4917ae50b894", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/AD4VaJQhVa0HoaI9VSnho334dCF3CKbVDA5TR1ZaI_A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b33d22c8d70749514afe2c0341d6e98fbd9bc5a", "width": 960, "height": 960}], "variants": {}, "id": "3PQ6V9J61vwaA_BPtKmfepsTa5LZQdP3FLeP2yWRfdo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jtp1b", "is_robot_indexable": true, "report_reasons": null, "author": "Pleaseclap4", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jtp1b/what_does_a_10gbe_sfp_uplink_port_mean/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jtp1b/what_does_a_10gbe_sfp_uplink_port_mean/", "subreddit_subscribers": 667018, "created_utc": 1674524007.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm reworking some of my backup mechanics (thanks to drive compatibility issues, thanks asrock...) and I realized I could just do incremental backups on my ZFS snapshot directory instead of doing incremental backups on the actual directories.\n\nThis has some benefits. Since the snapshot directory is read only from the client devices (unless you permit root login on SMB) it pretty much eliminates the chance of a configuration mishap or scripting error wiping out the actual directory. Also, I would get an independant copy of the dataset's history in case I misconfigure the retention policies or delete the wrong snapshot. I don't know if I can 'readd' accidentally deleted snapshots from a non ZFS source but that's potentially a problem for future me.\n\nSince its an archival dataset where nothing gets deleted, backing up the snapshots cost basically no extra storage space. From what I can tell, this is a win-win situation which makes me wonder if I missed something.", "author_fullname": "t2_upalof7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a \"proper\" way to back up ZFS snapshots?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jt0ad", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674522035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m reworking some of my backup mechanics (thanks to drive compatibility issues, thanks asrock...) and I realized I could just do incremental backups on my ZFS snapshot directory instead of doing incremental backups on the actual directories.&lt;/p&gt;\n\n&lt;p&gt;This has some benefits. Since the snapshot directory is read only from the client devices (unless you permit root login on SMB) it pretty much eliminates the chance of a configuration mishap or scripting error wiping out the actual directory. Also, I would get an independant copy of the dataset&amp;#39;s history in case I misconfigure the retention policies or delete the wrong snapshot. I don&amp;#39;t know if I can &amp;#39;readd&amp;#39; accidentally deleted snapshots from a non ZFS source but that&amp;#39;s potentially a problem for future me.&lt;/p&gt;\n\n&lt;p&gt;Since its an archival dataset where nothing gets deleted, backing up the snapshots cost basically no extra storage space. From what I can tell, this is a win-win situation which makes me wonder if I missed something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "vTrueNAS 72TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jt0ad", "is_robot_indexable": true, "report_reasons": null, "author": "Party_9001", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10jt0ad/is_there_a_proper_way_to_back_up_zfs_snapshots/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jt0ad/is_there_a_proper_way_to_back_up_zfs_snapshots/", "subreddit_subscribers": 667018, "created_utc": 1674522035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently began backing up all of my decade-old CDs and DVDs as they may become unreadable or damaged. However, I have noticed that my DVD drive is no longer detecting these DVDs. I have tried using my dad's PC to check some of the DVDs and some are working while others are not. I am unsure if the issue is with both of my DVD writers. Can you please suggest a solution for getting them to work again? I have also tried using ISO buster and it shows the DVDs as blank.", "author_fullname": "t2_7s5s7atf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DVD not detecting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10jxg0s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674535681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently began backing up all of my decade-old CDs and DVDs as they may become unreadable or damaged. However, I have noticed that my DVD drive is no longer detecting these DVDs. I have tried using my dad&amp;#39;s PC to check some of the DVDs and some are working while others are not. I am unsure if the issue is with both of my DVD writers. Can you please suggest a solution for getting them to work again? I have also tried using ISO buster and it shows the DVDs as blank.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jxg0s", "is_robot_indexable": true, "report_reasons": null, "author": "nagarajtg", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jxg0s/dvd_not_detecting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jxg0s/dvd_not_detecting/", "subreddit_subscribers": 667018, "created_utc": 1674535681.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm brand new to all of this (including this sub), but I'm willing to dive head first, and spend as long as it takes to figure this out. I just need a point in the right direction.\n\nI would like to consolidate all of the pictures my family has of my son on a personal storage device everyone in the family can access from anywhere. What is the best/cheapest option that includes security and ease of access to the family? I have some 750G solid state hard drives I'd like to use, but can also purchase whatever I'd need to (within reasonable limits). I would prefer to learn how to do this all in-house as opposed to getting a subscription and possibly having them lose/lock my data (even though I'd backup everything frequently). Any push in the right direction is greatly appreciated.\n\nI also have some 500G Mac hard drives, but if it's too complicated to incorporate those I won't use them.\n\nThanks for any help.", "author_fullname": "t2_cpn19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal Cloud Storage for Pictures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jsvxs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674521712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m brand new to all of this (including this sub), but I&amp;#39;m willing to dive head first, and spend as long as it takes to figure this out. I just need a point in the right direction.&lt;/p&gt;\n\n&lt;p&gt;I would like to consolidate all of the pictures my family has of my son on a personal storage device everyone in the family can access from anywhere. What is the best/cheapest option that includes security and ease of access to the family? I have some 750G solid state hard drives I&amp;#39;d like to use, but can also purchase whatever I&amp;#39;d need to (within reasonable limits). I would prefer to learn how to do this all in-house as opposed to getting a subscription and possibly having them lose/lock my data (even though I&amp;#39;d backup everything frequently). Any push in the right direction is greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;I also have some 500G Mac hard drives, but if it&amp;#39;s too complicated to incorporate those I won&amp;#39;t use them.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jsvxs", "is_robot_indexable": true, "report_reasons": null, "author": "hey-zues", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jsvxs/personal_cloud_storage_for_pictures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jsvxs/personal_cloud_storage_for_pictures/", "subreddit_subscribers": 667018, "created_utc": 1674521712.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone!\n\nSo my goal is building a NAS, ideally I would have waited a bit for some 12/13th Intel motherboards supporting ECC, for that extra safety, but as it turns out, they're not out there yet.\n\nSince my NAS is dying I need to make one asap, and I will ditch ECC and go non ECC. I'm aware this causes some risks, since the data is stored on the memory first and the disk later (risking corruption if not using ECC).\n\nMy question is, this is more of a \"let's make sure\" kind of thing, if I were to use rsync and enable checksum verifying, would I be safe and be able to detect potential issues from data corruption due to the non ECC memory?\n\nIs it ok for me to calculate the checksum while the data is still potentially cached onto memory? I guess what I'm getting at is: can data corruption in non ECC memory occur only during writing, or at any time the data is cached?\n\nIs the answer is that the corruption can happen during any time, it means that I would have to wait for the data to be fully written onto the disk and not be stored onto memory as cache, in such case, how can I ensure I'm reading and calculating a checksum of the data actually stored in the disk, and would it be a safe enough approach to mitigate the risks of data corruption with non ECC memory?\n\nOnce again, I would like to go for a modern CPU, ideally the 13500 which supports ECC, but I have found no 12th or 13th gen with ECC support, I saw an Intel chipset that should be released supporting, but I have no idea when and my NAS is on the brink of death (doing another backup right now for safety), so I can't wait any longer unfortunately.\n\n**Secondary unrelated question:**\n\nIn case in the future I wanted to swap motherboard in order to go with ECC memory, could I rebuild the same ZFS poll and avoid data loss? Realistically, I won't have enough space to backup all the data (I could of course consider temporarily purchasing some cloud storage, which is probably pretty expensive)?\n\nEssentially, can I reconstruct the pools on another motherboard using the same disks?\n\nThank's in advance, I'm sorry if those are noobs questions, this is my first NAS from scratch pretty much.", "author_fullname": "t2_7vil65ud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is stuff like rsync hash check a good idea with lack of ECC RAM?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jrdjz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674517521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;So my goal is building a NAS, ideally I would have waited a bit for some 12/13th Intel motherboards supporting ECC, for that extra safety, but as it turns out, they&amp;#39;re not out there yet.&lt;/p&gt;\n\n&lt;p&gt;Since my NAS is dying I need to make one asap, and I will ditch ECC and go non ECC. I&amp;#39;m aware this causes some risks, since the data is stored on the memory first and the disk later (risking corruption if not using ECC).&lt;/p&gt;\n\n&lt;p&gt;My question is, this is more of a &amp;quot;let&amp;#39;s make sure&amp;quot; kind of thing, if I were to use rsync and enable checksum verifying, would I be safe and be able to detect potential issues from data corruption due to the non ECC memory?&lt;/p&gt;\n\n&lt;p&gt;Is it ok for me to calculate the checksum while the data is still potentially cached onto memory? I guess what I&amp;#39;m getting at is: can data corruption in non ECC memory occur only during writing, or at any time the data is cached?&lt;/p&gt;\n\n&lt;p&gt;Is the answer is that the corruption can happen during any time, it means that I would have to wait for the data to be fully written onto the disk and not be stored onto memory as cache, in such case, how can I ensure I&amp;#39;m reading and calculating a checksum of the data actually stored in the disk, and would it be a safe enough approach to mitigate the risks of data corruption with non ECC memory?&lt;/p&gt;\n\n&lt;p&gt;Once again, I would like to go for a modern CPU, ideally the 13500 which supports ECC, but I have found no 12th or 13th gen with ECC support, I saw an Intel chipset that should be released supporting, but I have no idea when and my NAS is on the brink of death (doing another backup right now for safety), so I can&amp;#39;t wait any longer unfortunately.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Secondary unrelated question:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In case in the future I wanted to swap motherboard in order to go with ECC memory, could I rebuild the same ZFS poll and avoid data loss? Realistically, I won&amp;#39;t have enough space to backup all the data (I could of course consider temporarily purchasing some cloud storage, which is probably pretty expensive)?&lt;/p&gt;\n\n&lt;p&gt;Essentially, can I reconstruct the pools on another motherboard using the same disks?&lt;/p&gt;\n\n&lt;p&gt;Thank&amp;#39;s in advance, I&amp;#39;m sorry if those are noobs questions, this is my first NAS from scratch pretty much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jrdjz", "is_robot_indexable": true, "report_reasons": null, "author": "LynxesExe", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jrdjz/is_stuff_like_rsync_hash_check_a_good_idea_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jrdjz/is_stuff_like_rsync_hash_check_a_good_idea_with/", "subreddit_subscribers": 667018, "created_utc": 1674517521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have quite a few books, files, etc in which I need to host somewhere anonymously or just in general to allow anyone in my group to access them. It's a bunch of stuff. Only issue with Anonfiles is as you know, they expire. Could one lead me to a website that I can just save, any sized files I'd desire to share with others without pay, anonymously, and permanently? That's all I ask.", "author_fullname": "t2_9qz1zwov", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do any of you know of an Anonfiles alternative?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jq4l5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674514387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have quite a few books, files, etc in which I need to host somewhere anonymously or just in general to allow anyone in my group to access them. It&amp;#39;s a bunch of stuff. Only issue with Anonfiles is as you know, they expire. Could one lead me to a website that I can just save, any sized files I&amp;#39;d desire to share with others without pay, anonymously, and permanently? That&amp;#39;s all I ask.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jq4l5", "is_robot_indexable": true, "report_reasons": null, "author": "RisingFire2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jq4l5/do_any_of_you_know_of_an_anonfiles_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jq4l5/do_any_of_you_know_of_an_anonfiles_alternative/", "subreddit_subscribers": 667018, "created_utc": 1674514387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Had a pretty bad tornado watch recently and realized I have a lot of physical media that could be lost from the possible damages, I decided to go in the process of backing up everything as much as I can. Unfortunately my work station just has a dvd drive and not a blu Ray drive to play Blu-ray\u2019s off of, and buying a drive to install and replace might take more money and time than I thought, so I\u2019m thinking of getting an external disc drive to dump everything onto a 13TB drive and then return the external disc drive back to Amazon to get back and save money.\n\nWhat would you recommend that would cover both game discs and tv/movie discs in Blu-ray?", "author_fullname": "t2_v4boj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ideal external blu ray disc drive to rip and dump discs with", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jq1au", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674514152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a pretty bad tornado watch recently and realized I have a lot of physical media that could be lost from the possible damages, I decided to go in the process of backing up everything as much as I can. Unfortunately my work station just has a dvd drive and not a blu Ray drive to play Blu-ray\u2019s off of, and buying a drive to install and replace might take more money and time than I thought, so I\u2019m thinking of getting an external disc drive to dump everything onto a 13TB drive and then return the external disc drive back to Amazon to get back and save money.&lt;/p&gt;\n\n&lt;p&gt;What would you recommend that would cover both game discs and tv/movie discs in Blu-ray?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jq1au", "is_robot_indexable": true, "report_reasons": null, "author": "Lmnr01", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jq1au/ideal_external_blu_ray_disc_drive_to_rip_and_dump/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jq1au/ideal_external_blu_ray_disc_drive_to_rip_and_dump/", "subreddit_subscribers": 667018, "created_utc": 1674514152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have an entire milk crate of VHS tapes I stumbled upon last week of family birthday parties. A lot of them have loved ones that are no longer with us. I was hoping to archive these in a more digital-friendly format for my mother. \n\nDoes anyone have any suggestions for how I can get these converted to the best quality? I know that services do offer it. I'm just skeptical since I prefer to not let anyone actually destroy tapes that are already been sitting in a closet for years. I'd rather trust myself to do it &amp; use software for effort correction. Does anyone have any good suggestions for a VHS conversion setup on a budget?", "author_fullname": "t2_cl3zf0dv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best VCR Choice - Affordable", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jdsct", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674484242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an entire milk crate of VHS tapes I stumbled upon last week of family birthday parties. A lot of them have loved ones that are no longer with us. I was hoping to archive these in a more digital-friendly format for my mother. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any suggestions for how I can get these converted to the best quality? I know that services do offer it. I&amp;#39;m just skeptical since I prefer to not let anyone actually destroy tapes that are already been sitting in a closet for years. I&amp;#39;d rather trust myself to do it &amp;amp; use software for effort correction. Does anyone have any good suggestions for a VHS conversion setup on a budget?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jdsct", "is_robot_indexable": true, "report_reasons": null, "author": "No_Bit_1456", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jdsct/best_vcr_choice_affordable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jdsct/best_vcr_choice_affordable/", "subreddit_subscribers": 667018, "created_utc": 1674484242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to use ZFS for the error checking to make sure the source data integrity is correct as this would effect my backups as well, but don't need RAID as I do backups and uptime isn't that important. \n\nIs it possible to run ZFS with no parity and when an bitrot gets detected, ZFS will notify which files have been corrupted and I can manually replace them from my backup?\n\nAlso ZFS checks the file against the stored checksum when doing any read operations, but when using a Windows PC if I was to copy data from the ZFS shared drive to the Windows PC will ZFS still compare the file against the stored checksum?\n\nWith no parity drives are scrubs nessessary if I was to copy all data stored every week?", "author_fullname": "t2_5s1gh8m1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ZFS with no parity do manual correction from backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jbvhi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674478681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to use ZFS for the error checking to make sure the source data integrity is correct as this would effect my backups as well, but don&amp;#39;t need RAID as I do backups and uptime isn&amp;#39;t that important. &lt;/p&gt;\n\n&lt;p&gt;Is it possible to run ZFS with no parity and when an bitrot gets detected, ZFS will notify which files have been corrupted and I can manually replace them from my backup?&lt;/p&gt;\n\n&lt;p&gt;Also ZFS checks the file against the stored checksum when doing any read operations, but when using a Windows PC if I was to copy data from the ZFS shared drive to the Windows PC will ZFS still compare the file against the stored checksum?&lt;/p&gt;\n\n&lt;p&gt;With no parity drives are scrubs nessessary if I was to copy all data stored every week?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jbvhi", "is_robot_indexable": true, "report_reasons": null, "author": "pjkm123987", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jbvhi/zfs_with_no_parity_do_manual_correction_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jbvhi/zfs_with_no_parity_do_manual_correction_from/", "subreddit_subscribers": 667018, "created_utc": 1674478681.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It seems that if you grab videos from youtube and pornhub just after they've been released (im not talking of immediately released when only 360p may be available, but once 1080p is available), you get one file. but if you try and download the same files (i use jdownloader 2) after some time (days?), you get another file with a slightly different file size.\n\nJust to clarify i'm not talking of grabbing lives that have just finished streaming, and I'm not talking of jd grabbing sometimes both 'regular video' and '\\_hls video' that usually end up having very similar file sizes and quality.\n\nThe file size difference is small, however it's a shame because it messes up the easiest duplicate detection by file size (and file name may also end up being slightly different depending on your settings, and it's possible that sometimes even the autograbbed 'date' is different), and can also mess up jd 'already downloaded' detection.\n\nAnyone else encounter this and know why?", "author_fullname": "t2_111o3ncd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Youtube and PH seem to sometimes replace video files with very similar ones?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10jxf4b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674535599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems that if you grab videos from youtube and pornhub just after they&amp;#39;ve been released (im not talking of immediately released when only 360p may be available, but once 1080p is available), you get one file. but if you try and download the same files (i use jdownloader 2) after some time (days?), you get another file with a slightly different file size.&lt;/p&gt;\n\n&lt;p&gt;Just to clarify i&amp;#39;m not talking of grabbing lives that have just finished streaming, and I&amp;#39;m not talking of jd grabbing sometimes both &amp;#39;regular video&amp;#39; and &amp;#39;_hls video&amp;#39; that usually end up having very similar file sizes and quality.&lt;/p&gt;\n\n&lt;p&gt;The file size difference is small, however it&amp;#39;s a shame because it messes up the easiest duplicate detection by file size (and file name may also end up being slightly different depending on your settings, and it&amp;#39;s possible that sometimes even the autograbbed &amp;#39;date&amp;#39; is different), and can also mess up jd &amp;#39;already downloaded&amp;#39; detection.&lt;/p&gt;\n\n&lt;p&gt;Anyone else encounter this and know why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jxf4b", "is_robot_indexable": true, "report_reasons": null, "author": "BitsAndBobs304", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jxf4b/youtube_and_ph_seem_to_sometimes_replace_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jxf4b/youtube_and_ph_seem_to_sometimes_replace_video/", "subreddit_subscribers": 667018, "created_utc": 1674535599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "x-post from the OMV sub, I am new to all of this so any feedback would be appreciated.  \n\n\nI just started using OMV (new to Linux &amp; Raid). I just set up a Raid 5 with 4 14 TB Seagate Exos drives. I then got the power cables I was waiting on for my 2 remaining drives, so I grew the Raid by 2 drives (so 6 total now). It now says it is reshaping, but it has been stuck on 0.0% progress for hours. I have not set up the file system yet or added any data.   \n\n\nDid I do this in the wrong order? Can I fix it or stop it? Should I? What else did I do wrong?  \n\n\nThanks!", "author_fullname": "t2_75xl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Growing Raid 5 in OMV, stuck at 0.0% reshape status for hours now.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jv36t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674528160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;x-post from the OMV sub, I am new to all of this so any feedback would be appreciated.  &lt;/p&gt;\n\n&lt;p&gt;I just started using OMV (new to Linux &amp;amp; Raid). I just set up a Raid 5 with 4 14 TB Seagate Exos drives. I then got the power cables I was waiting on for my 2 remaining drives, so I grew the Raid by 2 drives (so 6 total now). It now says it is reshaping, but it has been stuck on 0.0% progress for hours. I have not set up the file system yet or added any data.   &lt;/p&gt;\n\n&lt;p&gt;Did I do this in the wrong order? Can I fix it or stop it? Should I? What else did I do wrong?  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jv36t", "is_robot_indexable": true, "report_reasons": null, "author": "angryjew", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jv36t/growing_raid_5_in_omv_stuck_at_00_reshape_status/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jv36t/growing_raid_5_in_omv_stuck_at_00_reshape_status/", "subreddit_subscribers": 667018, "created_utc": 1674528160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've looked at many options and they all fall short due to Twitter's poor achieving.\n\nI have my bookmarks in categorized folders, I would like to save the (Link to the post, Text in the tweet, and Username) that's in each folder to a CSV file for achieving.\n\nWhat automated method can be used for this?\n\nThank you", "author_fullname": "t2_11pyqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Save Twitter Bookmarks in Folders as CSV.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jp242", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674511719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve looked at many options and they all fall short due to Twitter&amp;#39;s poor achieving.&lt;/p&gt;\n\n&lt;p&gt;I have my bookmarks in categorized folders, I would like to save the (Link to the post, Text in the tweet, and Username) that&amp;#39;s in each folder to a CSV file for achieving.&lt;/p&gt;\n\n&lt;p&gt;What automated method can be used for this?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jp242", "is_robot_indexable": true, "report_reasons": null, "author": "Retrobolt1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jp242/how_to_save_twitter_bookmarks_in_folders_as_csv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jp242/how_to_save_twitter_bookmarks_in_folders_as_csv/", "subreddit_subscribers": 667018, "created_utc": 1674511719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't need to download any of the actual files, I just need a list of all the links that look like \"https://midgetscrotum.bandcamp.com/track/*\". \n\nI usually use WinHTTrack for scraping, but that downloads the site itself, which I don't need.", "author_fullname": "t2_7haig6fl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I get a list of all the individual track links of a bandcamp album/user?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jntud", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674508744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t need to download any of the actual files, I just need a list of all the links that look like &amp;quot;&lt;a href=\"https://midgetscrotum.bandcamp.com/track/*\"&gt;https://midgetscrotum.bandcamp.com/track/*&lt;/a&gt;&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;I usually use WinHTTrack for scraping, but that downloads the site itself, which I don&amp;#39;t need.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jntud", "is_robot_indexable": true, "report_reasons": null, "author": "spermo_chuggins", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jntud/how_do_i_get_a_list_of_all_the_individual_track/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jntud/how_do_i_get_a_list_of_all_the_individual_track/", "subreddit_subscribers": 667018, "created_utc": 1674508744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a TS-413K (firmware version 4.5.4.2012) with expansion TR-004 (fw 1.2.0) connected in software mode. TS has 2x8TB (raid 1) and 2x12TB (raid 1), while TR has 2x20TB (raid 1), 1x2TB (spare, can't be allocated) and 1x4TB (spare, can't be allocated).\n\nAssuming I can freely shuffle drives and not worry about data loss, is there a way to configure that setup to preserve current raid 1 groups and use spares as individual network drives or in raid 0? If possible, I'd rather have spares in faster device as they'll be heavily used.\n\nI've read in TR-004 manual that it can't have more than one raid group, but nothing about spare drives not being able to be used at all. Manual also mentions that the expansion can be run in \"External Device Mode\" (page 23, Storage Modes and Platform Support) which does support multiple groups, but nowhere it mention *how to set it up like so when connected to QNAP NAS*, only that it can be used with a \"NAS or a Windows, macOS, or Linux-based computer\". I have a NAS, the mentioned TS-413K, but it won't let me create raid 1 pool on TR if it's in Individual mode, and in Software Control mode I can't use spare drives after creating a raid 1 pool!\n\nI bought TR-004 because it was advertised as an expansion module that NAS could manage like a part of its own setup, but it's like a tumor that won't let me do as I please.", "author_fullname": "t2_vk4l07vu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TS-413K &amp; TR-004 setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jmd71", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674505234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a TS-413K (firmware version 4.5.4.2012) with expansion TR-004 (fw 1.2.0) connected in software mode. TS has 2x8TB (raid 1) and 2x12TB (raid 1), while TR has 2x20TB (raid 1), 1x2TB (spare, can&amp;#39;t be allocated) and 1x4TB (spare, can&amp;#39;t be allocated).&lt;/p&gt;\n\n&lt;p&gt;Assuming I can freely shuffle drives and not worry about data loss, is there a way to configure that setup to preserve current raid 1 groups and use spares as individual network drives or in raid 0? If possible, I&amp;#39;d rather have spares in faster device as they&amp;#39;ll be heavily used.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read in TR-004 manual that it can&amp;#39;t have more than one raid group, but nothing about spare drives not being able to be used at all. Manual also mentions that the expansion can be run in &amp;quot;External Device Mode&amp;quot; (page 23, Storage Modes and Platform Support) which does support multiple groups, but nowhere it mention &lt;em&gt;how to set it up like so when connected to QNAP NAS&lt;/em&gt;, only that it can be used with a &amp;quot;NAS or a Windows, macOS, or Linux-based computer&amp;quot;. I have a NAS, the mentioned TS-413K, but it won&amp;#39;t let me create raid 1 pool on TR if it&amp;#39;s in Individual mode, and in Software Control mode I can&amp;#39;t use spare drives after creating a raid 1 pool!&lt;/p&gt;\n\n&lt;p&gt;I bought TR-004 because it was advertised as an expansion module that NAS could manage like a part of its own setup, but it&amp;#39;s like a tumor that won&amp;#39;t let me do as I please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jmd71", "is_robot_indexable": true, "report_reasons": null, "author": "Aromatic-Pin-6947", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jmd71/ts413k_tr004_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jmd71/ts413k_tr004_setup/", "subreddit_subscribers": 667018, "created_utc": 1674505234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello hive mind. I\u2019m asking this on a few subreddits so apologies if you\u2019ve seen this elsewhere today.\n\nSo\u2026 yeah, pretty specific question, but I\u2019m slightly flexible on the solution if it works. Here goes:\n\n\nI have several 10 TB enterprise-level hard drives that SMART test great that I would like to create a RAID of to use with a Mac or on a network. I\u2019d like to make it RAID 5 or RAID 5+0. I\u2019d also like to find something rack mountable.\n\nI\u2019ve seen a few enclosures online that are 4U ( rack units ) high with 24 hard drive capacity that seem okay but so far none have jumped out at me as \u2018the one\u2019.\n\n\nDo you have any suggestions or recommendations of what equipment to look at or supplier to check that\u2019s comparable to these specs?\n\n\nYes, my Google-Fu works - I\u2019ve searched online and I\u2019ve found a few, but I want to ask the hive mind here what they think.\n\nThank you in advance for the recommendations and help.\n\nCheers", "author_fullname": "t2_3hikt1e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone know of a great 4U rack-mountable, 24 HDD enclosure that\u2019s RAID 5 capable and works with a Mac?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jlul0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674503992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello hive mind. I\u2019m asking this on a few subreddits so apologies if you\u2019ve seen this elsewhere today.&lt;/p&gt;\n\n&lt;p&gt;So\u2026 yeah, pretty specific question, but I\u2019m slightly flexible on the solution if it works. Here goes:&lt;/p&gt;\n\n&lt;p&gt;I have several 10 TB enterprise-level hard drives that SMART test great that I would like to create a RAID of to use with a Mac or on a network. I\u2019d like to make it RAID 5 or RAID 5+0. I\u2019d also like to find something rack mountable.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen a few enclosures online that are 4U ( rack units ) high with 24 hard drive capacity that seem okay but so far none have jumped out at me as \u2018the one\u2019.&lt;/p&gt;\n\n&lt;p&gt;Do you have any suggestions or recommendations of what equipment to look at or supplier to check that\u2019s comparable to these specs?&lt;/p&gt;\n\n&lt;p&gt;Yes, my Google-Fu works - I\u2019ve searched online and I\u2019ve found a few, but I want to ask the hive mind here what they think.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for the recommendations and help.&lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jlul0", "is_robot_indexable": true, "report_reasons": null, "author": "not_that_guy_at_work", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jlul0/anyone_know_of_a_great_4u_rackmountable_24_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jlul0/anyone_know_of_a_great_4u_rackmountable_24_hdd/", "subreddit_subscribers": 667018, "created_utc": 1674503992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a windows 10 HP laptop that i use primarily for studying. Almost 8 months ago I installed a new SSD in place of my old HDD. Since then I have barely downloaded any data on it but I began noticing that HD sentinel kept showing ever increasing amount of data writes (Approximately 19 gb an hour) to my kingston 128 gb internal drive. I turned off the page file thinking it might be an issue but it made no difference. I also noticed that its temperature always remains exactly at 40C even when other external drives' temperature fluctuates with room temperature. I thought that HDsentinel might be misreporting it but I used HWINFO and it showed the same values. I should add that I've noticed no performance or reliability issues thus far. I've attached an image to help you further understand the issue.\n\nhttps://preview.redd.it/dish1gyd6uda1.png?width=892&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=83f9c8a9ff73a16bd05fc4743c9838d5514f61c9", "author_fullname": "t2_5ydua9ca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "System SSD keeps writing huge amount of data (93.16 TB data writes in 128 GB SSD)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"dish1gyd6uda1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 176, "x": 108, "u": "https://preview.redd.it/dish1gyd6uda1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e58b59aa352b7c9b57e01af5c11ede96c9dd7d3"}, {"y": 352, "x": 216, "u": "https://preview.redd.it/dish1gyd6uda1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58de4c7d6b739111f5010deefceb067a619e82d0"}, {"y": 522, "x": 320, "u": "https://preview.redd.it/dish1gyd6uda1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a9cd5d8de19d3a3e52e0322c484b85a7110fa541"}, {"y": 1044, "x": 640, "u": "https://preview.redd.it/dish1gyd6uda1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f4e581e606366679edb2d106072022058532e8b"}], "s": {"y": 1456, "x": 892, "u": "https://preview.redd.it/dish1gyd6uda1.png?width=892&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=83f9c8a9ff73a16bd05fc4743c9838d5514f61c9"}, "id": "dish1gyd6uda1"}}, "name": "t3_10jjl30", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/0R2O7AGsoBvHpc3BllJU-Akuthkluu81dkLFMDBEou8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674498515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a windows 10 HP laptop that i use primarily for studying. Almost 8 months ago I installed a new SSD in place of my old HDD. Since then I have barely downloaded any data on it but I began noticing that HD sentinel kept showing ever increasing amount of data writes (Approximately 19 gb an hour) to my kingston 128 gb internal drive. I turned off the page file thinking it might be an issue but it made no difference. I also noticed that its temperature always remains exactly at 40C even when other external drives&amp;#39; temperature fluctuates with room temperature. I thought that HDsentinel might be misreporting it but I used HWINFO and it showed the same values. I should add that I&amp;#39;ve noticed no performance or reliability issues thus far. I&amp;#39;ve attached an image to help you further understand the issue.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dish1gyd6uda1.png?width=892&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=83f9c8a9ff73a16bd05fc4743c9838d5514f61c9\"&gt;https://preview.redd.it/dish1gyd6uda1.png?width=892&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=83f9c8a9ff73a16bd05fc4743c9838d5514f61c9&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jjl30", "is_robot_indexable": true, "report_reasons": null, "author": "rkj1990", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jjl30/system_ssd_keeps_writing_huge_amount_of_data_9316/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jjl30/system_ssd_keeps_writing_huge_amount_of_data_9316/", "subreddit_subscribers": 667018, "created_utc": 1674498515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello. I own a repair shop and during 20 years I have collected a large amount of data I have lots of data, firmware, tips, service bulletins, collections of tips, repair guide, spare parts, inventory etc.\n\nThe data that I have are saved as excel, word, notepad, pdfs, images.\nIm adding data weekly and its still organized but kinda hard keeping track of all the files as when im searching something I search purely based on my memory.\n\nIm using windows search and everything to search my files but sometimes they do not show up on search, so I have to recall my memory to find.\n\nWhat would be the best way to make everything searchable on windows search or on some type of other search programs?\nI was thinking to make every pdf searchable by using Abbyreader and then using search.\n\nWhat would be the best approach to solve my problem?\n\nCheers!", "author_fullname": "t2_lbnm6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me organize and make my data searchable", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jd32g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674483345.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674482293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I own a repair shop and during 20 years I have collected a large amount of data I have lots of data, firmware, tips, service bulletins, collections of tips, repair guide, spare parts, inventory etc.&lt;/p&gt;\n\n&lt;p&gt;The data that I have are saved as excel, word, notepad, pdfs, images.\nIm adding data weekly and its still organized but kinda hard keeping track of all the files as when im searching something I search purely based on my memory.&lt;/p&gt;\n\n&lt;p&gt;Im using windows search and everything to search my files but sometimes they do not show up on search, so I have to recall my memory to find.&lt;/p&gt;\n\n&lt;p&gt;What would be the best way to make everything searchable on windows search or on some type of other search programs?\nI was thinking to make every pdf searchable by using Abbyreader and then using search.&lt;/p&gt;\n\n&lt;p&gt;What would be the best approach to solve my problem?&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jd32g", "is_robot_indexable": true, "report_reasons": null, "author": "soediv_throwaway", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jd32g/help_me_organize_and_make_my_data_searchable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jd32g/help_me_organize_and_make_my_data_searchable/", "subreddit_subscribers": 667018, "created_utc": 1674482293.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}