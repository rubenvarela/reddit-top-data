{"kind": "Listing", "data": {"after": "t3_10jq4l5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_2hjf6lgy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Didn\u2019t really know which group to post this. Why is this so cheap. Is there a catch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10jyt6c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 193, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 193, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Oq9W0omgV9EbkVCH0r8Sdl8wxXUKPrRSCq66HXfQY68.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674540477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9yrysgok4zda1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9yrysgok4zda1.jpg?auto=webp&amp;v=enabled&amp;s=c5497fdb2666de6211696d85660a7ddb729c989a", "width": 1125, "height": 2436}, "resolutions": [{"url": "https://preview.redd.it/9yrysgok4zda1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f07b6c554be5155ec4847d2986f6a645ab7450d", "width": 108, "height": 216}, {"url": "https://preview.redd.it/9yrysgok4zda1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1df338125bf48348ec94f2d0976743ea3a5d1830", "width": 216, "height": 432}, {"url": "https://preview.redd.it/9yrysgok4zda1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86b33639840ef9b24c6c5dbaca4a4fe2e3cae1c0", "width": 320, "height": 640}, {"url": "https://preview.redd.it/9yrysgok4zda1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09490b060f7920bb9a2fe49171d050bce7662d22", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/9yrysgok4zda1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8e547d8918a1e1a8031a194f757dda4bc7dcca4", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/9yrysgok4zda1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a6d8f89afb8e55b2d56b7e7ed771abe47c985c3", "width": 1080, "height": 2160}], "variants": {}, "id": "-eiJfedWdVhQansmCF2XprviU7lHCw_XzMCXpIWcO34"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jyt6c", "is_robot_indexable": true, "report_reasons": null, "author": "AngelOfGod3", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jyt6c/didnt_really_know_which_group_to_post_this_why_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9yrysgok4zda1.jpg", "subreddit_subscribers": 667071, "created_utc": 1674540477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For this one, we are giving away an IronWolf Pro 125 960GB SSD to one lucky winner in this thread!\n\nHappy New Year! To kick 2023 off right, here's the terms for this one:\n\n**The prize is: one IronWolf Pro 125 960GB SSD**\n\n**How to enter:**\n\n**Just reply to this post once with a comment that includes the terms RunWithIronWolf and Seagate telling us how the prize would help further your data hoarding projects.**\n\n\nSelection process/rules\n\nOne entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. Entries are open until January 31st 2023, 23:59 UTC. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.\n\nGeographic restrictions:\n\nOur policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions (e.g. the obvious shipping restrictions to Russia and Belarus currently)\n\nUS\n\nCanada (exc. Quebec and will require a basic skills-based question if winner is chosen by law)\n\nBrazil\n\nSouth America\n\nUnited Kingdom\n\nGermany\n\nFrance\n\nIberia\n\nAustralia\n\nNew Zealand\n\nKorea\n\nIndia\n\nMalaysia\n\nSingapore\n\nChina", "author_fullname": "t2_16nn7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We're Back - Official January 2023 Seagate IronWolf Giveaway!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jjdfe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": "", "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "OFFICIAL", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674498007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For this one, we are giving away an IronWolf Pro 125 960GB SSD to one lucky winner in this thread!&lt;/p&gt;\n\n&lt;p&gt;Happy New Year! To kick 2023 off right, here&amp;#39;s the terms for this one:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The prize is: one IronWolf Pro 125 960GB SSD&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How to enter:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Just reply to this post once with a comment that includes the terms RunWithIronWolf and Seagate telling us how the prize would help further your data hoarding projects.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Selection process/rules&lt;/p&gt;\n\n&lt;p&gt;One entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. Entries are open until January 31st 2023, 23:59 UTC. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.&lt;/p&gt;\n\n&lt;p&gt;Geographic restrictions:&lt;/p&gt;\n\n&lt;p&gt;Our policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions (e.g. the obvious shipping restrictions to Russia and Belarus currently)&lt;/p&gt;\n\n&lt;p&gt;US&lt;/p&gt;\n\n&lt;p&gt;Canada (exc. Quebec and will require a basic skills-based question if winner is chosen by law)&lt;/p&gt;\n\n&lt;p&gt;Brazil&lt;/p&gt;\n\n&lt;p&gt;South America&lt;/p&gt;\n\n&lt;p&gt;United Kingdom&lt;/p&gt;\n\n&lt;p&gt;Germany&lt;/p&gt;\n\n&lt;p&gt;France&lt;/p&gt;\n\n&lt;p&gt;Iberia&lt;/p&gt;\n\n&lt;p&gt;Australia&lt;/p&gt;\n\n&lt;p&gt;New Zealand&lt;/p&gt;\n\n&lt;p&gt;Korea&lt;/p&gt;\n\n&lt;p&gt;India&lt;/p&gt;\n\n&lt;p&gt;Malaysia&lt;/p&gt;\n\n&lt;p&gt;Singapore&lt;/p&gt;\n\n&lt;p&gt;China&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "81f0a58e-b3f5-11ea-95d7-0e4db8ecc231", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "OFFICIAL SEAGATE", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "10jjdfe", "is_robot_indexable": true, "report_reasons": null, "author": "Seagate_Surfer", "discussion_type": null, "num_comments": 170, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10jjdfe/were_back_official_january_2023_seagate_ironwolf/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/10jjdfe/were_back_official_january_2023_seagate_ironwolf/", "subreddit_subscribers": 667071, "created_utc": 1674498007.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Pokemon Fossil Museum](https://my.matterport.com/show/?m=P9WCbyCBGBM)\n\n&amp;#x200B;\n\nThere is a wonderful virtual tour of the Pokemon Fossial Museum but I'm confident this tour won't last much longer as I believe the real event is now over. How would I best preserve its contents?", "author_fullname": "t2_a803a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would I best archive the Pokemon Fossil Museum ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k5p3p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674567551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://my.matterport.com/show/?m=P9WCbyCBGBM\"&gt;Pokemon Fossil Museum&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;There is a wonderful virtual tour of the Pokemon Fossial Museum but I&amp;#39;m confident this tour won&amp;#39;t last much longer as I believe the real event is now over. How would I best preserve its contents?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?auto=webp&amp;v=enabled&amp;s=31a49400b27cc8e895ed64fc1f661843df1702bd", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07e93e0352eaec3c9b7e195b45e46fc6bc95bfa7", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46ad8f09aaa117b0ea4f1ce4d6ae239af341aa5b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98878cf2f9655172cdf32cc0a9edc00a25e4ca4e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e0ee1036ad2c931eb6aa78eca4547f946465003", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=557dd207ca5a81d20dfd562b3da884c367cef3de", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/p-qAfDl00Uml6IwiotXZ_PXazPFzTq51xFy405T5Yfw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e75e7d52fe47a8c38dce79416de2f6ceba8d3a7e", "width": 1080, "height": 607}], "variants": {}, "id": "fGnKi5YB-vxBlAhbeSiM_drqVykiurxKM0A3j_rn0ds"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k5p3p", "is_robot_indexable": true, "report_reasons": null, "author": "shadowfax1007", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k5p3p/how_would_i_best_archive_the_pokemon_fossil_museum/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k5p3p/how_would_i_best_archive_the_pokemon_fossil_museum/", "subreddit_subscribers": 667071, "created_utc": 1674567551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m sitting on many now-deleted YouTube videos by one of my favorite creators. I only kept them for myself because they mean a lot to me, but I\u2019ve been contemplating whether or not to upload them somewhere. Afaik, I might be the only person who has those files. I have some moral concerns for obvious reasons. Those videos were made unavailable because the creator is clearly not proud of them anymore (outdated beliefs, bad quality, etc.). What should I do?", "author_fullname": "t2_srv95nt7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it ethical to reupload YouTube videos that were taken down by the creator?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jms85", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674506242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m sitting on many now-deleted YouTube videos by one of my favorite creators. I only kept them for myself because they mean a lot to me, but I\u2019ve been contemplating whether or not to upload them somewhere. Afaik, I might be the only person who has those files. I have some moral concerns for obvious reasons. Those videos were made unavailable because the creator is clearly not proud of them anymore (outdated beliefs, bad quality, etc.). What should I do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jms85", "is_robot_indexable": true, "report_reasons": null, "author": "Jaded_Answer_2188", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jms85/is_it_ethical_to_reupload_youtube_videos_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jms85/is_it_ethical_to_reupload_youtube_videos_that/", "subreddit_subscribers": 667071, "created_utc": 1674506242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not planning on using them constantly, I'd just grab what I need and copy it over to my main drive whenever I feel like it. Wouldn't be read from the drives either. Looking for pure storage.", "author_fullname": "t2_dsagmsmp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External HDDs or internals better for ROM and ISO hoarding?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k2mh2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674556760.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not planning on using them constantly, I&amp;#39;d just grab what I need and copy it over to my main drive whenever I feel like it. Wouldn&amp;#39;t be read from the drives either. Looking for pure storage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k2mh2", "is_robot_indexable": true, "report_reasons": null, "author": "metatron_ebooks", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k2mh2/external_hdds_or_internals_better_for_rom_and_iso/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k2mh2/external_hdds_or_internals_better_for_rom_and_iso/", "subreddit_subscribers": 667071, "created_utc": 1674556760.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello.\n\nIn 2003, a process was made for storing various documents, including media files and records. As a base, a server with Windows XP, NTFS and a local copy of the system was used as a backup in case the main one had a problem.\n\nTime went by and by May, I have about 51 TB of files, including PDFs, media files, documents, records and log files that need to be stored for exactly 30 years (due to legislation in the country where I live and work).\n\nI am unsure which file system to use and which operating system to backup. I have been a Linux user for a little over 2 years, I know that btrfs and ext4 are excellent for this, but what about in the long run? Would I have any problems? Could anyone with experience in \"cold\" storage give me some insight?", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "51 TB file storage - Where can you start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jzqmw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": "", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674544062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.&lt;/p&gt;\n\n&lt;p&gt;In 2003, a process was made for storing various documents, including media files and records. As a base, a server with Windows XP, NTFS and a local copy of the system was used as a backup in case the main one had a problem.&lt;/p&gt;\n\n&lt;p&gt;Time went by and by May, I have about 51 TB of files, including PDFs, media files, documents, records and log files that need to be stored for exactly 30 years (due to legislation in the country where I live and work).&lt;/p&gt;\n\n&lt;p&gt;I am unsure which file system to use and which operating system to backup. I have been a Linux user for a little over 2 years, I know that btrfs and ext4 are excellent for this, but what about in the long run? Would I have any problems? Could anyone with experience in &amp;quot;cold&amp;quot; storage give me some insight?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jzqmw", "is_robot_indexable": true, "report_reasons": null, "author": "[deleted]", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10jzqmw/51_tb_file_storage_where_can_you_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jzqmw/51_tb_file_storage_where_can_you_start/", "subreddit_subscribers": 667071, "created_utc": 1674544062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm reworking some of my backup mechanics (thanks to drive compatibility issues, thanks asrock...) and I realized I could just do incremental backups on my ZFS snapshot directory instead of doing incremental backups on the actual directories.\n\nThis has some benefits. Since the snapshot directory is read only from the client devices (unless you permit root login on SMB) it pretty much eliminates the chance of a configuration mishap or scripting error wiping out the actual directory. Also, I would get an independant copy of the dataset's history in case I misconfigure the retention policies or delete the wrong snapshot. I don't know if I can 'readd' accidentally deleted snapshots from a non ZFS source but that's potentially a problem for future me.\n\nSince its an archival dataset where nothing gets deleted, backing up the snapshots cost basically no extra storage space. From what I can tell, this is a win-win situation which makes me wonder if I missed something.", "author_fullname": "t2_upalof7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a \"proper\" way to back up ZFS snapshots?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jt0ad", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674522035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m reworking some of my backup mechanics (thanks to drive compatibility issues, thanks asrock...) and I realized I could just do incremental backups on my ZFS snapshot directory instead of doing incremental backups on the actual directories.&lt;/p&gt;\n\n&lt;p&gt;This has some benefits. Since the snapshot directory is read only from the client devices (unless you permit root login on SMB) it pretty much eliminates the chance of a configuration mishap or scripting error wiping out the actual directory. Also, I would get an independant copy of the dataset&amp;#39;s history in case I misconfigure the retention policies or delete the wrong snapshot. I don&amp;#39;t know if I can &amp;#39;readd&amp;#39; accidentally deleted snapshots from a non ZFS source but that&amp;#39;s potentially a problem for future me.&lt;/p&gt;\n\n&lt;p&gt;Since its an archival dataset where nothing gets deleted, backing up the snapshots cost basically no extra storage space. From what I can tell, this is a win-win situation which makes me wonder if I missed something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "vTrueNAS 72TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jt0ad", "is_robot_indexable": true, "report_reasons": null, "author": "Party_9001", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10jt0ad/is_there_a_proper_way_to_back_up_zfs_snapshots/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jt0ad/is_there_a_proper_way_to_back_up_zfs_snapshots/", "subreddit_subscribers": 667071, "created_utc": 1674522035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Please. Specifically letters P and N. I've had 1.0kB/s down for the past several months, 2 peers no seeders.", "author_fullname": "t2_8nbrguzr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shamelessly seeking seeders for the 2017 ihadp GW archives...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jpji4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674512912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please. Specifically letters P and N. I&amp;#39;ve had 1.0kB/s down for the past several months, 2 peers no seeders.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10jpji4", "is_robot_indexable": true, "report_reasons": null, "author": "parastro", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jpji4/shamelessly_seeking_seeders_for_the_2017_ihadp_gw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jpji4/shamelessly_seeking_seeders_for_the_2017_ihadp_gw/", "subreddit_subscribers": 667071, "created_utc": 1674512912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I would like to buy some cheap unpacked clening tapes that spend time as spared never used. I have HP LTO5 drive and cleaning tapes are IBM 35L2086 but on front cover is written \"for use with ultrium 1 and 2 drives\". Are they going fit to my drive/ is it back compatible from LTO5 to LTO2?", "author_fullname": "t2_4mfclctu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTO cleaning tapes compatibility", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k1n97", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674552371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to buy some cheap unpacked clening tapes that spend time as spared never used. I have HP LTO5 drive and cleaning tapes are IBM 35L2086 but on front cover is written &amp;quot;for use with ultrium 1 and 2 drives&amp;quot;. Are they going fit to my drive/ is it back compatible from LTO5 to LTO2?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k1n97", "is_robot_indexable": true, "report_reasons": null, "author": "ruffpl", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k1n97/lto_cleaning_tapes_compatibility/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k1n97/lto_cleaning_tapes_compatibility/", "subreddit_subscribers": 667071, "created_utc": 1674552371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am expanding my business and I want to connect my data collection machine(does all the drive imaging) to our infrastructure via 10Gbe.  [This guy here](https://www.tp-link.com/us/business-networking/managed-switch/tl-sg3428x/v1/) has 10Gbe 'uplink' SFP+ ports.  Is that just a regular port?  Can I connect a desktop computer to that via SFP, as well as my NAS, and the two will talk over fiber?", "author_fullname": "t2_6eiisul2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does a 10Gbe SFP+ 'uplink' port mean?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jtp1b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674524007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am expanding my business and I want to connect my data collection machine(does all the drive imaging) to our infrastructure via 10Gbe.  &lt;a href=\"https://www.tp-link.com/us/business-networking/managed-switch/tl-sg3428x/v1/\"&gt;This guy here&lt;/a&gt; has 10Gbe &amp;#39;uplink&amp;#39; SFP+ ports.  Is that just a regular port?  Can I connect a desktop computer to that via SFP, as well as my NAS, and the two will talk over fiber?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AD4VaJQhVa0HoaI9VSnho334dCF3CKbVDA5TR1ZaI_A.jpg?auto=webp&amp;v=enabled&amp;s=da0441d55b300308428998a85a4fd8606597b7a6", "width": 1000, "height": 1000}, "resolutions": [{"url": "https://external-preview.redd.it/AD4VaJQhVa0HoaI9VSnho334dCF3CKbVDA5TR1ZaI_A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d828f8c49bce3a53ea132ca7fdd461cf5de956c4", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/AD4VaJQhVa0HoaI9VSnho334dCF3CKbVDA5TR1ZaI_A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8be492d83cf02aebcc2ea0cd0c810e3c55a06097", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/AD4VaJQhVa0HoaI9VSnho334dCF3CKbVDA5TR1ZaI_A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6050307cd4c828f50c443de225e9e984e0f04946", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/AD4VaJQhVa0HoaI9VSnho334dCF3CKbVDA5TR1ZaI_A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d342ed95bcc8a9adfb762c8ca2b4917ae50b894", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/AD4VaJQhVa0HoaI9VSnho334dCF3CKbVDA5TR1ZaI_A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b33d22c8d70749514afe2c0341d6e98fbd9bc5a", "width": 960, "height": 960}], "variants": {}, "id": "3PQ6V9J61vwaA_BPtKmfepsTa5LZQdP3FLeP2yWRfdo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jtp1b", "is_robot_indexable": true, "report_reasons": null, "author": "Pleaseclap4", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jtp1b/what_does_a_10gbe_sfp_uplink_port_mean/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jtp1b/what_does_a_10gbe_sfp_uplink_port_mean/", "subreddit_subscribers": 667071, "created_utc": 1674524007.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Give me you view on my backup strategy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10ka3fg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_4p8n0nv3", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "homelab", "selftext": "I am looking for some advice / Critique for my backup strategy\n\n**My Devices are as follows:**\n\nLaptops/Phones (Windows / Android)Synology DS920 (X4 14TB in SHR2) Always powered onhp microserver n54l (X4 6TB in RAIDZ1 on TrueNAS Core) Switched on once a week over night on Sunday\n\n**I have 3 Main datasets:**\n\nPersonal (Photos, Home Folders...) 400GWork (Documents..) 10GISO (You know...) 10+TB\n\n&amp;#x200B;\n\n**My Current Backup data flow:**\n\n1. Laptops/Phones (Documents, Camera...) -&gt; syncthing to users home folder (Personal Dataset) on Synology\n2. HP &lt;- Pull sync all 3 datasets (Not all ISOs but enough for the family) from Synology\n3. Once a month I create a Mirror Pool on HP (TrueNAS) using 1TB Drives and Replicate Personal &amp; Work datasets to it. 1 Drive is kept in my garden shed and the other at my parents 1 hour away. Old drives are kept in shed once swapped for latest (I rotate 6 1TB Drives)\n\n**Snapshots**\n\nSynologyPersonal - Daily Kept for 2 Weeks / Weekly for 4 Weeks/ Monthly for 3 MonthsWork - Hourly for Day (Then the same as Personal)\n\nHP (TrueNAS)Personal - Weekly for 12 WeeksWork - Weekly for 12 Weeks\n\n&amp;#x200B;\n\nWhat do you think?  What's your backup strategy? How would you do using my hardware?  Is the Split Mirror pool a good idea or am I setting myself up for data loss doing it this way?\n\n&amp;#x200B;\n\nThank you &amp;Have a Great Evening", "author_fullname": "t2_4p8n0nv3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Give me you view on my backup strategy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/homelab", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10k9r9f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674579985.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674578625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.homelab", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for some advice / Critique for my backup strategy&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Devices are as follows:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Laptops/Phones (Windows / Android)Synology DS920 (X4 14TB in SHR2) Always powered onhp microserver n54l (X4 6TB in RAIDZ1 on TrueNAS Core) Switched on once a week over night on Sunday&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I have 3 Main datasets:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Personal (Photos, Home Folders...) 400GWork (Documents..) 10GISO (You know...) 10+TB&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Current Backup data flow:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Laptops/Phones (Documents, Camera...) -&amp;gt; syncthing to users home folder (Personal Dataset) on Synology&lt;/li&gt;\n&lt;li&gt;HP &amp;lt;- Pull sync all 3 datasets (Not all ISOs but enough for the family) from Synology&lt;/li&gt;\n&lt;li&gt;Once a month I create a Mirror Pool on HP (TrueNAS) using 1TB Drives and Replicate Personal &amp;amp; Work datasets to it. 1 Drive is kept in my garden shed and the other at my parents 1 hour away. Old drives are kept in shed once swapped for latest (I rotate 6 1TB Drives)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Snapshots&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;SynologyPersonal - Daily Kept for 2 Weeks / Weekly for 4 Weeks/ Monthly for 3 MonthsWork - Hourly for Day (Then the same as Personal)&lt;/p&gt;\n\n&lt;p&gt;HP (TrueNAS)Personal - Weekly for 12 WeeksWork - Weekly for 12 Weeks&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What do you think?  What&amp;#39;s your backup strategy? How would you do using my hardware?  Is the Split Mirror pool a good idea or am I setting myself up for data loss doing it this way?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you &amp;amp;Have a Great Evening&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7793475a-322a-11e6-a5cc-0e74ee60e56b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2ubz7", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#6b238e", "id": "10k9r9f", "is_robot_indexable": true, "report_reasons": null, "author": "IndependentDepth1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/homelab/comments/10k9r9f/give_me_you_view_on_my_backup_strategy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/homelab/comments/10k9r9f/give_me_you_view_on_my_backup_strategy/", "subreddit_subscribers": 547029, "created_utc": 1674578625.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1674579504.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.homelab", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/homelab/comments/10k9r9f/give_me_you_view_on_my_backup_strategy/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ka3fg", "is_robot_indexable": true, "report_reasons": null, "author": "IndependentDepth1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_10k9r9f", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10ka3fg/give_me_you_view_on_my_backup_strategy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/homelab/comments/10k9r9f/give_me_you_view_on_my_backup_strategy/", "subreddit_subscribers": 667071, "created_utc": 1674579504.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So recently one of my drives, a 5tb Seagate one, stopped working.  As in it won\u2019t show up on my Mac as mounted, I even open up Disk Utility which is usually helpful but it won\u2019t even show up there, that\u2019s the part that troubles me. I have tried just about everything that I\u2019ve looked up on a Google search down to using a different cord from a working drive, and still nothing. The only other thing I\u2019ve tried mounting it to was my tv and doesn\u2019t show up there as well.\n\nIs this drive pretty much done for or is there something I\u2019m missing?", "author_fullname": "t2_chp3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Drive not working. Is it a lost cause?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10k9l9j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674578190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So recently one of my drives, a 5tb Seagate one, stopped working.  As in it won\u2019t show up on my Mac as mounted, I even open up Disk Utility which is usually helpful but it won\u2019t even show up there, that\u2019s the part that troubles me. I have tried just about everything that I\u2019ve looked up on a Google search down to using a different cord from a working drive, and still nothing. The only other thing I\u2019ve tried mounting it to was my tv and doesn\u2019t show up there as well.&lt;/p&gt;\n\n&lt;p&gt;Is this drive pretty much done for or is there something I\u2019m missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k9l9j", "is_robot_indexable": true, "report_reasons": null, "author": "JaketheSnake54", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k9l9j/seagate_drive_not_working_is_it_a_lost_cause/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k9l9j/seagate_drive_not_working_is_it_a_lost_cause/", "subreddit_subscribers": 667071, "created_utc": 1674578190.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So im trying to locally archive a website that focuses on archiving everything on the Ultra64 (codename for nintendo 64 before release) I have all offline functionality complete except one part, the Gallery\n\nThe website is divided by Home, Blog, Resources, Gallery and Getting Started and im having trouble with the gallery. The gallery has a collection of images on the Developement Kits and Parts and is subdivided by the name of the company that made the dev kit with a drop down menu when you hover the mouse over gallery (I can navigate to all the pages listed on the dropdown except 1). \n\nNow my problem is wget grabbed all the files and images in the Gallery dropdown menu except 1, Looking at the command prompt (Using Windows wget) I found that one page gave out a 404 error and was skipped so only 6 of the 7 companies have working offline links to their respective pages on the website im trying to archive\n\nI already tried pasting the actual link for the page im missing in wget and I still get error 404 which I dont get why considering that \n\n1. I can navigate the page when browsing online so its obviously not down and to verify this when i reload the page in my browser with dev tools enabled I get status 200 not 404.\n\n2. Doubt its wordpress and js as its used through the whole site which I successfully archived and all images are uploaded to the same folder according to the dev tools on the actual website I also checked all the APIs being used between the page that gave 404 and one that didnt and all the names and versions matched up\n\nMy theory is that the page uses a non standard character in the url but i dont see why that would be an issue but who am i to say since i dont know how wget works it, the character is \u00b5 and ive tried both with the actual character which is the decoded and the orignal which has this  \"%c2%b5\" substituted for \u00b5\n\nive been using wget -m -p -k --follow-ftp --random-wait -w 7 --tries=4 -e robots=off -U Mozilla/5.0 url which worked with absolutely everything on the website except this specific page and have tried putting the url with quotes as well\n\nif anyone wants to see the page im having difficulty with, the domain is the codename used for the nintendo 64 with ca as the ccTop Level Domain and its the kyoto microcomputer page under the gallery, all other links under the gallery worked fine for wget (Not actually linking the site in my post in case its impolite or against rules especially since its a hobby site)\n\nAlso if anyone knows of a better subreddit that would be able to help me out pls link it", "author_fullname": "t2_ejb61oh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting 404 on a specific page on website when using wget", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10k8mmg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674575719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So im trying to locally archive a website that focuses on archiving everything on the Ultra64 (codename for nintendo 64 before release) I have all offline functionality complete except one part, the Gallery&lt;/p&gt;\n\n&lt;p&gt;The website is divided by Home, Blog, Resources, Gallery and Getting Started and im having trouble with the gallery. The gallery has a collection of images on the Developement Kits and Parts and is subdivided by the name of the company that made the dev kit with a drop down menu when you hover the mouse over gallery (I can navigate to all the pages listed on the dropdown except 1). &lt;/p&gt;\n\n&lt;p&gt;Now my problem is wget grabbed all the files and images in the Gallery dropdown menu except 1, Looking at the command prompt (Using Windows wget) I found that one page gave out a 404 error and was skipped so only 6 of the 7 companies have working offline links to their respective pages on the website im trying to archive&lt;/p&gt;\n\n&lt;p&gt;I already tried pasting the actual link for the page im missing in wget and I still get error 404 which I dont get why considering that &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I can navigate the page when browsing online so its obviously not down and to verify this when i reload the page in my browser with dev tools enabled I get status 200 not 404.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Doubt its wordpress and js as its used through the whole site which I successfully archived and all images are uploaded to the same folder according to the dev tools on the actual website I also checked all the APIs being used between the page that gave 404 and one that didnt and all the names and versions matched up&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My theory is that the page uses a non standard character in the url but i dont see why that would be an issue but who am i to say since i dont know how wget works it, the character is \u00b5 and ive tried both with the actual character which is the decoded and the orignal which has this  &amp;quot;%c2%b5&amp;quot; substituted for \u00b5&lt;/p&gt;\n\n&lt;p&gt;ive been using wget -m -p -k --follow-ftp --random-wait -w 7 --tries=4 -e robots=off -U Mozilla/5.0 url which worked with absolutely everything on the website except this specific page and have tried putting the url with quotes as well&lt;/p&gt;\n\n&lt;p&gt;if anyone wants to see the page im having difficulty with, the domain is the codename used for the nintendo 64 with ca as the ccTop Level Domain and its the kyoto microcomputer page under the gallery, all other links under the gallery worked fine for wget (Not actually linking the site in my post in case its impolite or against rules especially since its a hobby site)&lt;/p&gt;\n\n&lt;p&gt;Also if anyone knows of a better subreddit that would be able to help me out pls link it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k8mmg", "is_robot_indexable": true, "report_reasons": null, "author": "Bark_bark-im-a-doggo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k8mmg/getting_404_on_a_specific_page_on_website_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k8mmg/getting_404_on_a_specific_page_on_website_when/", "subreddit_subscribers": 667071, "created_utc": 1674575719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Man, this sub is a veritable cornucopia of knowledge.  Thanks to all of you for your help in my past couple of threads here.  Sorry for such a long post.\n\nI'm moving my business and expanding our operations.  I've decided it's time to upgrade all of my existing workflows and part of that is network infrastructure.  Larger, faster Synology unit, faster NIC's, faster switches, etc.\n\nWe do a ton of drive imaging using Macrium Reflect.  We used to just drag/copy clients' home folders to a backup folder on the NAS but there was an incident once(luckily we were able to resolve) where the copy didn't complete and the file system became corrupt in the process - scary.  So, despite the extra time it takes, when we are having anything to do with important data, EVERY OS PARTITION gets imaged first.  The way we've been doing this is every tech has their own workstation and dock which they do the imaging from.  I don't love the idea of doing other things on the machine that's imaging so I've decided I'd like to build a machine which is dedicated to imaging and can perform more than one image task at a time across multiple docks.  Does that sound good or bad?\n\nI don't know if I can run multiple instances of the same software be it Macrium or whatever, so I thought I would just run Proxmox and a bunch of VM's to do it?  I have a tendency to wildly overcomplicate things, so please...  your thoughts?", "author_fullname": "t2_6eiisul2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice on a data collection machine(basically a mass drive imaging rig)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k7q88", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674573306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Man, this sub is a veritable cornucopia of knowledge.  Thanks to all of you for your help in my past couple of threads here.  Sorry for such a long post.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m moving my business and expanding our operations.  I&amp;#39;ve decided it&amp;#39;s time to upgrade all of my existing workflows and part of that is network infrastructure.  Larger, faster Synology unit, faster NIC&amp;#39;s, faster switches, etc.&lt;/p&gt;\n\n&lt;p&gt;We do a ton of drive imaging using Macrium Reflect.  We used to just drag/copy clients&amp;#39; home folders to a backup folder on the NAS but there was an incident once(luckily we were able to resolve) where the copy didn&amp;#39;t complete and the file system became corrupt in the process - scary.  So, despite the extra time it takes, when we are having anything to do with important data, EVERY OS PARTITION gets imaged first.  The way we&amp;#39;ve been doing this is every tech has their own workstation and dock which they do the imaging from.  I don&amp;#39;t love the idea of doing other things on the machine that&amp;#39;s imaging so I&amp;#39;ve decided I&amp;#39;d like to build a machine which is dedicated to imaging and can perform more than one image task at a time across multiple docks.  Does that sound good or bad?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know if I can run multiple instances of the same software be it Macrium or whatever, so I thought I would just run Proxmox and a bunch of VM&amp;#39;s to do it?  I have a tendency to wildly overcomplicate things, so please...  your thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k7q88", "is_robot_indexable": true, "report_reasons": null, "author": "Pleaseclap4", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k7q88/looking_for_advice_on_a_data_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k7q88/looking_for_advice_on_a_data_collection/", "subreddit_subscribers": 667071, "created_utc": 1674573306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys, first post here looking for some help!I need to copy files to a disk, but I need the program to have \"resume\" function like downloaders like Jdownloader2 for example.   \nI want to copy files to an external USB drive but for some reason the disk sometimes fails when I'm copying big files to it.  \nI tested the disk with several disk checking tools and it shows no error at all. Even did surface test with no problem.  \nDo you guys know any copy tool for windows that has something like \"resume\" function but also needs to work if the USB external disk disconnects for any reason?  \nAlready tried with Teracopy and FileCopy but they don't have such function.  \n\n\nThanks!", "author_fullname": "t2_62608i1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copy file by parts - Windows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k7l1f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674572916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, first post here looking for some help!I need to copy files to a disk, but I need the program to have &amp;quot;resume&amp;quot; function like downloaders like Jdownloader2 for example.&lt;br/&gt;\nI want to copy files to an external USB drive but for some reason the disk sometimes fails when I&amp;#39;m copying big files to it.&lt;br/&gt;\nI tested the disk with several disk checking tools and it shows no error at all. Even did surface test with no problem.&lt;br/&gt;\nDo you guys know any copy tool for windows that has something like &amp;quot;resume&amp;quot; function but also needs to work if the USB external disk disconnects for any reason?&lt;br/&gt;\nAlready tried with Teracopy and FileCopy but they don&amp;#39;t have such function.  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k7l1f", "is_robot_indexable": true, "report_reasons": null, "author": "hikarusniper", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k7l1f/copy_file_by_parts_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k7l1f/copy_file_by_parts_windows/", "subreddit_subscribers": 667071, "created_utc": 1674572916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have some data drives and all are 8tb. I' got a deal on a 14tb drive and so it will become the parity drive. I was wondering if I could run the 14tb with a 2tb as parity and have the ability to recover two drives. Obviously if the 14tb goes I have nothing.\n\nI did multi drive parity when I first started with 4x2tb to cover 1x8tb failure. This is similar but a bit different.", "author_fullname": "t2_f8az5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does this is with snapraid make sense.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k63nk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674568761.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some data drives and all are 8tb. I&amp;#39; got a deal on a 14tb drive and so it will become the parity drive. I was wondering if I could run the 14tb with a 2tb as parity and have the ability to recover two drives. Obviously if the 14tb goes I have nothing.&lt;/p&gt;\n\n&lt;p&gt;I did multi drive parity when I first started with 4x2tb to cover 1x8tb failure. This is similar but a bit different.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k63nk", "is_robot_indexable": true, "report_reasons": null, "author": "light5out", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k63nk/does_this_is_with_snapraid_make_sense/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k63nk/does_this_is_with_snapraid_make_sense/", "subreddit_subscribers": 667071, "created_utc": 1674568761.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I need to backup my OS and also a storage drive but I'm running into issues with robocopy \n\nI've tried using \"Robocopy Sorce Destination /S /Copyall /xn /xo /xc \" But this doesn't update files if they have been changed, so now I started using \"Robocopy P: G: /MIR /b /copyall\" which runs into some system files and gets stuck in 30s loops \n\nAnd when I try backup normal folders on my OS, like videos, it doesn't seem to get everything, video folder is missing 8gb and 3 files, not sure where \n\nwhat do you guys use?", "author_fullname": "t2_lgib4i1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using robocopy to create rolling backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k5xbj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674568241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I need to backup my OS and also a storage drive but I&amp;#39;m running into issues with robocopy &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried using &amp;quot;Robocopy Sorce Destination /S /Copyall /xn /xo /xc &amp;quot; But this doesn&amp;#39;t update files if they have been changed, so now I started using &amp;quot;Robocopy P: G: /MIR /b /copyall&amp;quot; which runs into some system files and gets stuck in 30s loops &lt;/p&gt;\n\n&lt;p&gt;And when I try backup normal folders on my OS, like videos, it doesn&amp;#39;t seem to get everything, video folder is missing 8gb and 3 files, not sure where &lt;/p&gt;\n\n&lt;p&gt;what do you guys use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k5xbj", "is_robot_indexable": true, "report_reasons": null, "author": "Mockbubbles2628", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k5xbj/using_robocopy_to_create_rolling_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k5xbj/using_robocopy_to_create_rolling_backups/", "subreddit_subscribers": 667071, "created_utc": 1674568241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I use crystaldisk but surely that\u2019s not enough right?", "author_fullname": "t2_umytgvvj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to check for new HDD health?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k2jrq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674556413.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use crystaldisk but surely that\u2019s not enough right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "17TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k2jrq", "is_robot_indexable": true, "report_reasons": null, "author": "ElonTastical", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10k2jrq/how_to_check_for_new_hdd_health/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k2jrq/how_to_check_for_new_hdd_health/", "subreddit_subscribers": 667071, "created_utc": 1674556413.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Quick query regarding whether this scenario is possible :\n\nI have 4 3tb discs in the EX4, running in RAID 5 currently.\n\nI need to switch the drives at some point to the DS420+ and thought to ask if i could do following :\n\n* Pull one drive from EX4 and format\n* Insert to DS420+ and setup as SHR (Synology Hybrid Raid) in readiness for other 3 drives\n* Copy data from EX4 to DS420+ (will fit)\n* Remove other 3 drives, format and insert into DS420+ SHR unit\n\nI'm currently backing up to a USB3 HDD through PC but it's taking a while - jus thinking out loud as to whether this would be a suitable solution also.\n\nThanks in advance.", "author_fullname": "t2_9gciy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching from WD EX4 to Synology DS420+", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k1mwd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674552329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Quick query regarding whether this scenario is possible :&lt;/p&gt;\n\n&lt;p&gt;I have 4 3tb discs in the EX4, running in RAID 5 currently.&lt;/p&gt;\n\n&lt;p&gt;I need to switch the drives at some point to the DS420+ and thought to ask if i could do following :&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pull one drive from EX4 and format&lt;/li&gt;\n&lt;li&gt;Insert to DS420+ and setup as SHR (Synology Hybrid Raid) in readiness for other 3 drives&lt;/li&gt;\n&lt;li&gt;Copy data from EX4 to DS420+ (will fit)&lt;/li&gt;\n&lt;li&gt;Remove other 3 drives, format and insert into DS420+ SHR unit&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m currently backing up to a USB3 HDD through PC but it&amp;#39;s taking a while - jus thinking out loud as to whether this would be a suitable solution also.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k1mwd", "is_robot_indexable": true, "report_reasons": null, "author": "manguish", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k1mwd/switching_from_wd_ex4_to_synology_ds420/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k1mwd/switching_from_wd_ex4_to_synology_ds420/", "subreddit_subscribers": 667071, "created_utc": 1674552329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I bought a Synology DS-720+ with two 6TB HDD, but I'm running low on storage. So I would like to increase its capacity. I heard about the Synology DX517 5-bay expansion unit, but it's way too expensive. Is there a cheaper way to increase my storage capacity ?\n\nThanks", "author_fullname": "t2_3r6m4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to increase the capacity of a Synology DS-720+ 2-bay NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k1f7q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674551395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I bought a Synology DS-720+ with two 6TB HDD, but I&amp;#39;m running low on storage. So I would like to increase its capacity. I heard about the Synology DX517 5-bay expansion unit, but it&amp;#39;s way too expensive. Is there a cheaper way to increase my storage capacity ?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k1f7q", "is_robot_indexable": true, "report_reasons": null, "author": "Harkonnen", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k1f7q/what_is_the_best_way_to_increase_the_capacity_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k1f7q/what_is_the_best_way_to_increase_the_capacity_of/", "subreddit_subscribers": 667071, "created_utc": 1674551395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I hope someone can help, I have hundreds of movie files in their own folders, but I need to move them all into a different folder without taking the folders, \n\nThere are too many to do this manually, does anyone know how I can move all the media to a different location?\n\nThanks in advance! ( I can use windows or Synology to manipulate the data)", "author_fullname": "t2_3e06v9vo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there anyone intelligent who can help me with this problem I have please?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10k0uv6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674548851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I hope someone can help, I have hundreds of movie files in their own folders, but I need to move them all into a different folder without taking the folders, &lt;/p&gt;\n\n&lt;p&gt;There are too many to do this manually, does anyone know how I can move all the media to a different location?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance! ( I can use windows or Synology to manipulate the data)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10k0uv6", "is_robot_indexable": true, "report_reasons": null, "author": "Ash3000k", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10k0uv6/is_there_anyone_intelligent_who_can_help_me_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10k0uv6/is_there_anyone_intelligent_who_can_help_me_with/", "subreddit_subscribers": 667071, "created_utc": 1674548851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It seems that if you grab videos from youtube and pornhub just after they've been released (im not talking of immediately released when only 360p may be available, but once 1080p is available), you get one file. but if you try and download the same files (i use jdownloader 2) after some time (days?), you get another file with a slightly different file size.\n\nJust to clarify i'm not talking of grabbing lives that have just finished streaming, and I'm not talking of jd grabbing sometimes both 'regular video' and '\\_hls video' that usually end up having very similar file sizes and quality.\n\nThe file size difference is small, however it's a shame because it messes up the easiest duplicate detection by file size (and file name may also end up being slightly different depending on your settings, and it's possible that sometimes even the autograbbed 'date' is different), and can also mess up jd 'already downloaded' detection.\n\nAnyone else encounter this and know why?", "author_fullname": "t2_111o3ncd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Youtube and PH seem to sometimes replace video files with very similar ones?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jxf4b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674535599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems that if you grab videos from youtube and pornhub just after they&amp;#39;ve been released (im not talking of immediately released when only 360p may be available, but once 1080p is available), you get one file. but if you try and download the same files (i use jdownloader 2) after some time (days?), you get another file with a slightly different file size.&lt;/p&gt;\n\n&lt;p&gt;Just to clarify i&amp;#39;m not talking of grabbing lives that have just finished streaming, and I&amp;#39;m not talking of jd grabbing sometimes both &amp;#39;regular video&amp;#39; and &amp;#39;_hls video&amp;#39; that usually end up having very similar file sizes and quality.&lt;/p&gt;\n\n&lt;p&gt;The file size difference is small, however it&amp;#39;s a shame because it messes up the easiest duplicate detection by file size (and file name may also end up being slightly different depending on your settings, and it&amp;#39;s possible that sometimes even the autograbbed &amp;#39;date&amp;#39; is different), and can also mess up jd &amp;#39;already downloaded&amp;#39; detection.&lt;/p&gt;\n\n&lt;p&gt;Anyone else encounter this and know why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jxf4b", "is_robot_indexable": true, "report_reasons": null, "author": "BitsAndBobs304", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jxf4b/youtube_and_ph_seem_to_sometimes_replace_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jxf4b/youtube_and_ph_seem_to_sometimes_replace_video/", "subreddit_subscribers": 667071, "created_utc": 1674535599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm brand new to all of this (including this sub), but I'm willing to dive head first, and spend as long as it takes to figure this out. I just need a point in the right direction.\n\nI would like to consolidate all of the pictures my family has of my son on a personal storage device everyone in the family can access from anywhere. What is the best/cheapest option that includes security and ease of access to the family? I have some 750G solid state hard drives I'd like to use, but can also purchase whatever I'd need to (within reasonable limits). I would prefer to learn how to do this all in-house as opposed to getting a subscription and possibly having them lose/lock my data (even though I'd backup everything frequently). Any push in the right direction is greatly appreciated.\n\nI also have some 500G Mac hard drives, but if it's too complicated to incorporate those I won't use them.\n\nThanks for any help.", "author_fullname": "t2_cpn19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal Cloud Storage for Pictures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jsvxs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674521712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m brand new to all of this (including this sub), but I&amp;#39;m willing to dive head first, and spend as long as it takes to figure this out. I just need a point in the right direction.&lt;/p&gt;\n\n&lt;p&gt;I would like to consolidate all of the pictures my family has of my son on a personal storage device everyone in the family can access from anywhere. What is the best/cheapest option that includes security and ease of access to the family? I have some 750G solid state hard drives I&amp;#39;d like to use, but can also purchase whatever I&amp;#39;d need to (within reasonable limits). I would prefer to learn how to do this all in-house as opposed to getting a subscription and possibly having them lose/lock my data (even though I&amp;#39;d backup everything frequently). Any push in the right direction is greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;I also have some 500G Mac hard drives, but if it&amp;#39;s too complicated to incorporate those I won&amp;#39;t use them.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jsvxs", "is_robot_indexable": true, "report_reasons": null, "author": "hey-zues", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jsvxs/personal_cloud_storage_for_pictures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jsvxs/personal_cloud_storage_for_pictures/", "subreddit_subscribers": 667071, "created_utc": 1674521712.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone!\n\nSo my goal is building a NAS, ideally I would have waited a bit for some 12/13th Intel motherboards supporting ECC, for that extra safety, but as it turns out, they're not out there yet.\n\nSince my NAS is dying I need to make one asap, and I will ditch ECC and go non ECC. I'm aware this causes some risks, since the data is stored on the memory first and the disk later (risking corruption if not using ECC).\n\nMy question is, this is more of a \"let's make sure\" kind of thing, if I were to use rsync and enable checksum verifying, would I be safe and be able to detect potential issues from data corruption due to the non ECC memory?\n\nIs it ok for me to calculate the checksum while the data is still potentially cached onto memory? I guess what I'm getting at is: can data corruption in non ECC memory occur only during writing, or at any time the data is cached?\n\nIs the answer is that the corruption can happen during any time, it means that I would have to wait for the data to be fully written onto the disk and not be stored onto memory as cache, in such case, how can I ensure I'm reading and calculating a checksum of the data actually stored in the disk, and would it be a safe enough approach to mitigate the risks of data corruption with non ECC memory?\n\nOnce again, I would like to go for a modern CPU, ideally the 13500 which supports ECC, but I have found no 12th or 13th gen with ECC support, I saw an Intel chipset that should be released supporting, but I have no idea when and my NAS is on the brink of death (doing another backup right now for safety), so I can't wait any longer unfortunately.\n\n**Secondary unrelated question:**\n\nIn case in the future I wanted to swap motherboard in order to go with ECC memory, could I rebuild the same ZFS poll and avoid data loss? Realistically, I won't have enough space to backup all the data (I could of course consider temporarily purchasing some cloud storage, which is probably pretty expensive)?\n\nEssentially, can I reconstruct the pools on another motherboard using the same disks?\n\nThank's in advance, I'm sorry if those are noobs questions, this is my first NAS from scratch pretty much.", "author_fullname": "t2_7vil65ud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is stuff like rsync hash check a good idea with lack of ECC RAM?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jrdjz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674517521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;So my goal is building a NAS, ideally I would have waited a bit for some 12/13th Intel motherboards supporting ECC, for that extra safety, but as it turns out, they&amp;#39;re not out there yet.&lt;/p&gt;\n\n&lt;p&gt;Since my NAS is dying I need to make one asap, and I will ditch ECC and go non ECC. I&amp;#39;m aware this causes some risks, since the data is stored on the memory first and the disk later (risking corruption if not using ECC).&lt;/p&gt;\n\n&lt;p&gt;My question is, this is more of a &amp;quot;let&amp;#39;s make sure&amp;quot; kind of thing, if I were to use rsync and enable checksum verifying, would I be safe and be able to detect potential issues from data corruption due to the non ECC memory?&lt;/p&gt;\n\n&lt;p&gt;Is it ok for me to calculate the checksum while the data is still potentially cached onto memory? I guess what I&amp;#39;m getting at is: can data corruption in non ECC memory occur only during writing, or at any time the data is cached?&lt;/p&gt;\n\n&lt;p&gt;Is the answer is that the corruption can happen during any time, it means that I would have to wait for the data to be fully written onto the disk and not be stored onto memory as cache, in such case, how can I ensure I&amp;#39;m reading and calculating a checksum of the data actually stored in the disk, and would it be a safe enough approach to mitigate the risks of data corruption with non ECC memory?&lt;/p&gt;\n\n&lt;p&gt;Once again, I would like to go for a modern CPU, ideally the 13500 which supports ECC, but I have found no 12th or 13th gen with ECC support, I saw an Intel chipset that should be released supporting, but I have no idea when and my NAS is on the brink of death (doing another backup right now for safety), so I can&amp;#39;t wait any longer unfortunately.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Secondary unrelated question:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In case in the future I wanted to swap motherboard in order to go with ECC memory, could I rebuild the same ZFS poll and avoid data loss? Realistically, I won&amp;#39;t have enough space to backup all the data (I could of course consider temporarily purchasing some cloud storage, which is probably pretty expensive)?&lt;/p&gt;\n\n&lt;p&gt;Essentially, can I reconstruct the pools on another motherboard using the same disks?&lt;/p&gt;\n\n&lt;p&gt;Thank&amp;#39;s in advance, I&amp;#39;m sorry if those are noobs questions, this is my first NAS from scratch pretty much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jrdjz", "is_robot_indexable": true, "report_reasons": null, "author": "LynxesExe", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jrdjz/is_stuff_like_rsync_hash_check_a_good_idea_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jrdjz/is_stuff_like_rsync_hash_check_a_good_idea_with/", "subreddit_subscribers": 667071, "created_utc": 1674517521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have quite a few books, files, etc in which I need to host somewhere anonymously or just in general to allow anyone in my group to access them. It's a bunch of stuff. Only issue with Anonfiles is as you know, they expire. Could one lead me to a website that I can just save, any sized files I'd desire to share with others without pay, anonymously, and permanently? That's all I ask.", "author_fullname": "t2_9qz1zwov", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do any of you know of an Anonfiles alternative?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10jq4l5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674514387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have quite a few books, files, etc in which I need to host somewhere anonymously or just in general to allow anyone in my group to access them. It&amp;#39;s a bunch of stuff. Only issue with Anonfiles is as you know, they expire. Could one lead me to a website that I can just save, any sized files I&amp;#39;d desire to share with others without pay, anonymously, and permanently? That&amp;#39;s all I ask.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10jq4l5", "is_robot_indexable": true, "report_reasons": null, "author": "RisingFire2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10jq4l5/do_any_of_you_know_of_an_anonfiles_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10jq4l5/do_any_of_you_know_of_an_anonfiles_alternative/", "subreddit_subscribers": 667071, "created_utc": 1674514387.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}