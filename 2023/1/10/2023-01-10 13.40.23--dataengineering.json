{"kind": "Listing", "data": {"after": "t3_107jf2o", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have tried to work with this platform. I have given it every opportunity I can, but never have I seen so much trouble from a tool or framework I had to work with. I started the new year with a new years resolution: to write down every problem as I encounter them. I don't know if I should continue with it, since I might fill up my e-reader before the year is over.\n\nIn no particular order:\n\n1. Synapse expressions have no support for the case() function. Have fun chaining if else's!\n2. Not all activities have retries. What do you mean you want to put a retry on a pipeline? That's crazy talk!\n3. SQL scripts are saved as JSON. Not even JSON5. Fucking JSON. I hope you like diff checking one fucking long ass line of SQL!\n4. Browser IDE... Whoever thought this up deserves fish hooks up their ass. You can't even save properly in the stupid thing, because every single thing is counted as a commit. And before some asshat suggests committing after the work is done: you deserve the fish hook too.\n5. Pipeline variables do not include int's (and some other types for that matter) for whatever reason. Converting everything to string and back is fun isn't it?\n6. Pipelines don't have output parameters, so even if I wanted to make reusable modules for missing Synapse functionality I literally can't unless I start using them as error parameters (and if you try to suggest that I WILL shank you).\n7. No global parameters EVEN THOUGH DATA FACTORY HAD IT AND IT'S BEEN REQUESTED FOR ALMOST 2 YEARS.\n8. 2022 and still no dark mode. This has been requested for data factory in 2016. Yes you're old and so am I.\n9. The SQL editor has all the great functionalities notepad has.\n10. I hope you didn't name one of your workspaces incorrectly. Oh you did? RIP, time to remake it.\n11. Local timezones? You mean UTC? What do you mean + or -? You craycray!\n12. Nesting loops or if tests can't be done. You need to make separate pipelines for it and before someone asks: no, just because it's a nesting it REALLY doesn't mean it belongs in a separate pipeline.\n13. Speaking about nesting: if you use an if test in a foreach, you can't access the current item of the loop. Haven't you learned by now? You're using Synapse. Now eat shit.\n14. Self-Hosted Integration Runtimes cannot be shared, while you can actually do that in data factory. This means you need to run three separate runtimes just to get to on-premise sources.\n15. On the topic of self-hosted integration runtimes: why do you even need them at all? Why is it not possible to peer a vnet that contains synapse and be done with it?\n16. What's even the use of the SQL scripts if you can't access them from pipelines. You get the script activity, but that's just another fucking place to dump your SQL in. And if you dare to suggest to use the API to trigger/request the SQL script. I will adopt a dog just to feed you to it.\n17. Testing? HAHAHAHAHAHAHAHAHAHA- fuck you.\n18. You want to rename a variable? Lol you little shit now you have to search for every instance to rename it. Refactoring? Just become a 100x dev yo.\n19. Parameters in pipeline templates? Why would you want parameters in your pipeline templates? You talk like you want to reuse the code you wrote or something. Oh you do? ........oof.\n20. Git....lab? Sounds like a dangerous cocaine facility. Hope you weren't planning to attach that to synapse! Or like literally any other option since you have to attach git to synapse in the first place.\n\nThese are the ones from THIS year, so after one week. I have had the displeasure of using it for a year now. So here's my advice after this long-winded rant: use it for a quick prototype of anything and don't use it for anything bigger than that. If you do, you have learned nothing of all the improvements that people have brought to the art that is development and you should probably touch grass instead of sucking cock on LinkedIn.", "author_fullname": "t2_2pbhwnrm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Synapse Analytics is absolute trash for anything bigger than a few extractions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107rt91", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 85, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 85, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673302334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tried to work with this platform. I have given it every opportunity I can, but never have I seen so much trouble from a tool or framework I had to work with. I started the new year with a new years resolution: to write down every problem as I encounter them. I don&amp;#39;t know if I should continue with it, since I might fill up my e-reader before the year is over.&lt;/p&gt;\n\n&lt;p&gt;In no particular order:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Synapse expressions have no support for the case() function. Have fun chaining if else&amp;#39;s!&lt;/li&gt;\n&lt;li&gt;Not all activities have retries. What do you mean you want to put a retry on a pipeline? That&amp;#39;s crazy talk!&lt;/li&gt;\n&lt;li&gt;SQL scripts are saved as JSON. Not even JSON5. Fucking JSON. I hope you like diff checking one fucking long ass line of SQL!&lt;/li&gt;\n&lt;li&gt;Browser IDE... Whoever thought this up deserves fish hooks up their ass. You can&amp;#39;t even save properly in the stupid thing, because every single thing is counted as a commit. And before some asshat suggests committing after the work is done: you deserve the fish hook too.&lt;/li&gt;\n&lt;li&gt;Pipeline variables do not include int&amp;#39;s (and some other types for that matter) for whatever reason. Converting everything to string and back is fun isn&amp;#39;t it?&lt;/li&gt;\n&lt;li&gt;Pipelines don&amp;#39;t have output parameters, so even if I wanted to make reusable modules for missing Synapse functionality I literally can&amp;#39;t unless I start using them as error parameters (and if you try to suggest that I WILL shank you).&lt;/li&gt;\n&lt;li&gt;No global parameters EVEN THOUGH DATA FACTORY HAD IT AND IT&amp;#39;S BEEN REQUESTED FOR ALMOST 2 YEARS.&lt;/li&gt;\n&lt;li&gt;2022 and still no dark mode. This has been requested for data factory in 2016. Yes you&amp;#39;re old and so am I.&lt;/li&gt;\n&lt;li&gt;The SQL editor has all the great functionalities notepad has.&lt;/li&gt;\n&lt;li&gt;I hope you didn&amp;#39;t name one of your workspaces incorrectly. Oh you did? RIP, time to remake it.&lt;/li&gt;\n&lt;li&gt;Local timezones? You mean UTC? What do you mean + or -? You craycray!&lt;/li&gt;\n&lt;li&gt;Nesting loops or if tests can&amp;#39;t be done. You need to make separate pipelines for it and before someone asks: no, just because it&amp;#39;s a nesting it REALLY doesn&amp;#39;t mean it belongs in a separate pipeline.&lt;/li&gt;\n&lt;li&gt;Speaking about nesting: if you use an if test in a foreach, you can&amp;#39;t access the current item of the loop. Haven&amp;#39;t you learned by now? You&amp;#39;re using Synapse. Now eat shit.&lt;/li&gt;\n&lt;li&gt;Self-Hosted Integration Runtimes cannot be shared, while you can actually do that in data factory. This means you need to run three separate runtimes just to get to on-premise sources.&lt;/li&gt;\n&lt;li&gt;On the topic of self-hosted integration runtimes: why do you even need them at all? Why is it not possible to peer a vnet that contains synapse and be done with it?&lt;/li&gt;\n&lt;li&gt;What&amp;#39;s even the use of the SQL scripts if you can&amp;#39;t access them from pipelines. You get the script activity, but that&amp;#39;s just another fucking place to dump your SQL in. And if you dare to suggest to use the API to trigger/request the SQL script. I will adopt a dog just to feed you to it.&lt;/li&gt;\n&lt;li&gt;Testing? HAHAHAHAHAHAHAHAHAHA- fuck you.&lt;/li&gt;\n&lt;li&gt;You want to rename a variable? Lol you little shit now you have to search for every instance to rename it. Refactoring? Just become a 100x dev yo.&lt;/li&gt;\n&lt;li&gt;Parameters in pipeline templates? Why would you want parameters in your pipeline templates? You talk like you want to reuse the code you wrote or something. Oh you do? ........oof.&lt;/li&gt;\n&lt;li&gt;Git....lab? Sounds like a dangerous cocaine facility. Hope you weren&amp;#39;t planning to attach that to synapse! Or like literally any other option since you have to attach git to synapse in the first place.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;These are the ones from THIS year, so after one week. I have had the displeasure of using it for a year now. So here&amp;#39;s my advice after this long-winded rant: use it for a quick prototype of anything and don&amp;#39;t use it for anything bigger than that. If you do, you have learned nothing of all the improvements that people have brought to the art that is development and you should probably touch grass instead of sucking cock on LinkedIn.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "107rt91", "is_robot_indexable": true, "report_reasons": null, "author": "AirisuB", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107rt91/azure_synapse_analytics_is_absolute_trash_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107rt91/azure_synapse_analytics_is_absolute_trash_for/", "subreddit_subscribers": 85846, "created_utc": 1673302334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been a career jumper since I was 18. I\u2019ve attempted to get into about almost every career possible out there. Finally I narrowed things down to tech. Even in tech, I\u2019ve been jumping around. I\u2019m down to 2 final choices that I\u2019m considering \u2014 DE or Sales.\n\nI\u2019m currently trying to learn DE, and it\u2019s really  overwhelming. As I\u2019m approaching the readings, etc, I keep thinking maybe I should just stop this and go into sales.. \ud83d\ude2c\ud83d\ude2c\n\nHow does an absolute beginner with a data analyst background get over the mental mountain, persevere, and make it through the end of landing a DE job?", "author_fullname": "t2_ht40qce8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting over the mental mountain when starting out learning DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107ipn6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673281792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been a career jumper since I was 18. I\u2019ve attempted to get into about almost every career possible out there. Finally I narrowed things down to tech. Even in tech, I\u2019ve been jumping around. I\u2019m down to 2 final choices that I\u2019m considering \u2014 DE or Sales.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently trying to learn DE, and it\u2019s really  overwhelming. As I\u2019m approaching the readings, etc, I keep thinking maybe I should just stop this and go into sales.. \ud83d\ude2c\ud83d\ude2c&lt;/p&gt;\n\n&lt;p&gt;How does an absolute beginner with a data analyst background get over the mental mountain, persevere, and make it through the end of landing a DE job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "107ipn6", "is_robot_indexable": true, "report_reasons": null, "author": "phoot_in_the_door", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107ipn6/getting_over_the_mental_mountain_when_starting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107ipn6/getting_over_the_mental_mountain_when_starting/", "subreddit_subscribers": 85846, "created_utc": 1673281792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!I've  been tasked to evaluate whether we need dbt in our org and I have a\u00a0  feeling that either we need another\u00a0 approach or I am missing something.  \n\n\n**tldr:  Is there an out of the box solution for running heaviliy parametrized  queries with caching on different levels between them?**  \n\n\nHigh level view:  \nWe  do spatial data analysis, we have an algorithm to calculate a bunch of  metrics which accept a lot of parameters (area of calculation, different  weights for aggregation, etc.) which we currently calculate ina a LOT  of jupyter notebooks (so a lot of copypaste for each new  experiment/client).  \nWe want to move the calculation of these metrics into some DWH to:  \n\\- Keep track of all of the calculations  \n\\- Easily use the same code on the data from different providers (so it's transformed to the right format somehow)  \n\\- The calculations are multilayered - so some intermediate data can and should be reused between requests  \n\\- You have to be able to access intermediate data for calculations  \n\\-If  an analyst modifies some parameters it should be easy for them to pass  the new code for further generations such that they can compare this  version to the new ones.  \n\n\nSolution:  \nWe  can't just calculate this algorithm on all of the data with all of the  parameters - there are terrabytes of source data and it seems very  inefficient.  \nSo the algorithm is written as a series of dbt models  with a LOT of parameters passed during \\`dbt run\\` as a json object  through command line arguments. We also have a \\`request\\` table storing  the json of the requests and gives them certain ids.  \nThen for each  ongoing model we generate an sql request with dbt and add a cache\\_id  fully determined by the parameters. When we run other requests the  cache\\_id is calculated to be the same if the parameters that this model  depends on are the same and data is reused.  \n\n\nIt would be great to hear your thoughts. Cheers!", "author_fullname": "t2_8n7ze0vt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is dbt the right tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107fi0c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673273887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!I&amp;#39;ve  been tasked to evaluate whether we need dbt in our org and I have a\u00a0  feeling that either we need another\u00a0 approach or I am missing something.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;tldr:  Is there an out of the box solution for running heaviliy parametrized  queries with caching on different levels between them?&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;High level view:&lt;br/&gt;\nWe  do spatial data analysis, we have an algorithm to calculate a bunch of  metrics which accept a lot of parameters (area of calculation, different  weights for aggregation, etc.) which we currently calculate ina a LOT  of jupyter notebooks (so a lot of copypaste for each new  experiment/client).&lt;br/&gt;\nWe want to move the calculation of these metrics into some DWH to:&lt;br/&gt;\n- Keep track of all of the calculations&lt;br/&gt;\n- Easily use the same code on the data from different providers (so it&amp;#39;s transformed to the right format somehow)&lt;br/&gt;\n- The calculations are multilayered - so some intermediate data can and should be reused between requests&lt;br/&gt;\n- You have to be able to access intermediate data for calculations&lt;br/&gt;\n-If  an analyst modifies some parameters it should be easy for them to pass  the new code for further generations such that they can compare this  version to the new ones.  &lt;/p&gt;\n\n&lt;p&gt;Solution:&lt;br/&gt;\nWe  can&amp;#39;t just calculate this algorithm on all of the data with all of the  parameters - there are terrabytes of source data and it seems very  inefficient.&lt;br/&gt;\nSo the algorithm is written as a series of dbt models  with a LOT of parameters passed during `dbt run` as a json object  through command line arguments. We also have a `request` table storing  the json of the requests and gives them certain ids.&lt;br/&gt;\nThen for each  ongoing model we generate an sql request with dbt and add a cache_id  fully determined by the parameters. When we run other requests the  cache_id is calculated to be the same if the parameters that this model  depends on are the same and data is reused.  &lt;/p&gt;\n\n&lt;p&gt;It would be great to hear your thoughts. Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "107fi0c", "is_robot_indexable": true, "report_reasons": null, "author": "Outside_Banana_2138", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107fi0c/is_dbt_the_right_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107fi0c/is_dbt_the_right_tool/", "subreddit_subscribers": 85846, "created_utc": 1673273887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What's even a good answer for this?\n\nEdit: all great answers. Had this in a interview a few months ago while I am only beginning DE, so was wondering what was actually good lol", "author_fullname": "t2_8i81zdtc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Question: How fast are your ETL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107g616", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673314174.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673275643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s even a good answer for this?&lt;/p&gt;\n\n&lt;p&gt;Edit: all great answers. Had this in a interview a few months ago while I am only beginning DE, so was wondering what was actually good lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "107g616", "is_robot_indexable": true, "report_reasons": null, "author": "Particular-Sock5250", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107g616/interview_question_how_fast_are_your_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107g616/interview_question_how_fast_are_your_etl/", "subreddit_subscribers": 85846, "created_utc": 1673275643.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. After seeing /u/infiniteAggression- [pipeline for scraping Crinacle's headphone review site](https://www.reddit.com/r/dataengineering/comments/xyxpku/built_and_automated_a_complete_endtoend_elt/), I was inspired to do something similar to bolster my resume and hopefully upskill myself enough to land a better data-adjacent job! This is going to be the first \"real\" DE project that I will be working on, as well as learning about a cloud provider (GCP) from scratch. If anyone has pointers or advice they can share with me, I would greatly appreciate it. I've been reading around this subreddit for a while now, and I'm very excited to build something I can share with the fine folks here!  \n\nFor my project, I'm planning on creating a metabase dashboard (ironically, I work for one of their competitors \ud83d\ude06) that displays metrics from trending shows for each week and what streaming platforms they're **currently** available on. I don't exactly trust the trending sections from any of the streaming providers, so that's why I want to make my own. Also, it would be super helpful to know what's the hottest shows on each platform and whether or it's enough to justify resubbing to them. That way I don't need to sift through SEO garbage articles on /r/television.  \n\nI plan to host the dashboard on a micro-e2 VM; that way anyone looking at my resume can just click a link and interact with the dashboard without needing to `git clone` my repo and do any local setup.  \n\n### Pipeline\n1. make a weekly API request to fetch the top N trending TV shows from [TMDb](https://developers.themoviedb.org/3/trending/get-trending)\n2. dump the weekly trending shows into a Google cloud storage bucket\n3. append trending data files into a bigquery table. This will be a weekly grained fact table.\n4. run a `SELECT DISTINCT show_id` on the trending show table to get a list of unique show IDs that I can pass to this [endpoint](https://developers.themoviedb.org/3/tv/get-tv-details) to get all data needed to populate dimension tables (show details, genre, available watch providers, etc...)\n5. dump dimension table data into the same bucket from above and create the dimension tables in BQ\n6. use dbt to create models for Metabase to query on\n\n### Scalability Issues\nInitially, I was thinking of following /u/infiniteAggression- 's  idea of deploying an Airflow server on the micro-e2 VM that Metabase will be hosted on. However, I think this VM is too \"small\" to actually host Metabase as well as Airflow.  \n\nThe number of unique show IDs from the fact table will grow O(n)--which means the number of API calls needed to upsert dimension tables will increase linearly. O(n) is the worst case, as the same show can show up this week and in any of the following weeks. Scalability probably won't be *too* terrible, but it's still something for me to keep in mind.\n### Workaround\nAfter more research, I think using Google's CRON scheduler+cloud functions to handle data ingestion would be a better idea. That way, resources are freed up for the VM to solely host Metabase.  \n\nTo keep the number of API calls from blowing up, my idea is to delete records from my fact table after some condition is met. My two ideas are:\n\n1. delete records after N months from date of ingestion\n2. if the number of unique show IDs are &gt; some threshold, then delete records after N months from date of ingestion", "author_fullname": "t2_jk5r3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline architecture advice for my first side project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107rpu4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673333503.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673302123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. After seeing &lt;a href=\"/u/infiniteAggression-\"&gt;/u/infiniteAggression-&lt;/a&gt; &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/xyxpku/built_and_automated_a_complete_endtoend_elt/\"&gt;pipeline for scraping Crinacle&amp;#39;s headphone review site&lt;/a&gt;, I was inspired to do something similar to bolster my resume and hopefully upskill myself enough to land a better data-adjacent job! This is going to be the first &amp;quot;real&amp;quot; DE project that I will be working on, as well as learning about a cloud provider (GCP) from scratch. If anyone has pointers or advice they can share with me, I would greatly appreciate it. I&amp;#39;ve been reading around this subreddit for a while now, and I&amp;#39;m very excited to build something I can share with the fine folks here!  &lt;/p&gt;\n\n&lt;p&gt;For my project, I&amp;#39;m planning on creating a metabase dashboard (ironically, I work for one of their competitors \ud83d\ude06) that displays metrics from trending shows for each week and what streaming platforms they&amp;#39;re &lt;strong&gt;currently&lt;/strong&gt; available on. I don&amp;#39;t exactly trust the trending sections from any of the streaming providers, so that&amp;#39;s why I want to make my own. Also, it would be super helpful to know what&amp;#39;s the hottest shows on each platform and whether or it&amp;#39;s enough to justify resubbing to them. That way I don&amp;#39;t need to sift through SEO garbage articles on &lt;a href=\"/r/television\"&gt;/r/television&lt;/a&gt;.  &lt;/p&gt;\n\n&lt;p&gt;I plan to host the dashboard on a micro-e2 VM; that way anyone looking at my resume can just click a link and interact with the dashboard without needing to &lt;code&gt;git clone&lt;/code&gt; my repo and do any local setup.  &lt;/p&gt;\n\n&lt;h3&gt;Pipeline&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;make a weekly API request to fetch the top N trending TV shows from &lt;a href=\"https://developers.themoviedb.org/3/trending/get-trending\"&gt;TMDb&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;dump the weekly trending shows into a Google cloud storage bucket&lt;/li&gt;\n&lt;li&gt;append trending data files into a bigquery table. This will be a weekly grained fact table.&lt;/li&gt;\n&lt;li&gt;run a &lt;code&gt;SELECT DISTINCT show_id&lt;/code&gt; on the trending show table to get a list of unique show IDs that I can pass to this &lt;a href=\"https://developers.themoviedb.org/3/tv/get-tv-details\"&gt;endpoint&lt;/a&gt; to get all data needed to populate dimension tables (show details, genre, available watch providers, etc...)&lt;/li&gt;\n&lt;li&gt;dump dimension table data into the same bucket from above and create the dimension tables in BQ&lt;/li&gt;\n&lt;li&gt;use dbt to create models for Metabase to query on&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h3&gt;Scalability Issues&lt;/h3&gt;\n\n&lt;p&gt;Initially, I was thinking of following &lt;a href=\"/u/infiniteAggression-\"&gt;/u/infiniteAggression-&lt;/a&gt; &amp;#39;s  idea of deploying an Airflow server on the micro-e2 VM that Metabase will be hosted on. However, I think this VM is too &amp;quot;small&amp;quot; to actually host Metabase as well as Airflow.  &lt;/p&gt;\n\n&lt;p&gt;The number of unique show IDs from the fact table will grow O(n)--which means the number of API calls needed to upsert dimension tables will increase linearly. O(n) is the worst case, as the same show can show up this week and in any of the following weeks. Scalability probably won&amp;#39;t be &lt;em&gt;too&lt;/em&gt; terrible, but it&amp;#39;s still something for me to keep in mind.&lt;/p&gt;\n\n&lt;h3&gt;Workaround&lt;/h3&gt;\n\n&lt;p&gt;After more research, I think using Google&amp;#39;s CRON scheduler+cloud functions to handle data ingestion would be a better idea. That way, resources are freed up for the VM to solely host Metabase.  &lt;/p&gt;\n\n&lt;p&gt;To keep the number of API calls from blowing up, my idea is to delete records from my fact table after some condition is met. My two ideas are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;delete records after N months from date of ingestion&lt;/li&gt;\n&lt;li&gt;if the number of unique show IDs are &amp;gt; some threshold, then delete records after N months from date of ingestion&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "107rpu4", "is_robot_indexable": true, "report_reasons": null, "author": "bl4ckCloudz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107rpu4/pipeline_architecture_advice_for_my_first_side/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107rpu4/pipeline_architecture_advice_for_my_first_side/", "subreddit_subscribers": 85846, "created_utc": 1673302123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently standing up a new data analytics shop and we are in the process of designing our data lake.  I have read through the past posts on data lake design and read as much as I could find online and I thought I had a good handle on my approach.\n\nMy current thinking is to have three zones.\n\n1. Landing (immutable)\n2. Raw (This can convert file formats, maybe deduplicate, put in correct data types)\n3. Curated (Data modeled/aggregated to produce some type of product)\n\nThis seems to be where a lot of the literature ends, the actual design within these zones/containers is quite sparse.  So my design within these zones is something I am calling \"as flat as reasonable\".  This means instead on something like  domain/application/item/yyyy/mm/dd/data I am doing  domain\\_application\\_item/yyyy/mm/dd/data.  This does not work for all cases, but I like a flat structure if it makes sense.  So my first questions are, does this make sense?  Are there any pitfalls to this approach?\n\n&amp;#x200B;\n\nNext, I am wondering what people do in the curated zone.  I was asked to put data for a dashboard on the data lake.  This is data that will eventually be put into a pipeline, but is on someone's computer pulled from an api.  I received the data and it is broken into 9 different datasets broken up from 3 or 4 different original datasets.  So my questions are:\n\n1. Do people let analysts create a curated section for their data products?  My fear is this will become messy.\n2. Do you think it is ok for a curated section to have multiple datasets within it if it is for a specific data product?\n3. Should I take a more hands on approach and see if I can create a better product with the analyst?  My fear here is that I will not have enough time to do this with every product and I could step on some toes, or just slow the process down.\n\nI know this is a lot, but I would appreciate any feedback on the above issues.\n\nThanks", "author_fullname": "t2_bkzx0bcd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Lake Design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107gt0x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673277208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently standing up a new data analytics shop and we are in the process of designing our data lake.  I have read through the past posts on data lake design and read as much as I could find online and I thought I had a good handle on my approach.&lt;/p&gt;\n\n&lt;p&gt;My current thinking is to have three zones.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Landing (immutable)&lt;/li&gt;\n&lt;li&gt;Raw (This can convert file formats, maybe deduplicate, put in correct data types)&lt;/li&gt;\n&lt;li&gt;Curated (Data modeled/aggregated to produce some type of product)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This seems to be where a lot of the literature ends, the actual design within these zones/containers is quite sparse.  So my design within these zones is something I am calling &amp;quot;as flat as reasonable&amp;quot;.  This means instead on something like  domain/application/item/yyyy/mm/dd/data I am doing  domain_application_item/yyyy/mm/dd/data.  This does not work for all cases, but I like a flat structure if it makes sense.  So my first questions are, does this make sense?  Are there any pitfalls to this approach?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Next, I am wondering what people do in the curated zone.  I was asked to put data for a dashboard on the data lake.  This is data that will eventually be put into a pipeline, but is on someone&amp;#39;s computer pulled from an api.  I received the data and it is broken into 9 different datasets broken up from 3 or 4 different original datasets.  So my questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Do people let analysts create a curated section for their data products?  My fear is this will become messy.&lt;/li&gt;\n&lt;li&gt;Do you think it is ok for a curated section to have multiple datasets within it if it is for a specific data product?&lt;/li&gt;\n&lt;li&gt;Should I take a more hands on approach and see if I can create a better product with the analyst?  My fear here is that I will not have enough time to do this with every product and I could step on some toes, or just slow the process down.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I know this is a lot, but I would appreciate any feedback on the above issues.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "107gt0x", "is_robot_indexable": true, "report_reasons": null, "author": "SmothCerbrosoSimiae", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107gt0x/data_lake_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107gt0x/data_lake_design/", "subreddit_subscribers": 85846, "created_utc": 1673277208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys , I just got bombared with these questions that was waaay out of my league and expectations but I would like to continue learning about them and know how to prepare for any incoming questions like this in the future .\n\nFor context , the company that interviewed me is a AI &amp; Big Data Analytics company with products such as Fraud Risk Detection ,  Debt collection intelligence software  and etc.\n\n1.\u00a0\u00a0\u00a0\u00a0\u00a0How do you think external unstructured data can be used to augment internal data? What kind of additional insights can we derive from this ?\n\n2.\u00a0\u00a0\u00a0\u00a0The company  analyzes internal/external data and use the results in our proprietary influence platform to motivate users via a game-like process. Please think of and share two potential use cases for such a process.\n\n3.\u00a0\u00a0\u00a0\u00a0\u00a0The company  has a  project from an insurance firm to identify existing customers to be consider for upselling/cross-selling purposes (*a sales technique where a seller invites the customer to purchase more expensive items*) . They are willing to provide all kinds of data required, but these exist in data silos  without a data dictionary. What are the steps needed to begin and follow to achieve this ? ", "author_fullname": "t2_cl5vl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggled with DE interview questions as a Junior DE , any given perspectives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10801i2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673326809.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673323653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys , I just got bombared with these questions that was waaay out of my league and expectations but I would like to continue learning about them and know how to prepare for any incoming questions like this in the future .&lt;/p&gt;\n\n&lt;p&gt;For context , the company that interviewed me is a AI &amp;amp; Big Data Analytics company with products such as Fraud Risk Detection ,  Debt collection intelligence software  and etc.&lt;/p&gt;\n\n&lt;p&gt;1.\u00a0\u00a0\u00a0\u00a0\u00a0How do you think external unstructured data can be used to augment internal data? What kind of additional insights can we derive from this ?&lt;/p&gt;\n\n&lt;p&gt;2.\u00a0\u00a0\u00a0\u00a0The company  analyzes internal/external data and use the results in our proprietary influence platform to motivate users via a game-like process. Please think of and share two potential use cases for such a process.&lt;/p&gt;\n\n&lt;p&gt;3.\u00a0\u00a0\u00a0\u00a0\u00a0The company  has a  project from an insurance firm to identify existing customers to be consider for upselling/cross-selling purposes (&lt;em&gt;a sales technique where a seller invites the customer to purchase more expensive items&lt;/em&gt;) . They are willing to provide all kinds of data required, but these exist in data silos  without a data dictionary. What are the steps needed to begin and follow to achieve this ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10801i2", "is_robot_indexable": true, "report_reasons": null, "author": "Redxer", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10801i2/struggled_with_de_interview_questions_as_a_junior/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10801i2/struggled_with_de_interview_questions_as_a_junior/", "subreddit_subscribers": 85846, "created_utc": 1673323653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a data warehouse on Google Cloud that I ingest several data sources into. I am also hosting Apache Superset on Google Cloud to visualize data from this data warehouse. \n\nShould I have my data warehouse and Apache Superset deployment in a single project or is it best to split them into two different projects? One project makes sense in that everything is essentially in a single directory, but not sure this is the best practice as some of the data in the data warehouse may not be used in Superset.", "author_fullname": "t2_72tiq3y1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Cloud Data Warehousing - Best Practice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107nzje", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673293820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data warehouse on Google Cloud that I ingest several data sources into. I am also hosting Apache Superset on Google Cloud to visualize data from this data warehouse. &lt;/p&gt;\n\n&lt;p&gt;Should I have my data warehouse and Apache Superset deployment in a single project or is it best to split them into two different projects? One project makes sense in that everything is essentially in a single directory, but not sure this is the best practice as some of the data in the data warehouse may not be used in Superset.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "107nzje", "is_robot_indexable": true, "report_reasons": null, "author": "ROCKITZ15", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107nzje/google_cloud_data_warehousing_best_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107nzje/google_cloud_data_warehousing_best_practice/", "subreddit_subscribers": 85846, "created_utc": 1673293820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The coverage cutoff is set at 70% because that number is frequently used as a target point for \"good enough\" test coverage. \n\n[View Poll](https://www.reddit.com/poll/107g456)", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What level of testing does your data engineering codebase have?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107g456", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673275508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The coverage cutoff is set at 70% because that number is frequently used as a target point for &amp;quot;good enough&amp;quot; test coverage. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/107g456\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "107g456", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1673534708392, "options": [{"text": "Unit &amp; integration tests covering 70%+ of the codebase", "id": "20909852"}, {"text": "Any combination of unit &amp; integration Tests", "id": "20909853"}, {"text": "Some unit Tests", "id": "20909854"}, {"text": "Some integration Tests", "id": "20909855"}, {"text": "Tests... \ud83d\ude05", "id": "20909856"}, {"text": "See Results", "id": "20909857"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 290, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/107g456/what_level_of_testing_does_your_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/107g456/what_level_of_testing_does_your_data_engineering/", "subreddit_subscribers": 85846, "created_utc": 1673275508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to data engineering and I have learnt about the Parquet file. I have been able to take a csv file and using pandas and pyarrow, create a Parquet file.\n\nMy question now is, in real life/production/work environment how are Parquet files handled and stored? I can imagine in these environments, data will be retrieved from various sources: sensors, apis, databases etc and needs to be converted to Parquet files to enable analysis. But how will these files be managed?\n\nDo I need to create and manage directories filled with Parquet files? Or are there software systems that abstracts this away from you?", "author_fullname": "t2_3euic3tq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are Parquet files managed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1083os1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673335522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to data engineering and I have learnt about the Parquet file. I have been able to take a csv file and using pandas and pyarrow, create a Parquet file.&lt;/p&gt;\n\n&lt;p&gt;My question now is, in real life/production/work environment how are Parquet files handled and stored? I can imagine in these environments, data will be retrieved from various sources: sensors, apis, databases etc and needs to be converted to Parquet files to enable analysis. But how will these files be managed?&lt;/p&gt;\n\n&lt;p&gt;Do I need to create and manage directories filled with Parquet files? Or are there software systems that abstracts this away from you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1083os1", "is_robot_indexable": true, "report_reasons": null, "author": "finlaydotweber", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1083os1/how_are_parquet_files_managed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1083os1/how_are_parquet_files_managed/", "subreddit_subscribers": 85846, "created_utc": 1673335522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have always been a supporter of learning while doing and at 27 still support the idea that more or less you can do this job without a CS degree. That of course if you are motivated enough, like the job and don\u2019t mind losing your sleep over it from time to time because of learning and imposter syndrome.\n\nBut I (BD in finance and MD in Data science that were almost useless, with hindsight) also always felt like I\u2019m losing something without a CS degree. I\u2019m mulling over the idea of starting one, but i\u2019m pretty unsure about the benefits it gives compared to those I could get by using that time and money (about 6k \u20ac and maybe 5 years of studying while employed full time) for other activities (which ones?).\n\nAny thoughts or suggestions based on your experience?\n\nEDIT: forgot to mention that I work as a DE", "author_fullname": "t2_d0ifg2cb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts about CS and similar degrees in DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107m88k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673298604.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673289830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have always been a supporter of learning while doing and at 27 still support the idea that more or less you can do this job without a CS degree. That of course if you are motivated enough, like the job and don\u2019t mind losing your sleep over it from time to time because of learning and imposter syndrome.&lt;/p&gt;\n\n&lt;p&gt;But I (BD in finance and MD in Data science that were almost useless, with hindsight) also always felt like I\u2019m losing something without a CS degree. I\u2019m mulling over the idea of starting one, but i\u2019m pretty unsure about the benefits it gives compared to those I could get by using that time and money (about 6k \u20ac and maybe 5 years of studying while employed full time) for other activities (which ones?).&lt;/p&gt;\n\n&lt;p&gt;Any thoughts or suggestions based on your experience?&lt;/p&gt;\n\n&lt;p&gt;EDIT: forgot to mention that I work as a DE&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "107m88k", "is_robot_indexable": true, "report_reasons": null, "author": "Awkward-Cupcake6219", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107m88k/thoughts_about_cs_and_similar_degrees_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107m88k/thoughts_about_cs_and_similar_degrees_in_de/", "subreddit_subscribers": 85846, "created_utc": 1673289830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently wrote a blog on the top tools for data cleansing and preprocessing, and I wanted to share it with the community here on Reddit. Data cleansing and preprocessing are essential steps in the data pipeline, and having the right tools can make a big difference in the efficiency and effectiveness of these tasks.\n\n**OpenRefine**\n\n* OpenRefine is an open-source tool that is widely used for data cleansing and preprocessing.\n* It provides a user-friendly interface for working with large datasets and allows users to easily spot errors, inconsistencies, and duplicates in their data.\n* Example: Cleaning up a dataset of customer information, removing duplicates, standardizing phone numbers format.\n* Pros: open-source, large community of users and contributors, and intuitive interface.\n* Cons: not as powerful as some other tools, and may not be suitable for very large or complex datasets.\n\n**Trifacta Wrangler**\n\n* Trifacta Wrangler is a powerful data cleansing and preprocessing tool.\n* It is designed to make it easy for users to transform and shape their data in preparation for analysis.\n* Example: Preparing a dataset for analysis. Extracting relevant data and transforming it into a structured format.\n* Pros: advanced capabilities for data transformation, ability to handle large datasets with ease.\n* Cons: steeper learning curve, require technical expertise.\n\n**Talend**\n\n* Talend is a comprehensive data integration platform that includes a range of tools for data cleansing and preprocessing.\n* It is designed for use in enterprise environments and is capable of handling large volumes of data from a variety of sources.\n* Example: Integrating data from multiple sources for analysis. Extracting and combining data into a single dataset.\n* Pros: robust capabilities for data integration, ability to handle large volumes of data.\n* Cons: higher learning curve, requires investment of time and resources.\n\n**Pandas**\n\n* Pandas is a popular open-source library for data manipulation and analysis in Python.\n* It includes a range of tools for data cleansing and preprocessing, such as the ability to merge, split, reshape data, and handle missing values\n* Pros: Popular open-source, widely used among the data science community, provide a lot of functionality for data manipulation and cleaning, great for those already comfortable with python.\n\nLink to the full [post](https://medium.com/p/ebf98779b5d4)", "author_fullname": "t2_7obzgfpa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get Your Data Ready for Analysis: Top Tools for Data Cleansing and Preprocessing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1086c9u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673345653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently wrote a blog on the top tools for data cleansing and preprocessing, and I wanted to share it with the community here on Reddit. Data cleansing and preprocessing are essential steps in the data pipeline, and having the right tools can make a big difference in the efficiency and effectiveness of these tasks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;OpenRefine&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OpenRefine is an open-source tool that is widely used for data cleansing and preprocessing.&lt;/li&gt;\n&lt;li&gt;It provides a user-friendly interface for working with large datasets and allows users to easily spot errors, inconsistencies, and duplicates in their data.&lt;/li&gt;\n&lt;li&gt;Example: Cleaning up a dataset of customer information, removing duplicates, standardizing phone numbers format.&lt;/li&gt;\n&lt;li&gt;Pros: open-source, large community of users and contributors, and intuitive interface.&lt;/li&gt;\n&lt;li&gt;Cons: not as powerful as some other tools, and may not be suitable for very large or complex datasets.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Trifacta Wrangler&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Trifacta Wrangler is a powerful data cleansing and preprocessing tool.&lt;/li&gt;\n&lt;li&gt;It is designed to make it easy for users to transform and shape their data in preparation for analysis.&lt;/li&gt;\n&lt;li&gt;Example: Preparing a dataset for analysis. Extracting relevant data and transforming it into a structured format.&lt;/li&gt;\n&lt;li&gt;Pros: advanced capabilities for data transformation, ability to handle large datasets with ease.&lt;/li&gt;\n&lt;li&gt;Cons: steeper learning curve, require technical expertise.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Talend&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Talend is a comprehensive data integration platform that includes a range of tools for data cleansing and preprocessing.&lt;/li&gt;\n&lt;li&gt;It is designed for use in enterprise environments and is capable of handling large volumes of data from a variety of sources.&lt;/li&gt;\n&lt;li&gt;Example: Integrating data from multiple sources for analysis. Extracting and combining data into a single dataset.&lt;/li&gt;\n&lt;li&gt;Pros: robust capabilities for data integration, ability to handle large volumes of data.&lt;/li&gt;\n&lt;li&gt;Cons: higher learning curve, requires investment of time and resources.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Pandas&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pandas is a popular open-source library for data manipulation and analysis in Python.&lt;/li&gt;\n&lt;li&gt;It includes a range of tools for data cleansing and preprocessing, such as the ability to merge, split, reshape data, and handle missing values&lt;/li&gt;\n&lt;li&gt;Pros: Popular open-source, widely used among the data science community, provide a lot of functionality for data manipulation and cleaning, great for those already comfortable with python.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Link to the full &lt;a href=\"https://medium.com/p/ebf98779b5d4\"&gt;post&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8K2zc6Z3bUWPvEnpLa3ZMHPz6kBrrcye7JJfwj9DzCg.jpg?auto=webp&amp;s=a990ab91ce883fde2e88e452b7c098eccce7393d", "width": 1200, "height": 732}, "resolutions": [{"url": "https://external-preview.redd.it/8K2zc6Z3bUWPvEnpLa3ZMHPz6kBrrcye7JJfwj9DzCg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc8133dd302c0c46db245730c70ade0863ebfb24", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/8K2zc6Z3bUWPvEnpLa3ZMHPz6kBrrcye7JJfwj9DzCg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=537095c26f515b5e6ddeaedd2daf2f9b1c67f532", "width": 216, "height": 131}, {"url": "https://external-preview.redd.it/8K2zc6Z3bUWPvEnpLa3ZMHPz6kBrrcye7JJfwj9DzCg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a2e246f2038ab0cf467e83a658ad57d60f7b66ef", "width": 320, "height": 195}, {"url": "https://external-preview.redd.it/8K2zc6Z3bUWPvEnpLa3ZMHPz6kBrrcye7JJfwj9DzCg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d0116c018e33ff1da51c3bdea15856030447cca5", "width": 640, "height": 390}, {"url": "https://external-preview.redd.it/8K2zc6Z3bUWPvEnpLa3ZMHPz6kBrrcye7JJfwj9DzCg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ff3e994288d8293c19fede1c2bf39daeb8392980", "width": 960, "height": 585}, {"url": "https://external-preview.redd.it/8K2zc6Z3bUWPvEnpLa3ZMHPz6kBrrcye7JJfwj9DzCg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e62459433eb12af49cd68adff6de3b29ac520c5e", "width": 1080, "height": 658}], "variants": {}, "id": "b-E1SlF5WLOeVt1Gioctt2zLqvXh9Li4myKO24O0m80"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1086c9u", "is_robot_indexable": true, "report_reasons": null, "author": "Ill-Psychology-2407", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1086c9u/get_your_data_ready_for_analysis_top_tools_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1086c9u/get_your_data_ready_for_analysis_top_tools_for/", "subreddit_subscribers": 85846, "created_utc": 1673345653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI am implementing an ETL streaming job using Spark Structured Streaming. This job takes data from a kafka topic and stores it into separate Delta tables according to the value of a specific field.\n\nI keep the latest version for each row, and right now I am using the DeltaTable's merge function to upsert data into the destination table. I am already optimizing by using partition columns in the merge condition, and by doing so I am getting a latency in the order of 10 minutes.\n\nI was wondering if an \"append-only\" approach to the tables would decrease the latency or not.\n\nThe steps would be as follows:\n\nIn the batch, for each Delta table\n\n1. Calculate the latest versions for each event in the batch and keep them\n2. Append the latest event versions to the delta table\n3. Read the table partitions containing changes\n4. Filter the data to keep only the latest event versions\n5. Overwrite the involved table partitions using `replaceWhere`\n\nThe idea here is to avoid any joins that delta's merge [does](https://www.databricks.com/wp-content/uploads/2020/09/blog-diving-delta-4.png).\n\n&amp;#x200B;\n\nHas anyone tried this approach? Do you think it will give some performance gains?\n\nI'm wondering if I am reinventing the wheel and Delta is already optimized or not, or if there are some corner cases I am not considering.\n\n&amp;#x200B;\n\nEDIT1: The way I'd keep the latest event versions would be a window function:\n\nrow\\_number partitioning by key and ordering by event timestamp in descending order, then I'd keep only the rows where the value equals 1.", "author_fullname": "t2_do2bj7xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta's merge vs overwriting with replaceWhere", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10857r4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673343815.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673341273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am implementing an ETL streaming job using Spark Structured Streaming. This job takes data from a kafka topic and stores it into separate Delta tables according to the value of a specific field.&lt;/p&gt;\n\n&lt;p&gt;I keep the latest version for each row, and right now I am using the DeltaTable&amp;#39;s merge function to upsert data into the destination table. I am already optimizing by using partition columns in the merge condition, and by doing so I am getting a latency in the order of 10 minutes.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if an &amp;quot;append-only&amp;quot; approach to the tables would decrease the latency or not.&lt;/p&gt;\n\n&lt;p&gt;The steps would be as follows:&lt;/p&gt;\n\n&lt;p&gt;In the batch, for each Delta table&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Calculate the latest versions for each event in the batch and keep them&lt;/li&gt;\n&lt;li&gt;Append the latest event versions to the delta table&lt;/li&gt;\n&lt;li&gt;Read the table partitions containing changes&lt;/li&gt;\n&lt;li&gt;Filter the data to keep only the latest event versions&lt;/li&gt;\n&lt;li&gt;Overwrite the involved table partitions using &lt;code&gt;replaceWhere&lt;/code&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The idea here is to avoid any joins that delta&amp;#39;s merge &lt;a href=\"https://www.databricks.com/wp-content/uploads/2020/09/blog-diving-delta-4.png\"&gt;does&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anyone tried this approach? Do you think it will give some performance gains?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if I am reinventing the wheel and Delta is already optimized or not, or if there are some corner cases I am not considering.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EDIT1: The way I&amp;#39;d keep the latest event versions would be a window function:&lt;/p&gt;\n\n&lt;p&gt;row_number partitioning by key and ordering by event timestamp in descending order, then I&amp;#39;d keep only the rows where the value equals 1.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?auto=webp&amp;s=3c9c832e54ef81cae7aa454aa85685e08f1924b1", "width": 864, "height": 487}, "resolutions": [{"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e1c7cdc4202a0a9c529d42f2db034e7a18867d9d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c72f36147d383db174031d5408f1b3ade410a509", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8fbd9a41446e8d895ee1214187b892d445722005", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bffa0dffde9b766238f1287ee56b73c7f0d8a0ad", "width": 640, "height": 360}], "variants": {}, "id": "RHUM16d2JyL38mWZUyE_7-xAj3xsCEo-fboQVyXjq1M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10857r4", "is_robot_indexable": true, "report_reasons": null, "author": "somerandomdataeng", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10857r4/deltas_merge_vs_overwriting_with_replacewhere/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10857r4/deltas_merge_vs_overwriting_with_replacewhere/", "subreddit_subscribers": 85846, "created_utc": 1673341273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I\u2019m new to the DE world and I\u2019ve been tasked with essentially creating a small scale MES for our plant (structured production data and a few dashboards). \n\nInitially I thought it made sense to collect data by machine because each machine has slightly different data that needs to be recorded. Then I realized there was also a metric I needed by line that could only be calculated using data from all machines. Which makes me believe the data should be organized by line.\n\n But then I realize sometimes machines switch lines which would mess up the whole table for that line. There\u2019s also some data like hit count which could easily be captured only once every hour whereas something like vibration needs to be captured every second. Any advice or sources discussing this topic?", "author_fullname": "t2_fgc39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is manufacturing IoT data typically organized?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107srly", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673304479.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019m new to the DE world and I\u2019ve been tasked with essentially creating a small scale MES for our plant (structured production data and a few dashboards). &lt;/p&gt;\n\n&lt;p&gt;Initially I thought it made sense to collect data by machine because each machine has slightly different data that needs to be recorded. Then I realized there was also a metric I needed by line that could only be calculated using data from all machines. Which makes me believe the data should be organized by line.&lt;/p&gt;\n\n&lt;p&gt;But then I realize sometimes machines switch lines which would mess up the whole table for that line. There\u2019s also some data like hit count which could easily be captured only once every hour whereas something like vibration needs to be captured every second. Any advice or sources discussing this topic?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "107srly", "is_robot_indexable": true, "report_reasons": null, "author": "TheOnlinePolak", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107srly/how_is_manufacturing_iot_data_typically_organized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107srly/how_is_manufacturing_iot_data_typically_organized/", "subreddit_subscribers": 85846, "created_utc": 1673304479.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey peeps, trying to find a good solution for a hash table in AWS Glue.\n\nDue to GDPR reasons, several tables will have to go through a hash to anonymize data from id to external_id.\n\n\nFor example: I have a table with several columns and an id column.\nThe external_id will be the new id in this table, but I need to map it first and drop the original column.\n\nI have some limitations that all of this transformation need to occur inside AWS, nothing touching anything beyond this. (Again, GDPR)\n\nI wonder if there is a better way to do this within Glue or other service. At the beginning I thought that Databrew recipes would take care of this, but apparently it\u2019s not dynamic enough to extend the services to multiple tables.\n\nMy approach now would be to construct a dictionary with the tables and columns and do a join between datasets, but I\u2019m a little bit skeptical and trying to find a more refined solution for this. \n\nAny ideas?", "author_fullname": "t2_4j9daz4x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hash table with AWS Glue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107itn0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673282036.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey peeps, trying to find a good solution for a hash table in AWS Glue.&lt;/p&gt;\n\n&lt;p&gt;Due to GDPR reasons, several tables will have to go through a hash to anonymize data from id to external_id.&lt;/p&gt;\n\n&lt;p&gt;For example: I have a table with several columns and an id column.\nThe external_id will be the new id in this table, but I need to map it first and drop the original column.&lt;/p&gt;\n\n&lt;p&gt;I have some limitations that all of this transformation need to occur inside AWS, nothing touching anything beyond this. (Again, GDPR)&lt;/p&gt;\n\n&lt;p&gt;I wonder if there is a better way to do this within Glue or other service. At the beginning I thought that Databrew recipes would take care of this, but apparently it\u2019s not dynamic enough to extend the services to multiple tables.&lt;/p&gt;\n\n&lt;p&gt;My approach now would be to construct a dictionary with the tables and columns and do a join between datasets, but I\u2019m a little bit skeptical and trying to find a more refined solution for this. &lt;/p&gt;\n\n&lt;p&gt;Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "107itn0", "is_robot_indexable": true, "report_reasons": null, "author": "Dachsgp", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107itn0/hash_table_with_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107itn0/hash_table_with_aws_glue/", "subreddit_subscribers": 85846, "created_utc": 1673282036.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nDoes anyone have a good study guide that they would be willing to provide me for the certification exam? Any help is appreciated.", "author_fullname": "t2_adn81", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Data Engineer Cert Study Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107g4pe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673275542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Does anyone have a good study guide that they would be willing to provide me for the certification exam? Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "107g4pe", "is_robot_indexable": true, "report_reasons": null, "author": "milehighmecked", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107g4pe/databricks_data_engineer_cert_study_guide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107g4pe/databricks_data_engineer_cert_study_guide/", "subreddit_subscribers": 85846, "created_utc": 1673275542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My Airflow task is uploading JSON files to BigQuery every 2 hours. It takes longer than an hour to upload them all so it times out.\n\nI was thinking of using the async library but not sure - Because it has to read each file into memory and convert it to a dictionary so wondering if I should use multiprocessing instead?\n\nThe reason for reading the file is because we add some extra fields to each file", "author_fullname": "t2_ggg0wfmt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Time Outs - BigQuery Upload", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1084axv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "spoiler", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673337795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My Airflow task is uploading JSON files to BigQuery every 2 hours. It takes longer than an hour to upload them all so it times out.&lt;/p&gt;\n\n&lt;p&gt;I was thinking of using the async library but not sure - Because it has to read each file into memory and convert it to a dictionary so wondering if I should use multiprocessing instead?&lt;/p&gt;\n\n&lt;p&gt;The reason for reading the file is because we add some extra fields to each file&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": true, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1084axv", "is_robot_indexable": true, "report_reasons": null, "author": "camelCaseInsensitive", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1084axv/airflow_time_outs_bigquery_upload/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1084axv/airflow_time_outs_bigquery_upload/", "subreddit_subscribers": 85846, "created_utc": 1673337795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am thinking about taking a course on this topic and the book the class goes over is \u201cDatabase Management Systems (3rd ed) by Ramakrishnan and Gehrke\u201d. Should I focus on doing something like the data zoom camp instead and take one less class or stick to the class?", "author_fullname": "t2_cauyq044", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How relevant is database management systems for data engineering/data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10839qp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673334023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am thinking about taking a course on this topic and the book the class goes over is \u201cDatabase Management Systems (3rd ed) by Ramakrishnan and Gehrke\u201d. Should I focus on doing something like the data zoom camp instead and take one less class or stick to the class?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10839qp", "is_robot_indexable": true, "report_reasons": null, "author": "honey1337", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10839qp/how_relevant_is_database_management_systems_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10839qp/how_relevant_is_database_management_systems_for/", "subreddit_subscribers": 85846, "created_utc": 1673334023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm looking for a simple dataset catalog solution where we can store information (basic template and download link) about datasets/models and a couple of other data types. At the moment it's all maintained in excel and I'm looking for some simple and, if possible, accessible solutions to this problem so that different teams have some kind of data hub to share their data/outputs. So far the closest thing I found was CKAN, the other data catalogs include too much unnecessary functionality. Maybe I am looking for a too-basic case, and keeping everything in excel or writing simple solutions by ourselves will be a better option? Thank you in advance.", "author_fullname": "t2_zak9c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Very simple data catalog options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1082uea", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673332522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m looking for a simple dataset catalog solution where we can store information (basic template and download link) about datasets/models and a couple of other data types. At the moment it&amp;#39;s all maintained in excel and I&amp;#39;m looking for some simple and, if possible, accessible solutions to this problem so that different teams have some kind of data hub to share their data/outputs. So far the closest thing I found was CKAN, the other data catalogs include too much unnecessary functionality. Maybe I am looking for a too-basic case, and keeping everything in excel or writing simple solutions by ourselves will be a better option? Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1082uea", "is_robot_indexable": true, "report_reasons": null, "author": "pemens", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1082uea/very_simple_data_catalog_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1082uea/very_simple_data_catalog_options/", "subreddit_subscribers": 85846, "created_utc": 1673332522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello - I am looking to identify different storage services in SAP like S3, EBS, EFS in AWS. I found a lot of conflicting information and have come here for help.\n\nSo far I only have SAP Hana.", "author_fullname": "t2_uc7m4pgj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the different storage services in SAP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107v9jf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673310550.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello - I am looking to identify different storage services in SAP like S3, EBS, EFS in AWS. I found a lot of conflicting information and have come here for help.&lt;/p&gt;\n\n&lt;p&gt;So far I only have SAP Hana.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "107v9jf", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Packer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107v9jf/what_are_the_different_storage_services_in_sap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107v9jf/what_are_the_different_storage_services_in_sap/", "subreddit_subscribers": 85846, "created_utc": 1673310550.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was wondering if anyone on this sub has any experience working in a full-time job and as a freelancer/contractor. What mode of employment would you prefer in the data engineering space?\n\nPersonally I'm working full time with lots of benefits, but seeing those juicy hourly rates on contracting jobs makes me think if I should jump ship.", "author_fullname": "t2_l35gwhuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Full-Time Work Vs Freelance/Contracting Work", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107tpb1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673306663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if anyone on this sub has any experience working in a full-time job and as a freelancer/contractor. What mode of employment would you prefer in the data engineering space?&lt;/p&gt;\n\n&lt;p&gt;Personally I&amp;#39;m working full time with lots of benefits, but seeing those juicy hourly rates on contracting jobs makes me think if I should jump ship.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "107tpb1", "is_robot_indexable": true, "report_reasons": null, "author": "Senior_Anteater4688", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107tpb1/fulltime_work_vs_freelancecontracting_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107tpb1/fulltime_work_vs_freelancecontracting_work/", "subreddit_subscribers": 85846, "created_utc": 1673306663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a Synapse Serverless Pool setup that I am using to create parquet files in a ADLSGen2 container.  Is there any way to have a PySpark Notebook create a database view in a Serverless Pool?  \n\nI've been creating the parquet, then using a stored proc to create the view (in a pipeline), but is there a way to create a view right after the parquet files are created?  Or am I SOL because the notebook is connected to a Spark pool and can't do anything on the regular Serverless SQL pool because it's outside of Spark (without JDBC)?\n\nAny ideas?  I haven't gotten it to work.  Curious if it's theoretically possible.", "author_fullname": "t2_4270ek0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synapse Analytics - PySpark Notebooks and DDL statements", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107t6f0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673305430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Synapse Serverless Pool setup that I am using to create parquet files in a ADLSGen2 container.  Is there any way to have a PySpark Notebook create a database view in a Serverless Pool?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been creating the parquet, then using a stored proc to create the view (in a pipeline), but is there a way to create a view right after the parquet files are created?  Or am I SOL because the notebook is connected to a Spark pool and can&amp;#39;t do anything on the regular Serverless SQL pool because it&amp;#39;s outside of Spark (without JDBC)?&lt;/p&gt;\n\n&lt;p&gt;Any ideas?  I haven&amp;#39;t gotten it to work.  Curious if it&amp;#39;s theoretically possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "107t6f0", "is_robot_indexable": true, "report_reasons": null, "author": "mr_electric_wizard", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107t6f0/synapse_analytics_pyspark_notebooks_and_ddl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107t6f0/synapse_analytics_pyspark_notebooks_and_ddl/", "subreddit_subscribers": 85846, "created_utc": 1673305430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "let's say you want to pull rows from bigquery and put them into a redshift db - but you don't have access to gcp buckets.\n\nwhat would be the recommended methods to bring that data over, and how would they change with size in millions of rows: 1,10,50,100,500, 1billion, etc.\n\nif i hit bigquery for 15 million rows, put it in a polars dataframe, and then save that as a csv.  then load the csv to s3, then copy from redshift...that seems like a lot of moves, no?\n\nwould it be a good idea iterate over that bigquery result, and insert the data into redshift directly?\n\ntrying to understand how to design pipelines better rather than just getting it done.", "author_fullname": "t2_5qteskd9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "intermediary storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107qs90", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673300070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;let&amp;#39;s say you want to pull rows from bigquery and put them into a redshift db - but you don&amp;#39;t have access to gcp buckets.&lt;/p&gt;\n\n&lt;p&gt;what would be the recommended methods to bring that data over, and how would they change with size in millions of rows: 1,10,50,100,500, 1billion, etc.&lt;/p&gt;\n\n&lt;p&gt;if i hit bigquery for 15 million rows, put it in a polars dataframe, and then save that as a csv.  then load the csv to s3, then copy from redshift...that seems like a lot of moves, no?&lt;/p&gt;\n\n&lt;p&gt;would it be a good idea iterate over that bigquery result, and insert the data into redshift directly?&lt;/p&gt;\n\n&lt;p&gt;trying to understand how to design pipelines better rather than just getting it done.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "107qs90", "is_robot_indexable": true, "report_reasons": null, "author": "iseestupid", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107qs90/intermediary_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107qs90/intermediary_storage/", "subreddit_subscribers": 85846, "created_utc": 1673300070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI am doing my research on data engineering and I\u2019m very interested in becoming a healthcare-specific data engineer. I am passionate about healthcare and truly enjoy addressing the nuances of data engineering within the context of healthcare.\n\nI have a Bachelor\u2019s degree in Nursing &amp; a Master\u2019s degree in Data Analytics, the latter of which has afforded me the opportunity to get my current job as a business analyst (basically shifted from a medical biller to analyst before I even graduated grad school &amp; almost doubled my income).\n\nMy role currently involves creating &amp; maintaining Power BI dashboards, using basic Python to automate some things, pulling data from Snowflake &amp; doing analysis for revenue. In school, I did a lot of data modeling projects in Jupyter notebook in Python.\n\nMy research has led me to [awesomeengineering](https://awesomedataengineering.com/) so my plan is to follow that pathway, update my resume &amp; apply to jobs.\n\nWith that said, even with my master\u2019s I am a little concerned that my bachelor won\u2019t be sufficient in becoming competitive for the role. Should I consider getting a bachelor\u2019s in CS, IT or Software Dev on top of following the awesomeengineering pathway?", "author_fullname": "t2_alka6pjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Healthcare business analyst to data engineer - possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107jkt9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673285972.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673283789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I am doing my research on data engineering and I\u2019m very interested in becoming a healthcare-specific data engineer. I am passionate about healthcare and truly enjoy addressing the nuances of data engineering within the context of healthcare.&lt;/p&gt;\n\n&lt;p&gt;I have a Bachelor\u2019s degree in Nursing &amp;amp; a Master\u2019s degree in Data Analytics, the latter of which has afforded me the opportunity to get my current job as a business analyst (basically shifted from a medical biller to analyst before I even graduated grad school &amp;amp; almost doubled my income).&lt;/p&gt;\n\n&lt;p&gt;My role currently involves creating &amp;amp; maintaining Power BI dashboards, using basic Python to automate some things, pulling data from Snowflake &amp;amp; doing analysis for revenue. In school, I did a lot of data modeling projects in Jupyter notebook in Python.&lt;/p&gt;\n\n&lt;p&gt;My research has led me to &lt;a href=\"https://awesomedataengineering.com/\"&gt;awesomeengineering&lt;/a&gt; so my plan is to follow that pathway, update my resume &amp;amp; apply to jobs.&lt;/p&gt;\n\n&lt;p&gt;With that said, even with my master\u2019s I am a little concerned that my bachelor won\u2019t be sufficient in becoming competitive for the role. Should I consider getting a bachelor\u2019s in CS, IT or Software Dev on top of following the awesomeengineering pathway?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/a2ma3Lkie2EG-P-3gCgxSAWn7fKeMNOaVxbIoBBP908.jpg?auto=webp&amp;s=483e44ca7b0d86f177b188fbbd7371e4973bd0c4", "width": 818, "height": 428}, "resolutions": [{"url": "https://external-preview.redd.it/a2ma3Lkie2EG-P-3gCgxSAWn7fKeMNOaVxbIoBBP908.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=37f7f44ffa34248c8ba16aec9cc214d9765998f0", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/a2ma3Lkie2EG-P-3gCgxSAWn7fKeMNOaVxbIoBBP908.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=26f05f4d1f6d60cd5b5c27b29c2f25b6fad7da66", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/a2ma3Lkie2EG-P-3gCgxSAWn7fKeMNOaVxbIoBBP908.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c8afa81863199ac1a6833dc2bdccdb543194733", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/a2ma3Lkie2EG-P-3gCgxSAWn7fKeMNOaVxbIoBBP908.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e4bb337267b5e9acea6c092069a236f2c8df831", "width": 640, "height": 334}], "variants": {}, "id": "YApHHMbWpuH_VtbxYcfIqoEZcUYceWRq4U5PDmRKJVw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "107jkt9", "is_robot_indexable": true, "report_reasons": null, "author": "depressedbutsassy", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107jkt9/healthcare_business_analyst_to_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107jkt9/healthcare_business_analyst_to_data_engineer/", "subreddit_subscribers": 85846, "created_utc": 1673283789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_sppgx30r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Activity Selection Problem using Greedy Approach", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_107jf2o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/V0ZrLuIVzaY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Activity Selection Problem using Greedy Algorithm Method | DSA Interview Question Solved\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Activity Selection Problem using Greedy Algorithm Method | DSA Interview Question Solved", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/V0ZrLuIVzaY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Activity Selection Problem using Greedy Algorithm Method | DSA Interview Question Solved\"&gt;&lt;/iframe&gt;", "author_name": " Code with Scaler", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/V0ZrLuIVzaY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CodewithScaler"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/V0ZrLuIVzaY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Activity Selection Problem using Greedy Algorithm Method | DSA Interview Question Solved\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/107jf2o", "height": 200}, "link_flair_text": "Interview", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KcmAZrUo4BKEYusPQuAHY2dHqA7bYV4cANCOMT-0OLs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673283438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/V0ZrLuIVzaY", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GZX4YmlcM_paZrWMc9X7YImUzJNMbGtbkdocoHHcyw8.jpg?auto=webp&amp;s=1ee0eb6e602aa06fbd351bb644ff29693e8517fa", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/GZX4YmlcM_paZrWMc9X7YImUzJNMbGtbkdocoHHcyw8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cc70d549d19b062950e462892e866c27dee9ab5", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/GZX4YmlcM_paZrWMc9X7YImUzJNMbGtbkdocoHHcyw8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=667f5baa0ee0a1f68d171050f7c24a1d20943141", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/GZX4YmlcM_paZrWMc9X7YImUzJNMbGtbkdocoHHcyw8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=982d8c80afe44a9ed6c0e448a6b8fde0c0adcbe1", "width": 320, "height": 240}], "variants": {}, "id": "kn_DK6DZQnMoSFugEdYsIyTdOloZRZLcJNY1VFGStM4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "107jf2o", "is_robot_indexable": true, "report_reasons": null, "author": "thetech_learner", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107jf2o/sql_activity_selection_problem_using_greedy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/V0ZrLuIVzaY", "subreddit_subscribers": 85846, "created_utc": 1673283438.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Activity Selection Problem using Greedy Algorithm Method | DSA Interview Question Solved", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/V0ZrLuIVzaY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Activity Selection Problem using Greedy Algorithm Method | DSA Interview Question Solved\"&gt;&lt;/iframe&gt;", "author_name": " Code with Scaler", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/V0ZrLuIVzaY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CodewithScaler"}}, "is_video": false}}], "before": null}}