{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have tried to work with this platform. I have given it every opportunity I can, but never have I seen so much trouble from a tool or framework I had to work with. I started the new year with a new years resolution: to write down every problem as I encounter them. I don't know if I should continue with it, since I might fill up my e-reader before the year is over.\n\nIn no particular order:\n\n1. Synapse expressions have no support for the case() function. Have fun chaining if else's!\n2. Not all activities have retries. What do you mean you want to put a retry on a pipeline? That's crazy talk!\n3. SQL scripts are saved as JSON. Not even JSON5. Fucking JSON. I hope you like diff checking one fucking long ass line of SQL!\n4. Browser IDE... Whoever thought this up deserves fish hooks up their ass. You can't even save properly in the stupid thing, because every single thing is counted as a commit. And before some asshat suggests committing after the work is done: you deserve the fish hook too.\n5. Pipeline variables do not include int's (and some other types for that matter) for whatever reason. Converting everything to string and back is fun isn't it?\n6. Pipelines don't have output parameters, so even if I wanted to make reusable modules for missing Synapse functionality I literally can't unless I start using them as error parameters (and if you try to suggest that I WILL shank you).\n7. No global parameters EVEN THOUGH DATA FACTORY HAD IT AND IT'S BEEN REQUESTED FOR ALMOST 2 YEARS.\n8. 2022 and still no dark mode. This has been requested for data factory in 2016. Yes you're old and so am I.\n9. The SQL editor has all the great functionalities notepad has.\n10. I hope you didn't name one of your workspaces incorrectly. Oh you did? RIP, time to remake it.\n11. Local timezones? You mean UTC? What do you mean + or -? You craycray!\n12. Nesting loops or if tests can't be done. You need to make separate pipelines for it and before someone asks: no, just because it's a nesting it REALLY doesn't mean it belongs in a separate pipeline.\n13. Speaking about nesting: if you use an if test in a foreach, you can't access the current item of the loop. Haven't you learned by now? You're using Synapse. Now eat shit.\n14. Self-Hosted Integration Runtimes cannot be shared, while you can actually do that in data factory. This means you need to run three separate runtimes just to get to on-premise sources.\n15. On the topic of self-hosted integration runtimes: why do you even need them at all? Why is it not possible to peer a vnet that contains synapse and be done with it?\n16. What's even the use of the SQL scripts if you can't access them from pipelines. You get the script activity, but that's just another fucking place to dump your SQL in. And if you dare to suggest to use the API to trigger/request the SQL script. I will adopt a dog just to feed you to it.\n17. Testing? HAHAHAHAHAHAHAHAHAHA- fuck you.\n18. You want to rename a variable? Lol you little shit now you have to search for every instance to rename it. Refactoring? Just become a 100x dev yo.\n19. Parameters in pipeline templates? Why would you want parameters in your pipeline templates? You talk like you want to reuse the code you wrote or something. Oh you do? ........oof.\n20. Git....lab? Sounds like a dangerous cocaine facility. Hope you weren't planning to attach that to synapse! Or like literally any other option since you have to attach git to synapse in the first place.\n\nThese are the ones from THIS year, so after one week. I have had the displeasure of using it for a year now. So here's my advice after this long-winded rant: use it for a quick prototype of anything and don't use it for anything bigger than that. If you do, you have learned nothing of all the improvements that people have brought to the art that is development and you should probably touch grass instead of sucking cock on LinkedIn.", "author_fullname": "t2_2pbhwnrm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Synapse Analytics is absolute trash for anything bigger than a few extractions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107rt91", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 108, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 108, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673302334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tried to work with this platform. I have given it every opportunity I can, but never have I seen so much trouble from a tool or framework I had to work with. I started the new year with a new years resolution: to write down every problem as I encounter them. I don&amp;#39;t know if I should continue with it, since I might fill up my e-reader before the year is over.&lt;/p&gt;\n\n&lt;p&gt;In no particular order:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Synapse expressions have no support for the case() function. Have fun chaining if else&amp;#39;s!&lt;/li&gt;\n&lt;li&gt;Not all activities have retries. What do you mean you want to put a retry on a pipeline? That&amp;#39;s crazy talk!&lt;/li&gt;\n&lt;li&gt;SQL scripts are saved as JSON. Not even JSON5. Fucking JSON. I hope you like diff checking one fucking long ass line of SQL!&lt;/li&gt;\n&lt;li&gt;Browser IDE... Whoever thought this up deserves fish hooks up their ass. You can&amp;#39;t even save properly in the stupid thing, because every single thing is counted as a commit. And before some asshat suggests committing after the work is done: you deserve the fish hook too.&lt;/li&gt;\n&lt;li&gt;Pipeline variables do not include int&amp;#39;s (and some other types for that matter) for whatever reason. Converting everything to string and back is fun isn&amp;#39;t it?&lt;/li&gt;\n&lt;li&gt;Pipelines don&amp;#39;t have output parameters, so even if I wanted to make reusable modules for missing Synapse functionality I literally can&amp;#39;t unless I start using them as error parameters (and if you try to suggest that I WILL shank you).&lt;/li&gt;\n&lt;li&gt;No global parameters EVEN THOUGH DATA FACTORY HAD IT AND IT&amp;#39;S BEEN REQUESTED FOR ALMOST 2 YEARS.&lt;/li&gt;\n&lt;li&gt;2022 and still no dark mode. This has been requested for data factory in 2016. Yes you&amp;#39;re old and so am I.&lt;/li&gt;\n&lt;li&gt;The SQL editor has all the great functionalities notepad has.&lt;/li&gt;\n&lt;li&gt;I hope you didn&amp;#39;t name one of your workspaces incorrectly. Oh you did? RIP, time to remake it.&lt;/li&gt;\n&lt;li&gt;Local timezones? You mean UTC? What do you mean + or -? You craycray!&lt;/li&gt;\n&lt;li&gt;Nesting loops or if tests can&amp;#39;t be done. You need to make separate pipelines for it and before someone asks: no, just because it&amp;#39;s a nesting it REALLY doesn&amp;#39;t mean it belongs in a separate pipeline.&lt;/li&gt;\n&lt;li&gt;Speaking about nesting: if you use an if test in a foreach, you can&amp;#39;t access the current item of the loop. Haven&amp;#39;t you learned by now? You&amp;#39;re using Synapse. Now eat shit.&lt;/li&gt;\n&lt;li&gt;Self-Hosted Integration Runtimes cannot be shared, while you can actually do that in data factory. This means you need to run three separate runtimes just to get to on-premise sources.&lt;/li&gt;\n&lt;li&gt;On the topic of self-hosted integration runtimes: why do you even need them at all? Why is it not possible to peer a vnet that contains synapse and be done with it?&lt;/li&gt;\n&lt;li&gt;What&amp;#39;s even the use of the SQL scripts if you can&amp;#39;t access them from pipelines. You get the script activity, but that&amp;#39;s just another fucking place to dump your SQL in. And if you dare to suggest to use the API to trigger/request the SQL script. I will adopt a dog just to feed you to it.&lt;/li&gt;\n&lt;li&gt;Testing? HAHAHAHAHAHAHAHAHAHA- fuck you.&lt;/li&gt;\n&lt;li&gt;You want to rename a variable? Lol you little shit now you have to search for every instance to rename it. Refactoring? Just become a 100x dev yo.&lt;/li&gt;\n&lt;li&gt;Parameters in pipeline templates? Why would you want parameters in your pipeline templates? You talk like you want to reuse the code you wrote or something. Oh you do? ........oof.&lt;/li&gt;\n&lt;li&gt;Git....lab? Sounds like a dangerous cocaine facility. Hope you weren&amp;#39;t planning to attach that to synapse! Or like literally any other option since you have to attach git to synapse in the first place.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;These are the ones from THIS year, so after one week. I have had the displeasure of using it for a year now. So here&amp;#39;s my advice after this long-winded rant: use it for a quick prototype of anything and don&amp;#39;t use it for anything bigger than that. If you do, you have learned nothing of all the improvements that people have brought to the art that is development and you should probably touch grass instead of sucking cock on LinkedIn.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "107rt91", "is_robot_indexable": true, "report_reasons": null, "author": "AirisuB", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107rt91/azure_synapse_analytics_is_absolute_trash_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107rt91/azure_synapse_analytics_is_absolute_trash_for/", "subreddit_subscribers": 85888, "created_utc": 1673302334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys , I just got bombared with these questions that was waaay out of my league and expectations but I would like to continue learning about them and know how to prepare for any incoming questions like this in the future .\n\nFor context , the company that interviewed me is a AI &amp; Big Data Analytics company with products such as Fraud Risk Detection ,  Debt collection intelligence software  and etc.\n\n1.\u00a0\u00a0\u00a0\u00a0\u00a0How do you think external unstructured data can be used to augment internal data? What kind of additional insights can we derive from this ?\n\n2.\u00a0\u00a0\u00a0\u00a0The company  analyzes internal/external data and use the results in our proprietary influence platform to motivate users via a game-like process. Please think of and share two potential use cases for such a process.\n\n3.\u00a0\u00a0\u00a0\u00a0\u00a0The company  has a  project from an insurance firm to identify existing customers to be consider for upselling/cross-selling purposes (*a sales technique where a seller invites the customer to purchase more expensive items*) . They are willing to provide all kinds of data required, but these exist in data silos  without a data dictionary. What are the steps needed to begin and follow to achieve this ? ", "author_fullname": "t2_cl5vl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggled with DE interview questions as a Junior DE , any given perspectives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10801i2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673326809.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673323653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys , I just got bombared with these questions that was waaay out of my league and expectations but I would like to continue learning about them and know how to prepare for any incoming questions like this in the future .&lt;/p&gt;\n\n&lt;p&gt;For context , the company that interviewed me is a AI &amp;amp; Big Data Analytics company with products such as Fraud Risk Detection ,  Debt collection intelligence software  and etc.&lt;/p&gt;\n\n&lt;p&gt;1.\u00a0\u00a0\u00a0\u00a0\u00a0How do you think external unstructured data can be used to augment internal data? What kind of additional insights can we derive from this ?&lt;/p&gt;\n\n&lt;p&gt;2.\u00a0\u00a0\u00a0\u00a0The company  analyzes internal/external data and use the results in our proprietary influence platform to motivate users via a game-like process. Please think of and share two potential use cases for such a process.&lt;/p&gt;\n\n&lt;p&gt;3.\u00a0\u00a0\u00a0\u00a0\u00a0The company  has a  project from an insurance firm to identify existing customers to be consider for upselling/cross-selling purposes (&lt;em&gt;a sales technique where a seller invites the customer to purchase more expensive items&lt;/em&gt;) . They are willing to provide all kinds of data required, but these exist in data silos  without a data dictionary. What are the steps needed to begin and follow to achieve this ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10801i2", "is_robot_indexable": true, "report_reasons": null, "author": "Redxer", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10801i2/struggled_with_de_interview_questions_as_a_junior/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10801i2/struggled_with_de_interview_questions_as_a_junior/", "subreddit_subscribers": 85888, "created_utc": 1673323653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://vast.io/blog/parquet-and-feather-data-engineering-woes](https://vast.io/blog/parquet-and-feather-data-engineering-woes)\n\nNice article discussing the experience of integrating Parquet and Feather.", "author_fullname": "t2_4g2a7kl3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parquet &amp; Feather: Data Engineering Woes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108a9rt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673358558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://vast.io/blog/parquet-and-feather-data-engineering-woes\"&gt;https://vast.io/blog/parquet-and-feather-data-engineering-woes&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Nice article discussing the experience of integrating Parquet and Feather.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "108a9rt", "is_robot_indexable": true, "report_reasons": null, "author": "PsychologicalLoss829", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108a9rt/parquet_feather_data_engineering_woes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108a9rt/parquet_feather_data_engineering_woes/", "subreddit_subscribers": 85888, "created_utc": 1673358558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. After seeing /u/infiniteAggression- [pipeline for scraping Crinacle's headphone review site](https://www.reddit.com/r/dataengineering/comments/xyxpku/built_and_automated_a_complete_endtoend_elt/), I was inspired to do something similar to bolster my resume and hopefully upskill myself enough to land a better data-adjacent job! This is going to be the first \"real\" DE project that I will be working on, as well as learning about a cloud provider (GCP) from scratch. If anyone has pointers or advice they can share with me, I would greatly appreciate it. I've been reading around this subreddit for a while now, and I'm very excited to build something I can share with the fine folks here!  \n\nFor my project, I'm planning on creating a metabase dashboard (ironically, I work for one of their competitors \ud83d\ude06) that displays metrics from trending shows for each week and what streaming platforms they're **currently** available on. I don't exactly trust the trending sections from any of the streaming providers, so that's why I want to make my own. Also, it would be super helpful to know what's the hottest shows on each platform and whether or it's enough to justify resubbing to them. That way I don't need to sift through SEO garbage articles on /r/television.  \n\nI plan to host the dashboard on a micro-e2 VM; that way anyone looking at my resume can just click a link and interact with the dashboard without needing to `git clone` my repo and do any local setup.  \n\n### Pipeline\n1. make a weekly API request to fetch the top N trending TV shows from [TMDb](https://developers.themoviedb.org/3/trending/get-trending)\n2. dump the weekly trending shows into a Google cloud storage bucket\n3. append trending data files into a bigquery table. This will be a weekly grained fact table.\n4. run a `SELECT DISTINCT show_id` on the trending show table to get a list of unique show IDs that I can pass to this [endpoint](https://developers.themoviedb.org/3/tv/get-tv-details) to get all data needed to populate dimension tables (show details, genre, available watch providers, etc...)\n5. dump dimension table data into the same bucket from above and create the dimension tables in BQ\n6. use dbt to create models for Metabase to query on\n\n### Scalability Issues\nInitially, I was thinking of following /u/infiniteAggression- 's  idea of deploying an Airflow server on the micro-e2 VM that Metabase will be hosted on. However, I think this VM is too \"small\" to actually host Metabase as well as Airflow.  \n\nThe number of unique show IDs from the fact table will grow O(n)--which means the number of API calls needed to upsert dimension tables will increase linearly. O(n) is the worst case, as the same show can show up this week and in any of the following weeks. Scalability probably won't be *too* terrible, but it's still something for me to keep in mind.\n### Workaround\nAfter more research, I think using Google's CRON scheduler+cloud functions to handle data ingestion would be a better idea. That way, resources are freed up for the VM to solely host Metabase.  \n\nTo keep the number of API calls from blowing up, my idea is to delete records from my fact table after some condition is met. My two ideas are:\n\n1. delete records after N months from date of ingestion\n2. if the number of unique show IDs are &gt; some threshold, then delete records after N months from date of ingestion", "author_fullname": "t2_jk5r3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline architecture advice for my first side project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107rpu4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673333503.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673302123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. After seeing &lt;a href=\"/u/infiniteAggression-\"&gt;/u/infiniteAggression-&lt;/a&gt; &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/xyxpku/built_and_automated_a_complete_endtoend_elt/\"&gt;pipeline for scraping Crinacle&amp;#39;s headphone review site&lt;/a&gt;, I was inspired to do something similar to bolster my resume and hopefully upskill myself enough to land a better data-adjacent job! This is going to be the first &amp;quot;real&amp;quot; DE project that I will be working on, as well as learning about a cloud provider (GCP) from scratch. If anyone has pointers or advice they can share with me, I would greatly appreciate it. I&amp;#39;ve been reading around this subreddit for a while now, and I&amp;#39;m very excited to build something I can share with the fine folks here!  &lt;/p&gt;\n\n&lt;p&gt;For my project, I&amp;#39;m planning on creating a metabase dashboard (ironically, I work for one of their competitors \ud83d\ude06) that displays metrics from trending shows for each week and what streaming platforms they&amp;#39;re &lt;strong&gt;currently&lt;/strong&gt; available on. I don&amp;#39;t exactly trust the trending sections from any of the streaming providers, so that&amp;#39;s why I want to make my own. Also, it would be super helpful to know what&amp;#39;s the hottest shows on each platform and whether or it&amp;#39;s enough to justify resubbing to them. That way I don&amp;#39;t need to sift through SEO garbage articles on &lt;a href=\"/r/television\"&gt;/r/television&lt;/a&gt;.  &lt;/p&gt;\n\n&lt;p&gt;I plan to host the dashboard on a micro-e2 VM; that way anyone looking at my resume can just click a link and interact with the dashboard without needing to &lt;code&gt;git clone&lt;/code&gt; my repo and do any local setup.  &lt;/p&gt;\n\n&lt;h3&gt;Pipeline&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;make a weekly API request to fetch the top N trending TV shows from &lt;a href=\"https://developers.themoviedb.org/3/trending/get-trending\"&gt;TMDb&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;dump the weekly trending shows into a Google cloud storage bucket&lt;/li&gt;\n&lt;li&gt;append trending data files into a bigquery table. This will be a weekly grained fact table.&lt;/li&gt;\n&lt;li&gt;run a &lt;code&gt;SELECT DISTINCT show_id&lt;/code&gt; on the trending show table to get a list of unique show IDs that I can pass to this &lt;a href=\"https://developers.themoviedb.org/3/tv/get-tv-details\"&gt;endpoint&lt;/a&gt; to get all data needed to populate dimension tables (show details, genre, available watch providers, etc...)&lt;/li&gt;\n&lt;li&gt;dump dimension table data into the same bucket from above and create the dimension tables in BQ&lt;/li&gt;\n&lt;li&gt;use dbt to create models for Metabase to query on&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h3&gt;Scalability Issues&lt;/h3&gt;\n\n&lt;p&gt;Initially, I was thinking of following &lt;a href=\"/u/infiniteAggression-\"&gt;/u/infiniteAggression-&lt;/a&gt; &amp;#39;s  idea of deploying an Airflow server on the micro-e2 VM that Metabase will be hosted on. However, I think this VM is too &amp;quot;small&amp;quot; to actually host Metabase as well as Airflow.  &lt;/p&gt;\n\n&lt;p&gt;The number of unique show IDs from the fact table will grow O(n)--which means the number of API calls needed to upsert dimension tables will increase linearly. O(n) is the worst case, as the same show can show up this week and in any of the following weeks. Scalability probably won&amp;#39;t be &lt;em&gt;too&lt;/em&gt; terrible, but it&amp;#39;s still something for me to keep in mind.&lt;/p&gt;\n\n&lt;h3&gt;Workaround&lt;/h3&gt;\n\n&lt;p&gt;After more research, I think using Google&amp;#39;s CRON scheduler+cloud functions to handle data ingestion would be a better idea. That way, resources are freed up for the VM to solely host Metabase.  &lt;/p&gt;\n\n&lt;p&gt;To keep the number of API calls from blowing up, my idea is to delete records from my fact table after some condition is met. My two ideas are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;delete records after N months from date of ingestion&lt;/li&gt;\n&lt;li&gt;if the number of unique show IDs are &amp;gt; some threshold, then delete records after N months from date of ingestion&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "107rpu4", "is_robot_indexable": true, "report_reasons": null, "author": "bl4ckCloudz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107rpu4/pipeline_architecture_advice_for_my_first_side/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107rpu4/pipeline_architecture_advice_for_my_first_side/", "subreddit_subscribers": 85888, "created_utc": 1673302123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a data warehouse on Google Cloud that I ingest several data sources into. I am also hosting Apache Superset on Google Cloud to visualize data from this data warehouse. \n\nShould I have my data warehouse and Apache Superset deployment in a single project or is it best to split them into two different projects? One project makes sense in that everything is essentially in a single directory, but not sure this is the best practice as some of the data in the data warehouse may not be used in Superset.", "author_fullname": "t2_72tiq3y1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Cloud Data Warehousing - Best Practice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107nzje", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673293820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data warehouse on Google Cloud that I ingest several data sources into. I am also hosting Apache Superset on Google Cloud to visualize data from this data warehouse. &lt;/p&gt;\n\n&lt;p&gt;Should I have my data warehouse and Apache Superset deployment in a single project or is it best to split them into two different projects? One project makes sense in that everything is essentially in a single directory, but not sure this is the best practice as some of the data in the data warehouse may not be used in Superset.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "107nzje", "is_robot_indexable": true, "report_reasons": null, "author": "ROCKITZ15", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107nzje/google_cloud_data_warehousing_best_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107nzje/google_cloud_data_warehousing_best_practice/", "subreddit_subscribers": 85888, "created_utc": 1673293820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to data engineering and I have learnt about the Parquet file. I have been able to take a csv file and using pandas and pyarrow, create a Parquet file.\n\nMy question now is, in real life/production/work environment how are Parquet files handled and stored? I can imagine in these environments, data will be retrieved from various sources: sensors, apis, databases etc and needs to be converted to Parquet files to enable analysis. But how will these files be managed?\n\nDo I need to create and manage directories filled with Parquet files? Or are there software systems that abstracts this away from you?", "author_fullname": "t2_3euic3tq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are Parquet files managed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1083os1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673335522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to data engineering and I have learnt about the Parquet file. I have been able to take a csv file and using pandas and pyarrow, create a Parquet file.&lt;/p&gt;\n\n&lt;p&gt;My question now is, in real life/production/work environment how are Parquet files handled and stored? I can imagine in these environments, data will be retrieved from various sources: sensors, apis, databases etc and needs to be converted to Parquet files to enable analysis. But how will these files be managed?&lt;/p&gt;\n\n&lt;p&gt;Do I need to create and manage directories filled with Parquet files? Or are there software systems that abstracts this away from you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1083os1", "is_robot_indexable": true, "report_reasons": null, "author": "finlaydotweber", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1083os1/how_are_parquet_files_managed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1083os1/how_are_parquet_files_managed/", "subreddit_subscribers": 85888, "created_utc": 1673335522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have always been a supporter of learning while doing and at 27 still support the idea that more or less you can do this job without a CS degree. That of course if you are motivated enough, like the job and don\u2019t mind losing your sleep over it from time to time because of learning and imposter syndrome.\n\nBut I (BD in finance and MD in Data science that were almost useless, with hindsight) also always felt like I\u2019m losing something without a CS degree. I\u2019m mulling over the idea of starting one, but i\u2019m pretty unsure about the benefits it gives compared to those I could get by using that time and money (about 6k \u20ac and maybe 5 years of studying while employed full time) for other activities (which ones?).\n\nAny thoughts or suggestions based on your experience?\n\nEDIT: forgot to mention that I work as a DE", "author_fullname": "t2_d0ifg2cb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts about CS and similar degrees in DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107m88k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673298604.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673289830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have always been a supporter of learning while doing and at 27 still support the idea that more or less you can do this job without a CS degree. That of course if you are motivated enough, like the job and don\u2019t mind losing your sleep over it from time to time because of learning and imposter syndrome.&lt;/p&gt;\n\n&lt;p&gt;But I (BD in finance and MD in Data science that were almost useless, with hindsight) also always felt like I\u2019m losing something without a CS degree. I\u2019m mulling over the idea of starting one, but i\u2019m pretty unsure about the benefits it gives compared to those I could get by using that time and money (about 6k \u20ac and maybe 5 years of studying while employed full time) for other activities (which ones?).&lt;/p&gt;\n\n&lt;p&gt;Any thoughts or suggestions based on your experience?&lt;/p&gt;\n\n&lt;p&gt;EDIT: forgot to mention that I work as a DE&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "107m88k", "is_robot_indexable": true, "report_reasons": null, "author": "Awkward-Cupcake6219", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107m88k/thoughts_about_cs_and_similar_degrees_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107m88k/thoughts_about_cs_and_similar_degrees_in_de/", "subreddit_subscribers": 85888, "created_utc": 1673289830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_14i1sz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Faster PostgreSQL To BigQuery Transfers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_108fdj7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1673371433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tech.marksblogg.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tech.marksblogg.com/postgresql-to-bigquery.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "108fdj7", "is_robot_indexable": true, "report_reasons": null, "author": "pescennius", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108fdj7/faster_postgresql_to_bigquery_transfers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tech.marksblogg.com/postgresql-to-bigquery.html", "subreddit_subscribers": 85888, "created_utc": 1673371433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI am implementing an ETL streaming job using Spark Structured Streaming. This job takes data from a kafka topic and stores it into separate Delta tables according to the value of a specific field.\n\nI keep the latest version for each row, and right now I am using the DeltaTable's merge function to upsert data into the destination table. I am already optimizing by using partition columns in the merge condition, and by doing so I am getting a latency in the order of 10 minutes.\n\nI was wondering if an \"append-only\" approach to the tables would decrease the latency or not.\n\nThe steps would be as follows:\n\nIn the batch, for each Delta table\n\n1. Calculate the latest versions for each event in the batch and keep them\n2. Append the latest event versions to the delta table\n3. Read the table partitions containing changes\n4. Filter the data to keep only the latest event versions\n5. Overwrite the involved table partitions using `replaceWhere`\n\nThe idea here is to avoid any joins that delta's merge [does](https://www.databricks.com/wp-content/uploads/2020/09/blog-diving-delta-4.png).\n\n&amp;#x200B;\n\nHas anyone tried this approach? Do you think it will give some performance gains?\n\nI'm wondering if I am reinventing the wheel and Delta is already optimized or not, or if there are some corner cases I am not considering.\n\n&amp;#x200B;\n\nEDIT1: The way I'd keep the latest event versions would be a window function:\n\nrow\\_number partitioning by key and ordering by event timestamp in descending order, then I'd keep only the rows where the value equals 1.", "author_fullname": "t2_do2bj7xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta's merge vs overwriting with replaceWhere", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10857r4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673343815.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673341273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am implementing an ETL streaming job using Spark Structured Streaming. This job takes data from a kafka topic and stores it into separate Delta tables according to the value of a specific field.&lt;/p&gt;\n\n&lt;p&gt;I keep the latest version for each row, and right now I am using the DeltaTable&amp;#39;s merge function to upsert data into the destination table. I am already optimizing by using partition columns in the merge condition, and by doing so I am getting a latency in the order of 10 minutes.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if an &amp;quot;append-only&amp;quot; approach to the tables would decrease the latency or not.&lt;/p&gt;\n\n&lt;p&gt;The steps would be as follows:&lt;/p&gt;\n\n&lt;p&gt;In the batch, for each Delta table&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Calculate the latest versions for each event in the batch and keep them&lt;/li&gt;\n&lt;li&gt;Append the latest event versions to the delta table&lt;/li&gt;\n&lt;li&gt;Read the table partitions containing changes&lt;/li&gt;\n&lt;li&gt;Filter the data to keep only the latest event versions&lt;/li&gt;\n&lt;li&gt;Overwrite the involved table partitions using &lt;code&gt;replaceWhere&lt;/code&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The idea here is to avoid any joins that delta&amp;#39;s merge &lt;a href=\"https://www.databricks.com/wp-content/uploads/2020/09/blog-diving-delta-4.png\"&gt;does&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anyone tried this approach? Do you think it will give some performance gains?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if I am reinventing the wheel and Delta is already optimized or not, or if there are some corner cases I am not considering.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EDIT1: The way I&amp;#39;d keep the latest event versions would be a window function:&lt;/p&gt;\n\n&lt;p&gt;row_number partitioning by key and ordering by event timestamp in descending order, then I&amp;#39;d keep only the rows where the value equals 1.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?auto=webp&amp;v=enabled&amp;s=d87b9e1343bbe01113f9a5de248fda2154f4219f", "width": 864, "height": 487}, "resolutions": [{"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e4e607e4058490f4ca840cb1f85a678abd05adc", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f7613e6e083214441d71b5fe6259231bdde8dc1", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b86406f74a2bcce3bd2e75d927cbbf74c0a66430", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/6ENWyoV9Rp4ykyJ0aEcqquncv4lVJ8KlgxALvd2VR6A.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4408ac85003d8739b6be4eea63d6b673463bb9a4", "width": 640, "height": 360}], "variants": {}, "id": "RHUM16d2JyL38mWZUyE_7-xAj3xsCEo-fboQVyXjq1M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10857r4", "is_robot_indexable": true, "report_reasons": null, "author": "somerandomdataeng", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10857r4/deltas_merge_vs_overwriting_with_replacewhere/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10857r4/deltas_merge_vs_overwriting_with_replacewhere/", "subreddit_subscribers": 85888, "created_utc": 1673341273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my previous job we used SSIS to do some minor transformations to raw data and upload to our 'staging zones' in SQL Server, whereupon sprocs would run periodically and create DWH objects for analysts. Boring tech stack maybe but we had ample room to munge the data at any point before/after it hit our DBs if we wanted.\n\nAt my new job, they use Segment to directly load ('sync') data to Snowflake, and although I can see the queries that Segment is executing in Snowflake's query history, I seem unable to actually tinker with the queries in Segment -- am I missing something here? Or is this because Segment is a low-code 'plug and play' kind of deal? \n\nIf we wanted greater low-level access (i.e. ability to code transformations) to the data Segment is handling before it hits our DBs, do you think looking into Kafka/Kinesis and Snowpipe as an intermediary stage would be a good thing to explore? I'm entirely new to Snowflake and Segment here so I'd appreciate any information.", "author_fullname": "t2_418hrmde", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone used Segment with Snowflake before? What are the benefits/challenges?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_108ecbt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673368990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my previous job we used SSIS to do some minor transformations to raw data and upload to our &amp;#39;staging zones&amp;#39; in SQL Server, whereupon sprocs would run periodically and create DWH objects for analysts. Boring tech stack maybe but we had ample room to munge the data at any point before/after it hit our DBs if we wanted.&lt;/p&gt;\n\n&lt;p&gt;At my new job, they use Segment to directly load (&amp;#39;sync&amp;#39;) data to Snowflake, and although I can see the queries that Segment is executing in Snowflake&amp;#39;s query history, I seem unable to actually tinker with the queries in Segment -- am I missing something here? Or is this because Segment is a low-code &amp;#39;plug and play&amp;#39; kind of deal? &lt;/p&gt;\n\n&lt;p&gt;If we wanted greater low-level access (i.e. ability to code transformations) to the data Segment is handling before it hits our DBs, do you think looking into Kafka/Kinesis and Snowpipe as an intermediary stage would be a good thing to explore? I&amp;#39;m entirely new to Snowflake and Segment here so I&amp;#39;d appreciate any information.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "108ecbt", "is_robot_indexable": true, "report_reasons": null, "author": "Glaukosss", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108ecbt/anyone_used_segment_with_snowflake_before_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108ecbt/anyone_used_segment_with_snowflake_before_what/", "subreddit_subscribers": 85888, "created_utc": 1673368990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I\u2019m new to the DE world and I\u2019ve been tasked with essentially creating a small scale MES for our plant (structured production data and a few dashboards). \n\nInitially I thought it made sense to collect data by machine because each machine has slightly different data that needs to be recorded. Then I realized there was also a metric I needed by line that could only be calculated using data from all machines. Which makes me believe the data should be organized by line.\n\n But then I realize sometimes machines switch lines which would mess up the whole table for that line. There\u2019s also some data like hit count which could easily be captured only once every hour whereas something like vibration needs to be captured every second. Any advice or sources discussing this topic?", "author_fullname": "t2_fgc39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is manufacturing IoT data typically organized?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107srly", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673304479.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019m new to the DE world and I\u2019ve been tasked with essentially creating a small scale MES for our plant (structured production data and a few dashboards). &lt;/p&gt;\n\n&lt;p&gt;Initially I thought it made sense to collect data by machine because each machine has slightly different data that needs to be recorded. Then I realized there was also a metric I needed by line that could only be calculated using data from all machines. Which makes me believe the data should be organized by line.&lt;/p&gt;\n\n&lt;p&gt;But then I realize sometimes machines switch lines which would mess up the whole table for that line. There\u2019s also some data like hit count which could easily be captured only once every hour whereas something like vibration needs to be captured every second. Any advice or sources discussing this topic?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "107srly", "is_robot_indexable": true, "report_reasons": null, "author": "TheOnlinePolak", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107srly/how_is_manufacturing_iot_data_typically_organized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107srly/how_is_manufacturing_iot_data_typically_organized/", "subreddit_subscribers": 85888, "created_utc": 1673304479.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are happy to announce the release of Memphis v0.4.3!!\n\n&amp;#x200B;\n\nIn this release, we mainly focus on becoming compatible with NATS ecosystem, improving performance, more coverage of our famous dead-letter, and improving DevEx, \n\nto: \n\na) Utilize NATS broad ecosystem. \n\nb) Save precious dev time. \n\nc) Move faster.\n\n&amp;#x200B;\n\nThe main features added are:\n\n\\- Memphis is now compatible with all NATS SDKs\n\n\\- Schemaverse now supports GraphQL\n\n\\- Ability to filter system logs by type.\n\n\\- HTTP proxy. Generate access tokens with a dynamic expiration time.\n\n\\- Controlling messages types via triggers and supports storing messages that violate an attached schema. \n\n\\- Schemaverse users using Go SDK can send protobuf messages without storing the schemas locally.\n\n&amp;#x200B;\n\nFind more information on our release notes :arrow\\_down: \n\n[https://docs.memphis.dev/memphis/release-notes/releases/v0.4.3-beta](https://docs.memphis.dev/memphis/release-notes/releases/v0.4.3-beta)", "author_fullname": "t2_bqjrmnud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time data processing platform v0.4.3 is out!!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108e3fk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673368375.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are happy to announce the release of Memphis v0.4.3!!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In this release, we mainly focus on becoming compatible with NATS ecosystem, improving performance, more coverage of our famous dead-letter, and improving DevEx, &lt;/p&gt;\n\n&lt;p&gt;to: &lt;/p&gt;\n\n&lt;p&gt;a) Utilize NATS broad ecosystem. &lt;/p&gt;\n\n&lt;p&gt;b) Save precious dev time. &lt;/p&gt;\n\n&lt;p&gt;c) Move faster.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The main features added are:&lt;/p&gt;\n\n&lt;p&gt;- Memphis is now compatible with all NATS SDKs&lt;/p&gt;\n\n&lt;p&gt;- Schemaverse now supports GraphQL&lt;/p&gt;\n\n&lt;p&gt;- Ability to filter system logs by type.&lt;/p&gt;\n\n&lt;p&gt;- HTTP proxy. Generate access tokens with a dynamic expiration time.&lt;/p&gt;\n\n&lt;p&gt;- Controlling messages types via triggers and supports storing messages that violate an attached schema. &lt;/p&gt;\n\n&lt;p&gt;- Schemaverse users using Go SDK can send protobuf messages without storing the schemas locally.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Find more information on our release notes :arrow_down: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.memphis.dev/memphis/release-notes/releases/v0.4.3-beta\"&gt;https://docs.memphis.dev/memphis/release-notes/releases/v0.4.3-beta&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nY5H4mYxVrENamwyfiSFWZqyadYlQ6miv8aQjNjdg-Y.jpg?auto=webp&amp;v=enabled&amp;s=ea131f1654a6856c4e3af2e5aa0e98c86b647087", "width": 1137, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/nY5H4mYxVrENamwyfiSFWZqyadYlQ6miv8aQjNjdg-Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d36372ba6bc54ab1f0e591cda1129a9c1c94bf09", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/nY5H4mYxVrENamwyfiSFWZqyadYlQ6miv8aQjNjdg-Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c2de3d8b31d26a47f214dfd0c39163d2cf7884a4", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/nY5H4mYxVrENamwyfiSFWZqyadYlQ6miv8aQjNjdg-Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f141d217d6abbcd951cb0ccae9710628e2d2350", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/nY5H4mYxVrENamwyfiSFWZqyadYlQ6miv8aQjNjdg-Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=566a27bb06ce1dce6871d00bcdb624fae5aa360a", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/nY5H4mYxVrENamwyfiSFWZqyadYlQ6miv8aQjNjdg-Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dcd65ef62daaf08ff1cf1455b3ece4906ca61643", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/nY5H4mYxVrENamwyfiSFWZqyadYlQ6miv8aQjNjdg-Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=18cc31b351820a03ce681712796ad37915c5b1b2", "width": 1080, "height": 607}], "variants": {}, "id": "mx26JVaGlYYBIPS1LqUnS9J6mI7DKobHL4QQ785K03A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "108e3fk", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Bug105", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108e3fk/realtime_data_processing_platform_v043_is_out/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108e3fk/realtime_data_processing_platform_v043_is_out/", "subreddit_subscribers": 85888, "created_utc": 1673368375.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1bnhotlu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kubernetes for Data Science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 42, "top_awarded_type": null, "hide_score": false, "name": "t3_108crc6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/0-FVxHmIm7DmBIek9qgWAsvXVJFH36fbmjNxEN3LUC8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673365050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kanger.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://kanger.dev/kubernetes-for-data-science-practice/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B6mxUIcPmzG5y2ODMaUciFt698R0FPYi3vX9Xvsr6VM.jpg?auto=webp&amp;v=enabled&amp;s=773c8dda7588d6cdb666b996a01ebcdf3bdb3650", "width": 1184, "height": 358}, "resolutions": [{"url": "https://external-preview.redd.it/B6mxUIcPmzG5y2ODMaUciFt698R0FPYi3vX9Xvsr6VM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=792dbab352e30f633593140549300928aa698d74", "width": 108, "height": 32}, {"url": "https://external-preview.redd.it/B6mxUIcPmzG5y2ODMaUciFt698R0FPYi3vX9Xvsr6VM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90dd5a61977243b7b94957495530180f04c3b0ad", "width": 216, "height": 65}, {"url": "https://external-preview.redd.it/B6mxUIcPmzG5y2ODMaUciFt698R0FPYi3vX9Xvsr6VM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=122e1d4369b3f80a906fe2722f615c7b515aea24", "width": 320, "height": 96}, {"url": "https://external-preview.redd.it/B6mxUIcPmzG5y2ODMaUciFt698R0FPYi3vX9Xvsr6VM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e1c18324f7b60e28906e5a2f0eeda254a573301", "width": 640, "height": 193}, {"url": "https://external-preview.redd.it/B6mxUIcPmzG5y2ODMaUciFt698R0FPYi3vX9Xvsr6VM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a1b1c390a217b6e53dfd1a5ea220cfe6a7fbde9", "width": 960, "height": 290}, {"url": "https://external-preview.redd.it/B6mxUIcPmzG5y2ODMaUciFt698R0FPYi3vX9Xvsr6VM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd6118d998327074238e7756f52210daeac21c04", "width": 1080, "height": 326}], "variants": {}, "id": "tJvPkBie4qEWwialcqBlzzdQywHkzbd4ESKw3LUDe1M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "108crc6", "is_robot_indexable": true, "report_reasons": null, "author": "skj8", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108crc6/kubernetes_for_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://kanger.dev/kubernetes-for-data-science-practice/", "subreddit_subscribers": 85888, "created_utc": 1673365050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My Airflow task is uploading JSON files to BigQuery every 2 hours. It takes longer than an hour to upload them all so it times out.\n\nI was thinking of using the async library but not sure - Because it has to read each file into memory and convert it to a dictionary so wondering if I should use multiprocessing instead?\n\nThe reason for reading the file is because we add some extra fields to each file", "author_fullname": "t2_ggg0wfmt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Time Outs - BigQuery Upload", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1084axv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673337795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My Airflow task is uploading JSON files to BigQuery every 2 hours. It takes longer than an hour to upload them all so it times out.&lt;/p&gt;\n\n&lt;p&gt;I was thinking of using the async library but not sure - Because it has to read each file into memory and convert it to a dictionary so wondering if I should use multiprocessing instead?&lt;/p&gt;\n\n&lt;p&gt;The reason for reading the file is because we add some extra fields to each file&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1084axv", "is_robot_indexable": true, "report_reasons": null, "author": "camelCaseInsensitive", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1084axv/airflow_time_outs_bigquery_upload/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1084axv/airflow_time_outs_bigquery_upload/", "subreddit_subscribers": 85888, "created_utc": 1673337795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am thinking about taking a course on this topic and the book the class goes over is \u201cDatabase Management Systems (3rd ed) by Ramakrishnan and Gehrke\u201d. Should I focus on doing something like the data zoom camp instead and take one less class or stick to the class?", "author_fullname": "t2_cauyq044", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How relevant is database management systems for data engineering/data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10839qp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673334023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am thinking about taking a course on this topic and the book the class goes over is \u201cDatabase Management Systems (3rd ed) by Ramakrishnan and Gehrke\u201d. Should I focus on doing something like the data zoom camp instead and take one less class or stick to the class?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10839qp", "is_robot_indexable": true, "report_reasons": null, "author": "honey1337", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10839qp/how_relevant_is_database_management_systems_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10839qp/how_relevant_is_database_management_systems_for/", "subreddit_subscribers": 85888, "created_utc": 1673334023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm looking for a simple dataset catalog solution where we can store information (basic template and download link) about datasets/models and a couple of other data types. At the moment it's all maintained in excel and I'm looking for some simple and, if possible, accessible solutions to this problem so that different teams have some kind of data hub to share their data/outputs. So far the closest thing I found was CKAN, the other data catalogs include too much unnecessary functionality. Maybe I am looking for a too-basic case, and keeping everything in excel or writing simple solutions by ourselves will be a better option? Thank you in advance.", "author_fullname": "t2_zak9c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Very simple data catalog options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1082uea", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673332522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m looking for a simple dataset catalog solution where we can store information (basic template and download link) about datasets/models and a couple of other data types. At the moment it&amp;#39;s all maintained in excel and I&amp;#39;m looking for some simple and, if possible, accessible solutions to this problem so that different teams have some kind of data hub to share their data/outputs. So far the closest thing I found was CKAN, the other data catalogs include too much unnecessary functionality. Maybe I am looking for a too-basic case, and keeping everything in excel or writing simple solutions by ourselves will be a better option? Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1082uea", "is_robot_indexable": true, "report_reasons": null, "author": "pemens", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1082uea/very_simple_data_catalog_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1082uea/very_simple_data_catalog_options/", "subreddit_subscribers": 85888, "created_utc": 1673332522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello - I am looking to identify different storage services in SAP like S3, EBS, EFS in AWS. I found a lot of conflicting information and have come here for help.\n\nSo far I only have SAP Hana.", "author_fullname": "t2_uc7m4pgj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the different storage services in SAP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107v9jf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673310550.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello - I am looking to identify different storage services in SAP like S3, EBS, EFS in AWS. I found a lot of conflicting information and have come here for help.&lt;/p&gt;\n\n&lt;p&gt;So far I only have SAP Hana.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "107v9jf", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Packer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107v9jf/what_are_the_different_storage_services_in_sap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107v9jf/what_are_the_different_storage_services_in_sap/", "subreddit_subscribers": 85888, "created_utc": 1673310550.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was wondering if anyone on this sub has any experience working in a full-time job and as a freelancer/contractor. What mode of employment would you prefer in the data engineering space?\n\nPersonally I'm working full time with lots of benefits, but seeing those juicy hourly rates on contracting jobs makes me think if I should jump ship.", "author_fullname": "t2_l35gwhuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Full-Time Work Vs Freelance/Contracting Work", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107tpb1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673306663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if anyone on this sub has any experience working in a full-time job and as a freelancer/contractor. What mode of employment would you prefer in the data engineering space?&lt;/p&gt;\n\n&lt;p&gt;Personally I&amp;#39;m working full time with lots of benefits, but seeing those juicy hourly rates on contracting jobs makes me think if I should jump ship.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "107tpb1", "is_robot_indexable": true, "report_reasons": null, "author": "Senior_Anteater4688", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107tpb1/fulltime_work_vs_freelancecontracting_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107tpb1/fulltime_work_vs_freelancecontracting_work/", "subreddit_subscribers": 85888, "created_utc": 1673306663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a Synapse Serverless Pool setup that I am using to create parquet files in a ADLSGen2 container.  Is there any way to have a PySpark Notebook create a database view in a Serverless Pool?  \n\nI've been creating the parquet, then using a stored proc to create the view (in a pipeline), but is there a way to create a view right after the parquet files are created?  Or am I SOL because the notebook is connected to a Spark pool and can't do anything on the regular Serverless SQL pool because it's outside of Spark (without JDBC)?\n\nAny ideas?  I haven't gotten it to work.  Curious if it's theoretically possible.", "author_fullname": "t2_4270ek0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synapse Analytics - PySpark Notebooks and DDL statements", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107t6f0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673305430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Synapse Serverless Pool setup that I am using to create parquet files in a ADLSGen2 container.  Is there any way to have a PySpark Notebook create a database view in a Serverless Pool?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been creating the parquet, then using a stored proc to create the view (in a pipeline), but is there a way to create a view right after the parquet files are created?  Or am I SOL because the notebook is connected to a Spark pool and can&amp;#39;t do anything on the regular Serverless SQL pool because it&amp;#39;s outside of Spark (without JDBC)?&lt;/p&gt;\n\n&lt;p&gt;Any ideas?  I haven&amp;#39;t gotten it to work.  Curious if it&amp;#39;s theoretically possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "107t6f0", "is_robot_indexable": true, "report_reasons": null, "author": "mr_electric_wizard", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107t6f0/synapse_analytics_pyspark_notebooks_and_ddl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107t6f0/synapse_analytics_pyspark_notebooks_and_ddl/", "subreddit_subscribers": 85888, "created_utc": 1673305430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "let's say you want to pull rows from bigquery and put them into a redshift db - but you don't have access to gcp buckets.\n\nwhat would be the recommended methods to bring that data over, and how would they change with size in millions of rows: 1,10,50,100,500, 1billion, etc.\n\nif i hit bigquery for 15 million rows, put it in a polars dataframe, and then save that as a csv.  then load the csv to s3, then copy from redshift...that seems like a lot of moves, no?\n\nwould it be a good idea iterate over that bigquery result, and insert the data into redshift directly?\n\ntrying to understand how to design pipelines better rather than just getting it done.", "author_fullname": "t2_5qteskd9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "intermediary storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_107qs90", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673300070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;let&amp;#39;s say you want to pull rows from bigquery and put them into a redshift db - but you don&amp;#39;t have access to gcp buckets.&lt;/p&gt;\n\n&lt;p&gt;what would be the recommended methods to bring that data over, and how would they change with size in millions of rows: 1,10,50,100,500, 1billion, etc.&lt;/p&gt;\n\n&lt;p&gt;if i hit bigquery for 15 million rows, put it in a polars dataframe, and then save that as a csv.  then load the csv to s3, then copy from redshift...that seems like a lot of moves, no?&lt;/p&gt;\n\n&lt;p&gt;would it be a good idea iterate over that bigquery result, and insert the data into redshift directly?&lt;/p&gt;\n\n&lt;p&gt;trying to understand how to design pipelines better rather than just getting it done.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "107qs90", "is_robot_indexable": true, "report_reasons": null, "author": "iseestupid", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/107qs90/intermediary_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/107qs90/intermediary_storage/", "subreddit_subscribers": 85888, "created_utc": 1673300070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9dz8x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The easiest way to move data from MongoDB to a PostgreSQL DB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 94, "top_awarded_type": null, "hide_score": true, "name": "t3_108ef4b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/h_Pz_EKkGS-n_BWKGba3ZZ4yMKg9I3Gddlm5QzqiHLE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673369177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@henryivesjones/the-easiest-way-to-move-data-from-mongodb-to-a-postgresql-db-bc30846ac1b0", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/l3FjJ4b75QWKftB5myxTSEH7PUZYZ-TVWgVLJofbIDo.jpg?auto=webp&amp;v=enabled&amp;s=a0d1614a4bf46a7775639f63e6bd75d5510c957e", "width": 920, "height": 621}, "resolutions": [{"url": "https://external-preview.redd.it/l3FjJ4b75QWKftB5myxTSEH7PUZYZ-TVWgVLJofbIDo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eabb0302312d47b995fb33585f371d82db2d83c4", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/l3FjJ4b75QWKftB5myxTSEH7PUZYZ-TVWgVLJofbIDo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70cd943d2dd57e1a466af5d730671d21f6022b47", "width": 216, "height": 145}, {"url": "https://external-preview.redd.it/l3FjJ4b75QWKftB5myxTSEH7PUZYZ-TVWgVLJofbIDo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=047749125cc9fd9326429a95a874584e912db611", "width": 320, "height": 216}, {"url": "https://external-preview.redd.it/l3FjJ4b75QWKftB5myxTSEH7PUZYZ-TVWgVLJofbIDo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a3e514f2cfcf52944634fc1a498dd0778285cd6", "width": 640, "height": 432}], "variants": {}, "id": "O-WP4-c1TYayjTlfFwLLSbhh00h6ZAz8zGCDwlJiEH4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "108ef4b", "is_robot_indexable": true, "report_reasons": null, "author": "MidgetDufus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108ef4b/the_easiest_way_to_move_data_from_mongodb_to_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@henryivesjones/the-easiest-way-to-move-data-from-mongodb-to-a-postgresql-db-bc30846ac1b0", "subreddit_subscribers": 85888, "created_utc": 1673369177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve seen many posts on the subreddit about ELT, dbt and data warehousing, but I haven\u2019t seen a whole load of posts discussing orchestrators in an ML event-driven context.\n\nI\u2019ve had a lot of experience using Airflow and Dagster, but I know there are many more orchestrators out there I haven\u2019t had a chance to use, so wanted to get advice on which orchestrator would be appropriate for my current use case.\n\nUse case:\n\n1.\tFile arrives in an S3 bucket which kicks off various scripts\n2.\tAmong these scripts is sending the file to a ML model for inference\n3.\tAll scripts write the data extracted and inferred from file into a document in Elasticsearch\n\nWhich Orchestrator would be best for this? \n\n-\tIt needs to be event driven, so have solid support for sensors - instead of running periodically\n-\tIt needs to be able to initiate the scripts in Kubernetes or as Docker containers, or even initiate Lambda functions\n-\tIdeally it would support retrying singular tasks in the event of a task failure\n-\tIt needs lineage and task dependencies as there are currently race conditions between the scripts. This is the main reason I want an orchestrator\n-\tAnd finally it needs solid logging, monitoring and performance metrics\n\nRight now I\u2019m leaning towards Dagster over Airflow, but I can\u2019t speak to orchestrators like Prefect, Mage or Luigi because I\u2019ve never used them.", "author_fullname": "t2_9usjyayk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best orchestrator for event based ETL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_108bg7z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673361711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve seen many posts on the subreddit about ELT, dbt and data warehousing, but I haven\u2019t seen a whole load of posts discussing orchestrators in an ML event-driven context.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve had a lot of experience using Airflow and Dagster, but I know there are many more orchestrators out there I haven\u2019t had a chance to use, so wanted to get advice on which orchestrator would be appropriate for my current use case.&lt;/p&gt;\n\n&lt;p&gt;Use case:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; File arrives in an S3 bucket which kicks off various scripts&lt;/li&gt;\n&lt;li&gt; Among these scripts is sending the file to a ML model for inference&lt;/li&gt;\n&lt;li&gt; All scripts write the data extracted and inferred from file into a document in Elasticsearch&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Which Orchestrator would be best for this? &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;  It needs to be event driven, so have solid support for sensors - instead of running periodically&lt;/li&gt;\n&lt;li&gt;  It needs to be able to initiate the scripts in Kubernetes or as Docker containers, or even initiate Lambda functions&lt;/li&gt;\n&lt;li&gt;  Ideally it would support retrying singular tasks in the event of a task failure&lt;/li&gt;\n&lt;li&gt;  It needs lineage and task dependencies as there are currently race conditions between the scripts. This is the main reason I want an orchestrator&lt;/li&gt;\n&lt;li&gt;  And finally it needs solid logging, monitoring and performance metrics&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Right now I\u2019m leaning towards Dagster over Airflow, but I can\u2019t speak to orchestrators like Prefect, Mage or Luigi because I\u2019ve never used them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "108bg7z", "is_robot_indexable": true, "report_reasons": null, "author": "feathersurf", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/108bg7z/best_orchestrator_for_event_based_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/108bg7z/best_orchestrator_for_event_based_etl/", "subreddit_subscribers": 85888, "created_utc": 1673361711.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}