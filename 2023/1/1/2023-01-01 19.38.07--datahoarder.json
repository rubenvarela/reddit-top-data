{"kind": "Listing", "data": {"after": "t3_100jols", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good evening, datahoarders.\n\nOver the years I have posted here under various accounts, and lived various extremes of data hoarding. \n\nI have now reduced myself to a positively conservative 30TB in my main server, and 12 or so on my clients.\n\nIn the last 10 years I've used first Kodi and then Plex for my media server needs. Both worked, mostly.\n\nI remember checking in on this open-source flash in the pan called Jellyfin a few years back and moving back to Plex in a jiffy.\n\nWell, if you haven't tried it in a while...holy heck!\n\nI spent an hour or so today setting up a Jellyfin server and deployed the client app on all my TV's and computers etc.\n\nWow! That's about all I've got to say right now. I now have hardware acceleration. I now have the vast majority of my media playing with no transcoding whatsoever. I now have a client interface that is as fast as lightning and makes paid streaming services look like something for children.\n\nIn short, I'm blown away and so excited and nobody in my life really cares; you guys were the only people i could think of that might actually give half a rats.\n\nAnyway, if you haven't done so in a while - check out Jellyfin again. It's a whole new beast!", "author_fullname": "t2_r0lf8zqt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time to revisit Jellyfin", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100hye7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 310, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 310, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672573313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good evening, datahoarders.&lt;/p&gt;\n\n&lt;p&gt;Over the years I have posted here under various accounts, and lived various extremes of data hoarding. &lt;/p&gt;\n\n&lt;p&gt;I have now reduced myself to a positively conservative 30TB in my main server, and 12 or so on my clients.&lt;/p&gt;\n\n&lt;p&gt;In the last 10 years I&amp;#39;ve used first Kodi and then Plex for my media server needs. Both worked, mostly.&lt;/p&gt;\n\n&lt;p&gt;I remember checking in on this open-source flash in the pan called Jellyfin a few years back and moving back to Plex in a jiffy.&lt;/p&gt;\n\n&lt;p&gt;Well, if you haven&amp;#39;t tried it in a while...holy heck!&lt;/p&gt;\n\n&lt;p&gt;I spent an hour or so today setting up a Jellyfin server and deployed the client app on all my TV&amp;#39;s and computers etc.&lt;/p&gt;\n\n&lt;p&gt;Wow! That&amp;#39;s about all I&amp;#39;ve got to say right now. I now have hardware acceleration. I now have the vast majority of my media playing with no transcoding whatsoever. I now have a client interface that is as fast as lightning and makes paid streaming services look like something for children.&lt;/p&gt;\n\n&lt;p&gt;In short, I&amp;#39;m blown away and so excited and nobody in my life really cares; you guys were the only people i could think of that might actually give half a rats.&lt;/p&gt;\n\n&lt;p&gt;Anyway, if you haven&amp;#39;t done so in a while - check out Jellyfin again. It&amp;#39;s a whole new beast!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 300, "id": "award_725b427d-320b-4d02-8fb0-8bb7aa7b78aa", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=16&amp;height=16&amp;auto=webp&amp;s=b3bb991aac7c446063cc3b91d71d8547db0f7d6d", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=32&amp;height=32&amp;auto=webp&amp;s=881b998ff73380d3f02d27e7536aba842df055c1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=48&amp;height=48&amp;auto=webp&amp;s=325a8549233c6457eaf4eaef948230af4d062f0a", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=64&amp;height=64&amp;auto=webp&amp;s=9a5261140af96699d24ded7497d3b10c831464ba", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=128&amp;height=128&amp;auto=webp&amp;s=6069896f540b6928b86a55082ae4d55f823ce094", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Sometimes you just got to doot.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Updoot", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=16&amp;height=16&amp;auto=webp&amp;s=b3bb991aac7c446063cc3b91d71d8547db0f7d6d", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=32&amp;height=32&amp;auto=webp&amp;s=881b998ff73380d3f02d27e7536aba842df055c1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=48&amp;height=48&amp;auto=webp&amp;s=325a8549233c6457eaf4eaef948230af4d062f0a", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=64&amp;height=64&amp;auto=webp&amp;s=9a5261140af96699d24ded7497d3b10c831464ba", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=128&amp;height=128&amp;auto=webp&amp;s=6069896f540b6928b86a55082ae4d55f823ce094", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100hye7", "is_robot_indexable": true, "report_reasons": null, "author": "stopbanningmeplzfoo", "discussion_type": null, "num_comments": 182, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100hye7/time_to_revisit_jellyfin/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100hye7/time_to_revisit_jellyfin/", "subreddit_subscribers": 663235, "created_utc": 1672573313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_f6rxi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi guys, I'm just starting here. 4x8TB, RAID5, for my terramaster nas. I'll put linux on it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1009h4u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 108, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 108, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/u7nsiU8ZkiYm08JfnxEM9-fFELHaW3b5fgQ2fCLid50.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1672538901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/afkn3m1cbc9a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/afkn3m1cbc9a1.jpg?auto=webp&amp;s=07d6b1e1c1036c7830bd6db0b7f0ca8a67b92b40", "width": 3024, "height": 4032}, "resolutions": [{"url": "https://preview.redd.it/afkn3m1cbc9a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5beccf0dd9ea1562e8b6ccdffb7bacafb017d68a", "width": 108, "height": 144}, {"url": "https://preview.redd.it/afkn3m1cbc9a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e4ed379ce58c7973e47fc3858e4a8c86de103e9", "width": 216, "height": 288}, {"url": "https://preview.redd.it/afkn3m1cbc9a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=03d290ae8f0ed14711374f7e227e28e9a4871232", "width": 320, "height": 426}, {"url": "https://preview.redd.it/afkn3m1cbc9a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e3a7e91064bd348137cc84ce293e7a4c25e6878", "width": 640, "height": 853}, {"url": "https://preview.redd.it/afkn3m1cbc9a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8a1830f6178de94e86be843e45ea05eae31071b", "width": 960, "height": 1280}, {"url": "https://preview.redd.it/afkn3m1cbc9a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1501bce34f91b1406aa404f5cffab51b1b26d28c", "width": 1080, "height": 1440}], "variants": {}, "id": "4UsOfBjvoMFASt_nvGrBt7xBAzxsmQPbtlkmn2_8Vos"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1009h4u", "is_robot_indexable": true, "report_reasons": null, "author": "TheCharon77", "discussion_type": null, "num_comments": 29, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1009h4u/hi_guys_im_just_starting_here_4x8tb_raid5_for_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/afkn3m1cbc9a1.jpg", "subreddit_subscribers": 663235, "created_utc": 1672538901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_98m1og82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Canada] Is this too good to be true?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 69, "top_awarded_type": null, "hide_score": false, "name": "t3_10076nf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 80, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 80, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iGQCiZNGZAMDruKUcSrqjJrLnDNkdYure4PUR2pS9_U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1672531409.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1sgndwxhob9a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1sgndwxhob9a1.png?auto=webp&amp;s=e89515c467b7444c786ee7c4b52f94643bca129c", "width": 915, "height": 455}, "resolutions": [{"url": "https://preview.redd.it/1sgndwxhob9a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=749613691394a8f89d1a7dd6a0f1b04ed44b09ea", "width": 108, "height": 53}, {"url": "https://preview.redd.it/1sgndwxhob9a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=13a38269a0d014bee7248c8d84e7c953d9faa703", "width": 216, "height": 107}, {"url": "https://preview.redd.it/1sgndwxhob9a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=57c24ba1241730f422f4faee446e2d33fd0d30cc", "width": 320, "height": 159}, {"url": "https://preview.redd.it/1sgndwxhob9a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a756bb94382e20774f8155624593bbe1ca335fa7", "width": 640, "height": 318}], "variants": {}, "id": "l4qgNQrTQ845Hjyg_hFTog8RkgVXzrdFATgRVT6jH60"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10076nf", "is_robot_indexable": true, "report_reasons": null, "author": "cseye420", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10076nf/canada_is_this_too_good_to_be_true/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1sgndwxhob9a1.png", "subreddit_subscribers": 663235, "created_utc": 1672531409.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was researching NAS options and came across an offer for cloud storage priced at $1200 for 10TB lifetime. One time payment. By comparison that cost will get you 2 years of Google Drive storage. Based on that alone it looks like a win but what other factors do you consider when gauging the cost of data storage. I'm not talking about the pros and cons of cloud vs. local storage. I already get that. I'm asking, IF cloud storage could work for you, how valuable will 10TB be in 5, 10, 20 years. As media file sizes get bigger with 8K and photos grow in megapixels, will 10TB in the future get you what 500GB gets you today? Will the failure rate of NAS drives become a non issue when pricing storage? I have no use for 10TB of cloud storage now but down the line I probably will. If so, is this deal worth it?", "author_fullname": "t2_5xpb2e4x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Put 10TB lifetime in perspective for me", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100e0xk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672556255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was researching NAS options and came across an offer for cloud storage priced at $1200 for 10TB lifetime. One time payment. By comparison that cost will get you 2 years of Google Drive storage. Based on that alone it looks like a win but what other factors do you consider when gauging the cost of data storage. I&amp;#39;m not talking about the pros and cons of cloud vs. local storage. I already get that. I&amp;#39;m asking, IF cloud storage could work for you, how valuable will 10TB be in 5, 10, 20 years. As media file sizes get bigger with 8K and photos grow in megapixels, will 10TB in the future get you what 500GB gets you today? Will the failure rate of NAS drives become a non issue when pricing storage? I have no use for 10TB of cloud storage now but down the line I probably will. If so, is this deal worth it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "100e0xk", "is_robot_indexable": true, "report_reasons": null, "author": "Jaded-Function", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100e0xk/put_10tb_lifetime_in_perspective_for_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100e0xk/put_10tb_lifetime_in_perspective_for_me/", "subreddit_subscribers": 663235, "created_utc": 1672556255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello sub\nPlease advise what's a cheap Nas adapter that can help me put my 16tb wdd USB HDD on my own as a network drive?\n\nI'd eventually like to use rclone on my PCs/phone to backup content into it.\n\nThanks", "author_fullname": "t2_72g3b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advise : home backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100h1bm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672569280.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello sub\nPlease advise what&amp;#39;s a cheap Nas adapter that can help me put my 16tb wdd USB HDD on my own as a network drive?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d eventually like to use rclone on my PCs/phone to backup content into it.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100h1bm", "is_robot_indexable": true, "report_reasons": null, "author": "justbflat", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100h1bm/need_advise_home_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100h1bm/need_advise_home_backup/", "subreddit_subscribers": 663235, "created_utc": 1672569280.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hiya, I'm looking for any tool that downloads all images from a Reddit account, cheers", "author_fullname": "t2_ccp6tq5p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit account scraper?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100gw0k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672568632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hiya, I&amp;#39;m looking for any tool that downloads all images from a Reddit account, cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100gw0k", "is_robot_indexable": true, "report_reasons": null, "author": "LunaKindaExists", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100gw0k/reddit_account_scraper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100gw0k/reddit_account_scraper/", "subreddit_subscribers": 663235, "created_utc": 1672568632.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have audio, video, notes, project files, random files, etc. scattered in multiple places and it's been driving me crazy. Is there a platform or program that can help solve this? I want a homebase for all of my data, all in one place where I can easily access it.", "author_fullname": "t2_n8pzi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to organize everything into one place? Is it possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1009pph", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672539715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have audio, video, notes, project files, random files, etc. scattered in multiple places and it&amp;#39;s been driving me crazy. Is there a platform or program that can help solve this? I want a homebase for all of my data, all in one place where I can easily access it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1009pph", "is_robot_indexable": true, "report_reasons": null, "author": "Ty505", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1009pph/how_to_organize_everything_into_one_place_is_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1009pph/how_to_organize_everything_into_one_place_is_it/", "subreddit_subscribers": 663235, "created_utc": 1672539715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "and there are no CMR.", "author_fullname": "t2_ao2zj3p2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I understand correctly that all *recent* 4TB 2.5inch HDDs are SMR only?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1005jr0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672526318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;and there are no CMR.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1005jr0", "is_robot_indexable": true, "report_reasons": null, "author": "Rude-Engine440", "discussion_type": null, "num_comments": 13, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1005jr0/do_i_understand_correctly_that_all_recent_4tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1005jr0/do_i_understand_correctly_that_all_recent_4tb/", "subreddit_subscribers": 663235, "created_utc": 1672526318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**First, a disclaimer:** I'm aware media is generally already compress. Storage capacity is not an issue currently, but it's something I'm curious about and want to learn. \n\n**1)** Do any of you use compression, in any form, for either backups, local storage, or cloud?\n\n####Files I'm considering compressing:\n \n**2)** I have a LOT of games downloaded, which I do not *currently* play, but plan on returning to. One example would me Skyrim +150 Mods, which took weeks to figure out the right combination of mods \n\n**3)** I have about 500 GB - 1TB or so of old data that I'm **slowly** going through, and deleting or saving anything that's personally important. The data is from either: My old harddrives, my deceased dad's PC, an old WD passport, and an old backup folder that is partially corrupt but has some salvigable important files\n\n**4)** I'm not sure if RAR or 7-zip compression does anything for lossless media, so this part might not be relevant: but I have a lot of upscaled photos and videos; some of which are saved as PNG or ProRes, and are essentially lossless. Some pictures are 150mb each. I'm still fairly new at upscaling, and later on this will probably be less of an issue, but the reason some files are lossless is because:\n\n* Some select, important media I used ProRes or PNG for that I may plan on printing, or using in other software like Premiere and Photoshop. \n* Some photos had very small artifacts on the JPG Codec that weren't in PNG\n* some upscaling issues on various codec formats (either software-side errors, or time)\n* some issues at the time with using h.265 or vp9 format (software issue that was patched and fixed). But as a result I used h.264 or ProRes on\n* poor knowledge on compatibility issues when using various codecs (h.265, h.264, av1, vp9, etc) on supported devices (android, windows, Apple), transferring them, uploading to a cloud service (g.drive, g.photos), supported programs, etc.\n\nHopefully when I learn a bit more about the above, and with some software issues patched, I'll be able to use the right codec to compress files.", "author_fullname": "t2_cs86j6jo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some questions on compression", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100ikdy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672575819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;First, a disclaimer:&lt;/strong&gt; I&amp;#39;m aware media is generally already compress. Storage capacity is not an issue currently, but it&amp;#39;s something I&amp;#39;m curious about and want to learn. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1)&lt;/strong&gt; Do any of you use compression, in any form, for either backups, local storage, or cloud?&lt;/p&gt;\n\n&lt;h4&gt;Files I&amp;#39;m considering compressing:&lt;/h4&gt;\n\n&lt;p&gt;&lt;strong&gt;2)&lt;/strong&gt; I have a LOT of games downloaded, which I do not &lt;em&gt;currently&lt;/em&gt; play, but plan on returning to. One example would me Skyrim +150 Mods, which took weeks to figure out the right combination of mods &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3)&lt;/strong&gt; I have about 500 GB - 1TB or so of old data that I&amp;#39;m &lt;strong&gt;slowly&lt;/strong&gt; going through, and deleting or saving anything that&amp;#39;s personally important. The data is from either: My old harddrives, my deceased dad&amp;#39;s PC, an old WD passport, and an old backup folder that is partially corrupt but has some salvigable important files&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4)&lt;/strong&gt; I&amp;#39;m not sure if RAR or 7-zip compression does anything for lossless media, so this part might not be relevant: but I have a lot of upscaled photos and videos; some of which are saved as PNG or ProRes, and are essentially lossless. Some pictures are 150mb each. I&amp;#39;m still fairly new at upscaling, and later on this will probably be less of an issue, but the reason some files are lossless is because:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Some select, important media I used ProRes or PNG for that I may plan on printing, or using in other software like Premiere and Photoshop. &lt;/li&gt;\n&lt;li&gt;Some photos had very small artifacts on the JPG Codec that weren&amp;#39;t in PNG&lt;/li&gt;\n&lt;li&gt;some upscaling issues on various codec formats (either software-side errors, or time)&lt;/li&gt;\n&lt;li&gt;some issues at the time with using h.265 or vp9 format (software issue that was patched and fixed). But as a result I used h.264 or ProRes on&lt;/li&gt;\n&lt;li&gt;poor knowledge on compatibility issues when using various codecs (h.265, h.264, av1, vp9, etc) on supported devices (android, windows, Apple), transferring them, uploading to a cloud service (g.drive, g.photos), supported programs, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Hopefully when I learn a bit more about the above, and with some software issues patched, I&amp;#39;ll be able to use the right codec to compress files.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100ikdy", "is_robot_indexable": true, "report_reasons": null, "author": "NegativelyMagnetic", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100ikdy/some_questions_on_compression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100ikdy/some_questions_on_compression/", "subreddit_subscribers": 663235, "created_utc": 1672575819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Update: In digging I found [this chart](https://www.sonnettech.com/support/downloads/manuals/u2_ssds.pdf) in a different thread about U.2 PCIe cards for Mac Pros. Turns out WD doesn\u2019t make a drive compatible with Macs.\n\n___\n\nI'm a video editor and have been trying to find the best solution for high speed external storage. With some help from this subreddit I thought I hit the mark when I was turned towards using U.2 NVMe SSDs. I got the [OWC Helios 3S](https://eshop.macsales.com/shop/owc-mercury-helios-3s/thunderbolt-3). It's an external thunderbolt 3 PCIe card adapter that they sell with a caddy to mount U.2 NVMe SSDs.\n\nSo I get a 7.68TB WD SN640 U.2 SSD, stick it in the Helios and the machine sees the drive, I initialize it and try to format it. Couldn't do it between AFPS, MacOS Journaled, EXFAT, FAT, but I was able to do it on one of my machines into NTFS.\n\nOn that same machine it allowed me to reformat that into a MacOS Journaled drive and I thought I was good to go.\n\nTried to copy a file, system hangs and then them the drive is disconnected. Try to run a disk speed test, system hangs and drive is disconnected. Then I tried to reformat it to something else, hang and then disconnect on both my machines.\n\nI have no idea how to troubleshoot next. I read some stuff about compatibility issues with some drive manufacturers and Apple because of NVMe protocols, but WD seems to use the same one as Apple.\n\nAny ideas?\n\nMacBook Pro M1Max\n\niMac (2019) Core i9", "author_fullname": "t2_1bt7fjmn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trouble using external PCIe Adapter with U.2 NVMe SSD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100byw0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1672553801.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672547941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Update: In digging I found &lt;a href=\"https://www.sonnettech.com/support/downloads/manuals/u2_ssds.pdf\"&gt;this chart&lt;/a&gt; in a different thread about U.2 PCIe cards for Mac Pros. Turns out WD doesn\u2019t make a drive compatible with Macs.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;I&amp;#39;m a video editor and have been trying to find the best solution for high speed external storage. With some help from this subreddit I thought I hit the mark when I was turned towards using U.2 NVMe SSDs. I got the &lt;a href=\"https://eshop.macsales.com/shop/owc-mercury-helios-3s/thunderbolt-3\"&gt;OWC Helios 3S&lt;/a&gt;. It&amp;#39;s an external thunderbolt 3 PCIe card adapter that they sell with a caddy to mount U.2 NVMe SSDs.&lt;/p&gt;\n\n&lt;p&gt;So I get a 7.68TB WD SN640 U.2 SSD, stick it in the Helios and the machine sees the drive, I initialize it and try to format it. Couldn&amp;#39;t do it between AFPS, MacOS Journaled, EXFAT, FAT, but I was able to do it on one of my machines into NTFS.&lt;/p&gt;\n\n&lt;p&gt;On that same machine it allowed me to reformat that into a MacOS Journaled drive and I thought I was good to go.&lt;/p&gt;\n\n&lt;p&gt;Tried to copy a file, system hangs and then them the drive is disconnected. Try to run a disk speed test, system hangs and drive is disconnected. Then I tried to reformat it to something else, hang and then disconnect on both my machines.&lt;/p&gt;\n\n&lt;p&gt;I have no idea how to troubleshoot next. I read some stuff about compatibility issues with some drive manufacturers and Apple because of NVMe protocols, but WD seems to use the same one as Apple.&lt;/p&gt;\n\n&lt;p&gt;Any ideas?&lt;/p&gt;\n\n&lt;p&gt;MacBook Pro M1Max&lt;/p&gt;\n\n&lt;p&gt;iMac (2019) Core i9&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100byw0", "is_robot_indexable": true, "report_reasons": null, "author": "Canadian__Tired", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100byw0/trouble_using_external_pcie_adapter_with_u2_nvme/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100byw0/trouble_using_external_pcie_adapter_with_u2_nvme/", "subreddit_subscribers": 663235, "created_utc": 1672547941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey fellow Hoarders,\n\nA couple of year-ish back, the WD Blue SN550 with the go-to, with a nice fast 1.2TB or so per second write, and sustaining a respectable 850MBps post cache.\n\nEver since the chip swap in 2021, it remains fast (faster, in fact) for the SLC portion, then drops to 370MBps. Not acceptable.\n\n\nDoes anyone know who the budget king of NVME currently is?\n\nI'm really only looking for one key feature:\n\n**sustained writes that are notably faster than SATA, after cache is exhausted**\n\n750MB-1000MB would be nice. Price is a key factor.\n\nThanks all!", "author_fullname": "t2_ye03j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which cheap NVME, large writes.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1007ooi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Drive Choice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672533003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow Hoarders,&lt;/p&gt;\n\n&lt;p&gt;A couple of year-ish back, the WD Blue SN550 with the go-to, with a nice fast 1.2TB or so per second write, and sustaining a respectable 850MBps post cache.&lt;/p&gt;\n\n&lt;p&gt;Ever since the chip swap in 2021, it remains fast (faster, in fact) for the SLC portion, then drops to 370MBps. Not acceptable.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know who the budget king of NVME currently is?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m really only looking for one key feature:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;sustained writes that are notably faster than SATA, after cache is exhausted&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;750MB-1000MB would be nice. Price is a key factor.&lt;/p&gt;\n\n&lt;p&gt;Thanks all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "18TB-RaidZ2", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1007ooi", "is_robot_indexable": true, "report_reasons": null, "author": "Master_Scythe", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1007ooi/which_cheap_nvme_large_writes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1007ooi/which_cheap_nvme_large_writes/", "subreddit_subscribers": 663235, "created_utc": 1672533003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've just been moving the top image \"text\" from one page to another, deleting the background, then moving it back. Does anyone know if there's a quicker and more efficient way to do this?\n\nAlso (different but related question) -- is there a way to download pdfs from IA at higher image quality or as such that the pages don't get separated into mulitple separate \"layers\" in the case for books with a lot of pictures or illustrations?", "author_fullname": "t2_53xbirqx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any way to delete the background page image from Internet Archive pdfs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1002wnm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672518549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve just been moving the top image &amp;quot;text&amp;quot; from one page to another, deleting the background, then moving it back. Does anyone know if there&amp;#39;s a quicker and more efficient way to do this?&lt;/p&gt;\n\n&lt;p&gt;Also (different but related question) -- is there a way to download pdfs from IA at higher image quality or as such that the pages don&amp;#39;t get separated into mulitple separate &amp;quot;layers&amp;quot; in the case for books with a lot of pictures or illustrations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1002wnm", "is_robot_indexable": true, "report_reasons": null, "author": "fiocobra", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1002wnm/any_way_to_delete_the_background_page_image_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1002wnm/any_way_to_delete_the_background_page_image_from/", "subreddit_subscribers": 663235, "created_utc": 1672518549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Over the years I have amassed music &amp; audiobooks and have saved them places willy- nilly. I\u2019ve started to corral them in one place to weed out duplicates and keep the best versions, but I\u2019m stuck on one thing. How do I download things I\u2019ve purchased from various sites that are hosted on those sites and not my device. For example, I buy audiobooks from Chirp. I can listen to them using their app, but I want to get them saved so I can listen to them even if I don\u2019t have an internet connection.", "author_fullname": "t2_1ylgs4b8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Audio hoarder", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1002831", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672516580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the years I have amassed music &amp;amp; audiobooks and have saved them places willy- nilly. I\u2019ve started to corral them in one place to weed out duplicates and keep the best versions, but I\u2019m stuck on one thing. How do I download things I\u2019ve purchased from various sites that are hosted on those sites and not my device. For example, I buy audiobooks from Chirp. I can listen to them using their app, but I want to get them saved so I can listen to them even if I don\u2019t have an internet connection.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1002831", "is_robot_indexable": true, "report_reasons": null, "author": "CoveredInBeez", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1002831/audio_hoarder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1002831/audio_hoarder/", "subreddit_subscribers": 663235, "created_utc": 1672516580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I receive a few different periodic emails that I would like to use as input to automations like downloading data provided in a link in the email, or adding a newsletter to my wallabag.\n\nI could (but would rather not) write a python script to poll my actual email account, but I'd rather find free email account that I can automatically forward the emails to and then pull from that, then run an automation for each new email that comes in.  There must be a solution out there already, how are people getting this done?  I don't want to run an email server myself, that's a whole can of worms I don't want to deal with, and I don't need to be event-based; polling a couple times a day would be enough.\n\nI did a bit of searching here and /r/selfhosted, but mostly people are talking about syncing/archiving emails, which is not what I'm trying to solve; as soon as I grab the input and launch the automation I'll trash the email.", "author_fullname": "t2_14wfe6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to launch automations based on incoming emails?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100271m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672516497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I receive a few different periodic emails that I would like to use as input to automations like downloading data provided in a link in the email, or adding a newsletter to my wallabag.&lt;/p&gt;\n\n&lt;p&gt;I could (but would rather not) write a python script to poll my actual email account, but I&amp;#39;d rather find free email account that I can automatically forward the emails to and then pull from that, then run an automation for each new email that comes in.  There must be a solution out there already, how are people getting this done?  I don&amp;#39;t want to run an email server myself, that&amp;#39;s a whole can of worms I don&amp;#39;t want to deal with, and I don&amp;#39;t need to be event-based; polling a couple times a day would be enough.&lt;/p&gt;\n\n&lt;p&gt;I did a bit of searching here and &lt;a href=\"/r/selfhosted\"&gt;/r/selfhosted&lt;/a&gt;, but mostly people are talking about syncing/archiving emails, which is not what I&amp;#39;m trying to solve; as soon as I grab the input and launch the automation I&amp;#39;ll trash the email.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100271m", "is_robot_indexable": true, "report_reasons": null, "author": "often_wears_pants", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100271m/how_to_launch_automations_based_on_incoming_emails/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100271m/how_to_launch_automations_based_on_incoming_emails/", "subreddit_subscribers": 663235, "created_utc": 1672516497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First, I would like to apologize for the wall of text below. It's tough to digest since English is not my native language.\n\nUsing YouTube to store videos: I have done my homework and agree with the consensus. This is a bad idea. A significant risk of a ban at any moment without ever getting a chance to appeal or interact with a human being, let alone a chance to recover the data, a significant drop in quality across the board, the sudden apparition of exotic formats compatible with nothing, it goes on and on. Some have tried and are here to tell their horror stories on this sub.\n\nMy case is a bit different, though.\n\nA bit of background for you to get a rough picture of where I'll be standing for the next 18 months-ish (hopefully less than that).\n\nIn 48 hours, I will be homeless. I sold my decent NAS and the disks a few months ago to pay for gas and food. I've sold 90% of everything I had, and 10% left has no value and will be left on the sidewalk or discarded after calling a charity organization.\n\nI'm doing my best to bounce and have a realistic plan. Not some stuff written on a napkin, I've projected infrastructure, funding, I have a timeline to stick to, etc. Something solid.\n\nThis plan requires me to temporarily store about 900Gb of footage + without sound in high definition: mostly 4k but some 8k as well :(\n\nIn another life, I was running a small yet monetized YouTube channel. There are around 5k subs left (bots and dead people). This channel is 12 years old; everything is in the green with YT' ToS.\n\nThe post starts here: I've uploaded 200 Gb of footage in 10 days on my old YouTube channel. I've covered my tracks (I feel extra stupid after writing this) by populating correctly and accurately all the fields for each video, such as an original title, location, description, tags, etc. Since each vid is 20 s to 4 minutes long, this is a lot of work put towards failure. Remember, this content has no sound, so I've even added copyright-free music from the YouTube library to everything. This content is 100% original, I'm the only person appearing on some videos, but those videos can reveal vehicle location and rotation (no plates visible). So far, so good (standings). I've hit the daily upload limitations twice, though. No need to mention this is done manually. Videos are either flagged as unlisted or draft; high-definition rendering takes ages if I flag those private, and it's not the end of the world if it leaks.\n\nQuestion: should I keep on spending precious time on this? And finally, a critical question: please tell me if there is a better way to do this. I can pay just under the equivalent of $10 US a month if it comes with a price tag.\n\nCongratulations on making it that far into the post.", "author_fullname": "t2_inxy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A slightly different case of storing videos on YouTube. Comments and alternative solutions welcome.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_100qb56", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672599070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First, I would like to apologize for the wall of text below. It&amp;#39;s tough to digest since English is not my native language.&lt;/p&gt;\n\n&lt;p&gt;Using YouTube to store videos: I have done my homework and agree with the consensus. This is a bad idea. A significant risk of a ban at any moment without ever getting a chance to appeal or interact with a human being, let alone a chance to recover the data, a significant drop in quality across the board, the sudden apparition of exotic formats compatible with nothing, it goes on and on. Some have tried and are here to tell their horror stories on this sub.&lt;/p&gt;\n\n&lt;p&gt;My case is a bit different, though.&lt;/p&gt;\n\n&lt;p&gt;A bit of background for you to get a rough picture of where I&amp;#39;ll be standing for the next 18 months-ish (hopefully less than that).&lt;/p&gt;\n\n&lt;p&gt;In 48 hours, I will be homeless. I sold my decent NAS and the disks a few months ago to pay for gas and food. I&amp;#39;ve sold 90% of everything I had, and 10% left has no value and will be left on the sidewalk or discarded after calling a charity organization.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m doing my best to bounce and have a realistic plan. Not some stuff written on a napkin, I&amp;#39;ve projected infrastructure, funding, I have a timeline to stick to, etc. Something solid.&lt;/p&gt;\n\n&lt;p&gt;This plan requires me to temporarily store about 900Gb of footage + without sound in high definition: mostly 4k but some 8k as well :(&lt;/p&gt;\n\n&lt;p&gt;In another life, I was running a small yet monetized YouTube channel. There are around 5k subs left (bots and dead people). This channel is 12 years old; everything is in the green with YT&amp;#39; ToS.&lt;/p&gt;\n\n&lt;p&gt;The post starts here: I&amp;#39;ve uploaded 200 Gb of footage in 10 days on my old YouTube channel. I&amp;#39;ve covered my tracks (I feel extra stupid after writing this) by populating correctly and accurately all the fields for each video, such as an original title, location, description, tags, etc. Since each vid is 20 s to 4 minutes long, this is a lot of work put towards failure. Remember, this content has no sound, so I&amp;#39;ve even added copyright-free music from the YouTube library to everything. This content is 100% original, I&amp;#39;m the only person appearing on some videos, but those videos can reveal vehicle location and rotation (no plates visible). So far, so good (standings). I&amp;#39;ve hit the daily upload limitations twice, though. No need to mention this is done manually. Videos are either flagged as unlisted or draft; high-definition rendering takes ages if I flag those private, and it&amp;#39;s not the end of the world if it leaks.&lt;/p&gt;\n\n&lt;p&gt;Question: should I keep on spending precious time on this? And finally, a critical question: please tell me if there is a better way to do this. I can pay just under the equivalent of $10 US a month if it comes with a price tag.&lt;/p&gt;\n\n&lt;p&gt;Congratulations on making it that far into the post.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100qb56", "is_robot_indexable": true, "report_reasons": null, "author": "-Nicolas-", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100qb56/a_slightly_different_case_of_storing_videos_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100qb56/a_slightly_different_case_of_storing_videos_on/", "subreddit_subscribers": 663235, "created_utc": 1672599070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The easy answer: RETWEET.  \n\n\nWhen you download your archive, for some reason just a simple retweet saves all media contained in a tweet.  What I consider insane is what is shown by in the \"Likes\" tab in the archive is almost nothing - ONLY the text and 2 LINKS POINTING TO THE SAME TWEET (wtf?). A retweet has the same content but saves the **IMAGES** and **VIDEOS** and the author's **USERNAME** which for some reason that is beyond me the \"Likes\" tab does not.\n\n\"Quote Retweets\" suffer the same problem as \"Likes\" so it might be better to retweet before quoting.\n\nDownloading data from the Twitter itself is infinitely better since it is complete than using any app so there are two options:\n\n1. **Retweet Everything**\n2. **Extract Links from Archive then Create an App to Retweet Everything**\n\nThe problem with the 1st option is that it fills your follower's timeline and that is a problem. You can make a 2nd account to personally retweet the same stuff but that's just as exhausting.\n\nThe 2nd option is probably more feasible but the best I can probably do is extract the tweet link from the archive but that's basically it. I'd imagine a dummy account is needed where an app is fed with a list of links to retweet. Archiving the dummy account saves everything from the main account. \n\n&amp;#x200B;\n\n*I really don't know how to implement the 2nd option and I only know basic stuff in using the API which I think can be done there* (for now?) but I'm hoping some data god will see this and is able to lend their power in creating one. I'm just a bit sick of using apps - it's either incomplete, extremely slow, or does not have the complete data.  Also, the lack of the use of API for saving account data is kinda, in general, iffy for me.\n\nI think I understand the data storage implication of not saving media from likes since it probably outnumbers tweets by a lot, and they are not bounded by daily limits. Still, I think the way they are handling when you do want to archive is insanity. Speaking of limits - twitter has a daily limit of 2400 tweets but I think that is more than enough to save an entire account within a day.", "author_fullname": "t2_n62192z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to save media from tweets, likes, and bookmarks in Twitter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_100plzr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1672598186.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672597251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The easy answer: RETWEET.  &lt;/p&gt;\n\n&lt;p&gt;When you download your archive, for some reason just a simple retweet saves all media contained in a tweet.  What I consider insane is what is shown by in the &amp;quot;Likes&amp;quot; tab in the archive is almost nothing - ONLY the text and 2 LINKS POINTING TO THE SAME TWEET (wtf?). A retweet has the same content but saves the &lt;strong&gt;IMAGES&lt;/strong&gt; and &lt;strong&gt;VIDEOS&lt;/strong&gt; and the author&amp;#39;s &lt;strong&gt;USERNAME&lt;/strong&gt; which for some reason that is beyond me the &amp;quot;Likes&amp;quot; tab does not.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Quote Retweets&amp;quot; suffer the same problem as &amp;quot;Likes&amp;quot; so it might be better to retweet before quoting.&lt;/p&gt;\n\n&lt;p&gt;Downloading data from the Twitter itself is infinitely better since it is complete than using any app so there are two options:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Retweet Everything&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Extract Links from Archive then Create an App to Retweet Everything&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The problem with the 1st option is that it fills your follower&amp;#39;s timeline and that is a problem. You can make a 2nd account to personally retweet the same stuff but that&amp;#39;s just as exhausting.&lt;/p&gt;\n\n&lt;p&gt;The 2nd option is probably more feasible but the best I can probably do is extract the tweet link from the archive but that&amp;#39;s basically it. I&amp;#39;d imagine a dummy account is needed where an app is fed with a list of links to retweet. Archiving the dummy account saves everything from the main account. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;I really don&amp;#39;t know how to implement the 2nd option and I only know basic stuff in using the API which I think can be done there&lt;/em&gt; (for now?) but I&amp;#39;m hoping some data god will see this and is able to lend their power in creating one. I&amp;#39;m just a bit sick of using apps - it&amp;#39;s either incomplete, extremely slow, or does not have the complete data.  Also, the lack of the use of API for saving account data is kinda, in general, iffy for me.&lt;/p&gt;\n\n&lt;p&gt;I think I understand the data storage implication of not saving media from likes since it probably outnumbers tweets by a lot, and they are not bounded by daily limits. Still, I think the way they are handling when you do want to archive is insanity. Speaking of limits - twitter has a daily limit of 2400 tweets but I think that is more than enough to save an entire account within a day.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100plzr", "is_robot_indexable": true, "report_reasons": null, "author": "Altrigeo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100plzr/how_to_save_media_from_tweets_likes_and_bookmarks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100plzr/how_to_save_media_from_tweets_likes_and_bookmarks/", "subreddit_subscribers": 663235, "created_utc": 1672597251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey fellas, I have a few channels I scrape occasionally on Bitchute using a custom script I wrote and using yt-dlp. I've noticed that a lot of older videos no longer archive, and when I check the videos on Bitchute they no longer load - so it's not an issue with my script or yt-dlp. The latest channel I tried to archive cut off at a video on December 3, 2019 - and all videos older than that are no longer playing on the website. \n\nIs this an issue with Bitchute automatically no longer hosting videos once they've been uploaded for a certain amount of time? Anyone else run into this problem? Any input is helpful. Thanks guys.", "author_fullname": "t2_p9vg3wjl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone having problem archiving Bitchute channels using yt-dlp?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_100plvd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672597242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellas, I have a few channels I scrape occasionally on Bitchute using a custom script I wrote and using yt-dlp. I&amp;#39;ve noticed that a lot of older videos no longer archive, and when I check the videos on Bitchute they no longer load - so it&amp;#39;s not an issue with my script or yt-dlp. The latest channel I tried to archive cut off at a video on December 3, 2019 - and all videos older than that are no longer playing on the website. &lt;/p&gt;\n\n&lt;p&gt;Is this an issue with Bitchute automatically no longer hosting videos once they&amp;#39;ve been uploaded for a certain amount of time? Anyone else run into this problem? Any input is helpful. Thanks guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100plvd", "is_robot_indexable": true, "report_reasons": null, "author": "TCIE", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100plvd/anyone_having_problem_archiving_bitchute_channels/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100plvd/anyone_having_problem_archiving_bitchute_channels/", "subreddit_subscribers": 663235, "created_utc": 1672597242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is MTBF the same as \"Reliability\" Cycles...?\n\nHello\n\nI wanna buy a new HDD, im choosing between:\n\n&amp;#x200B;\n\nToshiba P300 6TB (HDWD260UZSVA)\n\nand\n\nSeagate BarraCuda 6TB ( ST6000DM003)\n\n&amp;#x200B;\n\nIn the shop in my country, in my language (not english) it says for Toshiba: \"MTBF 600000 hours\" and for Seagate: \"Reliability 300000 cycles\".\n\n&amp;#x200B;\n\nIs \"MTBF\" (Mean time Before Failure) the same as \"Reliability Cycles\"...? I did the translation from my language to english as good as i can.\n\nAre they the same?\n\nIf not, which one is Better?\n\n&amp;#x200B;\n\n600 000 Hours MTBF\n\nor\n\n300 000 Reliability Cycles?\n\n&amp;#x200B;\n\nThanks", "author_fullname": "t2_1sl55e5m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is MTBF the same as \"Reliability\" Cycles...?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100m09a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672587219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is MTBF the same as &amp;quot;Reliability&amp;quot; Cycles...?&lt;/p&gt;\n\n&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I wanna buy a new HDD, im choosing between:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Toshiba P300 6TB (HDWD260UZSVA)&lt;/p&gt;\n\n&lt;p&gt;and&lt;/p&gt;\n\n&lt;p&gt;Seagate BarraCuda 6TB ( ST6000DM003)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In the shop in my country, in my language (not english) it says for Toshiba: &amp;quot;MTBF 600000 hours&amp;quot; and for Seagate: &amp;quot;Reliability 300000 cycles&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is &amp;quot;MTBF&amp;quot; (Mean time Before Failure) the same as &amp;quot;Reliability Cycles&amp;quot;...? I did the translation from my language to english as good as i can.&lt;/p&gt;\n\n&lt;p&gt;Are they the same?&lt;/p&gt;\n\n&lt;p&gt;If not, which one is Better?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;600 000 Hours MTBF&lt;/p&gt;\n\n&lt;p&gt;or&lt;/p&gt;\n\n&lt;p&gt;300 000 Reliability Cycles?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100m09a", "is_robot_indexable": true, "report_reasons": null, "author": "ThomasHasThomas", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100m09a/is_mtbf_the_same_as_reliability_cycles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100m09a/is_mtbf_the_same_as_reliability_cycles/", "subreddit_subscribers": 663235, "created_utc": 1672587219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Greetings, apologies if this or similar questions have been asked already, but I wanted some sanity check with the community that specializes in data storage.\n\nI have a Minisforum UM300 box that I currently use to run Proxmox and virtualized pfSense as 1Gbps home network router. And now, I would like it to provide some storage capabilities. So my plan is to have a virtualized NAS, that will only be exposed to LAN. Now hooking up the storage drives to the box is where I'm having some difficulties to make the right choices. \n\nI do photography for a hoby and the storage would mostly be used as photo archive. My plan was to get 2x4-6TB drives and use RAID1 and also setup daily backups to something like AWS S3 or Backblaze B2 for impotant stuff. My data needs don't grow by a lot, ~20-40Gb a month. This should be enough for at least a few years and then I could decide what to do next\n\nI would like to keep the whole setup low power, the electricity here is 0.65\u20ac / kwh, so running a lot of power hungry hardware is eh... not pleasant. Also, ideally, I would like to keep the whole setup low profile and easy to transport, I move a lot, sometimes to different EU countries (the next move is in 2-3 months, same country though).\n\nNow about hooking up the drives to the box. The box has 3 x USB 3.1 ports.\n- I could use individual external 3.5 HDDs. The con is that it's messy. Lots of unnecessary wiring and no expansion options.\n- I could use individual external 2.5 HDDs or SSDs. Less wiring, because there is no need for PSU, but no expansion. The SSDs are actually attractive choice, because there will be much more reading than writing to the drives and those could potentially outlasts HDDs also less prone to accidental impact damage when moving.\n- I could get something like 2-4 bay fantec or yottamaster DAS. USB 3.0 model with built-in RAID should be enough for 4 drives. Quite compact and I could add 2x drives later on.\n- Maybe someone could also recommend some 2x or 4x 2.5 enclosure with USB 3.1? I know that the interface would become a bottleneck in case of 4 SSDs, for HDD it would be plenty of bandwidth though.\n\nMaybe also some recommendations on filesystem to use. My thought was using btrfs and raid1.", "author_fullname": "t2_9rdt91h4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Minisforum UM300 as home NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100i6bi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672574255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings, apologies if this or similar questions have been asked already, but I wanted some sanity check with the community that specializes in data storage.&lt;/p&gt;\n\n&lt;p&gt;I have a Minisforum UM300 box that I currently use to run Proxmox and virtualized pfSense as 1Gbps home network router. And now, I would like it to provide some storage capabilities. So my plan is to have a virtualized NAS, that will only be exposed to LAN. Now hooking up the storage drives to the box is where I&amp;#39;m having some difficulties to make the right choices. &lt;/p&gt;\n\n&lt;p&gt;I do photography for a hoby and the storage would mostly be used as photo archive. My plan was to get 2x4-6TB drives and use RAID1 and also setup daily backups to something like AWS S3 or Backblaze B2 for impotant stuff. My data needs don&amp;#39;t grow by a lot, ~20-40Gb a month. This should be enough for at least a few years and then I could decide what to do next&lt;/p&gt;\n\n&lt;p&gt;I would like to keep the whole setup low power, the electricity here is 0.65\u20ac / kwh, so running a lot of power hungry hardware is eh... not pleasant. Also, ideally, I would like to keep the whole setup low profile and easy to transport, I move a lot, sometimes to different EU countries (the next move is in 2-3 months, same country though).&lt;/p&gt;\n\n&lt;p&gt;Now about hooking up the drives to the box. The box has 3 x USB 3.1 ports.\n- I could use individual external 3.5 HDDs. The con is that it&amp;#39;s messy. Lots of unnecessary wiring and no expansion options.\n- I could use individual external 2.5 HDDs or SSDs. Less wiring, because there is no need for PSU, but no expansion. The SSDs are actually attractive choice, because there will be much more reading than writing to the drives and those could potentially outlasts HDDs also less prone to accidental impact damage when moving.\n- I could get something like 2-4 bay fantec or yottamaster DAS. USB 3.0 model with built-in RAID should be enough for 4 drives. Quite compact and I could add 2x drives later on.\n- Maybe someone could also recommend some 2x or 4x 2.5 enclosure with USB 3.1? I know that the interface would become a bottleneck in case of 4 SSDs, for HDD it would be plenty of bandwidth though.&lt;/p&gt;\n\n&lt;p&gt;Maybe also some recommendations on filesystem to use. My thought was using btrfs and raid1.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100i6bi", "is_robot_indexable": true, "report_reasons": null, "author": "No-Significance2877", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100i6bi/minisforum_um300_as_home_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100i6bi/minisforum_um300_as_home_nas/", "subreddit_subscribers": 663235, "created_utc": 1672574255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I realized that I'm subscribed to Google One, Dropbox, iCloud, and one drive. I would like to move all my files to one cloud. What's the easiest, fastest way to do it?", "author_fullname": "t2_80viwyd5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easiest wayto transfer cloud files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100du0w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672555431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I realized that I&amp;#39;m subscribed to Google One, Dropbox, iCloud, and one drive. I would like to move all my files to one cloud. What&amp;#39;s the easiest, fastest way to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100du0w", "is_robot_indexable": true, "report_reasons": null, "author": "Used-Shift9107", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100du0w/easiest_wayto_transfer_cloud_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100du0w/easiest_wayto_transfer_cloud_files/", "subreddit_subscribers": 663235, "created_utc": 1672555431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I want to transfer some photos from my iPhone to my computer. I found an online tool called qrcp ([https://github.com/claudiodangelis/qrcp](https://github.com/claudiodangelis/qrcp)). Downloaded it and ran it on my computer (Linux user). Then, I accessed it from my iPhone and uploaded a few files to test it out.\n\nWhile the content does get transferred over, the filesystem date isn't preserved. Roughly 1,000 photos to transfer so keying in the dates manually isn't an option.\n\nWhat can I do? I want to preserve the dates and times the photos were taken.", "author_fullname": "t2_20uo12nk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to transfer photos from iphone to linux computer while preserving date and time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100au5n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1672543687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I want to transfer some photos from my iPhone to my computer. I found an online tool called qrcp (&lt;a href=\"https://github.com/claudiodangelis/qrcp\"&gt;https://github.com/claudiodangelis/qrcp&lt;/a&gt;). Downloaded it and ran it on my computer (Linux user). Then, I accessed it from my iPhone and uploaded a few files to test it out.&lt;/p&gt;\n\n&lt;p&gt;While the content does get transferred over, the filesystem date isn&amp;#39;t preserved. Roughly 1,000 photos to transfer so keying in the dates manually isn&amp;#39;t an option.&lt;/p&gt;\n\n&lt;p&gt;What can I do? I want to preserve the dates and times the photos were taken.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/W2thSUPQl9oaNAWB1ls8eUZe5HoGkIiIpTGZSyEs-7c.jpg?auto=webp&amp;s=1cb65063aa2b9b03dfd62b542ee3617103ef653f", "width": 794, "height": 684}, "resolutions": [{"url": "https://external-preview.redd.it/W2thSUPQl9oaNAWB1ls8eUZe5HoGkIiIpTGZSyEs-7c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ebe5f9322f58bbf7fd9032740fd4cedcfb852cb", "width": 108, "height": 93}, {"url": "https://external-preview.redd.it/W2thSUPQl9oaNAWB1ls8eUZe5HoGkIiIpTGZSyEs-7c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=900a97acbaa891ed56ef43c03ebee49b47d58c0d", "width": 216, "height": 186}, {"url": "https://external-preview.redd.it/W2thSUPQl9oaNAWB1ls8eUZe5HoGkIiIpTGZSyEs-7c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d4ae546be4b1e47e2a29ef2cd3ee09df7dbdcaec", "width": 320, "height": 275}, {"url": "https://external-preview.redd.it/W2thSUPQl9oaNAWB1ls8eUZe5HoGkIiIpTGZSyEs-7c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a07702bc744ecace0d53282ad4c3bfba280d832d", "width": 640, "height": 551}], "variants": {}, "id": "rpd_tYMw3lHAI6xoCn3I_mwj-r6ZhF9Nv6uESMFkEVM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100au5n", "is_robot_indexable": true, "report_reasons": null, "author": "CrazybloxianEmpireNS", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100au5n/how_to_transfer_photos_from_iphone_to_linux/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100au5n/how_to_transfer_photos_from_iphone_to_linux/", "subreddit_subscribers": 663235, "created_utc": 1672543687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am hoping to digitize/archive some old photos but my laser printer also just went kaput. \n\nI am looking around to no avail and wondering if anyone knows of an ADF scanner and printer?\n\nTIA", "author_fullname": "t2_73b79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for ADF Scanner + Laser Printer Combo. Unicorn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1001coy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672514046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am hoping to digitize/archive some old photos but my laser printer also just went kaput. &lt;/p&gt;\n\n&lt;p&gt;I am looking around to no avail and wondering if anyone knows of an ADF scanner and printer?&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1001coy", "is_robot_indexable": true, "report_reasons": null, "author": "MILFHunterHearstHelm", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1001coy/looking_for_adf_scanner_laser_printer_combo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1001coy/looking_for_adf_scanner_laser_printer_combo/", "subreddit_subscribers": 663235, "created_utc": 1672514046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There are many reason for why data hoarding is more than just stockpiling publicly available information. Most people see content on the internet as continuous. However, take 5 pages from 10 different websites, comeback in 3 months and you will quickly realize half the links are broken, content has changed or has been completely lost. Everyday thousands of new websites are created and shut down.\n\nBelow is a short summery of how information or access to information can be lost forever and why its important to save everything that is relevant, inspirational or entertaining to you.\n\n# There are a number of external factors that can impact or influence the availability of information on the internet. \n\n&amp;#x200B;\n\n* Governments may seize entire websites or implement internet shutdowns without notice. Takedown notices may be issued legitimately or illegitimately based on copyright disputes, malice or in cases of companies like Nintendo \\[[1](https://gamerant.com/nintendo-takedown-notice-25-year-old-super-mario-64-strategy-guide/)\\] \\[[2](https://kotaku.com/nintendo-steamgriddb-dmca-takedown-steam-icons-fan-art-1849813480)\\] \\[[3](https://arstechnica.com/gaming/2018/08/emuparadise-shuts-down-rom-downloads-amid-lawsuit-fears/)\\] \\[[4](https://www.wired.co.uk/article/lets-play-youtube-crackdown)\\], for total control of their IP regardless of fan made content, preservation or regards to privately owned physical property. \n* Pages may change over time including the content and information contained within them. Links to pages and content may change, break or be removed. Owners may be unable or uninterested in maintaining or paying for their site. Choosing to shut it down instead. \n* Environmental disasters or internal societal discourse leading to the destruction or [sabotage](https://en.wikipedia.org/wiki/2021_South_African_unrest) of local and state infrastructure. \n* User accounts and posts may be deleted, banned, suspended or removed - either by the users themselves, moderators or automatically by content moderation algorithms. Content may be removed regardless of reasoning, justification or even out of spite/malice by third parties and moderators. Users have very little control over the lifespan and availability of their posts and are at the whim of algorithms, reports, sudden policy changes or users with elevated privileges. \n* Websites, webpages, media and information can all be paywalled, region locked or may change based on your geographic location, credit card issuer or nationality. These practices are predatory and even discriminatory and only serve to fragment/limit access to information based on regional stereotypes, obscure internal policies, Government regulations and greed. The only acceptable exception to paywalls are stores, user created content and on demand services such as streaming sites. However, most if not all of these stores and streaming sites have implemented region locking. \n\n&amp;#x200B;", "author_fullname": "t2_o8wjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reasons for why data hoarding is important and why you should start", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100o9fo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1672593652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are many reason for why data hoarding is more than just stockpiling publicly available information. Most people see content on the internet as continuous. However, take 5 pages from 10 different websites, comeback in 3 months and you will quickly realize half the links are broken, content has changed or has been completely lost. Everyday thousands of new websites are created and shut down.&lt;/p&gt;\n\n&lt;p&gt;Below is a short summery of how information or access to information can be lost forever and why its important to save everything that is relevant, inspirational or entertaining to you.&lt;/p&gt;\n\n&lt;h1&gt;There are a number of external factors that can impact or influence the availability of information on the internet.&lt;/h1&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Governments may seize entire websites or implement internet shutdowns without notice. Takedown notices may be issued legitimately or illegitimately based on copyright disputes, malice or in cases of companies like Nintendo [&lt;a href=\"https://gamerant.com/nintendo-takedown-notice-25-year-old-super-mario-64-strategy-guide/\"&gt;1&lt;/a&gt;] [&lt;a href=\"https://kotaku.com/nintendo-steamgriddb-dmca-takedown-steam-icons-fan-art-1849813480\"&gt;2&lt;/a&gt;] [&lt;a href=\"https://arstechnica.com/gaming/2018/08/emuparadise-shuts-down-rom-downloads-amid-lawsuit-fears/\"&gt;3&lt;/a&gt;] [&lt;a href=\"https://www.wired.co.uk/article/lets-play-youtube-crackdown\"&gt;4&lt;/a&gt;], for total control of their IP regardless of fan made content, preservation or regards to privately owned physical property. &lt;/li&gt;\n&lt;li&gt;Pages may change over time including the content and information contained within them. Links to pages and content may change, break or be removed. Owners may be unable or uninterested in maintaining or paying for their site. Choosing to shut it down instead. &lt;/li&gt;\n&lt;li&gt;Environmental disasters or internal societal discourse leading to the destruction or &lt;a href=\"https://en.wikipedia.org/wiki/2021_South_African_unrest\"&gt;sabotage&lt;/a&gt; of local and state infrastructure. &lt;/li&gt;\n&lt;li&gt;User accounts and posts may be deleted, banned, suspended or removed - either by the users themselves, moderators or automatically by content moderation algorithms. Content may be removed regardless of reasoning, justification or even out of spite/malice by third parties and moderators. Users have very little control over the lifespan and availability of their posts and are at the whim of algorithms, reports, sudden policy changes or users with elevated privileges. &lt;/li&gt;\n&lt;li&gt;Websites, webpages, media and information can all be paywalled, region locked or may change based on your geographic location, credit card issuer or nationality. These practices are predatory and even discriminatory and only serve to fragment/limit access to information based on regional stereotypes, obscure internal policies, Government regulations and greed. The only acceptable exception to paywalls are stores, user created content and on demand services such as streaming sites. However, most if not all of these stores and streaming sites have implemented region locking. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZybyKowCFEd-nfPD69Cl4b-aTM_0-kX-bMYCWx_N3zo.jpg?auto=webp&amp;s=c387f574702b9ed7c68f698f7fa60d3c32211671", "width": 1800, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/ZybyKowCFEd-nfPD69Cl4b-aTM_0-kX-bMYCWx_N3zo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb9ab3549c9f5cc04cb293a047279f42604c60fd", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ZybyKowCFEd-nfPD69Cl4b-aTM_0-kX-bMYCWx_N3zo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7290288a77475dc5eb9e611bf255005806be1f4f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ZybyKowCFEd-nfPD69Cl4b-aTM_0-kX-bMYCWx_N3zo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a84b6bc449101a95d8f26aba49a1e373c5f7f05b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ZybyKowCFEd-nfPD69Cl4b-aTM_0-kX-bMYCWx_N3zo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=41b14dbcff1f1c5660bbb986002af4d35c724c77", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ZybyKowCFEd-nfPD69Cl4b-aTM_0-kX-bMYCWx_N3zo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4eb9f48e1255ec76179e227b7f255cae2381d077", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ZybyKowCFEd-nfPD69Cl4b-aTM_0-kX-bMYCWx_N3zo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39312c08074837719f03b915499b7495eeb170f7", "width": 1080, "height": 540}], "variants": {}, "id": "8L_qxpBRz4_edl-TBfsKsZva8bCq3Eo_1KrJqCdCbVo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "100o9fo", "is_robot_indexable": true, "report_reasons": null, "author": "ReclusiveEagle", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100o9fo/reasons_for_why_data_hoarding_is_important_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100o9fo/reasons_for_why_data_hoarding_is_important_and/", "subreddit_subscribers": 663235, "created_utc": 1672593652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First off all - i hope this is the right subreddit for my question. Secondly: Hi to the NAS pros here! I want to get my first NAS - but need a  little bit of support. I have 10TB of data right now, plus an expected growth of 2.5-3TB per year. From what i've read so far Raid 5 should be good enough for me. And it should be as cheap as somehow possible. I'm capable of building a server myself if that saves some money, and am willing to use used parts (not for the hard drives though). I currently have 3 6Tb drives and one 8Tb drive (all external HDDs). One 6Tb and the 8Tb drive serve as backups, the other two 6TB drives are always connected to my PC. But honestly - that kind of sucks. So I'm thinking about a future-proof storage solution.\n\nMy initial calculations were around 600-800\u20ac - but maybe someone here has an idea how to do it cheaper?\n\nMy priority is secure, cheap mass storage, not necessarily high read/write speeds\n\nThanks for your support!", "author_fullname": "t2_7psvx29e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice Required - Getting my first NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100lbw9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1672585660.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672585163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First off all - i hope this is the right subreddit for my question. Secondly: Hi to the NAS pros here! I want to get my first NAS - but need a  little bit of support. I have 10TB of data right now, plus an expected growth of 2.5-3TB per year. From what i&amp;#39;ve read so far Raid 5 should be good enough for me. And it should be as cheap as somehow possible. I&amp;#39;m capable of building a server myself if that saves some money, and am willing to use used parts (not for the hard drives though). I currently have 3 6Tb drives and one 8Tb drive (all external HDDs). One 6Tb and the 8Tb drive serve as backups, the other two 6TB drives are always connected to my PC. But honestly - that kind of sucks. So I&amp;#39;m thinking about a future-proof storage solution.&lt;/p&gt;\n\n&lt;p&gt;My initial calculations were around 600-800\u20ac - but maybe someone here has an idea how to do it cheaper?&lt;/p&gt;\n\n&lt;p&gt;My priority is secure, cheap mass storage, not necessarily high read/write speeds&lt;/p&gt;\n\n&lt;p&gt;Thanks for your support!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100lbw9", "is_robot_indexable": true, "report_reasons": null, "author": "luka_anders", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100lbw9/advice_required_getting_my_first_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100lbw9/advice_required_getting_my_first_nas/", "subreddit_subscribers": 663235, "created_utc": 1672585163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, i need your help, \n\nI have 1 drive. One NVME, which has windows 10 installed.\n\nI will buy two others matching drives HDD WD 4TB.\n\nI want to setup RAID only on two matching HDD in windows 10 without reinstalling windows.\n\nMy current setup is follows:\n\none NVME with windows 10.\n\none samsung EVO 1 TB SSD.\n\nI want to safeguard my data, thus i am planning the above.\n\nany tips?", "author_fullname": "t2_59fj8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "help with setting RAID on windows 10, with two new HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_100jols", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672579663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, i need your help, &lt;/p&gt;\n\n&lt;p&gt;I have 1 drive. One NVME, which has windows 10 installed.&lt;/p&gt;\n\n&lt;p&gt;I will buy two others matching drives HDD WD 4TB.&lt;/p&gt;\n\n&lt;p&gt;I want to setup RAID only on two matching HDD in windows 10 without reinstalling windows.&lt;/p&gt;\n\n&lt;p&gt;My current setup is follows:&lt;/p&gt;\n\n&lt;p&gt;one NVME with windows 10.&lt;/p&gt;\n\n&lt;p&gt;one samsung EVO 1 TB SSD.&lt;/p&gt;\n\n&lt;p&gt;I want to safeguard my data, thus i am planning the above.&lt;/p&gt;\n\n&lt;p&gt;any tips?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "100jols", "is_robot_indexable": true, "report_reasons": null, "author": "As9s", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/100jols/help_with_setting_raid_on_windows_10_with_two_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/100jols/help_with_setting_raid_on_windows_10_with_two_new/", "subreddit_subscribers": 663235, "created_utc": 1672579663.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}