{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At what point would you consider utilizing\n- a specialized modern data stack (e.g. Fivetran + dbt + Snowflake) to build a data lake and warehouse vs.\n- just following a KISS principle and using an RDBMS (e.g. Postgres) and a free ETL tool or Postgres+FDW+java/python/whatever code? \n\nI understand that it is impossible to give a simple answer and that many other factors are at play here (data governance requirements...), but without complicating things, what is your dividing line or rule of thumb (if any)?\n\nContext: I have 10+ years of sw engineering experience and have dabbled in data engineering ~6-7 years ago while very complex and glued-together-with-a-bubbleum Big Data solutions were all the rage. Now I got myself into a project where I am required to give my input from both data engineering and software engineering perspectives. The customer has a small number of applications (3-5) from which it needs to make data available for analytics. The total (multiple-year worth) amount of data is currently around ~100GB and I don't see a potential for exponential growth from there. Because of this, my instinct is to go for a KISS solution of simply using Postgres plus something to transfer data into it. Also, the customer's software engineering team is currently used to using outdated technologies and I think introducing an entirely new stack on them would be very complex and cumbersome.\n\nI would be especially interested to hear from people that got burned by their approach. Either by utilizing a too-complicated stack that was not needed or utilizing a too-simple stack that they outgrew and wish they used a more complicated one from the start.", "author_fullname": "t2_hlvgo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Complex vs KISS data pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10bnt19", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673700957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At what point would you consider utilizing\n- a specialized modern data stack (e.g. Fivetran + dbt + Snowflake) to build a data lake and warehouse vs.\n- just following a KISS principle and using an RDBMS (e.g. Postgres) and a free ETL tool or Postgres+FDW+java/python/whatever code? &lt;/p&gt;\n\n&lt;p&gt;I understand that it is impossible to give a simple answer and that many other factors are at play here (data governance requirements...), but without complicating things, what is your dividing line or rule of thumb (if any)?&lt;/p&gt;\n\n&lt;p&gt;Context: I have 10+ years of sw engineering experience and have dabbled in data engineering ~6-7 years ago while very complex and glued-together-with-a-bubbleum Big Data solutions were all the rage. Now I got myself into a project where I am required to give my input from both data engineering and software engineering perspectives. The customer has a small number of applications (3-5) from which it needs to make data available for analytics. The total (multiple-year worth) amount of data is currently around ~100GB and I don&amp;#39;t see a potential for exponential growth from there. Because of this, my instinct is to go for a KISS solution of simply using Postgres plus something to transfer data into it. Also, the customer&amp;#39;s software engineering team is currently used to using outdated technologies and I think introducing an entirely new stack on them would be very complex and cumbersome.&lt;/p&gt;\n\n&lt;p&gt;I would be especially interested to hear from people that got burned by their approach. Either by utilizing a too-complicated stack that was not needed or utilizing a too-simple stack that they outgrew and wish they used a more complicated one from the start.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10bnt19", "is_robot_indexable": true, "report_reasons": null, "author": "frula00", "discussion_type": null, "num_comments": 20, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10bnt19/complex_vs_kiss_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10bnt19/complex_vs_kiss_data_pipeline/", "subreddit_subscribers": 86302, "created_utc": 1673700957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'd like to understand if other folk experience same **pain points/time consuming activities** in the process of creating the pipeline. \n\nI am currently working with Airflow. My DB is in BigQuery.  \nWhen building a pipeline from scratch I'd estimate my time is divided as follows (given I have the pipeline's business requirements):  \n\\- 20% of the time is spent on designing the pipeline's flow logic.  \n\\- 40% of the time is spent on building the initial code (that should cover 80+% of the cases)  \n\\- 40% of the time is spent to adjust the code to handle edge cases found in the data (unexpected data behavior/contaminated data/missing values...).  \nI'd be happy to know how others split their time, and if there are any suggestions on **how to reduce the time spent on initially generating and adjusting the code** (this part can take me between a few days and a few weeks)?\n\n1. What's the **most time consuming** step for you (ex.  Data cleaning, structuring, modeling, etc)?   \nIf you could dive into more specifics on what's the exact struggle you have/had that would be awesome.  \n2. Also, which tools/strategies do you use and can recommend to make the process more efficient?\n\nThanks!", "author_fullname": "t2_ft0j766", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dear, pipeline builders! Which step in your role is the most time consuming?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10bm148", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673698636.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673694592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to understand if other folk experience same &lt;strong&gt;pain points/time consuming activities&lt;/strong&gt; in the process of creating the pipeline. &lt;/p&gt;\n\n&lt;p&gt;I am currently working with Airflow. My DB is in BigQuery.&lt;br/&gt;\nWhen building a pipeline from scratch I&amp;#39;d estimate my time is divided as follows (given I have the pipeline&amp;#39;s business requirements):&lt;br/&gt;\n- 20% of the time is spent on designing the pipeline&amp;#39;s flow logic.&lt;br/&gt;\n- 40% of the time is spent on building the initial code (that should cover 80+% of the cases)&lt;br/&gt;\n- 40% of the time is spent to adjust the code to handle edge cases found in the data (unexpected data behavior/contaminated data/missing values...).&lt;br/&gt;\nI&amp;#39;d be happy to know how others split their time, and if there are any suggestions on &lt;strong&gt;how to reduce the time spent on initially generating and adjusting the code&lt;/strong&gt; (this part can take me between a few days and a few weeks)?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What&amp;#39;s the &lt;strong&gt;most time consuming&lt;/strong&gt; step for you (ex.  Data cleaning, structuring, modeling, etc)?&lt;br/&gt;\nIf you could dive into more specifics on what&amp;#39;s the exact struggle you have/had that would be awesome.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Also, which tools/strategies do you use and can recommend to make the process more efficient?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10bm148", "is_robot_indexable": true, "report_reasons": null, "author": "Dzimkaf", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10bm148/dear_pipeline_builders_which_step_in_your_role_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10bm148/dear_pipeline_builders_which_step_in_your_role_is/", "subreddit_subscribers": 86302, "created_utc": 1673694592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have:\n\n- ETL: Extract, transform in the pipeline, load to the destination;\n- ELT: Extract, load to destination \u2018as is\u2019 and then transform;\n- EtLT: same as ELT except de-identification in the pipeline to reduce audit &amp; risk issues;\n\nThen we have a pipeline that is Transformed in source (the data warehouse), then Extracted, then Loaded to the destination. And what do we call it? Not TEL, but Reverse ETL.\n\nSerious question: was this naming a marketing thing? Does anyone know the history behind it? Genuinely curious.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can someone explain why it\u2019s Reverse ETL not TEL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10bjhvr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673685149.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673684909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ETL: Extract, transform in the pipeline, load to the destination;&lt;/li&gt;\n&lt;li&gt;ELT: Extract, load to destination \u2018as is\u2019 and then transform;&lt;/li&gt;\n&lt;li&gt;EtLT: same as ELT except de-identification in the pipeline to reduce audit &amp;amp; risk issues;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Then we have a pipeline that is Transformed in source (the data warehouse), then Extracted, then Loaded to the destination. And what do we call it? Not TEL, but Reverse ETL.&lt;/p&gt;\n\n&lt;p&gt;Serious question: was this naming a marketing thing? Does anyone know the history behind it? Genuinely curious.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10bjhvr", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10bjhvr/can_someone_explain_why_its_reverse_etl_not_tel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10bjhvr/can_someone_explain_why_its_reverse_etl_not_tel/", "subreddit_subscribers": 86302, "created_utc": 1673684909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "1. Python/Pyspark/PyArrow/Pandas\n2. r/tidyverse/SparkR/ Arrow in R\n3. Java/Scala\n4. etc...", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which programming languages (or framework) skills will be recession-proof and worth learning in 2023 Data Engineering or Data science job market?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10bw5jp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673722426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;Python/Pyspark/PyArrow/Pandas&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"/r/tidyverse/SparkR/\"&gt;r/tidyverse/SparkR/&lt;/a&gt; Arrow in R&lt;/li&gt;\n&lt;li&gt;Java/Scala&lt;/li&gt;\n&lt;li&gt;etc...&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10bw5jp", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10bw5jp/which_programming_languages_or_framework_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10bw5jp/which_programming_languages_or_framework_skills/", "subreddit_subscribers": 86302, "created_utc": 1673722426.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a data engineer mostly working in the modern data stack (DBT, python, Snowflake..) I wanted to learn spark, kafka and more into distributed systems. What would be the right learning framework. My plan is do a real project and get myself comfortable with the end to end of streaming pile-line etc. Any suggestions would be of great. \nps: I am using a mac m1 and some of the setups could be different", "author_fullname": "t2_gzg2l4p2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning spark/kafka in mac m1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10c8hkv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673750910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a data engineer mostly working in the modern data stack (DBT, python, Snowflake..) I wanted to learn spark, kafka and more into distributed systems. What would be the right learning framework. My plan is do a real project and get myself comfortable with the end to end of streaming pile-line etc. Any suggestions would be of great. \nps: I am using a mac m1 and some of the setups could be different&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10c8hkv", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden-Pitch6371", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10c8hkv/learning_sparkkafka_in_mac_m1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10c8hkv/learning_sparkkafka_in_mac_m1/", "subreddit_subscribers": 86302, "created_utc": 1673750910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im trying to use cloud functions to process streaming data from pub sub. Since we get random unexpected spikes from the source (iOS apps) hard to predict when users will be active etc\n\nThe function is written In Python. It takes a record does a bunch of validation on the fields and forwards it to another queue. \n\nIt\u2019s pretty slow, execution time is ~1min per message.\n\nSince it\u2019s the compute time you pay for my first hunch is to re write it using Golang which we know is much faster.\n\nBut is there anything else I could do to reduce the cost? \n\nMaybe have a function convert the JSON record to avro format or something first?", "author_fullname": "t2_ggg0wfmt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reducing Cloud Functions Cost - Ideas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10bou0k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673704067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im trying to use cloud functions to process streaming data from pub sub. Since we get random unexpected spikes from the source (iOS apps) hard to predict when users will be active etc&lt;/p&gt;\n\n&lt;p&gt;The function is written In Python. It takes a record does a bunch of validation on the fields and forwards it to another queue. &lt;/p&gt;\n\n&lt;p&gt;It\u2019s pretty slow, execution time is ~1min per message.&lt;/p&gt;\n\n&lt;p&gt;Since it\u2019s the compute time you pay for my first hunch is to re write it using Golang which we know is much faster.&lt;/p&gt;\n\n&lt;p&gt;But is there anything else I could do to reduce the cost? &lt;/p&gt;\n\n&lt;p&gt;Maybe have a function convert the JSON record to avro format or something first?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10bou0k", "is_robot_indexable": true, "report_reasons": null, "author": "camelCaseInsensitive", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10bou0k/reducing_cloud_functions_cost_ideas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10bou0k/reducing_cloud_functions_cost_ideas/", "subreddit_subscribers": 86302, "created_utc": 1673704067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm a non trad(psychology degree). joining a web dev bootcamp next month. I know many that graduated and found a good paying job, it seems that web dev has a really low entry barrier.\n\nHowever, I'm thinking of switching and studying DE, the issue is that there is no DE bootcamps in Lebanon. And doing a CS degree is not feasible. I found some online courses and the bootcamp offered here but I'm wondering whether I can find a remote job after completing it.\n\nTl;Dr: does de has the same entry barrier compared to web dev", "author_fullname": "t2_6801ehfn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "entry barrier for non traditional", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10bias6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673680373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m a non trad(psychology degree). joining a web dev bootcamp next month. I know many that graduated and found a good paying job, it seems that web dev has a really low entry barrier.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m thinking of switching and studying DE, the issue is that there is no DE bootcamps in Lebanon. And doing a CS degree is not feasible. I found some online courses and the bootcamp offered here but I&amp;#39;m wondering whether I can find a remote job after completing it.&lt;/p&gt;\n\n&lt;p&gt;Tl;Dr: does de has the same entry barrier compared to web dev&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10bias6", "is_robot_indexable": true, "report_reasons": null, "author": "ezio313", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10bias6/entry_barrier_for_non_traditional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10bias6/entry_barrier_for_non_traditional/", "subreddit_subscribers": 86302, "created_utc": 1673680373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "so, basically i hit a bigquery db for only 10 rows and then only 1,000 as a test:\n\nfrom there, i tried two different approaches\n\n1. first, turned the result into arrow, and then into polars dataframe (bigquery only has a pandas version of `\\`to_dataframe`\\` so --&gt; \\``pl.from_arrow(result.to_arrow())`\\`), and from polars i used s3fs to write a parquet file to an s3 bucket\n2. then...i was like maybe i can just cut out the middle-men and turned the result into arrow, then into a string which i encoded to bytes (arrow doesn't seem to have direct to bytes?) \\``result.to_arrow()`\\` then \\``aws.put_object(Body=bytes(result1_arrow.to_string(),encoding='utf-8')`\\`\n\n&amp;#x200B;\n\n* for the arrow approach, both times i got  230.0 B of size\n* for the dataframe approach, 10 rows gave me 3 KB and 1k rows gave me 13 KB\n* &amp;#x200B;\n\nnow, maybe if i just used boto3 for the dataframe approach i would get a different result, but i couldnt figure out how to do the following with a context manager -- that's why i used s3fs:\n\n    with fs.open(f's3://{bucket}/{file.parquet}', 'wb') as f:\n    df.write_parquet(f)\n\ninterested in your guys thoughts.", "author_fullname": "t2_5qteskd9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "can anyone explain the reason for the difference in size for these two parquet conversions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10bgiv2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673674200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so, basically i hit a bigquery db for only 10 rows and then only 1,000 as a test:&lt;/p&gt;\n\n&lt;p&gt;from there, i tried two different approaches&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;first, turned the result into arrow, and then into polars dataframe (bigquery only has a pandas version of &lt;code&gt;\\&lt;/code&gt;to_dataframe&lt;code&gt;\\&lt;/code&gt; so --&amp;gt; `&lt;code&gt;pl.from_arrow(result.to_arrow())&lt;/code&gt;`), and from polars i used s3fs to write a parquet file to an s3 bucket&lt;/li&gt;\n&lt;li&gt;then...i was like maybe i can just cut out the middle-men and turned the result into arrow, then into a string which i encoded to bytes (arrow doesn&amp;#39;t seem to have direct to bytes?) `&lt;code&gt;result.to_arrow()&lt;/code&gt;` then `&lt;code&gt;aws.put_object(Body=bytes(result1_arrow.to_string(),encoding=&amp;#39;utf-8&amp;#39;)&lt;/code&gt;`&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;for the arrow approach, both times i got  230.0 B of size&lt;/li&gt;\n&lt;li&gt;for the dataframe approach, 10 rows gave me 3 KB and 1k rows gave me 13 KB&lt;/li&gt;\n&lt;li&gt;&amp;#x200B;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;now, maybe if i just used boto3 for the dataframe approach i would get a different result, but i couldnt figure out how to do the following with a context manager -- that&amp;#39;s why i used s3fs:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;with fs.open(f&amp;#39;s3://{bucket}/{file.parquet}&amp;#39;, &amp;#39;wb&amp;#39;) as f:\ndf.write_parquet(f)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;interested in your guys thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10bgiv2", "is_robot_indexable": true, "report_reasons": null, "author": "iseestupid", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10bgiv2/can_anyone_explain_the_reason_for_the_difference/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10bgiv2/can_anyone_explain_the_reason_for_the_difference/", "subreddit_subscribers": 86302, "created_utc": 1673674200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I'm looking at about 3 years worth of Pushshift reddit archive files.   These are very large line delimited json files that have been compressed with zstd.  There are 72 of them (3 years of monthly submissions files and 3 years of monthly comment files).  Compressed each one is between 8-10 GB for submissions and 15-30GB for comments.\n\nGiven that storage isn't an issue, part of me wanted to decompress these and store the raw data (wouldn't change a thing) as daily parquet files (instead of monthly json).   But....   whatever format the raw files end up in, I will be extracting a subset of the columns, doing a bunch of clean up, and saving the resulting data to an (as yet unspecifed) DB - likely BigQuery.   \n\nSounds like a job for parquet, right?  Except I will probably only ever do this once.  It is very unlikely that I, or anyone else, will ever go back and extract different columns from the raw data.  So.... I have a decent script that can pull data from the zstd files in chunks, run the data munging and save the relevant data to the DB.  Is there really any value in adding the extra step of saving to parquet?\n\nI'm kinda torn.  I have to iteratively load the data from zstd files anyway, either to save it to parquet or to the DB after munging.   I could presumably combine the munging with saving to parquet before also sending to the DB, but I'm struggling to come up with a rationale for the parquet files.\n\nAm I just way overthinking this?", "author_fullname": "t2_syav4clo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling to decide whether to keep raw files in compressed state or save to parquet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10bxkxq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673725908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m looking at about 3 years worth of Pushshift reddit archive files.   These are very large line delimited json files that have been compressed with zstd.  There are 72 of them (3 years of monthly submissions files and 3 years of monthly comment files).  Compressed each one is between 8-10 GB for submissions and 15-30GB for comments.&lt;/p&gt;\n\n&lt;p&gt;Given that storage isn&amp;#39;t an issue, part of me wanted to decompress these and store the raw data (wouldn&amp;#39;t change a thing) as daily parquet files (instead of monthly json).   But....   whatever format the raw files end up in, I will be extracting a subset of the columns, doing a bunch of clean up, and saving the resulting data to an (as yet unspecifed) DB - likely BigQuery.   &lt;/p&gt;\n\n&lt;p&gt;Sounds like a job for parquet, right?  Except I will probably only ever do this once.  It is very unlikely that I, or anyone else, will ever go back and extract different columns from the raw data.  So.... I have a decent script that can pull data from the zstd files in chunks, run the data munging and save the relevant data to the DB.  Is there really any value in adding the extra step of saving to parquet?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m kinda torn.  I have to iteratively load the data from zstd files anyway, either to save it to parquet or to the DB after munging.   I could presumably combine the munging with saving to parquet before also sending to the DB, but I&amp;#39;m struggling to come up with a rationale for the parquet files.&lt;/p&gt;\n\n&lt;p&gt;Am I just way overthinking this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10bxkxq", "is_robot_indexable": true, "report_reasons": null, "author": "Malignant-Koala", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10bxkxq/struggling_to_decide_whether_to_keep_raw_files_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10bxkxq/struggling_to_decide_whether_to_keep_raw_files_in/", "subreddit_subscribers": 86302, "created_utc": 1673725908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are testing bigquery and have seen some promising results. However I would love to talk to someone who has done a spark -&gt; bigquery migration to see how it turned out. Also if anyone would be willing to hop on a call and talk about it I would be willing to buy you a beer/coffee or give you reddit gold idk whatever you want lol.", "author_fullname": "t2_hfasi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "has anyone migrated from spark to bigquery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10c9kad", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673754046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are testing bigquery and have seen some promising results. However I would love to talk to someone who has done a spark -&amp;gt; bigquery migration to see how it turned out. Also if anyone would be willing to hop on a call and talk about it I would be willing to buy you a beer/coffee or give you reddit gold idk whatever you want lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10c9kad", "is_robot_indexable": true, "report_reasons": null, "author": "sturdyplum", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10c9kad/has_anyone_migrated_from_spark_to_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10c9kad/has_anyone_migrated_from_spark_to_bigquery/", "subreddit_subscribers": 86302, "created_utc": 1673754046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5siqrpo0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exclusive Interview with Payal - Placed at DELOITTE as a DATA ENGINEER |...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_10c97rm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.53, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/08Ct0qDGUJ0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Exclusive Interview with Payal - Placed at DELOITTE as a DATA ENGINEER | 360DigiTMG\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"oembed": {"provider_url": "https://www.youtube.com/", "title": "Exclusive Interview with Payal - Placed at DELOITTE as a DATA ENGINEER | 360DigiTMG", "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/08Ct0qDGUJ0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Exclusive Interview with Payal - Placed at DELOITTE as a DATA ENGINEER | 360DigiTMG\"&gt;&lt;/iframe&gt;", "thumbnail_width": 480, "height": 200, "width": 356, "version": "1.0", "author_name": "360DigiTMG", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/08Ct0qDGUJ0/hqdefault.jpg", "type": "video", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@360DigiTMG"}, "type": "youtube.com"}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/08Ct0qDGUJ0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Exclusive Interview with Payal - Placed at DELOITTE as a DATA ENGINEER | 360DigiTMG\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/10c97rm", "height": 200}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YjSvrH-griz_JtpQ6IGUCoE2KyBLAuGJsUm7FNsduyY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673753002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtube.com/watch?v=08Ct0qDGUJ0&amp;feature=share", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oythRvdq9cJwkrgiXq9cXXIhFFgGqjNVOIE2rgPd1ko.jpg?auto=webp&amp;v=enabled&amp;s=31b00358ea0e3005eee2ee66c5bb3b5536c06576", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/oythRvdq9cJwkrgiXq9cXXIhFFgGqjNVOIE2rgPd1ko.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ff16636b5a88da8698f9acb14f5870b4eb79f8f", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/oythRvdq9cJwkrgiXq9cXXIhFFgGqjNVOIE2rgPd1ko.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=347ddc445bc8bfe3a37ffd34e2069905a5ad67b5", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/oythRvdq9cJwkrgiXq9cXXIhFFgGqjNVOIE2rgPd1ko.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ee6f197535de5f3c1ea700c1c3ce9ae85acd237", "width": 320, "height": 240}], "variants": {}, "id": "Yu_J1j5G1l6LosDqs58v0EstrWi0uNOSi8l2BY7balg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10c97rm", "is_robot_indexable": true, "report_reasons": null, "author": "pmkvsreddy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10c97rm/exclusive_interview_with_payal_placed_at_deloitte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtube.com/watch?v=08Ct0qDGUJ0&amp;feature=share", "subreddit_subscribers": 86302, "created_utc": 1673753002.0, "num_crossposts": 0, "media": {"oembed": {"provider_url": "https://www.youtube.com/", "title": "Exclusive Interview with Payal - Placed at DELOITTE as a DATA ENGINEER | 360DigiTMG", "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/08Ct0qDGUJ0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Exclusive Interview with Payal - Placed at DELOITTE as a DATA ENGINEER | 360DigiTMG\"&gt;&lt;/iframe&gt;", "thumbnail_width": 480, "height": 200, "width": 356, "version": "1.0", "author_name": "360DigiTMG", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/08Ct0qDGUJ0/hqdefault.jpg", "type": "video", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@360DigiTMG"}, "type": "youtube.com"}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "CONSTRAINT: Inhouse deployment\n\nWhat could be your to be state architecture ? and why ? \\[Keep in mind a data LakeHouse architecture along with BI\\]\n\nWhat are the industry tech stack design patterns", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "End Goal: Executive Dashboard ( PetaByte scale of Data coming from multiple sources)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10c4x82", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673741087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;CONSTRAINT: Inhouse deployment&lt;/p&gt;\n\n&lt;p&gt;What could be your to be state architecture ? and why ? [Keep in mind a data LakeHouse architecture along with BI]&lt;/p&gt;\n\n&lt;p&gt;What are the industry tech stack design patterns&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10c4x82", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10c4x82/end_goal_executive_dashboard_petabyte_scale_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10c4x82/end_goal_executive_dashboard_petabyte_scale_of/", "subreddit_subscribers": 86302, "created_utc": 1673741087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Basically, as the title says. I have an exam on data-acquisition in about a week from now and the professor's slides are just garbage. I can't seem to wrap my head around the basic concepts of data management, and every time I try to search it online I get just more terms i've never heard of. There is no textbook available and his lessons aren't recorded so I cannot watch them again. This is kind of reallly my last resort (as you will have probably guessed by now:'))   \nFor those who want to help, thanks in advance!", "author_fullname": "t2_369qm6d1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Searching for someone who can explain to me in plain english what the difference is between MRAA, SMBus2 and UPM.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10bwl2f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673723457.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically, as the title says. I have an exam on data-acquisition in about a week from now and the professor&amp;#39;s slides are just garbage. I can&amp;#39;t seem to wrap my head around the basic concepts of data management, and every time I try to search it online I get just more terms i&amp;#39;ve never heard of. There is no textbook available and his lessons aren&amp;#39;t recorded so I cannot watch them again. This is kind of reallly my last resort (as you will have probably guessed by now:&amp;#39;))&lt;br/&gt;\nFor those who want to help, thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10bwl2f", "is_robot_indexable": true, "report_reasons": null, "author": "no-meme-lord69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10bwl2f/searching_for_someone_who_can_explain_to_me_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10bwl2f/searching_for_someone_who_can_explain_to_me_in/", "subreddit_subscribers": 86302, "created_utc": 1673723457.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is anyone recently appeared at Tradebyte, Germany for a data engineer position?\n\nWould like to understand what kind of task they give in the last round of interviews?", "author_fullname": "t2_b4meoomb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tradebyte, Germany data engineer interview!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10bjc4n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673684272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone recently appeared at Tradebyte, Germany for a data engineer position?&lt;/p&gt;\n\n&lt;p&gt;Would like to understand what kind of task they give in the last round of interviews?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10bjc4n", "is_robot_indexable": true, "report_reasons": null, "author": "Monika_Chavan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10bjc4n/tradebyte_germany_data_engineer_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10bjc4n/tradebyte_germany_data_engineer_interview/", "subreddit_subscribers": 86302, "created_utc": 1673684272.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}