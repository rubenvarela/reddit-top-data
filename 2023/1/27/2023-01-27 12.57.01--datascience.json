{"kind": "Listing", "data": {"after": "t3_10m95dq", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I have just wrapped up first round interviews for an open position on my team. I work in banking and most of my career involves building regression or logistic regression models.\n\nOne of the trends I've seen since the tech data science boom started is that there just seems to be a drop in the technical level for peoples with masters degree on fundamentals. It seems too many candidates with masters degrees  do not understand mathematical assumptions of most of the models they are using even at a conceptual level.   For example, during the interview I asked most candidates about regression and what assumptions are required.\n\nNearly every single masters level candidate didn't know why the specific assumptions were made (even if they could correctly list them), could not answer questions on what happens when you violate an assumption, and did not know how to test violation of those assumptions or how to address those issues. Whats disconcerting is these are candidates coming out of professional masters programs from the worlds leading universities and most of them will end up in jobs where modeling error can have multi-million dollar impacts.\n\nFor some additional context: The comment here is explicitly here about standard of candidates I interviewed for people with masters degrees. Most of the Ph.D jobs met standards we expect, even though the job does not require one. The job is one that is very specifically related to regression modeling, time series.", "author_fullname": "t2_je0a2jeu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm a tired of interviewing fresh graduates that don't know fundamentals.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10m6kpq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 290, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 290, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674786789.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674777528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have just wrapped up first round interviews for an open position on my team. I work in banking and most of my career involves building regression or logistic regression models.&lt;/p&gt;\n\n&lt;p&gt;One of the trends I&amp;#39;ve seen since the tech data science boom started is that there just seems to be a drop in the technical level for peoples with masters degree on fundamentals. It seems too many candidates with masters degrees  do not understand mathematical assumptions of most of the models they are using even at a conceptual level.   For example, during the interview I asked most candidates about regression and what assumptions are required.&lt;/p&gt;\n\n&lt;p&gt;Nearly every single masters level candidate didn&amp;#39;t know why the specific assumptions were made (even if they could correctly list them), could not answer questions on what happens when you violate an assumption, and did not know how to test violation of those assumptions or how to address those issues. Whats disconcerting is these are candidates coming out of professional masters programs from the worlds leading universities and most of them will end up in jobs where modeling error can have multi-million dollar impacts.&lt;/p&gt;\n\n&lt;p&gt;For some additional context: The comment here is explicitly here about standard of candidates I interviewed for people with masters degrees. Most of the Ph.D jobs met standards we expect, even though the job does not require one. The job is one that is very specifically related to regression modeling, time series.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 70, "id": "award_b1b44fa1-8179-4d84-a9ed-f25bb81f1c5f", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/ey2iodron2s41_Facepalm.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/ey2iodron2s41_Facepalm.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=a6f300e1309aa93756b3a36593bd8f606520e508", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/ey2iodron2s41_Facepalm.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=1af1af2596ad64df163b8b17cad4eee836939c2c", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/ey2iodron2s41_Facepalm.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=b0237705862865dce1a1db3ea40064155eb14d58", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/ey2iodron2s41_Facepalm.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=54dde5ae7d66ed63151707df52341663e3d9669e", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/ey2iodron2s41_Facepalm.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=2d0a1a9b902c3b3acaf3560c9d3662c19d3cd0cf", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "*Lowers face into palm*", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Facepalm", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/ey2iodron2s41_Facepalm.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=a6f300e1309aa93756b3a36593bd8f606520e508", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/ey2iodron2s41_Facepalm.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=1af1af2596ad64df163b8b17cad4eee836939c2c", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/ey2iodron2s41_Facepalm.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=b0237705862865dce1a1db3ea40064155eb14d58", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/ey2iodron2s41_Facepalm.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=54dde5ae7d66ed63151707df52341663e3d9669e", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/ey2iodron2s41_Facepalm.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=2d0a1a9b902c3b3acaf3560c9d3662c19d3cd0cf", "width": 128, "height": 128}], "icon_format": "PNG", "icon_height": 2048, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/ey2iodron2s41_Facepalm.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10m6kpq", "is_robot_indexable": true, "report_reasons": null, "author": "nanashiaoe2de", "discussion_type": null, "num_comments": 354, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10m6kpq/im_a_tired_of_interviewing_fresh_graduates_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10m6kpq/im_a_tired_of_interviewing_fresh_graduates_that/", "subreddit_subscribers": 841510, "created_utc": 1674777528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am DS director. I have 8 YOE in analytics and 6 YOE in DS. My base is 175k and I get 20% bonus which fluctuates. No equity, RSU, or ESPP.\n\nI don\u2019t want to move up because I don\u2019t want to do more management tasks. I see people on here talking about making 200k+.\n\nAny ideas on how to make more without moving further into management?", "author_fullname": "t2_kcl3tfwe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I make more money?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10lsdyo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 108, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 108, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674741472.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am DS director. I have 8 YOE in analytics and 6 YOE in DS. My base is 175k and I get 20% bonus which fluctuates. No equity, RSU, or ESPP.&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t want to move up because I don\u2019t want to do more management tasks. I see people on here talking about making 200k+.&lt;/p&gt;\n\n&lt;p&gt;Any ideas on how to make more without moving further into management?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10lsdyo", "is_robot_indexable": true, "report_reasons": null, "author": "NewEcho2940", "discussion_type": null, "num_comments": 134, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10lsdyo/how_do_i_make_more_money/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10lsdyo/how_do_i_make_more_money/", "subreddit_subscribers": 841510, "created_utc": 1674741472.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work as a data analyst and have a physics background, and to be honest I\u2019ve always had a relatively pragmatic approach to statistical programming. I\u2019ll write functions when it seems like I\u2019m going to be doing something a lot, otherwise I won\u2019t.\n\nIn my work I\u2019ll quite often start a new R script, or SQL script, and rather than write *everything* from scratch, I\u2019ll typically copy and paste the code across from previous scripts - E.g. package loading in R, functions I use a lot, things like read_csv but just change the file path. Even though a lot of this is just lazy stuff that I probably know how to do, I feel like I don\u2019t have the time to just write it all out from scratch again.\n\nJust wondering if anyone else does things like this?", "author_fullname": "t2_1x2kyanz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How often do you copy and paste code?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10lz6vf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674758887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a data analyst and have a physics background, and to be honest I\u2019ve always had a relatively pragmatic approach to statistical programming. I\u2019ll write functions when it seems like I\u2019m going to be doing something a lot, otherwise I won\u2019t.&lt;/p&gt;\n\n&lt;p&gt;In my work I\u2019ll quite often start a new R script, or SQL script, and rather than write &lt;em&gt;everything&lt;/em&gt; from scratch, I\u2019ll typically copy and paste the code across from previous scripts - E.g. package loading in R, functions I use a lot, things like read_csv but just change the file path. Even though a lot of this is just lazy stuff that I probably know how to do, I feel like I don\u2019t have the time to just write it all out from scratch again.&lt;/p&gt;\n\n&lt;p&gt;Just wondering if anyone else does things like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10lz6vf", "is_robot_indexable": true, "report_reasons": null, "author": "JLane1996", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10lz6vf/how_often_do_you_copy_and_paste_code/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10lz6vf/how_often_do_you_copy_and_paste_code/", "subreddit_subscribers": 841510, "created_utc": 1674758887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve been job searching since November 2022 and my severance just ran out.\n\nI received an official offer letter from company A over a week ago and today learned that another, company B, wants to make an offer, but probably won\u2019t have it until next week.\n\nFrom the quoted figures, I believe company B won\u2019t be able to match my current offer (they are also not yet public, whereas A), and I\u2019m already tired of negotiating/interviewing.\n\nIs there a way to gracefully withdraw with company B? I told them I already got an offer and it\u2019s a difficult decision. I just don\u2019t want to wait anymore to secure a job given the market conditions.", "author_fullname": "t2_3gi52xce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to withdraw from job offer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10m4ois", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674772497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been job searching since November 2022 and my severance just ran out.&lt;/p&gt;\n\n&lt;p&gt;I received an official offer letter from company A over a week ago and today learned that another, company B, wants to make an offer, but probably won\u2019t have it until next week.&lt;/p&gt;\n\n&lt;p&gt;From the quoted figures, I believe company B won\u2019t be able to match my current offer (they are also not yet public, whereas A), and I\u2019m already tired of negotiating/interviewing.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to gracefully withdraw with company B? I told them I already got an offer and it\u2019s a difficult decision. I just don\u2019t want to wait anymore to secure a job given the market conditions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "10m4ois", "is_robot_indexable": true, "report_reasons": null, "author": "questforthrowaway", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10m4ois/how_to_withdraw_from_job_offer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10m4ois/how_to_withdraw_from_job_offer/", "subreddit_subscribers": 841510, "created_utc": 1674772497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Could you give some practical examples of when this question has come up in your work and why you made your decision? Sorry if this is very vague I feel as though I'm missing a piece of information that makes the answer to this question very obvious", "author_fullname": "t2_aghpz5g9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When would you choose to conduct a hypothesis test vs run a linear regression?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10m1b5l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674764144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could you give some practical examples of when this question has come up in your work and why you made your decision? Sorry if this is very vague I feel as though I&amp;#39;m missing a piece of information that makes the answer to this question very obvious&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10m1b5l", "is_robot_indexable": true, "report_reasons": null, "author": "Careless-Tailor-2317", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10m1b5l/when_would_you_choose_to_conduct_a_hypothesis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10m1b5l/when_would_you_choose_to_conduct_a_hypothesis/", "subreddit_subscribers": 841510, "created_utc": 1674764144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As the title mentions, I\u2019ve been in this sub for around 5 months. Hoping to see more in-depth data science discussion rather than students trying to justify that data science is worth going into or how to break into data science from another career. I don\u2019t think I\u2019ve seen another sub quite like this one with the degree of questions asked like this from people outside of the field. Especially how many duplicate/repeat questions I see every day.\n\nAre there better sub reddits to be in besides this where more advanced topics are discussed?", "author_fullname": "t2_t3mak0w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this sub only for new grads, students, and career swaps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10mf4gx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674804509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title mentions, I\u2019ve been in this sub for around 5 months. Hoping to see more in-depth data science discussion rather than students trying to justify that data science is worth going into or how to break into data science from another career. I don\u2019t think I\u2019ve seen another sub quite like this one with the degree of questions asked like this from people outside of the field. Especially how many duplicate/repeat questions I see every day.&lt;/p&gt;\n\n&lt;p&gt;Are there better sub reddits to be in besides this where more advanced topics are discussed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10mf4gx", "is_robot_indexable": true, "report_reasons": null, "author": "801Fluidity", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10mf4gx/is_this_sub_only_for_new_grads_students_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10mf4gx/is_this_sub_only_for_new_grads_students_and/", "subreddit_subscribers": 841510, "created_utc": 1674804509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm following 2 papers by Fader et al. for calculating LTV for a subscription based business. \n\nI even found a post that breaks the method down in simpler terms. \n\nhttps://towardsdatascience.com/customer-lifetime-value-in-a-discrete-time-contractual-setting-math-and-python-implementation-af3ef606cefe\n\nFollowing the paper found below, the author picks a single imaginary yearly cohort and uses that to calculate LTV. It all makes sense and I get it, but what I don't understand is how do we account for multiple cohorts using this method? Would we perform the calculation for each yearly cohort and take an average result for the LTV? Or randomly sample users from all cohorts and perform survival calculations based on this.\n\nHas anybody got experience using this method? If you could provide any insight it would be much appreciated.\n\nhttp://brucehardie.com/notes/032/", "author_fullname": "t2_2gzsok4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modelling LTV in a discrete time contractual setting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10m379s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674768881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m following 2 papers by Fader et al. for calculating LTV for a subscription based business. &lt;/p&gt;\n\n&lt;p&gt;I even found a post that breaks the method down in simpler terms. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://towardsdatascience.com/customer-lifetime-value-in-a-discrete-time-contractual-setting-math-and-python-implementation-af3ef606cefe\"&gt;https://towardsdatascience.com/customer-lifetime-value-in-a-discrete-time-contractual-setting-math-and-python-implementation-af3ef606cefe&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Following the paper found below, the author picks a single imaginary yearly cohort and uses that to calculate LTV. It all makes sense and I get it, but what I don&amp;#39;t understand is how do we account for multiple cohorts using this method? Would we perform the calculation for each yearly cohort and take an average result for the LTV? Or randomly sample users from all cohorts and perform survival calculations based on this.&lt;/p&gt;\n\n&lt;p&gt;Has anybody got experience using this method? If you could provide any insight it would be much appreciated.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://brucehardie.com/notes/032/\"&gt;http://brucehardie.com/notes/032/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pXzYZYo4O7BfVBtyY-LFOXgKFpgK205LDtsV5IBdAaw.jpg?auto=webp&amp;v=enabled&amp;s=82068897da034df431f9bc4b5e4bae7cd520b185", "width": 612, "height": 390}, "resolutions": [{"url": "https://external-preview.redd.it/pXzYZYo4O7BfVBtyY-LFOXgKFpgK205LDtsV5IBdAaw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82d0f73815f3b97fd963f24044046de566a10d16", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/pXzYZYo4O7BfVBtyY-LFOXgKFpgK205LDtsV5IBdAaw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a43a7a8a4d6b5c77ecc4b2d28ce2368161b1254", "width": 216, "height": 137}, {"url": "https://external-preview.redd.it/pXzYZYo4O7BfVBtyY-LFOXgKFpgK205LDtsV5IBdAaw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fcd8c094f575c7411dd7f03666f617bbd78da933", "width": 320, "height": 203}], "variants": {}, "id": "is0nRhFUvVstxQPK9TAy3PJgeakQufJMQw2tfVCBmFg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10m379s", "is_robot_indexable": true, "report_reasons": null, "author": "ciarandeceol1", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10m379s/modelling_ltv_in_a_discrete_time_contractual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10m379s/modelling_ltv_in_a_discrete_time_contractual/", "subreddit_subscribers": 841510, "created_utc": 1674768881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm a senior data analyst and I regularly pull out data and make dashboards - basically grunt work. I'm curious as to what BI managers do? In my org, all analysts are just a part of the department they're in, there's no separate BI department. \n\nDo BI Managers just sit around and write a strategy document that says this is how requests will be fulfilled, dashboards should have these KPIs, models should be x etc?", "author_fullname": "t2_8uvbqmtb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do BI Managers do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10m2viw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674768073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a senior data analyst and I regularly pull out data and make dashboards - basically grunt work. I&amp;#39;m curious as to what BI managers do? In my org, all analysts are just a part of the department they&amp;#39;re in, there&amp;#39;s no separate BI department. &lt;/p&gt;\n\n&lt;p&gt;Do BI Managers just sit around and write a strategy document that says this is how requests will be fulfilled, dashboards should have these KPIs, models should be x etc?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10m2viw", "is_robot_indexable": true, "report_reasons": null, "author": "informatica6", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10m2viw/what_do_bi_managers_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10m2viw/what_do_bi_managers_do/", "subreddit_subscribers": 841510, "created_utc": 1674768073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m a full time insurtech data scientist for over a year, and looking to switch, what are some topics I should most definitely study for?", "author_fullname": "t2_tq6mxkv8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data scientist hiring managers, what is something you ask in an interview that makes or breaks the deal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10m9qtz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674786643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a full time insurtech data scientist for over a year, and looking to switch, what are some topics I should most definitely study for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "10m9qtz", "is_robot_indexable": true, "report_reasons": null, "author": "notmynameduh", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10m9qtz/data_scientist_hiring_managers_what_is_something/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10m9qtz/data_scientist_hiring_managers_what_is_something/", "subreddit_subscribers": 841510, "created_utc": 1674786643.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, \n\nI am trying to find the best multidimensional combination of parameters for an application based on a given metric by using bayesian optimisation to search the parameter space and efficiently find the most optimal parameters with the fewest number of evaluations. The model gives some sets of parameters, some that it has a high prediction of the metric, and others it has high uncertainty about. \n\nThese 2-300 outputs per cycle are then experimentally validated, and the accumulated results fed back into the model to get a better set of parameters for the next iterations for a total of about 6 iterations (12-1500 data points total) . The search space is large, and there are a limited amount of iterations that can be performed. \n\nBecause of this, I need to evaluate several surrogate models on their performance within this search space. I need to evaluate the search efficiency (how quickly can each one find the most optimal candidates e.g one will take 3 cycles, the other 8, other 20 etc) and the theoretical proportion of the search space that each model can search given the same data e.g 20% of the search space given 3% experimentally validated data from the search space.\n\nI am using the BoTorch library to build the bayesian optimisation model. I also already have a set of real world experimental data from different cycles from the first model I tried. \n\nI would like to know how to go about evaluating these models for the search efficiency and design space uncertainty, any thoughts are welcome. \n\nThanks.", "author_fullname": "t2_9zwp7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Evaluating Bayesian Optimisation search efficiency and uncertainty", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10mhggd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674814310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I am trying to find the best multidimensional combination of parameters for an application based on a given metric by using bayesian optimisation to search the parameter space and efficiently find the most optimal parameters with the fewest number of evaluations. The model gives some sets of parameters, some that it has a high prediction of the metric, and others it has high uncertainty about. &lt;/p&gt;\n\n&lt;p&gt;These 2-300 outputs per cycle are then experimentally validated, and the accumulated results fed back into the model to get a better set of parameters for the next iterations for a total of about 6 iterations (12-1500 data points total) . The search space is large, and there are a limited amount of iterations that can be performed. &lt;/p&gt;\n\n&lt;p&gt;Because of this, I need to evaluate several surrogate models on their performance within this search space. I need to evaluate the search efficiency (how quickly can each one find the most optimal candidates e.g one will take 3 cycles, the other 8, other 20 etc) and the theoretical proportion of the search space that each model can search given the same data e.g 20% of the search space given 3% experimentally validated data from the search space.&lt;/p&gt;\n\n&lt;p&gt;I am using the BoTorch library to build the bayesian optimisation model. I also already have a set of real world experimental data from different cycles from the first model I tried. &lt;/p&gt;\n\n&lt;p&gt;I would like to know how to go about evaluating these models for the search efficiency and design space uncertainty, any thoughts are welcome. &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10mhggd", "is_robot_indexable": true, "report_reasons": null, "author": "ogola89", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10mhggd/evaluating_bayesian_optimisation_search/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10mhggd/evaluating_bayesian_optimisation_search/", "subreddit_subscribers": 841510, "created_utc": 1674814310.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_hbglt7lp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An interesting major project idea for college level related to data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10mg9op", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674809272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10mg9op", "is_robot_indexable": true, "report_reasons": null, "author": "mukeshpilane", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10mg9op/an_interesting_major_project_idea_for_college/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10mg9op/an_interesting_major_project_idea_for_college/", "subreddit_subscribers": 841510, "created_utc": 1674809272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi! Im working on a project with huge amounts of data, actually, the model we are using is xgboost, im trying to increase the performance of my model and I came up with the idea of clusterize my data in an unsupervised way (with out the target in the dataset) and use the clusterization results as a new feature. So I fit several k means (k=3, k= 7, k= 10 and k = 15) to my training dataset and use the clusters as new features to try to improve my xgboost.\n\nDo you think my approach has sense?", "author_fullname": "t2_8j69r4rr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "K-means clusters as features for more complex models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10mc1mz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674793633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Im working on a project with huge amounts of data, actually, the model we are using is xgboost, im trying to increase the performance of my model and I came up with the idea of clusterize my data in an unsupervised way (with out the target in the dataset) and use the clusterization results as a new feature. So I fit several k means (k=3, k= 7, k= 10 and k = 15) to my training dataset and use the clusters as new features to try to improve my xgboost.&lt;/p&gt;\n\n&lt;p&gt;Do you think my approach has sense?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10mc1mz", "is_robot_indexable": true, "report_reasons": null, "author": "_bmph_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10mc1mz/kmeans_clusters_as_features_for_more_complex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10mc1mz/kmeans_clusters_as_features_for_more_complex/", "subreddit_subscribers": 841510, "created_utc": 1674793633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Good morning \n\nI am working on an interesting report for management. I am taking all the usage and cost reports for all mobile devices (hotspots, tablets, cell phones) in the organization (by cost center) and creating a dashboard to show utilization. The initial request was for a report to show outliers. But the more I dig into the data, the more I\u2019m convinced it isn\u2019t the outliers so much, it\u2019s the un/under utilization of devices that is noteworthy. \n\nIf an employee is using their company cell phone for thousands of minutes a month, but we are paying a standard flat rate for an unlimited plan, that is less meaningful to know than a user who isn\u2019t using their phone at all. \n\nUnused lines are easy, because it\u2019s a simple measure. Underutilized could be easy by simply taking everything below the average, but how do you qualify those lines that are underutilized is less valuable?\n\nSo question one is, how would you go about identifying and assigning a value to \u201cunderutilized\u201d lines?\n\nUnused lines are tricky as well. It would be easy to say, \u201cget rid of any device where minutes or data = 0.\u201d But what happens if a device needs to be replaced? And usage is a point in time measurement. A device could have 0 usage one month, and then above average the next. So it\u2019s really the duration the device is unused. \n\nSecond question is, what should that duration be? \n\nFinally, all the devices have upgrade eligibility dates. There are high demand users on very old devices and a few old, unused devices, and a few newer way underutilized devices. \n\nFinal question, how would you create a report/model that recommends a replacement?\n\nI appreciate any insight. \n\nThanks!", "author_fullname": "t2_9pmoyw4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mobile Device Inventory Management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10lssd2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674742613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good morning &lt;/p&gt;\n\n&lt;p&gt;I am working on an interesting report for management. I am taking all the usage and cost reports for all mobile devices (hotspots, tablets, cell phones) in the organization (by cost center) and creating a dashboard to show utilization. The initial request was for a report to show outliers. But the more I dig into the data, the more I\u2019m convinced it isn\u2019t the outliers so much, it\u2019s the un/under utilization of devices that is noteworthy. &lt;/p&gt;\n\n&lt;p&gt;If an employee is using their company cell phone for thousands of minutes a month, but we are paying a standard flat rate for an unlimited plan, that is less meaningful to know than a user who isn\u2019t using their phone at all. &lt;/p&gt;\n\n&lt;p&gt;Unused lines are easy, because it\u2019s a simple measure. Underutilized could be easy by simply taking everything below the average, but how do you qualify those lines that are underutilized is less valuable?&lt;/p&gt;\n\n&lt;p&gt;So question one is, how would you go about identifying and assigning a value to \u201cunderutilized\u201d lines?&lt;/p&gt;\n\n&lt;p&gt;Unused lines are tricky as well. It would be easy to say, \u201cget rid of any device where minutes or data = 0.\u201d But what happens if a device needs to be replaced? And usage is a point in time measurement. A device could have 0 usage one month, and then above average the next. So it\u2019s really the duration the device is unused. &lt;/p&gt;\n\n&lt;p&gt;Second question is, what should that duration be? &lt;/p&gt;\n\n&lt;p&gt;Finally, all the devices have upgrade eligibility dates. There are high demand users on very old devices and a few old, unused devices, and a few newer way underutilized devices. &lt;/p&gt;\n\n&lt;p&gt;Final question, how would you create a report/model that recommends a replacement?&lt;/p&gt;\n\n&lt;p&gt;I appreciate any insight. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10lssd2", "is_robot_indexable": true, "report_reasons": null, "author": "dat0dat", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10lssd2/mobile_device_inventory_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10lssd2/mobile_device_inventory_management/", "subreddit_subscribers": 841510, "created_utc": 1674742613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I just started browsing this sub again for the first time in maybe 6 months. I noticed all the hot threads had no users with mod-confirmed jobs or education. I recall the largerly popular and trusted posts/responses that helped me get into this field being from PhDs and highly experienced individuals. Is this practice still advertised by the mods?", "author_fullname": "t2_jfue2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Low prevalence of user education/job mod tags.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10mc40o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": "", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674793843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just started browsing this sub again for the first time in maybe 6 months. I noticed all the hot threads had no users with mod-confirmed jobs or education. I recall the largerly popular and trusted posts/responses that helped me get into this field being from PhDs and highly experienced individuals. Is this practice still advertised by the mods?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "MS | Data Scientist", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10mc40o", "is_robot_indexable": true, "report_reasons": null, "author": "skeerp", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/datascience/comments/10mc40o/low_prevalence_of_user_educationjob_mod_tags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10mc40o/low_prevalence_of_user_educationjob_mod_tags/", "subreddit_subscribers": 841510, "created_utc": 1674793843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019d like to brainstorm a problem I\u2019ve been mulling. I\u2019m working with a very large transactional database. (Billions of rows/ 30+ columns). \n\nEach row represents a single action from a user (e.g. one row for attempting to log in, another if the user buys a product, another if the user modifies their profile).\n\nI want to create stories (groupings) of events (rows) to summarize user activity better. E.g. this user logged in and searched, but never bought anything and made no changes to their profile. \n\nThere are dual goals with this project. One is to create a repeatable process (or potentially a new database) that categorizes user events. The second is to eventually build an algorithm (probably k-means, maybe others) to evaluate outlying activity trends. Eventually the algorithm would feed into a MLM, but that\u2019s a bit far off at the moment. \n\nI was going to do the grunt work of if/case statements based on understanding the different columns in the database. But I\u2019d like to brainstorm with others. Are there other methods that you\u2019d recommend?", "author_fullname": "t2_uyoinwrz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Categorizing groups of events in a transactional database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10mbewe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674791659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019d like to brainstorm a problem I\u2019ve been mulling. I\u2019m working with a very large transactional database. (Billions of rows/ 30+ columns). &lt;/p&gt;\n\n&lt;p&gt;Each row represents a single action from a user (e.g. one row for attempting to log in, another if the user buys a product, another if the user modifies their profile).&lt;/p&gt;\n\n&lt;p&gt;I want to create stories (groupings) of events (rows) to summarize user activity better. E.g. this user logged in and searched, but never bought anything and made no changes to their profile. &lt;/p&gt;\n\n&lt;p&gt;There are dual goals with this project. One is to create a repeatable process (or potentially a new database) that categorizes user events. The second is to eventually build an algorithm (probably k-means, maybe others) to evaluate outlying activity trends. Eventually the algorithm would feed into a MLM, but that\u2019s a bit far off at the moment. &lt;/p&gt;\n\n&lt;p&gt;I was going to do the grunt work of if/case statements based on understanding the different columns in the database. But I\u2019d like to brainstorm with others. Are there other methods that you\u2019d recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10mbewe", "is_robot_indexable": true, "report_reasons": null, "author": "Nerdy_birdy747", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10mbewe/categorizing_groups_of_events_in_a_transactional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10mbewe/categorizing_groups_of_events_in_a_transactional/", "subreddit_subscribers": 841510, "created_utc": 1674791659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm working on a project where data is naturally and necessarily collected in a row-by-row tabular format. I have a strong suspicious that analysis and exploitation of the data could be revolutionised if it were transposed into a graphDB, but I don't really have any direct experience or good examples to back this up.\n\nHas anyone here made a similar transition and made some kind of step change in either analysis efficiency or in terms of the value that can be extracted from a dataset?\n\nI know there are classic examples of where network analysis can be well utilized (social network analysis comes to the top of my mind), but I'd love to hear from anyone with direct industry experience. If it's an unexpected angle, all the better.", "author_fullname": "t2_82ygfecl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone transitioned a project from a relational DB to a graphDB and realised major improvements in performance or efficiency?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10mazhm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674790330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a project where data is naturally and necessarily collected in a row-by-row tabular format. I have a strong suspicious that analysis and exploitation of the data could be revolutionised if it were transposed into a graphDB, but I don&amp;#39;t really have any direct experience or good examples to back this up.&lt;/p&gt;\n\n&lt;p&gt;Has anyone here made a similar transition and made some kind of step change in either analysis efficiency or in terms of the value that can be extracted from a dataset?&lt;/p&gt;\n\n&lt;p&gt;I know there are classic examples of where network analysis can be well utilized (social network analysis comes to the top of my mind), but I&amp;#39;d love to hear from anyone with direct industry experience. If it&amp;#39;s an unexpected angle, all the better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10mazhm", "is_robot_indexable": true, "report_reasons": null, "author": "recovering_physicist", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10mazhm/has_anyone_transitioned_a_project_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10mazhm/has_anyone_transitioned_a_project_from_a/", "subreddit_subscribers": 841510, "created_utc": 1674790330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_vb57b4y3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quantum Machine Learning: An Advanced End-to-End Project Tutorial", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_10m8aq0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BO7YH2pvkQn7B9NooCFQmTdn1vzzIfBj9cJVJQ7Rw6w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674782361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/p/3316c542226d", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SVIx_UjYXatXGelCdaKd7katZKHIKg7PvZYaIK_OvKU.jpg?auto=webp&amp;v=enabled&amp;s=37499d290de5e0be1d980df91710e4b85fee2fe9", "width": 960, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/SVIx_UjYXatXGelCdaKd7katZKHIKg7PvZYaIK_OvKU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26e62a1280b3aa27203d8154d9874ea4b535a931", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/SVIx_UjYXatXGelCdaKd7katZKHIKg7PvZYaIK_OvKU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5da7a188277092155d2cee431e6d10528b6accfb", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/SVIx_UjYXatXGelCdaKd7katZKHIKg7PvZYaIK_OvKU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cef08db1caab416657c9941a55fa60124c8c9f11", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/SVIx_UjYXatXGelCdaKd7katZKHIKg7PvZYaIK_OvKU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=beaf1fa7dfb012773e11b7c47299f2722b165e6b", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/SVIx_UjYXatXGelCdaKd7katZKHIKg7PvZYaIK_OvKU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=52bbf7d1c1ed5a065bd17bba637dcb18bbeb50e3", "width": 960, "height": 640}], "variants": {}, "id": "hwO60Vy8qz7fzvS6lzOlw0e8uT-9LJaux15ofFCZjoE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "10m8aq0", "is_robot_indexable": true, "report_reasons": null, "author": "Historical-Pen9653", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10m8aq0/quantum_machine_learning_an_advanced_endtoend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/p/3316c542226d", "subreddit_subscribers": 841510, "created_utc": 1674782361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I would like to ask if someone of you has ever faced a problem in which the main available data are a combination between time series and static variables.\nAlso the anomaly has to be predicted in advance with respect to its occurring (i.e. predictive maintenance problem)\n\nLet\u2019s say that both of the type of data (dynamic-time series and static-fixed) are strictly important because based on information in static variables there are different behaviors in the time series for the same channel.\n\nWhat state of the art approaches could be used to manage early anomaly detection on this kind of data?", "author_fullname": "t2_i6lyoywf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "early anomaly detection on time series and additional static variables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10m6t3i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674778140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I would like to ask if someone of you has ever faced a problem in which the main available data are a combination between time series and static variables.\nAlso the anomaly has to be predicted in advance with respect to its occurring (i.e. predictive maintenance problem)&lt;/p&gt;\n\n&lt;p&gt;Let\u2019s say that both of the type of data (dynamic-time series and static-fixed) are strictly important because based on information in static variables there are different behaviors in the time series for the same channel.&lt;/p&gt;\n\n&lt;p&gt;What state of the art approaches could be used to manage early anomaly detection on this kind of data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10m6t3i", "is_robot_indexable": true, "report_reasons": null, "author": "ginotherhino1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10m6t3i/early_anomaly_detection_on_time_series_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10m6t3i/early_anomaly_detection_on_time_series_and/", "subreddit_subscribers": 841510, "created_utc": 1674778140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've spent this entire month so far refactoring code that relies on dependencies that broke in the new year. The carnage ranges from our highest-level internal libraries, all the way down to the basic development infrastructure we rely on. I've even had to troubleshoot critical new bugs in Zoom, Outlook, and OneDrive.\n\nIs there any clever phrase to describe this phenomenon, where every coder on the planet chooses to push breaking changes at the turn of the year? And then every broken dependency fix creates a domino effect of dependency conflicts.", "author_fullname": "t2_u0krl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Everything Breaks\" January", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10m6aq0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674776744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve spent this entire month so far refactoring code that relies on dependencies that broke in the new year. The carnage ranges from our highest-level internal libraries, all the way down to the basic development infrastructure we rely on. I&amp;#39;ve even had to troubleshoot critical new bugs in Zoom, Outlook, and OneDrive.&lt;/p&gt;\n\n&lt;p&gt;Is there any clever phrase to describe this phenomenon, where every coder on the planet chooses to push breaking changes at the turn of the year? And then every broken dependency fix creates a domino effect of dependency conflicts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10m6aq0", "is_robot_indexable": true, "report_reasons": null, "author": "TBSchemer", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10m6aq0/everything_breaks_january/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10m6aq0/everything_breaks_january/", "subreddit_subscribers": 841510, "created_utc": 1674776744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to build an optimal workspace set-up when working with code, specifically with machine learning models. So, what's your optimal workspace/workflow when integrating tools for building ML models? Please consider that:\n\n\\- I use **my local machine** **for running and testing** the data or ML models.  \n\\- I use **Python** language.  \n\\- I need to **run the code several times** (for testing).\n\nCurrently, I'm using jupyter notebook (anaconda), but my ideal world is something like this:\n\n\\- I edit the code in Visual Studio code and I test the code running in Visual Studio (so I have the code output in VS itself). Once I find the code gives good results, I use Git to merge it, and push it to GitHub.\n\n\\- For those that could recommend just using a script, you have to consider running the whole code each time you want to test some small code block it's a big waste of time in machine learning, because you have to load and run all the data, preprocessing, ML, everything each time you run the whole script (that's why Jupyter Notebook are so commonly used among Data Scientist because it allows you to run a specific code block without having to run everything, even when Jupyter Notebooks are so labeled as tools for bad coding practices from expert programmers)\n\nI would appreciate it if you talk from your own experience :D !", "author_fullname": "t2_3vzap1d5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimal tools for your Data Science workspace?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10m2ezr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674766943.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to build an optimal workspace set-up when working with code, specifically with machine learning models. So, what&amp;#39;s your optimal workspace/workflow when integrating tools for building ML models? Please consider that:&lt;/p&gt;\n\n&lt;p&gt;- I use &lt;strong&gt;my local machine&lt;/strong&gt; &lt;strong&gt;for running and testing&lt;/strong&gt; the data or ML models.&lt;br/&gt;\n- I use &lt;strong&gt;Python&lt;/strong&gt; language.&lt;br/&gt;\n- I need to &lt;strong&gt;run the code several times&lt;/strong&gt; (for testing).&lt;/p&gt;\n\n&lt;p&gt;Currently, I&amp;#39;m using jupyter notebook (anaconda), but my ideal world is something like this:&lt;/p&gt;\n\n&lt;p&gt;- I edit the code in Visual Studio code and I test the code running in Visual Studio (so I have the code output in VS itself). Once I find the code gives good results, I use Git to merge it, and push it to GitHub.&lt;/p&gt;\n\n&lt;p&gt;- For those that could recommend just using a script, you have to consider running the whole code each time you want to test some small code block it&amp;#39;s a big waste of time in machine learning, because you have to load and run all the data, preprocessing, ML, everything each time you run the whole script (that&amp;#39;s why Jupyter Notebook are so commonly used among Data Scientist because it allows you to run a specific code block without having to run everything, even when Jupyter Notebooks are so labeled as tools for bad coding practices from expert programmers)&lt;/p&gt;\n\n&lt;p&gt;I would appreciate it if you talk from your own experience :D !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10m2ezr", "is_robot_indexable": true, "report_reasons": null, "author": "conlake", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10m2ezr/optimal_tools_for_your_data_science_workspace/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10m2ezr/optimal_tools_for_your_data_science_workspace/", "subreddit_subscribers": 841510, "created_utc": 1674766943.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everybody!\n\nI am going to graduate in MSc Data Science in a month here in Italy and I've already finished my internship as a data scientist in a public institution that cannot hire me because there are no exams at the moment nor in the near future. I currently work as a teacher in Humanities but plan to switch my career definitively in September. Up until this date I want to keep learning things and applying what I know. I've already looked for intern on Linkedin but they are full time and not allowing remote job. I am considering to collaborate with startups or doing an intern for free, since my school contract doesn't allow me to be paid and now switching is impossible to me. \n\nWorking on my projects could be a solution, but I feel that I want to have feedbacks and dialogue with people with deeper understanding in this field. Plus, an intern or a collaboration are considered as more valuable in the CV. \n\nWhere can I find some startups or projects to practice my skills? Any suggestions is extremely valued. Thank you for your time!", "author_fullname": "t2_dukqcpgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "gain experience in DS and related roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10lym1l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674757396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody!&lt;/p&gt;\n\n&lt;p&gt;I am going to graduate in MSc Data Science in a month here in Italy and I&amp;#39;ve already finished my internship as a data scientist in a public institution that cannot hire me because there are no exams at the moment nor in the near future. I currently work as a teacher in Humanities but plan to switch my career definitively in September. Up until this date I want to keep learning things and applying what I know. I&amp;#39;ve already looked for intern on Linkedin but they are full time and not allowing remote job. I am considering to collaborate with startups or doing an intern for free, since my school contract doesn&amp;#39;t allow me to be paid and now switching is impossible to me. &lt;/p&gt;\n\n&lt;p&gt;Working on my projects could be a solution, but I feel that I want to have feedbacks and dialogue with people with deeper understanding in this field. Plus, an intern or a collaboration are considered as more valuable in the CV. &lt;/p&gt;\n\n&lt;p&gt;Where can I find some startups or projects to practice my skills? Any suggestions is extremely valued. Thank you for your time!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10lym1l", "is_robot_indexable": true, "report_reasons": null, "author": "Similar-Year4215", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10lym1l/gain_experience_in_ds_and_related_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10lym1l/gain_experience_in_ds_and_related_roles/", "subreddit_subscribers": 841510, "created_utc": 1674757396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "[Link](https://cdso.utexas.edu/msai) First term appears to be in a year with spring 2024.\n\n&gt; The Master of Science in Artificial Intelligence\u00a0(MSAI) will be the first large-scale degree program of its kind and the only master\u2019s degree program in AI from a top-ranked institution to be priced close to $10,000. The master\u2019s degree covers about two years\u2019 worth of course content, to be taken at the learner\u2019s own pace, and master\u2019s degree will be delivered in partnership with online course provider edX.\n\n&gt; AI master\u2019s programs from peer institutions carry costs five to 10 times as high as UT Austin\u2019s and serve only dozens of students \u2014 not the hundreds or thousands the Texas team projects it will reach annually within five years. Similarly priced online master\u2019s programs from the university, in\u00a0computer science and\u00a0data science, enroll 2,500 students within less than five years of their launch. Like those programs, the fully online MSAI program is both flexible and accessible.\n\n&gt; Enrolled students will receive advanced training in natural language processing, reinforcement learning, computer vision, deep learning and related topics, and will provide a critical framework for understanding the ethical implications of AI technologies. The degree will equip students for an array of potential career opportunities \u2014 from engineering to research and development, and product management to consulting...", "author_fullname": "t2_n90rekd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UT Austin Announcncing New AI Masters Program", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10lya3g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674756577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://cdso.utexas.edu/msai\"&gt;Link&lt;/a&gt; First term appears to be in a year with spring 2024.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The Master of Science in Artificial Intelligence\u00a0(MSAI) will be the first large-scale degree program of its kind and the only master\u2019s degree program in AI from a top-ranked institution to be priced close to $10,000. The master\u2019s degree covers about two years\u2019 worth of course content, to be taken at the learner\u2019s own pace, and master\u2019s degree will be delivered in partnership with online course provider edX.&lt;/p&gt;\n\n&lt;p&gt;AI master\u2019s programs from peer institutions carry costs five to 10 times as high as UT Austin\u2019s and serve only dozens of students \u2014 not the hundreds or thousands the Texas team projects it will reach annually within five years. Similarly priced online master\u2019s programs from the university, in\u00a0computer science and\u00a0data science, enroll 2,500 students within less than five years of their launch. Like those programs, the fully online MSAI program is both flexible and accessible.&lt;/p&gt;\n\n&lt;p&gt;Enrolled students will receive advanced training in natural language processing, reinforcement learning, computer vision, deep learning and related topics, and will provide a critical framework for understanding the ethical implications of AI technologies. The degree will equip students for an array of potential career opportunities \u2014 from engineering to research and development, and product management to consulting...&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10lya3g", "is_robot_indexable": true, "report_reasons": null, "author": "TapirTamer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10lya3g/ut_austin_announcncing_new_ai_masters_program/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10lya3g/ut_austin_announcncing_new_ai_masters_program/", "subreddit_subscribers": 841510, "created_utc": 1674756577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_rslw1rhc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Join us for a live webinar with Dr. Camille Nebeker (UCSD) Today, 1/26 at 1 PM CT as she discusses 'Strategies to Improve Ethical &amp; Responsible Digital Health Research' with Q&amp;A at the end of the hour. https://mhealth.md2k.org/training/webinars/451-camille-nebeker-webinar.html", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_10lxe23", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mh1pq0AnRI6oPG15ZreMrVETQjVjWRYk9R-cRyTiqrg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674754334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/i5ib0yjwafea1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/i5ib0yjwafea1.png?auto=webp&amp;v=enabled&amp;s=ae99b961273bbefb2c58dab6fe01b49718ca94d4", "width": 1440, "height": 810}, "resolutions": [{"url": "https://preview.redd.it/i5ib0yjwafea1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ef26705584df18f2412a347c8a46dc19305cd1d", "width": 108, "height": 60}, {"url": "https://preview.redd.it/i5ib0yjwafea1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50eebd74d743333904563c3ac341c5ca2f9d83d6", "width": 216, "height": 121}, {"url": "https://preview.redd.it/i5ib0yjwafea1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04782fe0745068706ce9ea6ad6c7f428d1cfbf56", "width": 320, "height": 180}, {"url": "https://preview.redd.it/i5ib0yjwafea1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=034d936145afa96107945416c342e37c4318d44f", "width": 640, "height": 360}, {"url": "https://preview.redd.it/i5ib0yjwafea1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e886c09f2f380645e482008288b2b19c2c42dff9", "width": 960, "height": 540}, {"url": "https://preview.redd.it/i5ib0yjwafea1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37fb180076701f1fe06db3e0432f18287398e260", "width": 1080, "height": 607}], "variants": {}, "id": "Xs4glP3wu8LGCMCO0RmJlDz_TEVfkGrtvmnI7ToihVE"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "10lxe23", "is_robot_indexable": true, "report_reasons": null, "author": "mDOT_Center", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10lxe23/join_us_for_a_live_webinar_with_dr_camille/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/i5ib0yjwafea1.png", "subreddit_subscribers": 841510, "created_utc": 1674754334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm not a data scientist, so please bear with me.\n\nI have a data set of items that are assigned tags, and I want to group them by the similarity of the sets of assigned tags. Below is the context:\n\n\\- Each of these items can be assigned many different tags (between 100 and 300). \n\n\\- The universe of possible tags is approximately 2500\n\n\\- Only 1000-1500 are used consistently.\n\nWhat would be the most efficient and/or accurate way to cluster items such that those with the most similar sets of tags are grouped together? I would imagine that - to start - I would need to represent the data like below:\n\n                 Tag1         Tag2          Tag3         Tag4         Tag5          Tag6\n    Item1        0            1             1            0            0             0\n    Item2        1            0             1            0            1             0\n    Item3        0            0             1            0            0             1\n    ...\n\nIs this the correct start? Once I've represented the data this way, what would I do next?\n\nThanks!", "author_fullname": "t2_4kns99rz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Efficiently Clustering Items Based on Many Tags (100's)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10lsx50", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674742993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not a data scientist, so please bear with me.&lt;/p&gt;\n\n&lt;p&gt;I have a data set of items that are assigned tags, and I want to group them by the similarity of the sets of assigned tags. Below is the context:&lt;/p&gt;\n\n&lt;p&gt;- Each of these items can be assigned many different tags (between 100 and 300). &lt;/p&gt;\n\n&lt;p&gt;- The universe of possible tags is approximately 2500&lt;/p&gt;\n\n&lt;p&gt;- Only 1000-1500 are used consistently.&lt;/p&gt;\n\n&lt;p&gt;What would be the most efficient and/or accurate way to cluster items such that those with the most similar sets of tags are grouped together? I would imagine that - to start - I would need to represent the data like below:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;             Tag1         Tag2          Tag3         Tag4         Tag5          Tag6\nItem1        0            1             1            0            0             0\nItem2        1            0             1            0            1             0\nItem3        0            0             1            0            0             1\n...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Is this the correct start? Once I&amp;#39;ve represented the data this way, what would I do next?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10lsx50", "is_robot_indexable": true, "report_reasons": null, "author": "JustinPooDough", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10lsx50/efficiently_clustering_items_based_on_many_tags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10lsx50/efficiently_clustering_items_based_on_many_tags/", "subreddit_subscribers": 841510, "created_utc": 1674742993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Explain Open Source != Security Issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10m95dq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_8hvlbe2j", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "rstats", "selftext": "TDLR: IT is panicked from a recent hack and thinks open source software is a security concern. What can I say / do to calm their nerves.\n\nI'm a data analyst for state government in the US (ie a boring bureaucrat paper pusher). For the last 2 years I've been building R scripts to pull our agency source data to build reports and statistical analysis. I've taken what used to take months to build in excel, now takes minutes with R. My direct management team has been all on board for my efforts.\n\nNow comes the monkey wrench. Our agency was recently the subject of a hack. Our IT department has gone full lockdown in response. Now everything is being nitpicked and our IT director is concerned with R being open source as a security risk.\n\nWhat can I say or do to show R is not a risk? How can I minimize their concerns? I have an opportunity to plead my case next week and am looking for support for my position.\n\nI truly appreciate any ideas or help you can offer. I don't want 2 years of my life to be trashed because of a nervous Nelly.", "author_fullname": "t2_8hvlbe2j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Explain Open Source != Security Issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/rstats", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10m91iz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674784590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.rstats", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TDLR: IT is panicked from a recent hack and thinks open source software is a security concern. What can I say / do to calm their nerves.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a data analyst for state government in the US (ie a boring bureaucrat paper pusher). For the last 2 years I&amp;#39;ve been building R scripts to pull our agency source data to build reports and statistical analysis. I&amp;#39;ve taken what used to take months to build in excel, now takes minutes with R. My direct management team has been all on board for my efforts.&lt;/p&gt;\n\n&lt;p&gt;Now comes the monkey wrench. Our agency was recently the subject of a hack. Our IT department has gone full lockdown in response. Now everything is being nitpicked and our IT director is concerned with R being open source as a security risk.&lt;/p&gt;\n\n&lt;p&gt;What can I say or do to show R is not a risk? How can I minimize their concerns? I have an opportunity to plead my case next week and am looking for support for my position.&lt;/p&gt;\n\n&lt;p&gt;I truly appreciate any ideas or help you can offer. I don&amp;#39;t want 2 years of my life to be trashed because of a nervous Nelly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r8n0", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10m91iz", "is_robot_indexable": true, "report_reasons": null, "author": "QuantumScribs", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/rstats/comments/10m91iz/how_to_explain_open_source_security_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/rstats/comments/10m91iz/how_to_explain_open_source_security_issue/", "subreddit_subscribers": 69897, "created_utc": 1674784590.0, "num_crossposts": 3, "media": null, "is_video": false}], "created": 1674784904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.rstats", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/rstats/comments/10m91iz/how_to_explain_open_source_security_issue/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10m95dq", "is_robot_indexable": true, "report_reasons": null, "author": "QuantumScribs", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_10m91iz", "author_flair_text_color": null, "permalink": "/r/datascience/comments/10m95dq/how_to_explain_open_source_security_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/rstats/comments/10m91iz/how_to_explain_open_source_security_issue/", "subreddit_subscribers": 841510, "created_utc": 1674784904.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}