{"kind": "Listing", "data": {"after": null, "dist": 16, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Kind of flipping the typical question around. What best practices would you ask a DE Team you were interviewing with to know the maturity/skill level of the team?\n\nI am thinking of technical questions but also how they structure their work?", "author_fullname": "t2_1n3qfa0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some questions you would ask a DE Team in an interview to see how advanced the team is?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102qkc6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 68, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 68, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672797230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kind of flipping the typical question around. What best practices would you ask a DE Team you were interviewing with to know the maturity/skill level of the team?&lt;/p&gt;\n\n&lt;p&gt;I am thinking of technical questions but also how they structure their work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "102qkc6", "is_robot_indexable": true, "report_reasons": null, "author": "Culpgrant21", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102qkc6/what_are_some_questions_you_would_ask_a_de_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102qkc6/what_are_some_questions_you_would_ask_a_de_team/", "subreddit_subscribers": 85144, "created_utc": 1672797230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my experience, our team had restricted the EMR spin up permissions to airflow-role only. Any DE will have to use an airflow dag to spin up an EMR cluster. And the dag will use an EMR operator to spin up and terminate the cluster in successive tasks. This is to ensure EMR is terminated right after the spark job is completed. \n\nOfcourse we can use cloudwatch to set alerts and what not. \n\nAre there any similar interesting strategies used by your teams to cut down cloud costs? I'm curious.", "author_fullname": "t2_msviuy1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some best practices in data engineering to cut cloud costs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102gj8q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672773090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my experience, our team had restricted the EMR spin up permissions to airflow-role only. Any DE will have to use an airflow dag to spin up an EMR cluster. And the dag will use an EMR operator to spin up and terminate the cluster in successive tasks. This is to ensure EMR is terminated right after the spark job is completed. &lt;/p&gt;\n\n&lt;p&gt;Ofcourse we can use cloudwatch to set alerts and what not. &lt;/p&gt;\n\n&lt;p&gt;Are there any similar interesting strategies used by your teams to cut down cloud costs? I&amp;#39;m curious.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "102gj8q", "is_robot_indexable": true, "report_reasons": null, "author": "vino_and_data", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102gj8q/what_are_some_best_practices_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102gj8q/what_are_some_best_practices_in_data_engineering/", "subreddit_subscribers": 85144, "created_utc": 1672773090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are building a data warehouse for our SaaS product data.\n\nThere are 2 methods to ingest data:\n\n1. Ingest Kafka events\n2. CDC from the relational database (when not possible to ingest events)\n\nIn both methods, we ingest data that is used for product purposes. The development teams are focused on product features and not on data that gets into the data warehouse so they might change the data we're ingesting in ways that will break the data warehouse.\n\n(Clarification - the concern is that we're consuming events and tables maintained by development teams and what happens when they change them on their side. We're not worried about them accessing the data warehouse and manually changing data that was ingested.)\n\nI would like to prevent developers from deploying breaking changes.\n\nIdeally, I would like to have tests that break if a developer changed an events schema in an incompatible way or ran a migration that changed a table which we're CDC'ing.\n\nWhat tools and processes can be used to do that?", "author_fullname": "t2_20muts76", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you prevent up stream developers from breaking the data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102hhn8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1672825862.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672775384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are building a data warehouse for our SaaS product data.&lt;/p&gt;\n\n&lt;p&gt;There are 2 methods to ingest data:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ingest Kafka events&lt;/li&gt;\n&lt;li&gt;CDC from the relational database (when not possible to ingest events)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;In both methods, we ingest data that is used for product purposes. The development teams are focused on product features and not on data that gets into the data warehouse so they might change the data we&amp;#39;re ingesting in ways that will break the data warehouse.&lt;/p&gt;\n\n&lt;p&gt;(Clarification - the concern is that we&amp;#39;re consuming events and tables maintained by development teams and what happens when they change them on their side. We&amp;#39;re not worried about them accessing the data warehouse and manually changing data that was ingested.)&lt;/p&gt;\n\n&lt;p&gt;I would like to prevent developers from deploying breaking changes.&lt;/p&gt;\n\n&lt;p&gt;Ideally, I would like to have tests that break if a developer changed an events schema in an incompatible way or ran a migration that changed a table which we&amp;#39;re CDC&amp;#39;ing.&lt;/p&gt;\n\n&lt;p&gt;What tools and processes can be used to do that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "102hhn8", "is_robot_indexable": true, "report_reasons": null, "author": "daramasala", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102hhn8/how_do_you_prevent_up_stream_developers_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102hhn8/how_do_you_prevent_up_stream_developers_from/", "subreddit_subscribers": 85144, "created_utc": 1672775384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI want to refresh my SQL skills for an upcoming Internship in Data Engineering and learn some advanced topics.\n\nWhat resources do you recommend for someone, who already knows some basics? \n\nIs Leetcode / Hackerrank suitable?", "author_fullname": "t2_jgn9fqry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way / source to refresh SQL and learn advanced topics for an upcoming internship", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1030pcr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672829680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I want to refresh my SQL skills for an upcoming Internship in Data Engineering and learn some advanced topics.&lt;/p&gt;\n\n&lt;p&gt;What resources do you recommend for someone, who already knows some basics? &lt;/p&gt;\n\n&lt;p&gt;Is Leetcode / Hackerrank suitable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1030pcr", "is_robot_indexable": true, "report_reasons": null, "author": "mar-lor", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1030pcr/best_way_source_to_refresh_sql_and_learn_advanced/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1030pcr/best_way_source_to_refresh_sql_and_learn_advanced/", "subreddit_subscribers": 85144, "created_utc": 1672829680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have worked in a couple of projects with the title of Big Data/ Hadoop developer. During this time , I have worked to set up data pipelines, building the framework for the same - to consume the data multiple sources , perform ETL operations and provide the data to end user (data scientists or input feed to a different app). I am currently for a similar set up but which is completely on AWS.\n\nThe tech stack I have mainly worked with: spark, hive , python, unix, shell scripts, sql, HDFS, CDH,CICD, Docker.\n\nAWS stack: EMR, EKS, airflow, lambda, s3, Athena,sqs \n\nI have development experience, but wouldn\u2019t say I have worked with a very object oriented and structured programing.\n\nI sort of feel like I am going with the flow, but don\u2019t have enough idea on how should I progress and the path that I should choose.\nI see that a lot of different job titles have similar requirement. \n\nPlease advise on which area/skillset I should focus on and the kind of jobs I should look for.", "author_fullname": "t2_4ewnsti5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some guidance on career path.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102z9vh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672824837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have worked in a couple of projects with the title of Big Data/ Hadoop developer. During this time , I have worked to set up data pipelines, building the framework for the same - to consume the data multiple sources , perform ETL operations and provide the data to end user (data scientists or input feed to a different app). I am currently for a similar set up but which is completely on AWS.&lt;/p&gt;\n\n&lt;p&gt;The tech stack I have mainly worked with: spark, hive , python, unix, shell scripts, sql, HDFS, CDH,CICD, Docker.&lt;/p&gt;\n\n&lt;p&gt;AWS stack: EMR, EKS, airflow, lambda, s3, Athena,sqs &lt;/p&gt;\n\n&lt;p&gt;I have development experience, but wouldn\u2019t say I have worked with a very object oriented and structured programing.&lt;/p&gt;\n\n&lt;p&gt;I sort of feel like I am going with the flow, but don\u2019t have enough idea on how should I progress and the path that I should choose.\nI see that a lot of different job titles have similar requirement. &lt;/p&gt;\n\n&lt;p&gt;Please advise on which area/skillset I should focus on and the kind of jobs I should look for.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "102z9vh", "is_robot_indexable": true, "report_reasons": null, "author": "Disastrous_Tax_2911", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102z9vh/need_some_guidance_on_career_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102z9vh/need_some_guidance_on_career_path/", "subreddit_subscribers": 85144, "created_utc": 1672824837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently started a new job as a business analyst. I\u2019m from a strong statistical and coding background, but only did a few basic units about database design, modelling and management, so my knowledge is quite limited and my terminology may be a little less technical. \n\nIt is my first job out of uni, and seem to have been thrown into the deep end. My team leader told me that during my first month I\u2019d be helping him by converting the existing architecture into data marts. \n\nI had never heard of data marts before so decided to do some research. From what I can see, it involves collecting data from various sources and formatting it, and then creating a collective database and then generating datasets out of the database as per business requirements. He said our goal is to then use the resulting datasets to generate reports on Power BI. \n\nFrom what I understood about the present architecture, data is extracted from the company software using a specific tool, which is then stored into a SQL database with different tables for different departments, which is then called upon to generate power BI reports. These tables also contain historical data.\n\nAm i correct in assuming that when we move to a data mart based approach, we would instead only have the more recent pertinent data fed to the reporting tools, and it would possibly contain fewer tables, allowing for improved performance, while the historical data would be stored in the data warehouse for analytics and machine learning purposes?", "author_fullname": "t2_9i23t85r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELI5 data marts and how they fit into reporting and analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102v6fl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672810458.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently started a new job as a business analyst. I\u2019m from a strong statistical and coding background, but only did a few basic units about database design, modelling and management, so my knowledge is quite limited and my terminology may be a little less technical. &lt;/p&gt;\n\n&lt;p&gt;It is my first job out of uni, and seem to have been thrown into the deep end. My team leader told me that during my first month I\u2019d be helping him by converting the existing architecture into data marts. &lt;/p&gt;\n\n&lt;p&gt;I had never heard of data marts before so decided to do some research. From what I can see, it involves collecting data from various sources and formatting it, and then creating a collective database and then generating datasets out of the database as per business requirements. He said our goal is to then use the resulting datasets to generate reports on Power BI. &lt;/p&gt;\n\n&lt;p&gt;From what I understood about the present architecture, data is extracted from the company software using a specific tool, which is then stored into a SQL database with different tables for different departments, which is then called upon to generate power BI reports. These tables also contain historical data.&lt;/p&gt;\n\n&lt;p&gt;Am i correct in assuming that when we move to a data mart based approach, we would instead only have the more recent pertinent data fed to the reporting tools, and it would possibly contain fewer tables, allowing for improved performance, while the historical data would be stored in the data warehouse for analytics and machine learning purposes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "102v6fl", "is_robot_indexable": true, "report_reasons": null, "author": "faxgebofk2451", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102v6fl/eli5_data_marts_and_how_they_fit_into_reporting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102v6fl/eli5_data_marts_and_how_they_fit_into_reporting/", "subreddit_subscribers": 85144, "created_utc": 1672810458.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3kxbd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Iceberg promises to change cloud-based data analytics - Adopted by Snowflake, Google and Cloudera, we look at why the Netflix-developed table format is important", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_102f2lc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": "#46d160", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0Srmt6nzjUOv5uMtihwHBDDvYu870YhRp48Xmw2GB1Y.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1672769702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "theregister.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.theregister.com/2023/01/03/apache_iceberg/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vpmO3WQNm-xbpWvJr-VoV4IaHQiUz4InQb_rH-m3it8.jpg?auto=webp&amp;s=ace2899b5f604e147825053fe19dfbcd4dec2ddf", "width": 648, "height": 432}, "resolutions": [{"url": "https://external-preview.redd.it/vpmO3WQNm-xbpWvJr-VoV4IaHQiUz4InQb_rH-m3it8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=59abd3b81132b28ec3bf0f394e8606dc63c28876", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/vpmO3WQNm-xbpWvJr-VoV4IaHQiUz4InQb_rH-m3it8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6dacd5907d5346db8fd240feb50aeebb189f6df0", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/vpmO3WQNm-xbpWvJr-VoV4IaHQiUz4InQb_rH-m3it8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9d26f2ea5846507e572eea33912c240131ca7a2", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/vpmO3WQNm-xbpWvJr-VoV4IaHQiUz4InQb_rH-m3it8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8799097a7441c9a9080e0d8be3ff50d5d482d97", "width": 640, "height": 426}], "variants": {}, "id": "lvFoT8qyKhCVnt3i0mB46e_6DHBh0Tp4GrtMpgbm_UE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "honorary mod | Snowflake", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "102f2lc", "is_robot_indexable": true, "report_reasons": null, "author": "fhoffa", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/102f2lc/apache_iceberg_promises_to_change_cloudbased_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.theregister.com/2023/01/03/apache_iceberg/", "subreddit_subscribers": 85144, "created_utc": 1672769702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI would like to determine which cloud platform to use by doing an cost determination exercise on all 3. i want to start with Azure. How do I go about doing the costing?", "author_fullname": "t2_5zlk5o8b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I go about determining the cost of a cloud platform?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102wuin", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672815955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I would like to determine which cloud platform to use by doing an cost determination exercise on all 3. i want to start with Azure. How do I go about doing the costing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "102wuin", "is_robot_indexable": true, "report_reasons": null, "author": "AggravatingWish1019", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102wuin/how_do_i_go_about_determining_the_cost_of_a_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102wuin/how_do_i_go_about_determining_the_cost_of_a_cloud/", "subreddit_subscribers": 85144, "created_utc": 1672815955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have enabled CDC on SQL Server (RDS). I am seeing the below WaitFor session running continuously. Does this have any impact on DB performance? Any way we can avoid this? Also included a query behind the waitfor. I have very limited experience with DBS. Any help would be appreciated. Thank you!!\n\n&amp;#x200B;\n\n    waitfor delay @waittime\n    \t\telse\n    \t\t\treturn @retcode\n    \n        end\n    \n    Failure:\n    \t\tif (@@TRANCOUNT &gt; @activetrancount)\n    \t\tbegin\n    \t\t\tif XACT_STATE() = 1\n    \t\t\tbegin\n    \t\t\t\trollback tran tr_sp_cdc_scan\n    \t\t\t\tcommit tran\n    \t\t\tend\n    \t\t\telse if XACT_STATE() = -1\n    \t\t\tbegin\n    \t\t\t\trollback tran\n    \t\t\tend\n    \t\tend\n    \t\t\n    \t\t-- If Azure then we don't have access to msdb\n    \t\tif((@job_id is not null) and (serverproperty('EngineEdition') &lt;&gt; 5))\n    \t\tbegin\n    \t\t\tdeclare @session_id int\n    \t\t\t\t\t\t,@error_id int\n    \t\t\t\t\t\t,@sev int\n    \t\t\t\t\t\t,@\n\nhttps://preview.redd.it/0ndsmu4d2x9a1.png?width=2580&amp;format=png&amp;auto=webp&amp;s=98cb2888e70e2d93528e03f4b51853fe37e15d14", "author_fullname": "t2_2adeipr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC create continuously running waitfor session. Impact on DB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 61, "top_awarded_type": null, "hide_score": false, "media_metadata": {"0ndsmu4d2x9a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/0ndsmu4d2x9a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=406ea640e7da248e931c6d793d9bebe7b3056760"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/0ndsmu4d2x9a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f2003864a2a2fe76accf7eb7af57f285c47af13f"}, {"y": 140, "x": 320, "u": "https://preview.redd.it/0ndsmu4d2x9a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f3fe55181f48f6be9621656db4457ddd60f55cb"}, {"y": 280, "x": 640, "u": "https://preview.redd.it/0ndsmu4d2x9a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4932e4f48bd0b98b7150753f942ff1a4faee5961"}, {"y": 420, "x": 960, "u": "https://preview.redd.it/0ndsmu4d2x9a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=51cb91daf8fe7332b8786715fb7e90661898d458"}, {"y": 473, "x": 1080, "u": "https://preview.redd.it/0ndsmu4d2x9a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=458c438ad6019c93b5451dccdb7b4c6c62839735"}], "s": {"y": 1130, "x": 2580, "u": "https://preview.redd.it/0ndsmu4d2x9a1.png?width=2580&amp;format=png&amp;auto=webp&amp;s=98cb2888e70e2d93528e03f4b51853fe37e15d14"}, "id": "0ndsmu4d2x9a1"}}, "name": "t3_102nsk7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sioxdbGDRJY-iajlBx98UYkc6pb9PIxQZyKbYcR950g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672790108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have enabled CDC on SQL Server (RDS). I am seeing the below WaitFor session running continuously. Does this have any impact on DB performance? Any way we can avoid this? Also included a query behind the waitfor. I have very limited experience with DBS. Any help would be appreciated. Thank you!!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;waitfor delay @waittime\n        else\n            return @retcode\n\n    end\n\nFailure:\n        if (@@TRANCOUNT &amp;gt; @activetrancount)\n        begin\n            if XACT_STATE() = 1\n            begin\n                rollback tran tr_sp_cdc_scan\n                commit tran\n            end\n            else if XACT_STATE() = -1\n            begin\n                rollback tran\n            end\n        end\n\n        -- If Azure then we don&amp;#39;t have access to msdb\n        if((@job_id is not null) and (serverproperty(&amp;#39;EngineEdition&amp;#39;) &amp;lt;&amp;gt; 5))\n        begin\n            declare @session_id int\n                        ,@error_id int\n                        ,@sev int\n                        ,@\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0ndsmu4d2x9a1.png?width=2580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98cb2888e70e2d93528e03f4b51853fe37e15d14\"&gt;https://preview.redd.it/0ndsmu4d2x9a1.png?width=2580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98cb2888e70e2d93528e03f4b51853fe37e15d14&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "102nsk7", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Outlandishness-74", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102nsk7/cdc_create_continuously_running_waitfor_session/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102nsk7/cdc_create_continuously_running_waitfor_session/", "subreddit_subscribers": 85144, "created_utc": 1672790108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How are folks using data \"time travel\" (aka temporal tables, history tables, etc) in the real world today? I've used it for backtesting signals in quant finance where we need to understand exactly the way the world looked at a given point-in-time.", "author_fullname": "t2_116kc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ask Reddit: Data Time Travel Use Cases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102fk82", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672770821.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are folks using data &amp;quot;time travel&amp;quot; (aka temporal tables, history tables, etc) in the real world today? I&amp;#39;ve used it for backtesting signals in quant finance where we need to understand exactly the way the world looked at a given point-in-time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "102fk82", "is_robot_indexable": true, "report_reasons": null, "author": "daeisfresh", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102fk82/ask_reddit_data_time_travel_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102fk82/ask_reddit_data_time_travel_use_cases/", "subreddit_subscribers": 85144, "created_utc": 1672770821.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\n\nI\u2019m not currently in a DE role, but I\u2019ve been asked to centralise &amp; visualise data for my (small) team using Power BI &amp; SharePoint\n\nWe have data on the team members, client projects each of us are working on, skills/tools we\u2019re competent in, codes associated with each project we can put into our timecards, among other things.\n\nRight now this data either exists in our heads or in random excel spreadsheets scattered about.\n\nI\u2019ve considered setting up a MS Access database given that there won\u2019t be much data &amp; that there are many-to-many relationships (e.g, skills matrix), and perhaps linking some of the tables to SharePoint Lists so the team can more easily update the data - then connecting all this to Power BI and linking this within my Team\u2019s SharePoint Site.\n\nJust wondering if this is a bad idea, and if anyone has any suggestions. I\u2019m trying to keep things simple &amp; maintainable, and allow for Team members to update their skills or projects they are working in without much fuss.", "author_fullname": "t2_osh2xtjb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SharePoint &amp; MS Access ok solutions for data centralisation &amp; visualisation in a small team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1034f9r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672840995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not currently in a DE role, but I\u2019ve been asked to centralise &amp;amp; visualise data for my (small) team using Power BI &amp;amp; SharePoint&lt;/p&gt;\n\n&lt;p&gt;We have data on the team members, client projects each of us are working on, skills/tools we\u2019re competent in, codes associated with each project we can put into our timecards, among other things.&lt;/p&gt;\n\n&lt;p&gt;Right now this data either exists in our heads or in random excel spreadsheets scattered about.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve considered setting up a MS Access database given that there won\u2019t be much data &amp;amp; that there are many-to-many relationships (e.g, skills matrix), and perhaps linking some of the tables to SharePoint Lists so the team can more easily update the data - then connecting all this to Power BI and linking this within my Team\u2019s SharePoint Site.&lt;/p&gt;\n\n&lt;p&gt;Just wondering if this is a bad idea, and if anyone has any suggestions. I\u2019m trying to keep things simple &amp;amp; maintainable, and allow for Team members to update their skills or projects they are working in without much fuss.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1034f9r", "is_robot_indexable": true, "report_reasons": null, "author": "TheDataPanda", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1034f9r/sharepoint_ms_access_ok_solutions_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1034f9r/sharepoint_ms_access_ok_solutions_for_data/", "subreddit_subscribers": 85144, "created_utc": 1672840995.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI have a set of 22 .sql files (generated from pg\\_dump), each containing data for a different chromosome. Each file is around 300-400GB in size, so I'm looking for the most efficient way to load all of this data into a Postgres database.\n\nI would also be interested in hearing about any scripts or techniques for splitting the .sql files into smaller chunks and loading them in parallel using the psql command-line interface.\n\nRight now, it takes around 2-5 Days to load one file. \n\nThank you in advance for your help!", "author_fullname": "t2_cw7qs3v3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting multiple large .sql files into Postgres database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102t84o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672804631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I have a set of 22 .sql files (generated from pg_dump), each containing data for a different chromosome. Each file is around 300-400GB in size, so I&amp;#39;m looking for the most efficient way to load all of this data into a Postgres database.&lt;/p&gt;\n\n&lt;p&gt;I would also be interested in hearing about any scripts or techniques for splitting the .sql files into smaller chunks and loading them in parallel using the psql command-line interface.&lt;/p&gt;\n\n&lt;p&gt;Right now, it takes around 2-5 Days to load one file. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "102t84o", "is_robot_indexable": true, "report_reasons": null, "author": "babypinkgoyard", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102t84o/ingesting_multiple_large_sql_files_into_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102t84o/ingesting_multiple_large_sql_files_into_postgres/", "subreddit_subscribers": 85144, "created_utc": 1672804631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm evaluating options for running a queue of N long-running jobs. They don't have to be finished sequentially, but our compute resources are limited so we can only process K jobs at a time. Every X days, the entire queue will be ran again. Each job will report information at the end. It'd be nice to have good monitoring for failures.\n\nWhat are current best practices for this type of thing? Is airflow still well supported for this type of thing?", "author_fullname": "t2_rs400v2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running recurring a queue of recurring long-running jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102h9cs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672774817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m evaluating options for running a queue of N long-running jobs. They don&amp;#39;t have to be finished sequentially, but our compute resources are limited so we can only process K jobs at a time. Every X days, the entire queue will be ran again. Each job will report information at the end. It&amp;#39;d be nice to have good monitoring for failures.&lt;/p&gt;\n\n&lt;p&gt;What are current best practices for this type of thing? Is airflow still well supported for this type of thing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "102h9cs", "is_robot_indexable": true, "report_reasons": null, "author": "ship-flipper", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102h9cs/running_recurring_a_queue_of_recurring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102h9cs/running_recurring_a_queue_of_recurring/", "subreddit_subscribers": 85144, "created_utc": 1672774817.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some resources for interview preparation to answer questions such as \"How would you design a data warehouse?\" or \"How would you design a data pipeline?\"   \n\nThanks!!", "author_fullname": "t2_hfgzs7v2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Theoretical Resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_102fnvv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672771060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some resources for interview preparation to answer questions such as &amp;quot;How would you design a data warehouse?&amp;quot; or &amp;quot;How would you design a data pipeline?&amp;quot;   &lt;/p&gt;\n\n&lt;p&gt;Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "102fnvv", "is_robot_indexable": true, "report_reasons": null, "author": "AlternativeDish5596", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/102fnvv/interview_theoretical_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/102fnvv/interview_theoretical_resources/", "subreddit_subscribers": 85144, "created_utc": 1672771060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5r54hksr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get Your ML Data Back: Open-Source Tool to Export Your MLFlow Data to SQLite", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_103a1de", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/wXN1kwi7w4hRgvN57lDikPY8juBMJ0huUQAEaBSvho8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1672854862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ploomber.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ploomber.io/blog/mlflow2sql/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?auto=webp&amp;s=a01611b4c500985bec4eea7c81b9625f4fc84a8e", "width": 2402, "height": 1262}, "resolutions": [{"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d1d9ae27f0ab4cdf6194eaccce264d2df0ec7eb", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=59f9f616fd7113e5fd2b154d26c36fb0c6c92b19", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b6566be246f6de95ceb35ad40145567b4fee505", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=98b5bf2fa38b93b98c3d05573b811b24efd21bbb", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=49a1defd0a799cb42ab84d35032b27f24a29b5f9", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/hfTF3Q4_x-cMAq8YTaz4fhcsyvt7F4FdJfqKbGLOekI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f206bfc6ef7a6b1ff19803d79315e77c5f727f05", "width": 1080, "height": 567}], "variants": {}, "id": "Objw1E7bxh1q2DHkIbVVfKXOpJIwmJvOhOKLMKIjEEs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "103a1de", "is_robot_indexable": true, "report_reasons": null, "author": "ploomber-io", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/103a1de/get_your_ml_data_back_opensource_tool_to_export/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ploomber.io/blog/mlflow2sql/", "subreddit_subscribers": 85144, "created_utc": 1672854862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a2zxqic6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Meltano can now run any Airbyte source connector thanks to a community contribution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": true, "name": "t3_1039njv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yanii5NteK25H5LV0gyfgeycmLXX6ot1nn0-48RvJsA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1672853981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "meltano.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://meltano.com/blog/meltano-community-contribution-enables-running-over-200-airbyte-connectors/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?auto=webp&amp;s=c281f7f1f173c9d757c14c22bd02904df6bbe926", "width": 2048, "height": 1365}, "resolutions": [{"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=542e31c66973c837670c355c9b39789836012881", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a0f1441845dca9f1f291d639b7b6bef2b15e3f7b", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f28b7d959f0f9abe9b78874b258d5d2db84e826e", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9dcf2528426908213926e342fd45708c44169b5", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=39a2699a04982e2cfa4c07fb50348c5629825b3f", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/ADIADkUMY_u_XuXeGZiVpmQJhh6FSfiFqFik4iKF_pM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=354068f1be19deb6fe35dee8f01873f6817f072a", "width": 1080, "height": 719}], "variants": {}, "id": "UaWSMIAUKV85uXdc7bNp-jFBUuKP9UuqUorN-KPwqrg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1039njv", "is_robot_indexable": true, "report_reasons": null, "author": "tayloramurphy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1039njv/meltano_can_now_run_any_airbyte_source_connector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://meltano.com/blog/meltano-community-contribution-enables-running-over-200-airbyte-connectors/", "subreddit_subscribers": 85144, "created_utc": 1672853981.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}