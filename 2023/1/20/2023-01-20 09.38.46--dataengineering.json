{"kind": "Listing", "data": {"after": "t3_10gbzvu", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Could be something very minor like a specific gotcha in a library or a major architecture decision that bit you in the behind.", "author_fullname": "t2_14i1sz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What mistake have you vowed never to repeat?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g3g1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 60, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 60, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674139484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could be something very minor like a specific gotcha in a library or a major architecture decision that bit you in the behind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10g3g1r", "is_robot_indexable": true, "report_reasons": null, "author": "pescennius", "discussion_type": null, "num_comments": 79, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g3g1r/what_mistake_have_you_vowed_never_to_repeat/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g3g1r/what_mistake_have_you_vowed_never_to_repeat/", "subreddit_subscribers": 86849, "created_utc": 1674139484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I posted the same in r/datascience, but thought it would get a better traction here instead, since many of you are actual data engineers. The original post is [here](https://www.reddit.com/r/datascience/comments/10fpmi2/has_anyone_here_transitioned_from_data_scientist/?utm_source=share&amp;utm_medium=web2x&amp;context=3) for anyone curious.\n\nSo basically, I'm curious to hear from people who work as data engineers that transitioned from a data scientist. What made you switch and do you regret you decision? Or was it one of the best decisions for your career? Thanks!", "author_fullname": "t2_rc5lfgnr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone here transitioned from Data Scientist to Data Engineer? What was your motivation, and do you regret the move now, or are you happier as a Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ghhal", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674172792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I posted the same in &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt;, but thought it would get a better traction here instead, since many of you are actual data engineers. The original post is &lt;a href=\"https://www.reddit.com/r/datascience/comments/10fpmi2/has_anyone_here_transitioned_from_data_scientist/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;here&lt;/a&gt; for anyone curious.&lt;/p&gt;\n\n&lt;p&gt;So basically, I&amp;#39;m curious to hear from people who work as data engineers that transitioned from a data scientist. What made you switch and do you regret you decision? Or was it one of the best decisions for your career? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ghhal", "is_robot_indexable": true, "report_reasons": null, "author": "Subject_Ad_9680", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ghhal/has_anyone_here_transitioned_from_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ghhal/has_anyone_here_transitioned_from_data_scientist/", "subreddit_subscribers": 86849, "created_utc": 1674172792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_31ilk7t8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to speed up range joins joins in Snowflake by 300x - SELECT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "name": "t3_10g65bg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yVrMGv9ob-fHob-nsAMDUf1Wxgufd4sgTPddBt3tMfo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674145957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "select.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://select.dev/posts/snowflake-range-join-optimization", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?auto=webp&amp;v=enabled&amp;s=c6102393c1fc139ebe3a192d54baefc9269fa3d6", "width": 1200, "height": 733}, "resolutions": [{"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e24f19fe617b042b54367889609e4aa75d7d6135", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc01477758940100756397660527fd80cfa738ad", "width": 216, "height": 131}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2db23948c752c587caa1f5bd2ecbfb81b13c713f", "width": 320, "height": 195}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac762f60c4c6caf1fa44521997dda65fed7b824c", "width": 640, "height": 390}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=956b255f528f54123a9598503aa68e2eef59b025", "width": 960, "height": 586}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c8a705f31099be5dd86194da0794355c6a7b829", "width": 1080, "height": 659}], "variants": {}, "id": "Kl2clgMmndy65WOWu6IHcN4IPoSI4AphoMLolknNnyA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10g65bg", "is_robot_indexable": true, "report_reasons": null, "author": "ian-whitestone", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g65bg/how_to_speed_up_range_joins_joins_in_snowflake_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://select.dev/posts/snowflake-range-join-optimization", "subreddit_subscribers": 86849, "created_utc": 1674145957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I kinda find the tools both use are same...so it makes both the roles same only ?", "author_fullname": "t2_99z52qfs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Data Engineers and Big Data Engineers same?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g7wyt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674150089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I kinda find the tools both use are same...so it makes both the roles same only ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10g7wyt", "is_robot_indexable": true, "report_reasons": null, "author": "Vortex_007s", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g7wyt/are_data_engineers_and_big_data_engineers_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g7wyt/are_data_engineers_and_big_data_engineers_same/", "subreddit_subscribers": 86849, "created_utc": 1674150089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j33i3nha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Requesting feedback on my resume - Data Engineer with a little over 2 years of experience, was laid off recently", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10g7wus", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": "transparent", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EYob2aCUBnKMaS02mKE8gOPld9yR-gCcrmYmsOtDUuw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674150081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zxbize3zd1da1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?auto=webp&amp;v=enabled&amp;s=9cca47ece1fc5ac2ac8ce349298c45c45aad2427", "width": 1700, "height": 2200}, "resolutions": [{"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be479760d91ee62bb8c1471afe5961e9285b2286", "width": 108, "height": 139}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0237c1248dafa3dfa220c7a14c15a52fa4896d13", "width": 216, "height": 279}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c234a772ce92e14f3642beb70560a6fc7e173be", "width": 320, "height": 414}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f5fc705a3bdac8a314c9016aa1ec77add7eed5f", "width": 640, "height": 828}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f68926ee615705f32c8cb6f0ee9fd672b2a9f16c", "width": 960, "height": 1242}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7aa391976255f8543c948d67f3fefd76e888af0", "width": 1080, "height": 1397}], "variants": {}, "id": "8eDElsZ4niQq2Lvas-ev5VOLUMDDsCuf4bubD9ehkf8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10g7wus", "is_robot_indexable": true, "report_reasons": null, "author": "CS_throwaway_DE", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10g7wus/requesting_feedback_on_my_resume_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zxbize3zd1da1.jpg", "subreddit_subscribers": 86849, "created_utc": 1674150081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE community!  \nI've been working on the data migration to the star schema architecture, and here are a couple of questions I could not confidently decide how to proceed with.\n\n1. What are the common strategies for handling the loaded\\_at timestamp fields in dwh, when the target table gets the data from multiple different sources? I was thinking of creating a 'last' and/or the 'first' loaded\\_at fields, not sure if there's any better way.\n2. A similar question about specifying the 'source' field into the dimensions. When the dimension gets created out of two different sources, in the way that attributes of a single dimension record are coming from various sources, how do you usually handle the 'source' field? Here I though of two options, one was to simply indicate the main dimension record source, i.e. source from where the platform's unique identifier is coming. But this way we lose the source of additional dimension attributes.  \n\n\nThanks!", "author_fullname": "t2_gwqijajo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Star Schema - questions about loading the data from multiple sources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fzw1q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674129466.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE community!&lt;br/&gt;\nI&amp;#39;ve been working on the data migration to the star schema architecture, and here are a couple of questions I could not confidently decide how to proceed with.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What are the common strategies for handling the loaded_at timestamp fields in dwh, when the target table gets the data from multiple different sources? I was thinking of creating a &amp;#39;last&amp;#39; and/or the &amp;#39;first&amp;#39; loaded_at fields, not sure if there&amp;#39;s any better way.&lt;/li&gt;\n&lt;li&gt;A similar question about specifying the &amp;#39;source&amp;#39; field into the dimensions. When the dimension gets created out of two different sources, in the way that attributes of a single dimension record are coming from various sources, how do you usually handle the &amp;#39;source&amp;#39; field? Here I though of two options, one was to simply indicate the main dimension record source, i.e. source from where the platform&amp;#39;s unique identifier is coming. But this way we lose the source of additional dimension attributes.&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fzw1q", "is_robot_indexable": true, "report_reasons": null, "author": "orm_the_stalker", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fzw1q/star_schema_questions_about_loading_the_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fzw1q/star_schema_questions_about_loading_the_data_from/", "subreddit_subscribers": 86849, "created_utc": 1674129466.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Earlier this week, we started seeing an issue where a DBT build seems to clog up Redshift. The build starts and runs for a bit, then it freezes (in terminal). Inside Redshift, the CPU load spikes to 100% for a few minutes, then comes back down. After 5 min or so the load is back to baseline around 6%, but if I check `svv_transactions` there are 423 entries. \n\nThis just started after running without issue for a couple months. We've been adding more models of course, but nothing crazy has changed. I have a few ideas like turning down the number of threads, but I'm not sure what the root cause is, so I'm afraid this will just come back again in the future.   \n\n\nHas anyone seen anything like this?", "author_fullname": "t2_4mt61lqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Crushing / Locking Up Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gmkn3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674186606.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Earlier this week, we started seeing an issue where a DBT build seems to clog up Redshift. The build starts and runs for a bit, then it freezes (in terminal). Inside Redshift, the CPU load spikes to 100% for a few minutes, then comes back down. After 5 min or so the load is back to baseline around 6%, but if I check &lt;code&gt;svv_transactions&lt;/code&gt; there are 423 entries. &lt;/p&gt;\n\n&lt;p&gt;This just started after running without issue for a couple months. We&amp;#39;ve been adding more models of course, but nothing crazy has changed. I have a few ideas like turning down the number of threads, but I&amp;#39;m not sure what the root cause is, so I&amp;#39;m afraid this will just come back again in the future.   &lt;/p&gt;\n\n&lt;p&gt;Has anyone seen anything like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10gmkn3", "is_robot_indexable": true, "report_reasons": null, "author": "cbc-bear", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gmkn3/dbt_crushing_locking_up_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gmkn3/dbt_crushing_locking_up_redshift/", "subreddit_subscribers": 86849, "created_utc": 1674186606.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Tracking down all the args and import statements for providers was sometimes a pain. This VS Code extension has all of the providers loaded so you can autocomplete all of the operators and hooks. [https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates](https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates)", "author_fullname": "t2_shq2slu0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "VS Code extension for Airflow Provider packages: Airflow Templates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g7tsp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674149886.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tracking down all the args and import statements for providers was sometimes a pain. This VS Code extension has all of the providers loaded so you can autocomplete all of the operators and hooks. &lt;a href=\"https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates\"&gt;https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5Vwgalc18IqnYE7aIh59IAOgguPxfM4Agv9EhnfeWJc.jpg?auto=webp&amp;v=enabled&amp;s=f519d587d5d576b6d3824ecfeb08e435ac409c9d", "width": 140, "height": 139}, "resolutions": [{"url": "https://external-preview.redd.it/5Vwgalc18IqnYE7aIh59IAOgguPxfM4Agv9EhnfeWJc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dfd5cd2e7d92a6db1b462a32c97900911d11d93", "width": 108, "height": 107}], "variants": {}, "id": "vuJ3SPN1HSQh3YWOb1ZAo50TA1_Ol-O149S9O3Ne1Es"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10g7tsp", "is_robot_indexable": true, "report_reasons": null, "author": "compound-cluster", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g7tsp/vs_code_extension_for_airflow_provider_packages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g7tsp/vs_code_extension_for_airflow_provider_packages/", "subreddit_subscribers": 86849, "created_utc": 1674149886.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI have a Kafka topic containing different event types and these events may have significantly different schemas.\n\nI am reading from the topic with a spark structured streaming job, values are in avro format.\n\nIn snowflake, this is perfectly fine because these values are stored as variant type, which allows completely different data structures and schema is enforced only when data is read.\n\nIs there anything similar in Spark? It is true that I can keep the values as byte arrays, but then I need to connect to the schema registry everytime I want to deserialize data. Moreover, if I wanted to consume this data in Athena, I am not sure how to provide the proper schema from the registry (or keep it up to date).\n\nIt is perfectly fine if the soluton won't let me read one or more event types at once, I would like to partition by event_type and read each type individually.", "author_fullname": "t2_do2bj7xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark equivalent of Snowflake's variant", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gqt0f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674204756.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674200211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have a Kafka topic containing different event types and these events may have significantly different schemas.&lt;/p&gt;\n\n&lt;p&gt;I am reading from the topic with a spark structured streaming job, values are in avro format.&lt;/p&gt;\n\n&lt;p&gt;In snowflake, this is perfectly fine because these values are stored as variant type, which allows completely different data structures and schema is enforced only when data is read.&lt;/p&gt;\n\n&lt;p&gt;Is there anything similar in Spark? It is true that I can keep the values as byte arrays, but then I need to connect to the schema registry everytime I want to deserialize data. Moreover, if I wanted to consume this data in Athena, I am not sure how to provide the proper schema from the registry (or keep it up to date).&lt;/p&gt;\n\n&lt;p&gt;It is perfectly fine if the soluton won&amp;#39;t let me read one or more event types at once, I would like to partition by event_type and read each type individually.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10gqt0f", "is_robot_indexable": true, "report_reasons": null, "author": "somerandomdataeng", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10gqt0f/spark_equivalent_of_snowflakes_variant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gqt0f/spark_equivalent_of_snowflakes_variant/", "subreddit_subscribers": 86849, "created_utc": 1674200211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm wondering if you guys have deprecation timelines for pipelines.\n\nI was thinking a system where after a year (or some other predetermined amount of time), pipelines get evaluated and deprecated if unnecessary.\n\n I suggested this as an idea, but there were concerns about implementing such a system:\n\n1. It doesn't add business value, so if we spend too much time doing this, it will be counterproductive.\n2. There are pipelines feeding datasets that are used very infrequently, it is still important to keep old data.\n3. Ideally, all pipelines we create are used forever, so it doesn't make sense to plan on deprecating them.\n\nThe reason I feel a need for some way to systematically decrease the number of pipelines are:\n\n1. Having many pipelines makes onboarding more difficult.\n2. Costs of running pipelines and holding data unnecessarily.\n3. Increasing complexity of responsibilities and maintenance \n\nDo you guys have any thoughts on this matter? I agree with many of the counterpoints, but have no doubts we are maintaining some unnecessary pipelines.", "author_fullname": "t2_wuozf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline Deprecation Timelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gntsu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674190368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering if you guys have deprecation timelines for pipelines.&lt;/p&gt;\n\n&lt;p&gt;I was thinking a system where after a year (or some other predetermined amount of time), pipelines get evaluated and deprecated if unnecessary.&lt;/p&gt;\n\n&lt;p&gt;I suggested this as an idea, but there were concerns about implementing such a system:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;It doesn&amp;#39;t add business value, so if we spend too much time doing this, it will be counterproductive.&lt;/li&gt;\n&lt;li&gt;There are pipelines feeding datasets that are used very infrequently, it is still important to keep old data.&lt;/li&gt;\n&lt;li&gt;Ideally, all pipelines we create are used forever, so it doesn&amp;#39;t make sense to plan on deprecating them.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The reason I feel a need for some way to systematically decrease the number of pipelines are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Having many pipelines makes onboarding more difficult.&lt;/li&gt;\n&lt;li&gt;Costs of running pipelines and holding data unnecessarily.&lt;/li&gt;\n&lt;li&gt;Increasing complexity of responsibilities and maintenance &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Do you guys have any thoughts on this matter? I agree with many of the counterpoints, but have no doubts we are maintaining some unnecessary pipelines.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10gntsu", "is_robot_indexable": true, "report_reasons": null, "author": "KdyLoL", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gntsu/pipeline_deprecation_timelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gntsu/pipeline_deprecation_timelines/", "subreddit_subscribers": 86849, "created_utc": 1674190368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_74pfheof", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tabular Product (creators of Iceberg)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_10gjfak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Yn9zmgvxMadrHKq1mwEaHAwwkq6neUfwKX12jqg-UA4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674177864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "m.youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://m.youtube.com/watch?v=KsdMhH0_nG0", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?auto=webp&amp;v=enabled&amp;s=1c65b0d5734f6dbfc2663ade3ac64a269dc95721", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b32bdcf4fd882f23b49d9e83023d96db5ba6bada", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4e97b57ccb4f1512aa740de15163568d7811ce1", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=851d4016207f0867d93fc4e196a79f87acad25ba", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06fe08ee1a9e6db78a39f94a81f62be8ebf8c309", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b51934f2f795502cc50ffae10631e5b6c03d07cc", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2acac09c4a8f723a0eaed2702fdcf2c7d35043f7", "width": 1080, "height": 607}], "variants": {}, "id": "Vrb4wmE0MD8BDsTYANfeTCD-cV-li3waoGAmJhdiE74"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10gjfak", "is_robot_indexable": true, "report_reasons": null, "author": "No_Equivalent5942", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gjfak/tabular_product_creators_of_iceberg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://m.youtube.com/watch?v=KsdMhH0_nG0", "subreddit_subscribers": 86849, "created_utc": 1674177864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! We are a medium sized data engineering team at my work and currently have a little quiz/learning exercise going on to help our juniors learn more about data engineering. In particular we are discussing different design decisions that were made for our systems in the past. One of those is that we have a default Lookback period of 3 days in many of our Airflow DAGs. One reason for that is that we don't have to do a lot of backfills in case a job fails and we fix it within that time window. However, our manager said there is another reason for why having a lookback window makes sense. Does anyone here have an idea what that could be? What are general benefits of having lookback windows aside from potentially covering for job failures?\n\nThanks all for your suggestions!!", "author_fullname": "t2_3wknjffr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reasons for Lookback Periods", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gfw9u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674168679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! We are a medium sized data engineering team at my work and currently have a little quiz/learning exercise going on to help our juniors learn more about data engineering. In particular we are discussing different design decisions that were made for our systems in the past. One of those is that we have a default Lookback period of 3 days in many of our Airflow DAGs. One reason for that is that we don&amp;#39;t have to do a lot of backfills in case a job fails and we fix it within that time window. However, our manager said there is another reason for why having a lookback window makes sense. Does anyone here have an idea what that could be? What are general benefits of having lookback windows aside from potentially covering for job failures?&lt;/p&gt;\n\n&lt;p&gt;Thanks all for your suggestions!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10gfw9u", "is_robot_indexable": true, "report_reasons": null, "author": "katerdoener", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gfw9u/reasons_for_lookback_periods/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gfw9u/reasons_for_lookback_periods/", "subreddit_subscribers": 86849, "created_utc": 1674168679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi ,\n\nWe are currently trying to build data quality checks like outlier detection for our streaming data such as kafka streams , iot devices etc.\n\nCan anyone suggest an approach that can be used to acheive this using databricks?\n\nThanks!", "author_fullname": "t2_jysbojws", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data quality checks in spark streaming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gftev", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674168489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi ,&lt;/p&gt;\n\n&lt;p&gt;We are currently trying to build data quality checks like outlier detection for our streaming data such as kafka streams , iot devices etc.&lt;/p&gt;\n\n&lt;p&gt;Can anyone suggest an approach that can be used to acheive this using databricks?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10gftev", "is_robot_indexable": true, "report_reasons": null, "author": "Quick_Poem_4344", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gftev/data_quality_checks_in_spark_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gftev/data_quality_checks_in_spark_streaming/", "subreddit_subscribers": 86849, "created_utc": 1674168489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI've been interviewing to move from data analyst role to more data engineering / software dev.\n\nI'm interested in/have extensive experience with pandas, pyodbc, querying APIs (+have graphql exposure), automation, and 24/7 running of scripts. Did a small snowflake workshop with aws glue / pyspark.\n\nI've had a lot of interviews with no pulls(many late stage as well). I feel I've improved my technical portion of interview since beginning. Last interview I did a data loading project to mariadb in python, ended up making this into a github repo and have been releasing to it. Did some new branch pull merges etc.\n\nI'm wondering what I should focus on to show I can do the work? Have been working with pandas maybe 7 years now, thought transition to finding a shop interested in that would be doable.\n\nRight now I'm working on implementing great expectations in a scalable way for data quality. Have been using chatgpt to boost my code writing as well.\n\nAny thoughts everyone?", "author_fullname": "t2_pkwxz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Portfolio projects for interview?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gdiqi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674162997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been interviewing to move from data analyst role to more data engineering / software dev.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in/have extensive experience with pandas, pyodbc, querying APIs (+have graphql exposure), automation, and 24/7 running of scripts. Did a small snowflake workshop with aws glue / pyspark.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had a lot of interviews with no pulls(many late stage as well). I feel I&amp;#39;ve improved my technical portion of interview since beginning. Last interview I did a data loading project to mariadb in python, ended up making this into a github repo and have been releasing to it. Did some new branch pull merges etc.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering what I should focus on to show I can do the work? Have been working with pandas maybe 7 years now, thought transition to finding a shop interested in that would be doable.&lt;/p&gt;\n\n&lt;p&gt;Right now I&amp;#39;m working on implementing great expectations in a scalable way for data quality. Have been using chatgpt to boost my code writing as well.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts everyone?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10gdiqi", "is_robot_indexable": true, "report_reasons": null, "author": "iupuiclubs", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gdiqi/portfolio_projects_for_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gdiqi/portfolio_projects_for_interview/", "subreddit_subscribers": 86849, "created_utc": 1674162997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on calculating the total cost of ownership (TCO) for tools like Apache Kafka to determine when to build vs. buy.\n\nI'd love your feedback -- what am I missing? What did I underestimate/overestimate? How can I improve this?\n\n**First, the criteria to consider when calculating TCO:**\n\nUp-front costs\n\n* software cost &amp; licensing, if applicable\n* learning &amp; education\n* implementation &amp; testing (including data migration costs)\n* documentation &amp; knowledge sharing\n* customization\n\nOngoing costs\n\n* direct infrastructure costs (e.g., hosting &amp; storage)\n* backup infrastructure costs (e.g., failover &amp; additional AZs)\n* supporting infrastructure costs (e.g., monitoring &amp; alerting)\n* maintenance, patches/upgrades, &amp; support\n* feature additions\n\nTeam &amp; opportunity costs\n\n* hiring to replace the engineers now working with the new software\n* time spent on infrastructure that could otherwise be spent on core product\n\n**Now, an example using the above criteria:**\n\nDesired specs for our example deployment (I picked one of the smaller Heroku plans):\n\n* Capacity: 300GB\n* Retention: 2 weeks\n* vCPU: 4\n* Ram: 16GB\n* Brokers: 3\n\nAssuming an engineer has an all-in comp package of $200k/yr (this would obviously be different in every situation, for every geo), year one would look like:\n\n||**Building (on AWS)**|**Buying (Heroku)**|\n|:-|:-|:-|\n|software cost &amp; licensing|$0|$21,600|\n|learning &amp; education|$7,692 (2 eng \\* 1 week)|$3,846 (1 eng \\* 1 week)|\n|implementation &amp; testing|$15,384 (2 eng \\* 2 weeks)|$7,692 (1 eng \\* 1 week)|\n|infrastructure costs (see above specs)|$12,117.60|$0 (included in software cost)|\n|supporting infrastructure costs (monitoring, etc.)|$1,200/yr|$1,200/yr|\n|maintenance, patches/upgrades|$15,384 (2 eng \\* 2 weeks spread throughout the year)|$7,692 (1 eng \\* 2 weeks spread throughout the year)|\n|**Year 1 TCO**|**$51,777.60**|**$42,030**|\n\nDirectionally, this example seems correct.\n\nWhat do you think? What am I missing? What did I underestimate/overestimate? How can I improve this?\n\nThanks!", "author_fullname": "t2_fv515", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feedback Request: TCO Calculation for Apache Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gaucp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674156743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on calculating the total cost of ownership (TCO) for tools like Apache Kafka to determine when to build vs. buy.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love your feedback -- what am I missing? What did I underestimate/overestimate? How can I improve this?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;First, the criteria to consider when calculating TCO:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Up-front costs&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;software cost &amp;amp; licensing, if applicable&lt;/li&gt;\n&lt;li&gt;learning &amp;amp; education&lt;/li&gt;\n&lt;li&gt;implementation &amp;amp; testing (including data migration costs)&lt;/li&gt;\n&lt;li&gt;documentation &amp;amp; knowledge sharing&lt;/li&gt;\n&lt;li&gt;customization&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Ongoing costs&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;direct infrastructure costs (e.g., hosting &amp;amp; storage)&lt;/li&gt;\n&lt;li&gt;backup infrastructure costs (e.g., failover &amp;amp; additional AZs)&lt;/li&gt;\n&lt;li&gt;supporting infrastructure costs (e.g., monitoring &amp;amp; alerting)&lt;/li&gt;\n&lt;li&gt;maintenance, patches/upgrades, &amp;amp; support&lt;/li&gt;\n&lt;li&gt;feature additions&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Team &amp;amp; opportunity costs&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;hiring to replace the engineers now working with the new software&lt;/li&gt;\n&lt;li&gt;time spent on infrastructure that could otherwise be spent on core product&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Now, an example using the above criteria:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Desired specs for our example deployment (I picked one of the smaller Heroku plans):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Capacity: 300GB&lt;/li&gt;\n&lt;li&gt;Retention: 2 weeks&lt;/li&gt;\n&lt;li&gt;vCPU: 4&lt;/li&gt;\n&lt;li&gt;Ram: 16GB&lt;/li&gt;\n&lt;li&gt;Brokers: 3&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Assuming an engineer has an all-in comp package of $200k/yr (this would obviously be different in every situation, for every geo), year one would look like:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;Building (on AWS)&lt;/strong&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;Buying (Heroku)&lt;/strong&gt;&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;software cost &amp;amp; licensing&lt;/td&gt;\n&lt;td align=\"left\"&gt;$0&lt;/td&gt;\n&lt;td align=\"left\"&gt;$21,600&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;learning &amp;amp; education&lt;/td&gt;\n&lt;td align=\"left\"&gt;$7,692 (2 eng * 1 week)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$3,846 (1 eng * 1 week)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;implementation &amp;amp; testing&lt;/td&gt;\n&lt;td align=\"left\"&gt;$15,384 (2 eng * 2 weeks)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$7,692 (1 eng * 1 week)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;infrastructure costs (see above specs)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$12,117.60&lt;/td&gt;\n&lt;td align=\"left\"&gt;$0 (included in software cost)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;supporting infrastructure costs (monitoring, etc.)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$1,200/yr&lt;/td&gt;\n&lt;td align=\"left\"&gt;$1,200/yr&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;maintenance, patches/upgrades&lt;/td&gt;\n&lt;td align=\"left\"&gt;$15,384 (2 eng * 2 weeks spread throughout the year)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$7,692 (1 eng * 2 weeks spread throughout the year)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Year 1 TCO&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;$51,777.60&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;$42,030&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Directionally, this example seems correct.&lt;/p&gt;\n\n&lt;p&gt;What do you think? What am I missing? What did I underestimate/overestimate? How can I improve this?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10gaucp", "is_robot_indexable": true, "report_reasons": null, "author": "propjames", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gaucp/feedback_request_tco_calculation_for_apache_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gaucp/feedback_request_tco_calculation_for_apache_kafka/", "subreddit_subscribers": 86849, "created_utc": 1674156743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I just started working in a Consultancy firm a few months ago. It's my first job and I am a fresher just out of college. I am working as an analyst there but the Domain in which I am working isn't exactly what I was looking forward to join when I joined the firm. I am really interested in Data related fields and it'll be a great help if someone could just guide me on what specific fields and tools to master so that I can crack an interview. I'll try to switch in this very firm itself first, but if there isn't any provision to switch, I'll try in some other firm. \n\nI am familiar with PostgreSQL, MySQL, Power BI, a little bit of R and Python, and Java. \nI am not really good at DSA.", "author_fullname": "t2_v738hvv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips on how to get into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gq8c9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674198133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I just started working in a Consultancy firm a few months ago. It&amp;#39;s my first job and I am a fresher just out of college. I am working as an analyst there but the Domain in which I am working isn&amp;#39;t exactly what I was looking forward to join when I joined the firm. I am really interested in Data related fields and it&amp;#39;ll be a great help if someone could just guide me on what specific fields and tools to master so that I can crack an interview. I&amp;#39;ll try to switch in this very firm itself first, but if there isn&amp;#39;t any provision to switch, I&amp;#39;ll try in some other firm. &lt;/p&gt;\n\n&lt;p&gt;I am familiar with PostgreSQL, MySQL, Power BI, a little bit of R and Python, and Java. \nI am not really good at DSA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10gq8c9", "is_robot_indexable": true, "report_reasons": null, "author": "curvedcurrent", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gq8c9/tips_on_how_to_get_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gq8c9/tips_on_how_to_get_into_data_engineering/", "subreddit_subscribers": 86849, "created_utc": 1674198133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is me multiple times a day:\n\nhttps://preview.redd.it/aajtxc8p44da1.jpg?width=500&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=fee1002c06dde822acbca49cca9f2ce85be125a5\n\nIf this is also you, you should check out this experts panel I moderated with Shirshanka Das &amp; Chad Sanderson to nail down the what, the why, and the how behind Data Contracts\n\n[https://youtu.be/jBMvb039RFU](https://youtu.be/jBMvb039RFU)\n\nWhat questions did I miss? What's still unclear when it comes to data contracts? ", "author_fullname": "t2_g7cej1g1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm once again asking: WTF is a Data Contract?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"aajtxc8p44da1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 108, "x": 108, "u": "https://preview.redd.it/aajtxc8p44da1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bb91630cb16e9915dbf19438982e80dfef34a71"}, {"y": 216, "x": 216, "u": "https://preview.redd.it/aajtxc8p44da1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4192462824bf843aff0baff302bb9ee9cb71cfcc"}, {"y": 320, "x": 320, "u": "https://preview.redd.it/aajtxc8p44da1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a030839c3202aeadb64db0d57684cccb82d01518"}], "s": {"y": 500, "x": 500, "u": "https://preview.redd.it/aajtxc8p44da1.jpg?width=500&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=fee1002c06dde822acbca49cca9f2ce85be125a5"}, "id": "aajtxc8p44da1"}}, "name": "t3_10glhpm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/jBMvb039RFU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Contracts Experts Panel &amp;amp; AMA\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Contracts Experts Panel &amp; AMA", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/jBMvb039RFU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Contracts Experts Panel &amp;amp; AMA\"&gt;&lt;/iframe&gt;", "author_name": "DataHub", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/jBMvb039RFU/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataHubProject"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/jBMvb039RFU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Contracts Experts Panel &amp;amp; AMA\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/10glhpm", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/UziC05KLYIAO4vNGNJMdD5UNztio0FyB6smsiyt9Rsg.jpg", "edited": 1674183785.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1674183528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is me multiple times a day:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/aajtxc8p44da1.jpg?width=500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fee1002c06dde822acbca49cca9f2ce85be125a5\"&gt;https://preview.redd.it/aajtxc8p44da1.jpg?width=500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fee1002c06dde822acbca49cca9f2ce85be125a5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If this is also you, you should check out this experts panel I moderated with Shirshanka Das &amp;amp; Chad Sanderson to nail down the what, the why, and the how behind Data Contracts&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/jBMvb039RFU\"&gt;https://youtu.be/jBMvb039RFU&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What questions did I miss? What&amp;#39;s still unclear when it comes to data contracts? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rLNDsz2lGsNrU0u8ofa0QDpCQhzq0_twnDJgh0wDaFI.jpg?auto=webp&amp;v=enabled&amp;s=197d5b5d6224b216daefc252040cbd38222a0e03", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/rLNDsz2lGsNrU0u8ofa0QDpCQhzq0_twnDJgh0wDaFI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c85a2ad438df963b88581022bf16ca9ff41ffb9", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/rLNDsz2lGsNrU0u8ofa0QDpCQhzq0_twnDJgh0wDaFI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7c0faf3a7ddce93de615849827ebd1ac84ad4034", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/rLNDsz2lGsNrU0u8ofa0QDpCQhzq0_twnDJgh0wDaFI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fb9b7383b116483c5c21f231d0511ce5086c5e4", "width": 320, "height": 240}], "variants": {}, "id": "68LvsX5bMLbwnnSC9RI90jGgXkWzG4t6OxEjnflKKwY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10glhpm", "is_robot_indexable": true, "report_reasons": null, "author": "Brief_Actuator_8731", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10glhpm/im_once_again_asking_wtf_is_a_data_contract/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10glhpm/im_once_again_asking_wtf_is_a_data_contract/", "subreddit_subscribers": 86849, "created_utc": 1674183528.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Contracts Experts Panel &amp; AMA", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/jBMvb039RFU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Contracts Experts Panel &amp;amp; AMA\"&gt;&lt;/iframe&gt;", "author_name": "DataHub", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/jBMvb039RFU/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataHubProject"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jxndm14v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shrinking the insurance data dump", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_10ge002", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Xojqd17zHMTFJBEeEcp6MXVfLSyHvVMC_EMzmtASjMQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674164086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dolthub.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dolthub.com/blog/2023-01-11-mrf-data-deduplication/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?auto=webp&amp;v=enabled&amp;s=ff7e9e7c0865b33e725e14e88be32132231f142c", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1fffa753b9da18dff55e9cd6fcb32fcae3a0b947", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=841814dbfc79490d718e5470150e97e546fcc804", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3d0b2240c6bab768449c7f80727b8052e8f297e", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=756d71014433a18b6e188cadaa83b501cbd27212", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5f17aa8be3098b485f2d8f9e7f3bf93846d04bc", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3636361baf2b03548ceea8856410d92029f3a028", "width": 1080, "height": 564}], "variants": {}, "id": "VnvCeEEknT_S34IC_m16ttV2ZKmlOUcP-P7IbZwRlds"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10ge002", "is_robot_indexable": true, "report_reasons": null, "author": "alecs-dolt", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ge002/shrinking_the_insurance_data_dump/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dolthub.com/blog/2023-01-11-mrf-data-deduplication/", "subreddit_subscribers": 86849, "created_utc": 1674164086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was tasked to aggregate data in DynamoDB. It was only around 5GB of data. Is it better if I aggregate data using AWS Glue and load it to s3 as csv? Or should I export the table to s3 and query using athena?", "author_fullname": "t2_6542em0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Query from Athena or Transform in AWS Glue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fzw3l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674129471.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was tasked to aggregate data in DynamoDB. It was only around 5GB of data. Is it better if I aggregate data using AWS Glue and load it to s3 as csv? Or should I export the table to s3 and query using athena?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fzw3l", "is_robot_indexable": true, "report_reasons": null, "author": "EngrRhys", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fzw3l/query_from_athena_or_transform_in_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fzw3l/query_from_athena_or_transform_in_aws_glue/", "subreddit_subscribers": 86849, "created_utc": 1674129471.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does one attach a dollar value to data engineering projects that concern tightening of security \u2013 improving access control for Snowflake data, improved methods of authentication into AWS from within GitHub, Jenkins, etc. I\u2019m pushing for initiatives like these but need to justify it to the business using monetary benefits.", "author_fullname": "t2_4y7rb87m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Assigning dollar value benefits to projects concerning security", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gpnj4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674196145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does one attach a dollar value to data engineering projects that concern tightening of security \u2013 improving access control for Snowflake data, improved methods of authentication into AWS from within GitHub, Jenkins, etc. I\u2019m pushing for initiatives like these but need to justify it to the business using monetary benefits.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10gpnj4", "is_robot_indexable": true, "report_reasons": null, "author": "ElectricalFilm2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gpnj4/assigning_dollar_value_benefits_to_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gpnj4/assigning_dollar_value_benefits_to_projects/", "subreddit_subscribers": 86849, "created_utc": 1674196145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,   \n\n\nIs there a canonical resource (a-la Clean Code) that spells out best practices for creating large SQL queries for report creations? In particular, I would like to have more confidence in how I'm designing CTE's.  \n\n\nThanks!", "author_fullname": "t2_7yr26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Resource for SQL Best Practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gh95u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674172210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,   &lt;/p&gt;\n\n&lt;p&gt;Is there a canonical resource (a-la Clean Code) that spells out best practices for creating large SQL queries for report creations? In particular, I would like to have more confidence in how I&amp;#39;m designing CTE&amp;#39;s.  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10gh95u", "is_robot_indexable": true, "report_reasons": null, "author": "nathanak21", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gh95u/best_resource_for_sql_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gh95u/best_resource_for_sql_best_practices/", "subreddit_subscribers": 86849, "created_utc": 1674172210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have you ever designed a lakehouse/datawarehouse - Top Down ( KPI-&gt; Use Case-&gt;Architecture -&gt; ETL) or Bottom Up ( Architecture -&gt; ETL -&gt; Use Case -&gt; KPI ) ? What were the lessons learned ?", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trade off - Top Down vs Bottom up Approach of Data Estate Set Up", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10grc86", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674202189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you ever designed a lakehouse/datawarehouse - Top Down ( KPI-&amp;gt; Use Case-&amp;gt;Architecture -&amp;gt; ETL) or Bottom Up ( Architecture -&amp;gt; ETL -&amp;gt; Use Case -&amp;gt; KPI ) ? What were the lessons learned ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10grc86", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10grc86/trade_off_top_down_vs_bottom_up_approach_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10grc86/trade_off_top_down_vs_bottom_up_approach_of_data/", "subreddit_subscribers": 86849, "created_utc": 1674202189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nLooking for the tangible business benefits of a move from Snowflake to Lakehouse design (Delta/Hudi/Iceberg tbc.) managed by the team - not a switch to DataBricks.\n\nCompany uses AWS for our microservices, and Kafka for event messaging.\n\nWould love to move from a technical perspective, but need sound reasoning from a business perspective to back it up.\n\nWe\u2019re paying a little over $100k/year to Snowflake, but have not cost optimised (could possibly get to $75k but hard to justify the opportunity cost in comparison to other projects). Cost in part due to volume of data in variant columns and requirement to de-dup Kafkas \u2018guaranteed at least once delivery\u2019, along with a business requirement for point in time reporting on data that doesn\u2019t neatly conform to fact/dim relationships. Access would be via Power BI.\n\nSo this is a loose framing I know, but keen to understand thoughts.\n\nEdit: for clarity, Kafka events drop straight into Snowflake right now with a table per publication and a record per event. Two variant columns per table - first containing Kafka metadata (offset, partition etc) and the second containing the payload which has business key in the json.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lakehouse vs Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10gr5l8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674201796.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674201533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Looking for the tangible business benefits of a move from Snowflake to Lakehouse design (Delta/Hudi/Iceberg tbc.) managed by the team - not a switch to DataBricks.&lt;/p&gt;\n\n&lt;p&gt;Company uses AWS for our microservices, and Kafka for event messaging.&lt;/p&gt;\n\n&lt;p&gt;Would love to move from a technical perspective, but need sound reasoning from a business perspective to back it up.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re paying a little over $100k/year to Snowflake, but have not cost optimised (could possibly get to $75k but hard to justify the opportunity cost in comparison to other projects). Cost in part due to volume of data in variant columns and requirement to de-dup Kafkas \u2018guaranteed at least once delivery\u2019, along with a business requirement for point in time reporting on data that doesn\u2019t neatly conform to fact/dim relationships. Access would be via Power BI.&lt;/p&gt;\n\n&lt;p&gt;So this is a loose framing I know, but keen to understand thoughts.&lt;/p&gt;\n\n&lt;p&gt;Edit: for clarity, Kafka events drop straight into Snowflake right now with a table per publication and a record per event. Two variant columns per table - first containing Kafka metadata (offset, partition etc) and the second containing the payload which has business key in the json.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10gr5l8", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gr5l8/lakehouse_vs_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gr5l8/lakehouse_vs_snowflake/", "subreddit_subscribers": 86849, "created_utc": 1674201533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI've been lurking on this subreddit for awhile and have been learning a lot based on all the great posts here. Recently I completed an end to end project after also working through the DataTalksClub zoomcamp based on their 2022 youtube videos. I'm in a semi technical role now although it's similar to an SAP-FI specialist so I don't really use python or SQL as part of my job.\n\nI've been spending a lot of time practicing both my SQL/python skills and learning data warehousing fundamentals (normalization/denormalization, fact/dimension/star/snowflake, OLAP/OLTP, etc.) and am hopefully at a point where I can start applying to junior DE jobs. I'm also half way through the computational track of the Georgia Tech Analytics masters so I have been getting a lot of practice through that program as well.\n\nIf anyone has any feedback on either my project or my resume it would be much appreciated. Also, on the off chance anyone knows of a job opportunity that might be a good fit feel free to DM me.\n\n[End to End Data Pipeline Project](https://medium.com/@tgrady101/building-an-end-to-end-data-engineering-project-f8f34b334648)\n\nhttps://preview.redd.it/wbof37dvg4da1.jpg?width=2550&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=c186095cf9e362bb437d00e732a5462ad6c30150", "author_fullname": "t2_re0fe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Another Aspiring Data Engineer: Project/Resume Feedback Request", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 104, "top_awarded_type": null, "hide_score": false, "media_metadata": {"wbof37dvg4da1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 139, "x": 108, "u": "https://preview.redd.it/wbof37dvg4da1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e5536281935af4a83333c0b654f674ca55f49062"}, {"y": 279, "x": 216, "u": "https://preview.redd.it/wbof37dvg4da1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c60a542a81cfe391f8a63fd3bb25f42049e37fd"}, {"y": 414, "x": 320, "u": "https://preview.redd.it/wbof37dvg4da1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a5ada7f1219bbd37e55373ab5adb25f11b06d7f"}, {"y": 828, "x": 640, "u": "https://preview.redd.it/wbof37dvg4da1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08771363d1297fe0518c50eed15851ebb3fba285"}, {"y": 1242, "x": 960, "u": "https://preview.redd.it/wbof37dvg4da1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e13636acb2f846cef9b80dd69fee25c1b88fa9e7"}, {"y": 1397, "x": 1080, "u": "https://preview.redd.it/wbof37dvg4da1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be6523433d4a33bbfa21b724f628dae1f6a6bc6b"}], "s": {"y": 3300, "x": 2550, "u": "https://preview.redd.it/wbof37dvg4da1.jpg?width=2550&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=c186095cf9e362bb437d00e732a5462ad6c30150"}, "id": "wbof37dvg4da1"}}, "name": "t3_10gmrri", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/aQc5y0lxhJhMGVycOBo4WyqhyFfV8TxZE2brmR6h0hY.jpg", "edited": 1674187648.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1674187212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been lurking on this subreddit for awhile and have been learning a lot based on all the great posts here. Recently I completed an end to end project after also working through the DataTalksClub zoomcamp based on their 2022 youtube videos. I&amp;#39;m in a semi technical role now although it&amp;#39;s similar to an SAP-FI specialist so I don&amp;#39;t really use python or SQL as part of my job.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been spending a lot of time practicing both my SQL/python skills and learning data warehousing fundamentals (normalization/denormalization, fact/dimension/star/snowflake, OLAP/OLTP, etc.) and am hopefully at a point where I can start applying to junior DE jobs. I&amp;#39;m also half way through the computational track of the Georgia Tech Analytics masters so I have been getting a lot of practice through that program as well.&lt;/p&gt;\n\n&lt;p&gt;If anyone has any feedback on either my project or my resume it would be much appreciated. Also, on the off chance anyone knows of a job opportunity that might be a good fit feel free to DM me.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@tgrady101/building-an-end-to-end-data-engineering-project-f8f34b334648\"&gt;End to End Data Pipeline Project&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wbof37dvg4da1.jpg?width=2550&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c186095cf9e362bb437d00e732a5462ad6c30150\"&gt;https://preview.redd.it/wbof37dvg4da1.jpg?width=2550&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c186095cf9e362bb437d00e732a5462ad6c30150&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cSXQdx83l1kvWx-14UgEzElDTBgxIxoUXRN7euH9RP4.jpg?auto=webp&amp;v=enabled&amp;s=245e1498f971802d5663723abac835660827a3f9", "width": 432, "height": 323}, "resolutions": [{"url": "https://external-preview.redd.it/cSXQdx83l1kvWx-14UgEzElDTBgxIxoUXRN7euH9RP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3207c935ad3d75a9e84d137477638b8f660a94e7", "width": 108, "height": 80}, {"url": "https://external-preview.redd.it/cSXQdx83l1kvWx-14UgEzElDTBgxIxoUXRN7euH9RP4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=940ab457c95586781743feb0704187328f7760be", "width": 216, "height": 161}, {"url": "https://external-preview.redd.it/cSXQdx83l1kvWx-14UgEzElDTBgxIxoUXRN7euH9RP4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82e904ffa019de64bf74c02821063702dc65f53d", "width": 320, "height": 239}], "variants": {}, "id": "uzoEiS--JT9dP6jq8Uo379W-p0eaSiphEgIm90RzdbY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "10gmrri", "is_robot_indexable": true, "report_reasons": null, "author": "Jovius10", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gmrri/another_aspiring_data_engineer_projectresume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gmrri/another_aspiring_data_engineer_projectresume/", "subreddit_subscribers": 86849, "created_utc": 1674187212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a pipeline that would look something like this:\n\n[Pipeline](https://preview.redd.it/aaaitll422da1.png?width=1662&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5e20276f445483765304633b7408ca66c642aed6)\n\nI intend to implement it in Prefect 2.0 but there are two things that I can't actually figure out:\n\n1. Flow B has a dependency on both flows A and X. However, flow X is triggered by a different process and X and A don't necessarily run at the same time (\\~15 min gap), can these two flows interact with each other and wait for each other to run a third flow given that both flows are running separately (i.e. separate deployment )\n2. Flow C is triggered by flow A on the 1st of every month, in normal circumstances, it would get executed, however, in occasional events, if it's Monday and also the 1st of the month, flow C should wait for B to finish, how does this work? I can just add an if-else or a similar implementation but wouldn't it mess up the radar (the visual flow representation) since the pythonic code can mess up with\n\nI would really appreciate it if anyone can lead me to the right path or maybe if there is any similar implementation, that'd be great too, Thanks!\n\nDiagram: You can consider the yellow box as job A and the green box as job B\n\nEdit 1: Flow C has no time schedule, it gets triggered from flow A, 2 PM is marked wrong\n\n^(Disclaimer: Apologies if this is a dumb question and doesn't relate to this sub, please feel free to remove this.)", "author_fullname": "t2_2xpdiv8k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can two different perfect flows interact with each other?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 99, "top_awarded_type": null, "hide_score": false, "media_metadata": {"aaaitll422da1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/aaaitll422da1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db84c3e387775b2ee0870dc3be0ddb74c8f8d822"}, {"y": 153, "x": 216, "u": "https://preview.redd.it/aaaitll422da1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbef575bd5ea417c89fb32a863f9afb688473127"}, {"y": 227, "x": 320, "u": "https://preview.redd.it/aaaitll422da1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=208c45f98ba15df36bd3fc701f3996f07e9c3e10"}, {"y": 454, "x": 640, "u": "https://preview.redd.it/aaaitll422da1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3fbe97132e10c73dfa67765d1e9ecf762fc55f86"}, {"y": 682, "x": 960, "u": "https://preview.redd.it/aaaitll422da1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cda3bd29b3866665a015e51bf8e002a449d8d276"}, {"y": 767, "x": 1080, "u": "https://preview.redd.it/aaaitll422da1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae6937b07c1287e4385f8702d8a9f6208849d7a4"}], "s": {"y": 1181, "x": 1662, "u": "https://preview.redd.it/aaaitll422da1.png?width=1662&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5e20276f445483765304633b7408ca66c642aed6"}, "id": "aaaitll422da1"}}, "name": "t3_10gbzvu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3uV6ZuGGEkvzaJvbz613V1a30VOQKMCXJO327EJSROs.jpg", "edited": 1674159639.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674159447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a pipeline that would look something like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/aaaitll422da1.png?width=1662&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5e20276f445483765304633b7408ca66c642aed6\"&gt;Pipeline&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I intend to implement it in Prefect 2.0 but there are two things that I can&amp;#39;t actually figure out:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Flow B has a dependency on both flows A and X. However, flow X is triggered by a different process and X and A don&amp;#39;t necessarily run at the same time (~15 min gap), can these two flows interact with each other and wait for each other to run a third flow given that both flows are running separately (i.e. separate deployment )&lt;/li&gt;\n&lt;li&gt;Flow C is triggered by flow A on the 1st of every month, in normal circumstances, it would get executed, however, in occasional events, if it&amp;#39;s Monday and also the 1st of the month, flow C should wait for B to finish, how does this work? I can just add an if-else or a similar implementation but wouldn&amp;#39;t it mess up the radar (the visual flow representation) since the pythonic code can mess up with&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I would really appreciate it if anyone can lead me to the right path or maybe if there is any similar implementation, that&amp;#39;d be great too, Thanks!&lt;/p&gt;\n\n&lt;p&gt;Diagram: You can consider the yellow box as job A and the green box as job B&lt;/p&gt;\n\n&lt;p&gt;Edit 1: Flow C has no time schedule, it gets triggered from flow A, 2 PM is marked wrong&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;Disclaimer: Apologies if this is a dumb question and doesn&amp;#39;t relate to this sub, please feel free to remove this.&lt;/sup&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "SSIS developer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10gbzvu", "is_robot_indexable": true, "report_reasons": null, "author": "_whitezetsu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10gbzvu/can_two_different_perfect_flows_interact_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gbzvu/can_two_different_perfect_flows_interact_with/", "subreddit_subscribers": 86849, "created_utc": 1674159447.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}