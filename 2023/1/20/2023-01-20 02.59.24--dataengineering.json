{"kind": "Listing", "data": {"after": null, "dist": 20, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Could be something very minor like a specific gotcha in a library or a major architecture decision that bit you in the behind.", "author_fullname": "t2_14i1sz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What mistake have you vowed never to repeat?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g3g1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674139484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could be something very minor like a specific gotcha in a library or a major architecture decision that bit you in the behind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10g3g1r", "is_robot_indexable": true, "report_reasons": null, "author": "pescennius", "discussion_type": null, "num_comments": 71, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g3g1r/what_mistake_have_you_vowed_never_to_repeat/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g3g1r/what_mistake_have_you_vowed_never_to_repeat/", "subreddit_subscribers": 86830, "created_utc": 1674139484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_31ilk7t8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to speed up range joins joins in Snowflake by 300x - SELECT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "name": "t3_10g65bg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yVrMGv9ob-fHob-nsAMDUf1Wxgufd4sgTPddBt3tMfo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674145957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "select.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://select.dev/posts/snowflake-range-join-optimization", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?auto=webp&amp;v=enabled&amp;s=c6102393c1fc139ebe3a192d54baefc9269fa3d6", "width": 1200, "height": 733}, "resolutions": [{"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e24f19fe617b042b54367889609e4aa75d7d6135", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc01477758940100756397660527fd80cfa738ad", "width": 216, "height": 131}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2db23948c752c587caa1f5bd2ecbfb81b13c713f", "width": 320, "height": 195}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac762f60c4c6caf1fa44521997dda65fed7b824c", "width": 640, "height": 390}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=956b255f528f54123a9598503aa68e2eef59b025", "width": 960, "height": 586}, {"url": "https://external-preview.redd.it/ypMFVD-R60POJ7omlFD3hQzXoTCAGXeGEI_iDDfosDU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c8a705f31099be5dd86194da0794355c6a7b829", "width": 1080, "height": 659}], "variants": {}, "id": "Kl2clgMmndy65WOWu6IHcN4IPoSI4AphoMLolknNnyA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10g65bg", "is_robot_indexable": true, "report_reasons": null, "author": "ian-whitestone", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g65bg/how_to_speed_up_range_joins_joins_in_snowflake_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://select.dev/posts/snowflake-range-join-optimization", "subreddit_subscribers": 86830, "created_utc": 1674145957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE community!  \nI've been working on the data migration to the star schema architecture, and here are a couple of questions I could not confidently decide how to proceed with.\n\n1. What are the common strategies for handling the loaded\\_at timestamp fields in dwh, when the target table gets the data from multiple different sources? I was thinking of creating a 'last' and/or the 'first' loaded\\_at fields, not sure if there's any better way.\n2. A similar question about specifying the 'source' field into the dimensions. When the dimension gets created out of two different sources, in the way that attributes of a single dimension record are coming from various sources, how do you usually handle the 'source' field? Here I though of two options, one was to simply indicate the main dimension record source, i.e. source from where the platform's unique identifier is coming. But this way we lose the source of additional dimension attributes.  \n\n\nThanks!", "author_fullname": "t2_gwqijajo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Star Schema - questions about loading the data from multiple sources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fzw1q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674129466.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE community!&lt;br/&gt;\nI&amp;#39;ve been working on the data migration to the star schema architecture, and here are a couple of questions I could not confidently decide how to proceed with.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What are the common strategies for handling the loaded_at timestamp fields in dwh, when the target table gets the data from multiple different sources? I was thinking of creating a &amp;#39;last&amp;#39; and/or the &amp;#39;first&amp;#39; loaded_at fields, not sure if there&amp;#39;s any better way.&lt;/li&gt;\n&lt;li&gt;A similar question about specifying the &amp;#39;source&amp;#39; field into the dimensions. When the dimension gets created out of two different sources, in the way that attributes of a single dimension record are coming from various sources, how do you usually handle the &amp;#39;source&amp;#39; field? Here I though of two options, one was to simply indicate the main dimension record source, i.e. source from where the platform&amp;#39;s unique identifier is coming. But this way we lose the source of additional dimension attributes.&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fzw1q", "is_robot_indexable": true, "report_reasons": null, "author": "orm_the_stalker", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fzw1q/star_schema_questions_about_loading_the_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fzw1q/star_schema_questions_about_loading_the_data_from/", "subreddit_subscribers": 86830, "created_utc": 1674129466.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I posted the same in r/datascience, but thought it would get a better traction here instead, since many of you are actual data engineers. The original post is [here](https://www.reddit.com/r/datascience/comments/10fpmi2/has_anyone_here_transitioned_from_data_scientist/?utm_source=share&amp;utm_medium=web2x&amp;context=3) for anyone curious.\n\nSo basically, I'm curious to hear from people who work as data engineers that transitioned from a data scientist. What made you switch and do you regret you decision? Or was it one of the best decisions for your career? Thanks!", "author_fullname": "t2_rc5lfgnr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone here transitioned from Data Scientist to Data Engineer? What was your motivation, and do you regret the move now, or are you happier as a Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ghhal", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674172792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I posted the same in &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt;, but thought it would get a better traction here instead, since many of you are actual data engineers. The original post is &lt;a href=\"https://www.reddit.com/r/datascience/comments/10fpmi2/has_anyone_here_transitioned_from_data_scientist/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;here&lt;/a&gt; for anyone curious.&lt;/p&gt;\n\n&lt;p&gt;So basically, I&amp;#39;m curious to hear from people who work as data engineers that transitioned from a data scientist. What made you switch and do you regret you decision? Or was it one of the best decisions for your career? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ghhal", "is_robot_indexable": true, "report_reasons": null, "author": "Subject_Ad_9680", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ghhal/has_anyone_here_transitioned_from_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ghhal/has_anyone_here_transitioned_from_data_scientist/", "subreddit_subscribers": 86830, "created_utc": 1674172792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I kinda find the tools both use are same...so it makes both the roles same only ?", "author_fullname": "t2_99z52qfs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Data Engineers and Big Data Engineers same?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g7wyt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674150089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I kinda find the tools both use are same...so it makes both the roles same only ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10g7wyt", "is_robot_indexable": true, "report_reasons": null, "author": "Vortex_007s", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g7wyt/are_data_engineers_and_big_data_engineers_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g7wyt/are_data_engineers_and_big_data_engineers_same/", "subreddit_subscribers": 86830, "created_utc": 1674150089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j33i3nha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Requesting feedback on my resume - Data Engineer with a little over 2 years of experience, was laid off recently", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10g7wus", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": "transparent", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EYob2aCUBnKMaS02mKE8gOPld9yR-gCcrmYmsOtDUuw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674150081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zxbize3zd1da1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?auto=webp&amp;v=enabled&amp;s=9cca47ece1fc5ac2ac8ce349298c45c45aad2427", "width": 1700, "height": 2200}, "resolutions": [{"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be479760d91ee62bb8c1471afe5961e9285b2286", "width": 108, "height": 139}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0237c1248dafa3dfa220c7a14c15a52fa4896d13", "width": 216, "height": 279}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c234a772ce92e14f3642beb70560a6fc7e173be", "width": 320, "height": 414}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f5fc705a3bdac8a314c9016aa1ec77add7eed5f", "width": 640, "height": 828}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f68926ee615705f32c8cb6f0ee9fd672b2a9f16c", "width": 960, "height": 1242}, {"url": "https://preview.redd.it/zxbize3zd1da1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7aa391976255f8543c948d67f3fefd76e888af0", "width": 1080, "height": 1397}], "variants": {}, "id": "8eDElsZ4niQq2Lvas-ev5VOLUMDDsCuf4bubD9ehkf8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10g7wus", "is_robot_indexable": true, "report_reasons": null, "author": "CS_throwaway_DE", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10g7wus/requesting_feedback_on_my_resume_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zxbize3zd1da1.jpg", "subreddit_subscribers": 86830, "created_utc": 1674150081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! We are a medium sized data engineering team at my work and currently have a little quiz/learning exercise going on to help our juniors learn more about data engineering. In particular we are discussing different design decisions that were made for our systems in the past. One of those is that we have a default Lookback period of 3 days in many of our Airflow DAGs. One reason for that is that we don't have to do a lot of backfills in case a job fails and we fix it within that time window. However, our manager said there is another reason for why having a lookback window makes sense. Does anyone here have an idea what that could be? What are general benefits of having lookback windows aside from potentially covering for job failures?\n\nThanks all for your suggestions!!", "author_fullname": "t2_3wknjffr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reasons for Lookback Periods", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gfw9u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674168679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! We are a medium sized data engineering team at my work and currently have a little quiz/learning exercise going on to help our juniors learn more about data engineering. In particular we are discussing different design decisions that were made for our systems in the past. One of those is that we have a default Lookback period of 3 days in many of our Airflow DAGs. One reason for that is that we don&amp;#39;t have to do a lot of backfills in case a job fails and we fix it within that time window. However, our manager said there is another reason for why having a lookback window makes sense. Does anyone here have an idea what that could be? What are general benefits of having lookback windows aside from potentially covering for job failures?&lt;/p&gt;\n\n&lt;p&gt;Thanks all for your suggestions!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10gfw9u", "is_robot_indexable": true, "report_reasons": null, "author": "katerdoener", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gfw9u/reasons_for_lookback_periods/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gfw9u/reasons_for_lookback_periods/", "subreddit_subscribers": 86830, "created_utc": 1674168679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi ,\n\nWe are currently trying to build data quality checks like outlier detection for our streaming data such as kafka streams , iot devices etc.\n\nCan anyone suggest an approach that can be used to acheive this using databricks?\n\nThanks!", "author_fullname": "t2_jysbojws", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data quality checks in spark streaming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gftev", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674168489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi ,&lt;/p&gt;\n\n&lt;p&gt;We are currently trying to build data quality checks like outlier detection for our streaming data such as kafka streams , iot devices etc.&lt;/p&gt;\n\n&lt;p&gt;Can anyone suggest an approach that can be used to acheive this using databricks?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10gftev", "is_robot_indexable": true, "report_reasons": null, "author": "Quick_Poem_4344", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gftev/data_quality_checks_in_spark_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gftev/data_quality_checks_in_spark_streaming/", "subreddit_subscribers": 86830, "created_utc": 1674168489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Tracking down all the args and import statements for providers was sometimes a pain. This VS Code extension has all of the providers loaded so you can autocomplete all of the operators and hooks. [https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates](https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates)", "author_fullname": "t2_shq2slu0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "VS Code extension for Airflow Provider packages: Airflow Templates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g7tsp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674149886.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tracking down all the args and import statements for providers was sometimes a pain. This VS Code extension has all of the providers loaded so you can autocomplete all of the operators and hooks. &lt;a href=\"https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates\"&gt;https://marketplace.visualstudio.com/items?itemName=GraysonStream.airflow-templates&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5Vwgalc18IqnYE7aIh59IAOgguPxfM4Agv9EhnfeWJc.jpg?auto=webp&amp;v=enabled&amp;s=f519d587d5d576b6d3824ecfeb08e435ac409c9d", "width": 140, "height": 139}, "resolutions": [{"url": "https://external-preview.redd.it/5Vwgalc18IqnYE7aIh59IAOgguPxfM4Agv9EhnfeWJc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dfd5cd2e7d92a6db1b462a32c97900911d11d93", "width": 108, "height": 107}], "variants": {}, "id": "vuJ3SPN1HSQh3YWOb1ZAo50TA1_Ol-O149S9O3Ne1Es"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10g7tsp", "is_robot_indexable": true, "report_reasons": null, "author": "compound-cluster", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g7tsp/vs_code_extension_for_airflow_provider_packages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g7tsp/vs_code_extension_for_airflow_provider_packages/", "subreddit_subscribers": 86830, "created_utc": 1674149886.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was tasked to aggregate data in DynamoDB. It was only around 5GB of data. Is it better if I aggregate data using AWS Glue and load it to s3 as csv? Or should I export the table to s3 and query using athena?", "author_fullname": "t2_6542em0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Query from Athena or Transform in AWS Glue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10fzw3l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674129471.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was tasked to aggregate data in DynamoDB. It was only around 5GB of data. Is it better if I aggregate data using AWS Glue and load it to s3 as csv? Or should I export the table to s3 and query using athena?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10fzw3l", "is_robot_indexable": true, "report_reasons": null, "author": "EngrRhys", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10fzw3l/query_from_athena_or_transform_in_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10fzw3l/query_from_athena_or_transform_in_aws_glue/", "subreddit_subscribers": 86830, "created_utc": 1674129471.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jxndm14v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shrinking the insurance data dump", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_10ge002", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Xojqd17zHMTFJBEeEcp6MXVfLSyHvVMC_EMzmtASjMQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674164086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dolthub.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dolthub.com/blog/2023-01-11-mrf-data-deduplication/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?auto=webp&amp;v=enabled&amp;s=ff7e9e7c0865b33e725e14e88be32132231f142c", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1fffa753b9da18dff55e9cd6fcb32fcae3a0b947", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=841814dbfc79490d718e5470150e97e546fcc804", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3d0b2240c6bab768449c7f80727b8052e8f297e", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=756d71014433a18b6e188cadaa83b501cbd27212", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5f17aa8be3098b485f2d8f9e7f3bf93846d04bc", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/rBLXYgOek5SsbIPlHjta7IfBNMqnn47xJjV2NwqTIn8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3636361baf2b03548ceea8856410d92029f3a028", "width": 1080, "height": 564}], "variants": {}, "id": "VnvCeEEknT_S34IC_m16ttV2ZKmlOUcP-P7IbZwRlds"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10ge002", "is_robot_indexable": true, "report_reasons": null, "author": "alecs-dolt", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ge002/shrinking_the_insurance_data_dump/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dolthub.com/blog/2023-01-11-mrf-data-deduplication/", "subreddit_subscribers": 86830, "created_utc": 1674164086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on calculating the total cost of ownership (TCO) for tools like Apache Kafka to determine when to build vs. buy.\n\nI'd love your feedback -- what am I missing? What did I underestimate/overestimate? How can I improve this?\n\n**First, the criteria to consider when calculating TCO:**\n\nUp-front costs\n\n* software cost &amp; licensing, if applicable\n* learning &amp; education\n* implementation &amp; testing (including data migration costs)\n* documentation &amp; knowledge sharing\n* customization\n\nOngoing costs\n\n* direct infrastructure costs (e.g., hosting &amp; storage)\n* backup infrastructure costs (e.g., failover &amp; additional AZs)\n* supporting infrastructure costs (e.g., monitoring &amp; alerting)\n* maintenance, patches/upgrades, &amp; support\n* feature additions\n\nTeam &amp; opportunity costs\n\n* hiring to replace the engineers now working with the new software\n* time spent on infrastructure that could otherwise be spent on core product\n\n**Now, an example using the above criteria:**\n\nDesired specs for our example deployment (I picked one of the smaller Heroku plans):\n\n* Capacity: 300GB\n* Retention: 2 weeks\n* vCPU: 4\n* Ram: 16GB\n* Brokers: 3\n\nAssuming an engineer has an all-in comp package of $200k/yr (this would obviously be different in every situation, for every geo), year one would look like:\n\n||**Building (on AWS)**|**Buying (Heroku)**|\n|:-|:-|:-|\n|software cost &amp; licensing|$0|$21,600|\n|learning &amp; education|$7,692 (2 eng \\* 1 week)|$3,846 (1 eng \\* 1 week)|\n|implementation &amp; testing|$15,384 (2 eng \\* 2 weeks)|$7,692 (1 eng \\* 1 week)|\n|infrastructure costs (see above specs)|$12,117.60|$0 (included in software cost)|\n|supporting infrastructure costs (monitoring, etc.)|$1,200/yr|$1,200/yr|\n|maintenance, patches/upgrades|$15,384 (2 eng \\* 2 weeks spread throughout the year)|$7,692 (1 eng \\* 2 weeks spread throughout the year)|\n|**Year 1 TCO**|**$51,777.60**|**$42,030**|\n\nDirectionally, this example seems correct.\n\nWhat do you think? What am I missing? What did I underestimate/overestimate? How can I improve this?\n\nThanks!", "author_fullname": "t2_fv515", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feedback Request: TCO Calculation for Apache Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gaucp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674156743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on calculating the total cost of ownership (TCO) for tools like Apache Kafka to determine when to build vs. buy.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love your feedback -- what am I missing? What did I underestimate/overestimate? How can I improve this?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;First, the criteria to consider when calculating TCO:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Up-front costs&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;software cost &amp;amp; licensing, if applicable&lt;/li&gt;\n&lt;li&gt;learning &amp;amp; education&lt;/li&gt;\n&lt;li&gt;implementation &amp;amp; testing (including data migration costs)&lt;/li&gt;\n&lt;li&gt;documentation &amp;amp; knowledge sharing&lt;/li&gt;\n&lt;li&gt;customization&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Ongoing costs&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;direct infrastructure costs (e.g., hosting &amp;amp; storage)&lt;/li&gt;\n&lt;li&gt;backup infrastructure costs (e.g., failover &amp;amp; additional AZs)&lt;/li&gt;\n&lt;li&gt;supporting infrastructure costs (e.g., monitoring &amp;amp; alerting)&lt;/li&gt;\n&lt;li&gt;maintenance, patches/upgrades, &amp;amp; support&lt;/li&gt;\n&lt;li&gt;feature additions&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Team &amp;amp; opportunity costs&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;hiring to replace the engineers now working with the new software&lt;/li&gt;\n&lt;li&gt;time spent on infrastructure that could otherwise be spent on core product&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Now, an example using the above criteria:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Desired specs for our example deployment (I picked one of the smaller Heroku plans):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Capacity: 300GB&lt;/li&gt;\n&lt;li&gt;Retention: 2 weeks&lt;/li&gt;\n&lt;li&gt;vCPU: 4&lt;/li&gt;\n&lt;li&gt;Ram: 16GB&lt;/li&gt;\n&lt;li&gt;Brokers: 3&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Assuming an engineer has an all-in comp package of $200k/yr (this would obviously be different in every situation, for every geo), year one would look like:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;Building (on AWS)&lt;/strong&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;Buying (Heroku)&lt;/strong&gt;&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;software cost &amp;amp; licensing&lt;/td&gt;\n&lt;td align=\"left\"&gt;$0&lt;/td&gt;\n&lt;td align=\"left\"&gt;$21,600&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;learning &amp;amp; education&lt;/td&gt;\n&lt;td align=\"left\"&gt;$7,692 (2 eng * 1 week)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$3,846 (1 eng * 1 week)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;implementation &amp;amp; testing&lt;/td&gt;\n&lt;td align=\"left\"&gt;$15,384 (2 eng * 2 weeks)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$7,692 (1 eng * 1 week)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;infrastructure costs (see above specs)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$12,117.60&lt;/td&gt;\n&lt;td align=\"left\"&gt;$0 (included in software cost)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;supporting infrastructure costs (monitoring, etc.)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$1,200/yr&lt;/td&gt;\n&lt;td align=\"left\"&gt;$1,200/yr&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;maintenance, patches/upgrades&lt;/td&gt;\n&lt;td align=\"left\"&gt;$15,384 (2 eng * 2 weeks spread throughout the year)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$7,692 (1 eng * 2 weeks spread throughout the year)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Year 1 TCO&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;$51,777.60&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;$42,030&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Directionally, this example seems correct.&lt;/p&gt;\n\n&lt;p&gt;What do you think? What am I missing? What did I underestimate/overestimate? How can I improve this?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10gaucp", "is_robot_indexable": true, "report_reasons": null, "author": "propjames", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gaucp/feedback_request_tco_calculation_for_apache_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gaucp/feedback_request_tco_calculation_for_apache_kafka/", "subreddit_subscribers": 86830, "created_utc": 1674156743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know how to directly get the status of a specific sync job?\n\nThe round about way I\u2019m attempting to do this is by\n1. fetching all the logs of the extractions\n2. filtering them so that the extraction\u2019s job matches the name of the current job\n3. fetching the loads\n4. filtering them where the job name matches the current job name\n5. Parse the extraction logs to see if any of the streams for that extraction fetched any rows\n6. If a stream fetched rows based on the extraction log, then check the streams from the load\n7. If the load for a stream has a specific value designating it is done, and if it had rows extracted based on the logs, mark that stream as done\n8. Keep polling and once all streams are \u201cmarked\u201d as done, consider the job done\n\n\nProblem: a ton of api calls, and potential rate limiting.\n\nAnyone else solve this issue?\n\nHere are there docs: https://www.stitchdata.com/docs/developers/stitch-connect/api", "author_fullname": "t2_9aeao1ey", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stitch Connect API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ftnmc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674106423.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know how to directly get the status of a specific sync job?&lt;/p&gt;\n\n&lt;p&gt;The round about way I\u2019m attempting to do this is by\n1. fetching all the logs of the extractions\n2. filtering them so that the extraction\u2019s job matches the name of the current job\n3. fetching the loads\n4. filtering them where the job name matches the current job name\n5. Parse the extraction logs to see if any of the streams for that extraction fetched any rows\n6. If a stream fetched rows based on the extraction log, then check the streams from the load\n7. If the load for a stream has a specific value designating it is done, and if it had rows extracted based on the logs, mark that stream as done\n8. Keep polling and once all streams are \u201cmarked\u201d as done, consider the job done&lt;/p&gt;\n\n&lt;p&gt;Problem: a ton of api calls, and potential rate limiting.&lt;/p&gt;\n\n&lt;p&gt;Anyone else solve this issue?&lt;/p&gt;\n\n&lt;p&gt;Here are there docs: &lt;a href=\"https://www.stitchdata.com/docs/developers/stitch-connect/api\"&gt;https://www.stitchdata.com/docs/developers/stitch-connect/api&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "ex-Airbnb engineer (data, ML)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ftnmc", "is_robot_indexable": true, "report_reasons": null, "author": "ironplaneswalker", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10ftnmc/stitch_connect_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ftnmc/stitch_connect_api/", "subreddit_subscribers": 86830, "created_utc": 1674106423.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_74pfheof", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tabular Product (creators of Iceberg)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": true, "name": "t3_10gjfak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Yn9zmgvxMadrHKq1mwEaHAwwkq6neUfwKX12jqg-UA4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674177864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "m.youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://m.youtube.com/watch?v=KsdMhH0_nG0", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?auto=webp&amp;v=enabled&amp;s=1c65b0d5734f6dbfc2663ade3ac64a269dc95721", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b32bdcf4fd882f23b49d9e83023d96db5ba6bada", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4e97b57ccb4f1512aa740de15163568d7811ce1", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=851d4016207f0867d93fc4e196a79f87acad25ba", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06fe08ee1a9e6db78a39f94a81f62be8ebf8c309", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b51934f2f795502cc50ffae10631e5b6c03d07cc", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/QORNsvC-6z6Bbm8ecT5kVYrveVW6a6meZ0IoeOWDlaQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2acac09c4a8f723a0eaed2702fdcf2c7d35043f7", "width": 1080, "height": 607}], "variants": {}, "id": "Vrb4wmE0MD8BDsTYANfeTCD-cV-li3waoGAmJhdiE74"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10gjfak", "is_robot_indexable": true, "report_reasons": null, "author": "No_Equivalent5942", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gjfak/tabular_product_creators_of_iceberg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://m.youtube.com/watch?v=KsdMhH0_nG0", "subreddit_subscribers": 86830, "created_utc": 1674177864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,   \n\n\nIs there a canonical resource (a-la Clean Code) that spells out best practices for creating large SQL queries for report creations? In particular, I would like to have more confidence in how I'm designing CTE's.  \n\n\nThanks!", "author_fullname": "t2_7yr26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Resource for SQL Best Practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gh95u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674172210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,   &lt;/p&gt;\n\n&lt;p&gt;Is there a canonical resource (a-la Clean Code) that spells out best practices for creating large SQL queries for report creations? In particular, I would like to have more confidence in how I&amp;#39;m designing CTE&amp;#39;s.  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10gh95u", "is_robot_indexable": true, "report_reasons": null, "author": "nathanak21", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gh95u/best_resource_for_sql_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gh95u/best_resource_for_sql_best_practices/", "subreddit_subscribers": 86830, "created_utc": 1674172210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI've been interviewing to move from data analyst role to more data engineering / software dev.\n\nI'm interested in/have extensive experience with pandas, pyodbc, querying APIs (+have graphql exposure), automation, and 24/7 running of scripts. Did a small snowflake workshop with aws glue / pyspark.\n\nI've had a lot of interviews with no pulls(many late stage as well). I feel I've improved my technical portion of interview since beginning. Last interview I did a data loading project to mariadb in python, ended up making this into a github repo and have been releasing to it. Did some new branch pull merges etc.\n\nI'm wondering what I should focus on to show I can do the work? Have been working with pandas maybe 7 years now, thought transition to finding a shop interested in that would be doable.\n\nRight now I'm working on implementing great expectations in a scalable way for data quality. Have been using chatgpt to boost my code writing as well.\n\nAny thoughts everyone?", "author_fullname": "t2_pkwxz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Portfolio projects for interview?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gdiqi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674162997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been interviewing to move from data analyst role to more data engineering / software dev.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in/have extensive experience with pandas, pyodbc, querying APIs (+have graphql exposure), automation, and 24/7 running of scripts. Did a small snowflake workshop with aws glue / pyspark.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had a lot of interviews with no pulls(many late stage as well). I feel I&amp;#39;ve improved my technical portion of interview since beginning. Last interview I did a data loading project to mariadb in python, ended up making this into a github repo and have been releasing to it. Did some new branch pull merges etc.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering what I should focus on to show I can do the work? Have been working with pandas maybe 7 years now, thought transition to finding a shop interested in that would be doable.&lt;/p&gt;\n\n&lt;p&gt;Right now I&amp;#39;m working on implementing great expectations in a scalable way for data quality. Have been using chatgpt to boost my code writing as well.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts everyone?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10gdiqi", "is_robot_indexable": true, "report_reasons": null, "author": "iupuiclubs", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gdiqi/portfolio_projects_for_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gdiqi/portfolio_projects_for_interview/", "subreddit_subscribers": 86830, "created_utc": 1674162997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a pipeline that would look something like this:\n\n[Pipeline](https://preview.redd.it/aaaitll422da1.png?width=1662&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5e20276f445483765304633b7408ca66c642aed6)\n\nI intend to implement it in Prefect 2.0 but there are two things that I can't actually figure out:\n\n1. Flow B has a dependency on both flows A and X. However, flow X is triggered by a different process and X and A don't necessarily run at the same time (\\~15 min gap), can these two flows interact with each other and wait for each other to run a third flow given that both flows are running separately (i.e. separate deployment )\n2. Flow C is triggered by flow A on the 1st of every month, in normal circumstances, it would get executed, however, in occasional events, if it's Monday and also the 1st of the month, flow C should wait for B to finish, how does this work? I can just add an if-else or a similar implementation but wouldn't it mess up the radar (the visual flow representation) since the pythonic code can mess up with\n\nI would really appreciate it if anyone can lead me to the right path or maybe if there is any similar implementation, that'd be great too, Thanks!\n\nDiagram: You can consider the yellow box as job A and the green box as job B\n\nEdit 1: Flow C has no time schedule, it gets triggered from flow A, 2 PM is marked wrong\n\n^(Disclaimer: Apologies if this is a dumb question and doesn't relate to this sub, please feel free to remove this.)", "author_fullname": "t2_2xpdiv8k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can two different perfect flows interact with each other?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 99, "top_awarded_type": null, "hide_score": false, "media_metadata": {"aaaitll422da1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/aaaitll422da1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db84c3e387775b2ee0870dc3be0ddb74c8f8d822"}, {"y": 153, "x": 216, "u": "https://preview.redd.it/aaaitll422da1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbef575bd5ea417c89fb32a863f9afb688473127"}, {"y": 227, "x": 320, "u": "https://preview.redd.it/aaaitll422da1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=208c45f98ba15df36bd3fc701f3996f07e9c3e10"}, {"y": 454, "x": 640, "u": "https://preview.redd.it/aaaitll422da1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3fbe97132e10c73dfa67765d1e9ecf762fc55f86"}, {"y": 682, "x": 960, "u": "https://preview.redd.it/aaaitll422da1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cda3bd29b3866665a015e51bf8e002a449d8d276"}, {"y": 767, "x": 1080, "u": "https://preview.redd.it/aaaitll422da1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae6937b07c1287e4385f8702d8a9f6208849d7a4"}], "s": {"y": 1181, "x": 1662, "u": "https://preview.redd.it/aaaitll422da1.png?width=1662&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5e20276f445483765304633b7408ca66c642aed6"}, "id": "aaaitll422da1"}}, "name": "t3_10gbzvu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3uV6ZuGGEkvzaJvbz613V1a30VOQKMCXJO327EJSROs.jpg", "edited": 1674159639.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674159447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a pipeline that would look something like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/aaaitll422da1.png?width=1662&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5e20276f445483765304633b7408ca66c642aed6\"&gt;Pipeline&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I intend to implement it in Prefect 2.0 but there are two things that I can&amp;#39;t actually figure out:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Flow B has a dependency on both flows A and X. However, flow X is triggered by a different process and X and A don&amp;#39;t necessarily run at the same time (~15 min gap), can these two flows interact with each other and wait for each other to run a third flow given that both flows are running separately (i.e. separate deployment )&lt;/li&gt;\n&lt;li&gt;Flow C is triggered by flow A on the 1st of every month, in normal circumstances, it would get executed, however, in occasional events, if it&amp;#39;s Monday and also the 1st of the month, flow C should wait for B to finish, how does this work? I can just add an if-else or a similar implementation but wouldn&amp;#39;t it mess up the radar (the visual flow representation) since the pythonic code can mess up with&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I would really appreciate it if anyone can lead me to the right path or maybe if there is any similar implementation, that&amp;#39;d be great too, Thanks!&lt;/p&gt;\n\n&lt;p&gt;Diagram: You can consider the yellow box as job A and the green box as job B&lt;/p&gt;\n\n&lt;p&gt;Edit 1: Flow C has no time schedule, it gets triggered from flow A, 2 PM is marked wrong&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;Disclaimer: Apologies if this is a dumb question and doesn&amp;#39;t relate to this sub, please feel free to remove this.&lt;/sup&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "SSIS developer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10gbzvu", "is_robot_indexable": true, "report_reasons": null, "author": "_whitezetsu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10gbzvu/can_two_different_perfect_flows_interact_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gbzvu/can_two_different_perfect_flows_interact_with/", "subreddit_subscribers": 86830, "created_utc": 1674159447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title\n\nI'm currently working as a Data Migration Analyst for a tech startup. I enjoy the job, as I'm developing a diverse set of skills. I work with a ton of SQL, JSON and Azure. In regards to Azure, I generally just deploy DB's to Azure and add them to elastic pools - nothing crazy.\n\nI'm currently working on my MS in DS while working full time. It's quite a bit of work, but I WFH so I do school work during the day as well. I also have an undergrad double major in Data Analytics &amp; Finance.\n\nI've been interviewing for a WFH  Jr. Cloud Engineer job. The company is in a field I'm extremely interested in and I was ecstatic to find out I got an interview. Below are the general job duties:\n\n* Identifying and implementing optimal cloud-based solutions, including team education and training\n* Participating in the software development life cycle, including planning, requirements, development, testing, and quality assurance\n* Building tools and systems to improve the time to market of product offerings in partnership with Product, Trading, and Data Science\n* Troubleshooting incidents, identifying root causes, fixing and documenting problems, and implementing preventive measures\n* Orchestrating and automating cloud operations and processes\n* Collaborating with stakeholders across the business to balance competing objectives\n* Working with third-party vendors to meet business requirements.\n\nAs I mentioned before, I don't have any experience in building a Cloud pipeline by any means. I understand the functionalities of  Cloud platforms and how they interact with SQL ide's, but nothing in terms of actual development.\n\nI had my first interview yesterday as a general screening to see my fit for the role and I think it went very well. I answered everything honestly and my resume doesn't make it seem like I'm more qualified than I actually am.\n\nToday I received multiple calls from the company asking if I saw their email and if I had time for a second interview today. The hiring manager told me their emails sometimes end up in spam/junk folders - which is exactly where it ended up. Long story short, I have another interview in 3 hours. I knew after yesterdays interview that they were very interested based on our conversations.\n\nHypothetically - lets say they give me an offer:\n\n* Firstly, does it seem like I have imposter syndrome? I worry that I'm just not qualified for this job and I won't deliver what they expect\n* Do I continue my MS if I'm getting legitimate experience as a Cloud Engineer?\n* If I do continue my MS, am I going to be able to manage the workload with this new role?\n* The expected pay is about $10,000 higher in the new role with a better job title - if my current job offers a similar pay raise and a new title + added responsibilities, should I just stay?\n\nThank you for reading and any advice would be awesome. I love the field of Data in general and the helpfulness of the community, you guys are the best.", "author_fullname": "t2_1x7s010", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Crisis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10gb99d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674158029.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674157724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working as a Data Migration Analyst for a tech startup. I enjoy the job, as I&amp;#39;m developing a diverse set of skills. I work with a ton of SQL, JSON and Azure. In regards to Azure, I generally just deploy DB&amp;#39;s to Azure and add them to elastic pools - nothing crazy.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on my MS in DS while working full time. It&amp;#39;s quite a bit of work, but I WFH so I do school work during the day as well. I also have an undergrad double major in Data Analytics &amp;amp; Finance.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been interviewing for a WFH  Jr. Cloud Engineer job. The company is in a field I&amp;#39;m extremely interested in and I was ecstatic to find out I got an interview. Below are the general job duties:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Identifying and implementing optimal cloud-based solutions, including team education and training&lt;/li&gt;\n&lt;li&gt;Participating in the software development life cycle, including planning, requirements, development, testing, and quality assurance&lt;/li&gt;\n&lt;li&gt;Building tools and systems to improve the time to market of product offerings in partnership with Product, Trading, and Data Science&lt;/li&gt;\n&lt;li&gt;Troubleshooting incidents, identifying root causes, fixing and documenting problems, and implementing preventive measures&lt;/li&gt;\n&lt;li&gt;Orchestrating and automating cloud operations and processes&lt;/li&gt;\n&lt;li&gt;Collaborating with stakeholders across the business to balance competing objectives&lt;/li&gt;\n&lt;li&gt;Working with third-party vendors to meet business requirements.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As I mentioned before, I don&amp;#39;t have any experience in building a Cloud pipeline by any means. I understand the functionalities of  Cloud platforms and how they interact with SQL ide&amp;#39;s, but nothing in terms of actual development.&lt;/p&gt;\n\n&lt;p&gt;I had my first interview yesterday as a general screening to see my fit for the role and I think it went very well. I answered everything honestly and my resume doesn&amp;#39;t make it seem like I&amp;#39;m more qualified than I actually am.&lt;/p&gt;\n\n&lt;p&gt;Today I received multiple calls from the company asking if I saw their email and if I had time for a second interview today. The hiring manager told me their emails sometimes end up in spam/junk folders - which is exactly where it ended up. Long story short, I have another interview in 3 hours. I knew after yesterdays interview that they were very interested based on our conversations.&lt;/p&gt;\n\n&lt;p&gt;Hypothetically - lets say they give me an offer:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Firstly, does it seem like I have imposter syndrome? I worry that I&amp;#39;m just not qualified for this job and I won&amp;#39;t deliver what they expect&lt;/li&gt;\n&lt;li&gt;Do I continue my MS if I&amp;#39;m getting legitimate experience as a Cloud Engineer?&lt;/li&gt;\n&lt;li&gt;If I do continue my MS, am I going to be able to manage the workload with this new role?&lt;/li&gt;\n&lt;li&gt;The expected pay is about $10,000 higher in the new role with a better job title - if my current job offers a similar pay raise and a new title + added responsibilities, should I just stay?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you for reading and any advice would be awesome. I love the field of Data in general and the helpfulness of the community, you guys are the best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10gb99d", "is_robot_indexable": true, "report_reasons": null, "author": "HercHuntsdirty", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10gb99d/career_crisis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10gb99d/career_crisis/", "subreddit_subscribers": 86830, "created_utc": 1674157724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Same as the title.", "author_fullname": "t2_57e44nxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any way to load a public airtable into a pandas dataframe without using airtable api ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g9n7x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674153948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Same as the title.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10g9n7x", "is_robot_indexable": true, "report_reasons": null, "author": "RstarPhoneix", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g9n7x/any_way_to_load_a_public_airtable_into_a_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g9n7x/any_way_to_load_a_public_airtable_into_a_pandas/", "subreddit_subscribers": 86830, "created_utc": 1674153948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apologies if this question in basic. I understand that they're the ones who handle data before it reaches any data science/engineering teams. But it's still pretty confusing. if those guys are incharge of cleaning up the data, where to Database Admins/devs come in the picture. is there any pictictional representation for how each role is related to the entire data lifecycle journey?\n\nI'm on a data literacy project and need help figuring out how they relate to one another", "author_fullname": "t2_58hz9jwy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does Data Owner/Custodian/Steward correlate with Data Analyst/Scientists/Engineers/BI etc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10g3ymx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674140758.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if this question in basic. I understand that they&amp;#39;re the ones who handle data before it reaches any data science/engineering teams. But it&amp;#39;s still pretty confusing. if those guys are incharge of cleaning up the data, where to Database Admins/devs come in the picture. is there any pictictional representation for how each role is related to the entire data lifecycle journey?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on a data literacy project and need help figuring out how they relate to one another&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10g3ymx", "is_robot_indexable": true, "report_reasons": null, "author": "kantaBane", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10g3ymx/how_does_data_ownercustodiansteward_correlate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10g3ymx/how_does_data_ownercustodiansteward_correlate/", "subreddit_subscribers": 86830, "created_utc": 1674140758.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}