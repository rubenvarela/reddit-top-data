{"kind": "Listing", "data": {"after": "t3_10a2g8s", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5z5rj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "YouTubers said they destroyed over 100 VHS tapes of an obscure 1987 movie to increase the value of their final copy. They sold it on eBay for $80,600.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_10a50y4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 1203, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 1203, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Qv6VDMJ9RIpS_qPBQ49h4TZuGnbi7vn-81cpkqngm2Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673543913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "insider.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.insider.com/youtubers-destroy-nukie-vhs-tape-collectable-ebay-sale-redlettermedia-2023-1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?auto=webp&amp;v=enabled&amp;s=eeec15dd0fc4a21f4b70116ef9efdaab0d529fca", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c58fe37eba227f4d81b2839e2086d8bf3e388eb5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22769873551441f606718b7c7af2fc76eaeba25b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4187a265dcbbb07b8e58d84e8bd6a093e2e80ee", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b27a59c96af5d6556352631ef545aaa0eb11992b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d7b3a5c0168f9b727303ef54e8c063e24841299", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be08317c9a35c9da4e5399b1c5ba6898750ecbd4", "width": 1080, "height": 540}], "variants": {}, "id": "xRZTtLBIe0INmo3ybLtCCz7LqQtFpG1yBU1egx6NDfk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a50y4", "is_robot_indexable": true, "report_reasons": null, "author": "ET2-SW", "discussion_type": null, "num_comments": 284, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a50y4/youtubers_said_they_destroyed_over_100_vhs_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.insider.com/youtubers-destroy-nukie-vhs-tape-collectable-ebay-sale-redlettermedia-2023-1", "subreddit_subscribers": 665671, "created_utc": 1673543913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Micron Unveils 9400 SSD: 30 TB Capacities With Best-In-Class Performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": false, "name": "t3_10a4nvv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 80, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 80, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/AC0ueZqYqdOMzCdm3ntxWB65qmLoxW0SUkHoeuH1Bds.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673543009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "wccftech.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://wccftech.com/micron-unveils-9400-ssd-30-tb-capacities-with-best-in-class-performance/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?auto=webp&amp;v=enabled&amp;s=c5683a71bf7f47729b0ff2c5cc693e5e54236a37", "width": 980, "height": 620}, "resolutions": [{"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f096790349c2231fec800a7e162ac38ff971e054", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c6c9f335d72366983fe496f7108b7ba824b5f74", "width": 216, "height": 136}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6be074be75dbd483261d4e50615feebc1a996a10", "width": 320, "height": 202}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14a9c87bb2670e5df83de7748e5ddaa8c8813a88", "width": 640, "height": 404}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5652b2fb0ca0e91c36cb053db1abddf73351f32", "width": 960, "height": 607}], "variants": {}, "id": "Z0sSJbkM797eMfo6AVw0nJG4sVsDUTAEeTjiWS9-Vlc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a4nvv", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10a4nvv/micron_unveils_9400_ssd_30_tb_capacities_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://wccftech.com/micron-unveils-9400-ssd-30-tb-capacities-with-best-in-class-performance/", "subreddit_subscribers": 665671, "created_utc": 1673543009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_7l2p5s3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2010-Present: At ten years old, I started collecting the music I was listening to after finding an ipod nano next to a drunk guy sleeping on my front lawn. 11 GB isn't a lot, but I'm still proud of hauling these tracks around for 12 years straight.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10abzbp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Audio Data", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/M-t7fnovqe2WstYdUHEZWYUyII_PtebRC--jIOobsr0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673560512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/5q6309u4aoba1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/5q6309u4aoba1.png?auto=webp&amp;v=enabled&amp;s=c345d10e74ba82f3d114f97725d2229e0b7b13bc", "width": 303, "height": 313}, "resolutions": [{"url": "https://preview.redd.it/5q6309u4aoba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d95b3105e04660a6b7e3340da5f57b32f074ab59", "width": 108, "height": 111}, {"url": "https://preview.redd.it/5q6309u4aoba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce58d3ded722b1eb2c8754f251a3afef62b16628", "width": 216, "height": 223}], "variants": {}, "id": "_PXHrjLaqDIiAWLyr_Xmd27OGgLdjJA4oqHgm7Z9Vf0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10abzbp", "is_robot_indexable": true, "report_reasons": null, "author": "shinnith", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10abzbp/2010present_at_ten_years_old_i_started_collecting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/5q6309u4aoba1.png", "subreddit_subscribers": 665671, "created_utc": 1673560512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got about 300,000 emails going back to 1993.  I love having the whole thing indexed and searchable for practical and nostalgic purposes, but the ratio of \"good\" to \"bad\" is ridiculous.  Even with gmail doing a lot of the heavy spam filtering, it's got a lot of garbage.\n\nDoes anyone have any pointers on how to efficiently spam filter large quantities of email spread across about 30 unix mbox files?  \n\nIf I had to, I'd be OK resorting to something somewhat iterative and convoluted (somehow feed all the mbox files to a mail server running on one of my linux boxes?) but it would be nice if there were tools out there to make this as easy as possible.", "author_fullname": "t2_tthrp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "30 years of saved email - spam filtering options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a02eo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673531289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got about 300,000 emails going back to 1993.  I love having the whole thing indexed and searchable for practical and nostalgic purposes, but the ratio of &amp;quot;good&amp;quot; to &amp;quot;bad&amp;quot; is ridiculous.  Even with gmail doing a lot of the heavy spam filtering, it&amp;#39;s got a lot of garbage.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any pointers on how to efficiently spam filter large quantities of email spread across about 30 unix mbox files?  &lt;/p&gt;\n\n&lt;p&gt;If I had to, I&amp;#39;d be OK resorting to something somewhat iterative and convoluted (somehow feed all the mbox files to a mail server running on one of my linux boxes?) but it would be nice if there were tools out there to make this as easy as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a02eo", "is_robot_indexable": true, "report_reasons": null, "author": "lectures", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a02eo/30_years_of_saved_email_spam_filtering_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a02eo/30_years_of_saved_email_spam_filtering_options/", "subreddit_subscribers": 665671, "created_utc": 1673531289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got the HDDs of almost every computer I've ever owned sitting in a box. I recently built myself a NAS and would like to save the contents of the disks to the NAS. I want to keep a full copy of the disk and not just common locations such as \"My Documents\" etc.\n\nI originally tried the na\u00efve approach of copying the entire contents of the drive into a folder. This doesn't work too well for Windows boot drives, as there are millions of tiny files all over the place. Even my 128GB SSD took over 24 hours to transfer.\n\nWhat I'm looking for is:\n\n* Ideally filesystem agnostic - while most of my drives are NTFS I have several from machines running Linux and macOS. Windows-only is better than nothing though.\n* Something that creates a single file that can be opened/mounted to view files.\n* Only the size of used space - I believe this rules out most types of disk image. I don't want to store an 8TB file for a drive that only has 100GB saved on it.\n* Free and preferably open source tools available for creation and viewing.\n\nI'm not sure if there's anything that exists that meets all my requirements but I'd be interested to know of anything that comes close. I'd also be interested to know what other people do to archive their old hard drives.\n\nThanks!", "author_fullname": "t2_rf5b8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to archive contents of old HDDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10agls0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673571698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got the HDDs of almost every computer I&amp;#39;ve ever owned sitting in a box. I recently built myself a NAS and would like to save the contents of the disks to the NAS. I want to keep a full copy of the disk and not just common locations such as &amp;quot;My Documents&amp;quot; etc.&lt;/p&gt;\n\n&lt;p&gt;I originally tried the na\u00efve approach of copying the entire contents of the drive into a folder. This doesn&amp;#39;t work too well for Windows boot drives, as there are millions of tiny files all over the place. Even my 128GB SSD took over 24 hours to transfer.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m looking for is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ideally filesystem agnostic - while most of my drives are NTFS I have several from machines running Linux and macOS. Windows-only is better than nothing though.&lt;/li&gt;\n&lt;li&gt;Something that creates a single file that can be opened/mounted to view files.&lt;/li&gt;\n&lt;li&gt;Only the size of used space - I believe this rules out most types of disk image. I don&amp;#39;t want to store an 8TB file for a drive that only has 100GB saved on it.&lt;/li&gt;\n&lt;li&gt;Free and preferably open source tools available for creation and viewing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if there&amp;#39;s anything that exists that meets all my requirements but I&amp;#39;d be interested to know of anything that comes close. I&amp;#39;d also be interested to know what other people do to archive their old hard drives.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10agls0", "is_robot_indexable": true, "report_reasons": null, "author": "Nogtail", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10agls0/best_way_to_archive_contents_of_old_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10agls0/best_way_to_archive_contents_of_old_hdds/", "subreddit_subscribers": 665671, "created_utc": 1673571698.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I\u2019ve been somewhat of a DataHoarder for a bit. AKA, I download a crap ton of files and feel apart of me die inside whenever I run out of room and have to delete stuff. I\u2019ve also had my PC growing up crash 3 times and lose *everything*, including projects that I had made and never backed up anywhere. 13 year old me was devastated.\n\nI\u2019m all ready to jump head first in, but the hard part is I really don\u2019t understand any of this technical jargon. I understand the basics, as I\u2019m currently learning how to do front end work on websites, but I\u2019ve never hosted my own server or even built my own computer before.\n\nI currently have a shitty laptop with 1TB of storage, and have only ever bought an SSD for my PS4.\n\nI do have a basic understanding of how the internet *actually* works, and can grasp the ideas behind internet protocols and how servers work.\n\nCurrently, I mainly wanna hoarde music files, (I have visited the music hoarding subreddit for tips on that) and a *l o t* of niche content / files. Mainly USTs, VSQs, SVPs, some midis, etc. If you don\u2019t know what those are, they\u2019re file types that are used by some vocal synths, like VOCALOID. Think Hatsune Miku.\n\nI do want to expand to having my own media collection in general, but that\u2019s something further down the line once I am more knowledgeable and can get a server running.\n\nShould I first build my own setup? If I stick to my laptop for the time being, is cloud storage + an external backup (like USBs or something) my best bet? Should I even bother with getting an external hard drive and just wait until I can build my own setup?\n\nAre USBs even comparable to having your data on HDDs and SSDs?\n\nOr, do any of you have any good resources for absolute newbies like me?\n\nGoogle keeps sending me to unhelpful websites, and honestly? Getting some tips from people who love data hoarding seems much more reliable then whatever google will show me.\n\nEither way, I am eternally thankful for this subreddit making me realize I should embrace my hoarding.", "author_fullname": "t2_47z0zk6b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Absolute Newbie DataHoarder with limited technical experience, any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ad410", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673563083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I\u2019ve been somewhat of a DataHoarder for a bit. AKA, I download a crap ton of files and feel apart of me die inside whenever I run out of room and have to delete stuff. I\u2019ve also had my PC growing up crash 3 times and lose &lt;em&gt;everything&lt;/em&gt;, including projects that I had made and never backed up anywhere. 13 year old me was devastated.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m all ready to jump head first in, but the hard part is I really don\u2019t understand any of this technical jargon. I understand the basics, as I\u2019m currently learning how to do front end work on websites, but I\u2019ve never hosted my own server or even built my own computer before.&lt;/p&gt;\n\n&lt;p&gt;I currently have a shitty laptop with 1TB of storage, and have only ever bought an SSD for my PS4.&lt;/p&gt;\n\n&lt;p&gt;I do have a basic understanding of how the internet &lt;em&gt;actually&lt;/em&gt; works, and can grasp the ideas behind internet protocols and how servers work.&lt;/p&gt;\n\n&lt;p&gt;Currently, I mainly wanna hoarde music files, (I have visited the music hoarding subreddit for tips on that) and a &lt;em&gt;l o t&lt;/em&gt; of niche content / files. Mainly USTs, VSQs, SVPs, some midis, etc. If you don\u2019t know what those are, they\u2019re file types that are used by some vocal synths, like VOCALOID. Think Hatsune Miku.&lt;/p&gt;\n\n&lt;p&gt;I do want to expand to having my own media collection in general, but that\u2019s something further down the line once I am more knowledgeable and can get a server running.&lt;/p&gt;\n\n&lt;p&gt;Should I first build my own setup? If I stick to my laptop for the time being, is cloud storage + an external backup (like USBs or something) my best bet? Should I even bother with getting an external hard drive and just wait until I can build my own setup?&lt;/p&gt;\n\n&lt;p&gt;Are USBs even comparable to having your data on HDDs and SSDs?&lt;/p&gt;\n\n&lt;p&gt;Or, do any of you have any good resources for absolute newbies like me?&lt;/p&gt;\n\n&lt;p&gt;Google keeps sending me to unhelpful websites, and honestly? Getting some tips from people who love data hoarding seems much more reliable then whatever google will show me.&lt;/p&gt;\n\n&lt;p&gt;Either way, I am eternally thankful for this subreddit making me realize I should embrace my hoarding.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ad410", "is_robot_indexable": true, "report_reasons": null, "author": "Ellestyx", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ad410/absolute_newbie_datahoarder_with_limited/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ad410/absolute_newbie_datahoarder_with_limited/", "subreddit_subscribers": 665671, "created_utc": 1673563083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I got bunch of **14TB** EasyStores on sale to shuck later.\n\nI haven't opened them all, but the ones that I did - I noticed there are two varieties (with slightly different packaging too) of drives inside the shell. They are both marked as WD140EDGZ and both *made in Thailand*, but the r/n and company contact address are different:\n\n=========================================\n\n\\- **us7sar180**\n\n(contacts in France, made in September 2021)\n\nShows up as  **WDC WD140EDGZ-11B2DA2** in CrystalDisk.\n\nThey are significantly ***noisier***.\n\nI read that they are rebranded *Ultrastar HC550* ***18TB*** binned down to **14TB**.  \n\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- **us7sap140**\n\n(contacts in Ireland, made in August 2022)\n\nShows up as  **WDC WD140EDGZ-11B1PA0** in CrystalDisk.\n\nThey are significantly ***quieter***.\n\nI read that they are rebranded *Ultrastar HC530*\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nSpeed is the same on all - 230 MBs. I expected \"18TB\" ones to be faster.\n\n&amp;#x200B;\n\n===========================================\n\n&amp;#x200B;\n\nI would appreciate if anyone has answers to these QUESTIONS:\n\n\\- Which one would you pick?\n\nI.e., is the extra noise in **us7sar180** warranted by any added benefits? (workload rating, performance)\n\nOr should I go with a far more quiet **us7sap140** and return their noisy siblings that I haven't unboxed yet back to BestBuy?\n\n&amp;#x200B;\n\n\\- Would the \"18TB\" **us7sar180** drive be better because  it would have more available space to reallocate bad blocks (since it's originally an 18TB drive)? Or is the space above 14TB not accessible even for block reallocation? \n\n\\- Is it possible unlock the disabled extra space in the **us7sar180** firmware to make them 18TB again?\n\n\\- People mention that these are 7200 slowed down to 5400 in firmware, but they all report 230-240 MBs in CrystalDisk Mark (4 GiB sample file, 3 runs). Can 5400 yield that kind of speed? I though 540 should be around 100-150.\n\n&amp;#x200B;\n\nI plan to use them in a ZFS/TrueNAS server if that matters.\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_2r0xql9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TWO types of WD EasyStore drives. Which one is better?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10akvbd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673583406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I got bunch of &lt;strong&gt;14TB&lt;/strong&gt; EasyStores on sale to shuck later.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t opened them all, but the ones that I did - I noticed there are two varieties (with slightly different packaging too) of drives inside the shell. They are both marked as WD140EDGZ and both &lt;em&gt;made in Thailand&lt;/em&gt;, but the r/n and company contact address are different:&lt;/p&gt;\n\n&lt;h1&gt;&lt;/h1&gt;\n\n&lt;p&gt;- &lt;strong&gt;us7sar180&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;(contacts in France, made in September 2021)&lt;/p&gt;\n\n&lt;p&gt;Shows up as  &lt;strong&gt;WDC WD140EDGZ-11B2DA2&lt;/strong&gt; in CrystalDisk.&lt;/p&gt;\n\n&lt;p&gt;They are significantly &lt;strong&gt;&lt;em&gt;noisier&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;I read that they are rebranded &lt;em&gt;Ultrastar HC550&lt;/em&gt; &lt;strong&gt;&lt;em&gt;18TB&lt;/em&gt;&lt;/strong&gt; binned down to &lt;strong&gt;14TB&lt;/strong&gt;.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;us7sap140&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;(contacts in Ireland, made in August 2022)&lt;/p&gt;\n\n&lt;p&gt;Shows up as  &lt;strong&gt;WDC WD140EDGZ-11B1PA0&lt;/strong&gt; in CrystalDisk.&lt;/p&gt;\n\n&lt;p&gt;They are significantly &lt;strong&gt;&lt;em&gt;quieter&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;I read that they are rebranded &lt;em&gt;Ultrastar HC530&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Speed is the same on all - 230 MBs. I expected &amp;quot;18TB&amp;quot; ones to be faster.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;&lt;/h1&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I would appreciate if anyone has answers to these QUESTIONS:&lt;/p&gt;\n\n&lt;p&gt;- Which one would you pick?&lt;/p&gt;\n\n&lt;p&gt;I.e., is the extra noise in &lt;strong&gt;us7sar180&lt;/strong&gt; warranted by any added benefits? (workload rating, performance)&lt;/p&gt;\n\n&lt;p&gt;Or should I go with a far more quiet &lt;strong&gt;us7sap140&lt;/strong&gt; and return their noisy siblings that I haven&amp;#39;t unboxed yet back to BestBuy?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Would the &amp;quot;18TB&amp;quot; &lt;strong&gt;us7sar180&lt;/strong&gt; drive be better because  it would have more available space to reallocate bad blocks (since it&amp;#39;s originally an 18TB drive)? Or is the space above 14TB not accessible even for block reallocation? &lt;/p&gt;\n\n&lt;p&gt;- Is it possible unlock the disabled extra space in the &lt;strong&gt;us7sar180&lt;/strong&gt; firmware to make them 18TB again?&lt;/p&gt;\n\n&lt;p&gt;- People mention that these are 7200 slowed down to 5400 in firmware, but they all report 230-240 MBs in CrystalDisk Mark (4 GiB sample file, 3 runs). Can 5400 yield that kind of speed? I though 540 should be around 100-150.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I plan to use them in a ZFS/TrueNAS server if that matters.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10akvbd", "is_robot_indexable": true, "report_reasons": null, "author": "Infinite100p", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10akvbd/two_types_of_wd_easystore_drives_which_one_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10akvbd/two_types_of_wd_easystore_drives_which_one_is/", "subreddit_subscribers": 665671, "created_utc": 1673583406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI download a lot of content for archival purposes. The files I download are typically packaged as ZIP or RAR archives which then have to be extracted.\n\nI am wondering if there is any way I can automatically add read-only attributes to all files as they are being extracted (eg. as an option inside WinRAR or 7-Zip) instead of having to manually select the folder afterwards, then right-clicking and selecting the read-only checkbox (which sometimes I forget to do)\n\nThanks in advance.", "author_fullname": "t2_fcjv96r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Windows] Is there any way to automatically add read-only attributes to all files extracted by WinRAR or 7-Zip?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a3vd9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673603262.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673541140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I download a lot of content for archival purposes. The files I download are typically packaged as ZIP or RAR archives which then have to be extracted.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if there is any way I can automatically add read-only attributes to all files as they are being extracted (eg. as an option inside WinRAR or 7-Zip) instead of having to manually select the folder afterwards, then right-clicking and selecting the read-only checkbox (which sometimes I forget to do)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a3vd9", "is_robot_indexable": true, "report_reasons": null, "author": "P650SE", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a3vd9/windows_is_there_any_way_to_automatically_add/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a3vd9/windows_is_there_any_way_to_automatically_add/", "subreddit_subscribers": 665671, "created_utc": 1673541140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for some help in finding ways to better saturate my 10Gb network. I'm currently maxing out at ~5Gbps using a 10GB zip file but I know the storage is capable of far more (internal transfers and tar'd mbuffered external transfers are far faster) and the network is fine (iperf registers 9.8Gbps both ways). Tests are done between the pools on the server and a storage NVMe on my entirely overspec'd workstation. Any help is appreciated.\n\nServer Hardware:\n\n- Box: Dell T620\n- RAM: 220GB DDR3 ECC\n- CPU: 2x E5-2643v2 - 12C/24T@3.5GHz\n- HBA: LSI 9305-16i in IT mode\n- NIC: MCX354A-FCCT in ethernet mode, using a QSFP to SFP+ adapter\n- OS: Proxmox 7.3-4\n- File server: Samba directly on Proxmox, shares through ZFS pool settings\n\nServer Storage: \n\n- ssdpool: 8x 8TB Samsung 870QVO drives in ZFS RAID0, typically tops out at 3-4Gbps.\n- hddpool: 12x 16TB Seagate Exos HDDs in ZFS RAID0 + 2x 960GB Intel Optane 905P as a mirrored special vdev, typically tops out at 3-4Gbps.\n- cache: 1x 1.92TB Samsung DCT983 NVMe on ZFS, tops out at 5Gbps.\n\n---\n\nBefore anyone asks: I'm not concerned about the large RAID0 arrays, I have backups and am looking for lots of speed and scratch space. The 870QVO pool is read-only to the services that access it. Writes are infrequent and manual (download to cache, categorize and clean, transfer to pool). The shares are based on Proxmox because every attempt at setting up a file share container results in a ~2Gbps bottleneck. Jumbo frames are on for every device in the network. Network connections are all SFP+ DACs.", "author_fullname": "t2_u4u6f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me squeeze some more speed out of my file shares", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10aezg0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673567591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for some help in finding ways to better saturate my 10Gb network. I&amp;#39;m currently maxing out at ~5Gbps using a 10GB zip file but I know the storage is capable of far more (internal transfers and tar&amp;#39;d mbuffered external transfers are far faster) and the network is fine (iperf registers 9.8Gbps both ways). Tests are done between the pools on the server and a storage NVMe on my entirely overspec&amp;#39;d workstation. Any help is appreciated.&lt;/p&gt;\n\n&lt;p&gt;Server Hardware:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Box: Dell T620&lt;/li&gt;\n&lt;li&gt;RAM: 220GB DDR3 ECC&lt;/li&gt;\n&lt;li&gt;CPU: 2x E5-2643v2 - 12C/&lt;a href=\"mailto:24T@3.5GHz\"&gt;24T@3.5GHz&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;HBA: LSI 9305-16i in IT mode&lt;/li&gt;\n&lt;li&gt;NIC: MCX354A-FCCT in ethernet mode, using a QSFP to SFP+ adapter&lt;/li&gt;\n&lt;li&gt;OS: Proxmox 7.3-4&lt;/li&gt;\n&lt;li&gt;File server: Samba directly on Proxmox, shares through ZFS pool settings&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Server Storage: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ssdpool: 8x 8TB Samsung 870QVO drives in ZFS RAID0, typically tops out at 3-4Gbps.&lt;/li&gt;\n&lt;li&gt;hddpool: 12x 16TB Seagate Exos HDDs in ZFS RAID0 + 2x 960GB Intel Optane 905P as a mirrored special vdev, typically tops out at 3-4Gbps.&lt;/li&gt;\n&lt;li&gt;cache: 1x 1.92TB Samsung DCT983 NVMe on ZFS, tops out at 5Gbps.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Before anyone asks: I&amp;#39;m not concerned about the large RAID0 arrays, I have backups and am looking for lots of speed and scratch space. The 870QVO pool is read-only to the services that access it. Writes are infrequent and manual (download to cache, categorize and clean, transfer to pool). The shares are based on Proxmox because every attempt at setting up a file share container results in a ~2Gbps bottleneck. Jumbo frames are on for every device in the network. Network connections are all SFP+ DACs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10aezg0", "is_robot_indexable": true, "report_reasons": null, "author": "certifiedintelligent", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10aezg0/help_me_squeeze_some_more_speed_out_of_my_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10aezg0/help_me_squeeze_some_more_speed_out_of_my_file/", "subreddit_subscribers": 665671, "created_utc": 1673567591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't know if it's on point with the subreddit but I don't where else I could find opinions.\nI have some games and anime series backed up in HDDs but I also wanted a second backup on a storage of a different type. Bluray is too pricey for me so I went for DVD-r and DVD-dl. \n\nThe other option would have been a chain of USB but I wanted to keep a better aesthetic /s\n\nAnyway, I am now undecided on which package to use to keep the DVDs. Considering also their shelf position. \nMy library is not directly exposed to sunlight but \"diagonally\" there is some light. Normally summer is not really harsh. Maybe there is a bit of humidity problem in some months.\n\n[Plastic case] But I recently read/heard about problems coming from black pvc cases, since they would stick to the disk and may have come chemical reaction that can ruin the disk. Is it true that hot/humidity+enclosed pvc case really makes a mess?\n\n[Transparent jewel case] So I looked for the smaller cases, the ones with the flimsy hinge that always breaks. Majority of them seems to be made of PP and other non toxic materials.\n\nJewel seems the best choice, but I want opinions on the truth of the pvc/pp/other materials debate. I don't think there are many more type of rigid cases. I don't need the folder/booklet one with 4 discs, single pockets of paper/plastic are too \"nude\".\n\nAlso, for proper temperature, is it a good idea to put the cases in a second line, behind books/other stuff?\n\n\nThen there is the external HDD placement. I know that I should plug them in sometimes to check if files are corrupted/spin the disk inside.\nSo, is it a good idea to put my HDD on a shelf, plugged to a multi-port USB that routinely I connect to an external battery? So I can power them up with minimal hassle. They should have no problem with data \"safely remove hardware\" when unplugged since they only transfer energy with the battery, I suppose. \nThe hdd shelf is also easier to manage rather than dump them in a box or pile over a desk, and it's better covered from sunlight", "author_fullname": "t2_3n8go9g5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DVD and HDD:how and where to shelf/store/encase?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10adzyc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673565201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know if it&amp;#39;s on point with the subreddit but I don&amp;#39;t where else I could find opinions.\nI have some games and anime series backed up in HDDs but I also wanted a second backup on a storage of a different type. Bluray is too pricey for me so I went for DVD-r and DVD-dl. &lt;/p&gt;\n\n&lt;p&gt;The other option would have been a chain of USB but I wanted to keep a better aesthetic /s&lt;/p&gt;\n\n&lt;p&gt;Anyway, I am now undecided on which package to use to keep the DVDs. Considering also their shelf position. \nMy library is not directly exposed to sunlight but &amp;quot;diagonally&amp;quot; there is some light. Normally summer is not really harsh. Maybe there is a bit of humidity problem in some months.&lt;/p&gt;\n\n&lt;p&gt;[Plastic case] But I recently read/heard about problems coming from black pvc cases, since they would stick to the disk and may have come chemical reaction that can ruin the disk. Is it true that hot/humidity+enclosed pvc case really makes a mess?&lt;/p&gt;\n\n&lt;p&gt;[Transparent jewel case] So I looked for the smaller cases, the ones with the flimsy hinge that always breaks. Majority of them seems to be made of PP and other non toxic materials.&lt;/p&gt;\n\n&lt;p&gt;Jewel seems the best choice, but I want opinions on the truth of the pvc/pp/other materials debate. I don&amp;#39;t think there are many more type of rigid cases. I don&amp;#39;t need the folder/booklet one with 4 discs, single pockets of paper/plastic are too &amp;quot;nude&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Also, for proper temperature, is it a good idea to put the cases in a second line, behind books/other stuff?&lt;/p&gt;\n\n&lt;p&gt;Then there is the external HDD placement. I know that I should plug them in sometimes to check if files are corrupted/spin the disk inside.\nSo, is it a good idea to put my HDD on a shelf, plugged to a multi-port USB that routinely I connect to an external battery? So I can power them up with minimal hassle. They should have no problem with data &amp;quot;safely remove hardware&amp;quot; when unplugged since they only transfer energy with the battery, I suppose. \nThe hdd shelf is also easier to manage rather than dump them in a box or pile over a desk, and it&amp;#39;s better covered from sunlight&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10adzyc", "is_robot_indexable": true, "report_reasons": null, "author": "CourierLordProcione2", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10adzyc/dvd_and_hddhow_and_where_to_shelfstoreencase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10adzyc/dvd_and_hddhow_and_where_to_shelfstoreencase/", "subreddit_subscribers": 665671, "created_utc": 1673565201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys,\n\nI am not that much of a hoarder rather a storage/compute/backup specialist.\n\nLast couple of months I found that torrent trackers don't have movies in my native language. So for me the tube is best source of movies and even 4k nature videos. Can you share with me what proof to be best way for you to download YT videos/playlist both 720p and 4k?\n\nPS\n\nHere is a snip from session that will time out:\n\nuser@host:\\~$ ls -lh\n\ntotal 235M\n\n\\-rw-rw-r-- 1 user 235M Jan  4 21:02 '\u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm.part'\n\nuser@host:\\~$ rm '\u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm.part'\n\nuser@host:\\~$ youtube-dl -f 315 [https://www.youtube.com/watch?v=LCDDzLK-sys](https://www.youtube.com/watch?v=LCDDzLK-sys)\n\n\\[youtube\\] LCDDzLK-sys: Downloading webpage\n\n\\[download\\] Destination: \u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm\n\n\\[download\\]   0.9% of 2.76GiB at 70.46KiB/s ETA 11:17:32", "author_fullname": "t2_db0rv0yp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you like to dump 4k Youtube videos ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10aa20l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673556026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I am not that much of a hoarder rather a storage/compute/backup specialist.&lt;/p&gt;\n\n&lt;p&gt;Last couple of months I found that torrent trackers don&amp;#39;t have movies in my native language. So for me the tube is best source of movies and even 4k nature videos. Can you share with me what proof to be best way for you to download YT videos/playlist both 720p and 4k?&lt;/p&gt;\n\n&lt;p&gt;PS&lt;/p&gt;\n\n&lt;p&gt;Here is a snip from session that will time out:&lt;/p&gt;\n\n&lt;p&gt;user@host:~$ ls -lh&lt;/p&gt;\n\n&lt;p&gt;total 235M&lt;/p&gt;\n\n&lt;p&gt;-rw-rw-r-- 1 user 235M Jan  4 21:02 &amp;#39;\u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm.part&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;user@host:~$ rm &amp;#39;\u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm.part&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;user@host:~$ youtube-dl -f 315 &lt;a href=\"https://www.youtube.com/watch?v=LCDDzLK-sys\"&gt;https://www.youtube.com/watch?v=LCDDzLK-sys&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[youtube] LCDDzLK-sys: Downloading webpage&lt;/p&gt;\n\n&lt;p&gt;[download] Destination: \u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm&lt;/p&gt;\n\n&lt;p&gt;[download]   0.9% of 2.76GiB at 70.46KiB/s ETA 11:17:32&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B_95ltT36M0Z_42W0ff5ZhRSP8_V3dIr00yEl4_xSyg.jpg?auto=webp&amp;v=enabled&amp;s=0750adbce2c293bab5e98c3db1fc417259085eff", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/B_95ltT36M0Z_42W0ff5ZhRSP8_V3dIr00yEl4_xSyg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a42c015f4b6d258225a8ce691de9bbef630ed1ca", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/B_95ltT36M0Z_42W0ff5ZhRSP8_V3dIr00yEl4_xSyg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9cd0fe9388dd7504c6be08fa00f019a728d63f3c", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/B_95ltT36M0Z_42W0ff5ZhRSP8_V3dIr00yEl4_xSyg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a55a3af333fb19a2e81021b7da43de8dd7ea89b", "width": 320, "height": 240}], "variants": {}, "id": "UW-6yU-dqtHKKqWFY9jvmOw8xW-z6xI7jzKq6jBnulw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "9TB TerraMaster", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10aa20l", "is_robot_indexable": true, "report_reasons": null, "author": "Accomplished-Eye1673", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10aa20l/how_do_you_like_to_dump_4k_youtube_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10aa20l/how_do_you_like_to_dump_4k_youtube_videos/", "subreddit_subscribers": 665671, "created_utc": 1673556026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anybody know a service that offers large amounts of storage for very short periods of time and how much would this cost? cheers", "author_fullname": "t2_h36vzasl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to buy 2tb cloud hosted storage for a very short period of time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a7x3c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673550889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anybody know a service that offers large amounts of storage for very short periods of time and how much would this cost? cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a7x3c", "is_robot_indexable": true, "report_reasons": null, "author": "JustYourJoe_", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a7x3c/looking_to_buy_2tb_cloud_hosted_storage_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a7x3c/looking_to_buy_2tb_cloud_hosted_storage_for_a/", "subreddit_subscribers": 665671, "created_utc": 1673550889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "title says it all , been trying many github scripts but nothing works", "author_fullname": "t2_aj30rvtl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "any way to download photo album from weibo ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109x642", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673521913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title says it all , been trying many github scripts but nothing works&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109x642", "is_robot_indexable": true, "report_reasons": null, "author": "Digital-Nuke", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109x642/any_way_to_download_photo_album_from_weibo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109x642/any_way_to_download_photo_album_from_weibo/", "subreddit_subscribers": 665671, "created_utc": 1673521913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have approximately 400-700 videos that have an annoying, redundant, 4 second intro from the site they originated from. I would like to cut this pre-roll in a single batch operation.\n\nI have ffmpeg and Linux; I'm not sure of the command to run to accomplish this.\n\nThanks for your help :)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&gt;!Yes, it's porn!&lt;", "author_fullname": "t2_v9x1ekbb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch cut the first 4 seconds from a group of video's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10aney2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673591366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have approximately 400-700 videos that have an annoying, redundant, 4 second intro from the site they originated from. I would like to cut this pre-roll in a single batch operation.&lt;/p&gt;\n\n&lt;p&gt;I have ffmpeg and Linux; I&amp;#39;m not sure of the command to run to accomplish this.&lt;/p&gt;\n\n&lt;p&gt;Thanks for your help :)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;span class=\"md-spoiler-text\"&gt;Yes, it&amp;#39;s porn&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10aney2", "is_robot_indexable": true, "report_reasons": null, "author": "asyoucommandstan", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10aney2/batch_cut_the_first_4_seconds_from_a_group_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10aney2/batch_cut_the_first_4_seconds_from_a_group_of/", "subreddit_subscribers": 665671, "created_utc": 1673591366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been scrolling through Reddit recently and I just remembered a rarely known subreddit that I was often scrolling into. It's now about more than a year since I've last visited that subreddit that when I searched it, it was banned due to being unmoderated. I searched it through unddit and I can see the sub's comments, but not the posts and when I clicked it, I get taken back to Reddit and a message pops up saying 'It looks like you aren't allowed to do that.'. I've keep reloading the page, logging out and back in on Reddit, and searched the internet on a way to view those posts, but still nothing. Any way on how to get around this message?", "author_fullname": "t2_9pox10mc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "'It looks like you aren't allowed to do that.' Message", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ajnmc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673579898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been scrolling through Reddit recently and I just remembered a rarely known subreddit that I was often scrolling into. It&amp;#39;s now about more than a year since I&amp;#39;ve last visited that subreddit that when I searched it, it was banned due to being unmoderated. I searched it through unddit and I can see the sub&amp;#39;s comments, but not the posts and when I clicked it, I get taken back to Reddit and a message pops up saying &amp;#39;It looks like you aren&amp;#39;t allowed to do that.&amp;#39;. I&amp;#39;ve keep reloading the page, logging out and back in on Reddit, and searched the internet on a way to view those posts, but still nothing. Any way on how to get around this message?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ajnmc", "is_robot_indexable": true, "report_reasons": null, "author": "CupRevolutionary5599", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ajnmc/it_looks_like_you_arent_allowed_to_do_that_message/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ajnmc/it_looks_like_you_arent_allowed_to_do_that_message/", "subreddit_subscribers": 665671, "created_utc": 1673579898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Wonder if YouTube-DL would work, however this site isn\u2019t mainstream \n\nAny ideas? \n\nThanks", "author_fullname": "t2_jp6vscfv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download from a \u201cstreaming only\u201d site", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a8exh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673552079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wonder if YouTube-DL would work, however this site isn\u2019t mainstream &lt;/p&gt;\n\n&lt;p&gt;Any ideas? &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a8exh", "is_robot_indexable": true, "report_reasons": null, "author": "StoryThrowAway916", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a8exh/download_from_a_streaming_only_site/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a8exh/download_from_a_streaming_only_site/", "subreddit_subscribers": 665671, "created_utc": 1673552079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone found a software they like for managing a library of *arbitrary* files? I have Plex for Video, Kavita for ebooks, Paperless-ng for personal documents, etc.\n\nIs there anything like these tools but for any and all file types? Basically I'd want a tool that could read metadata, potentially file contexts for text, and also allow any file to be tagged \u00e0 la Gmail labels, making searching easy. If it's something common like HTML/PDF/mp4 it could just view the file in browser, but otherwise provide a file location reference or download link.\n\nAnyone aware of anything in this vein existing?", "author_fullname": "t2_32pyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Library Management Software for Arbitrary Files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a5ttt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673545888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone found a software they like for managing a library of &lt;em&gt;arbitrary&lt;/em&gt; files? I have Plex for Video, Kavita for ebooks, Paperless-ng for personal documents, etc.&lt;/p&gt;\n\n&lt;p&gt;Is there anything like these tools but for any and all file types? Basically I&amp;#39;d want a tool that could read metadata, potentially file contexts for text, and also allow any file to be tagged \u00e0 la Gmail labels, making searching easy. If it&amp;#39;s something common like HTML/PDF/mp4 it could just view the file in browser, but otherwise provide a file location reference or download link.&lt;/p&gt;\n\n&lt;p&gt;Anyone aware of anything in this vein existing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "12TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a5ttt", "is_robot_indexable": true, "report_reasons": null, "author": "eldridgea", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10a5ttt/library_management_software_for_arbitrary_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a5ttt/library_management_software_for_arbitrary_files/", "subreddit_subscribers": 665671, "created_utc": 1673545888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't actually have the hardware yet, I'm doing some research before purchase (always advisable lol). I'm interested in buying a used 3.2TB Micron 7300 MAX, but I want to bump that up to 3.84TB for more capacity since I won't be using anywhere near the listed 3DWPD spec. Hell, assuming that makes it equal to the \"Pro\" version of the drive, 1DWPD is still waaaay overkill.\n\nIt seems like Micron calls it \"Flex Capacity\" and it seems to be a part of \"Storage Executive Software\". Is this where I change the capacity? Can I even increase the capacity? I've heard of people reducing capacity to increase endurance, but I've never heard of someone doing the reverse.", "author_fullname": "t2_upalof7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I change the capacity / overprovisioning on a Micron 7300 MAX?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109z0cp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673528186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t actually have the hardware yet, I&amp;#39;m doing some research before purchase (always advisable lol). I&amp;#39;m interested in buying a used 3.2TB Micron 7300 MAX, but I want to bump that up to 3.84TB for more capacity since I won&amp;#39;t be using anywhere near the listed 3DWPD spec. Hell, assuming that makes it equal to the &amp;quot;Pro&amp;quot; version of the drive, 1DWPD is still waaaay overkill.&lt;/p&gt;\n\n&lt;p&gt;It seems like Micron calls it &amp;quot;Flex Capacity&amp;quot; and it seems to be a part of &amp;quot;Storage Executive Software&amp;quot;. Is this where I change the capacity? Can I even increase the capacity? I&amp;#39;ve heard of people reducing capacity to increase endurance, but I&amp;#39;ve never heard of someone doing the reverse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109z0cp", "is_robot_indexable": true, "report_reasons": null, "author": "Party_9001", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109z0cp/how_do_i_change_the_capacity_overprovisioning_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109z0cp/how_do_i_change_the_capacity_overprovisioning_on/", "subreddit_subscribers": 665671, "created_utc": 1673528186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There are some search engines out there that provide direct links to images (duck duck go, metager, brave that I know of) though they are still somewhat hidden (you have to click on the image to then see the direct link, and you have to scroll down to see more results) by java i guess it is? Anyway, its quite convenient to be able to specify what you want to see and then hover up the results :)\n\nAnyway know of a way to get past this?", "author_fullname": "t2_f5t3js0e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "scraping image search results?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_109yn4c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673527007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are some search engines out there that provide direct links to images (duck duck go, metager, brave that I know of) though they are still somewhat hidden (you have to click on the image to then see the direct link, and you have to scroll down to see more results) by java i guess it is? Anyway, its quite convenient to be able to specify what you want to see and then hover up the results :)&lt;/p&gt;\n\n&lt;p&gt;Anyway know of a way to get past this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "109yn4c", "is_robot_indexable": true, "report_reasons": null, "author": "AlfieMcLuvin", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/109yn4c/scraping_image_search_results/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/109yn4c/scraping_image_search_results/", "subreddit_subscribers": 665671, "created_utc": 1673527007.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI am planning to upload big chunks of free2use CC 3.0 Flight Simulator Photoscenery and I wanted to share it (non profit) too. \n\nI am having a hard time finding anything more than 20GB which isn't MEGA and doesn't look like a scam.\n\nIf I need to pay, what would be the best options for paid sites? I would use a few TB if the prices are reasonable as it is just a non profit fun project for the community.\n\nSorry if this is the wrong sub, this one came to my mind ;)", "author_fullname": "t2_r430vw1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to get more than 20GB reliable cloud storage as a free user?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10aqauk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673602122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am planning to upload big chunks of free2use CC 3.0 Flight Simulator Photoscenery and I wanted to share it (non profit) too. &lt;/p&gt;\n\n&lt;p&gt;I am having a hard time finding anything more than 20GB which isn&amp;#39;t MEGA and doesn&amp;#39;t look like a scam.&lt;/p&gt;\n\n&lt;p&gt;If I need to pay, what would be the best options for paid sites? I would use a few TB if the prices are reasonable as it is just a non profit fun project for the community.&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is the wrong sub, this one came to my mind ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10aqauk", "is_robot_indexable": true, "report_reasons": null, "author": "Zentralschaden", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10aqauk/where_to_get_more_than_20gb_reliable_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10aqauk/where_to_get_more_than_20gb_reliable_cloud/", "subreddit_subscribers": 665671, "created_utc": 1673602122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm just thinking here. In a RAID 5 array of four drives, the data is split across each drive, and the data will be corrupted or lost if two separate drives fail at the same time, right? So in theory, all those spare drives I've got sitting around that still work but got taken out for being too small should each individually just have the equivalent of corrupted data if I just accessed one on its own, right? Am I thinking right or is there something I'm missing?\n\nI'd test this myself but at the moment I don't have any spare drive slots or enclosures to put the drive in", "author_fullname": "t2_qf6hcoj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the data on one single drive meaningless without the rest of the RAID array?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10aqank", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673602098.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just thinking here. In a RAID 5 array of four drives, the data is split across each drive, and the data will be corrupted or lost if two separate drives fail at the same time, right? So in theory, all those spare drives I&amp;#39;ve got sitting around that still work but got taken out for being too small should each individually just have the equivalent of corrupted data if I just accessed one on its own, right? Am I thinking right or is there something I&amp;#39;m missing?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d test this myself but at the moment I don&amp;#39;t have any spare drive slots or enclosures to put the drive in&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "18TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10aqank", "is_robot_indexable": true, "report_reasons": null, "author": "ian9921", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10aqank/is_the_data_on_one_single_drive_meaningless/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10aqank/is_the_data_on_one_single_drive_meaningless/", "subreddit_subscribers": 665671, "created_utc": 1673602098.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Long time data hoarder (though I usually refer to myself as digital hoarder), but new to the group and not a NAS/RAID/Server expert, though with decent computer knowledge.\n\nI currently have 2 EOL Lenovo NAS (PX4-400D with 4x4TB and IX4-300D with 4x3TB) but would like to \"consolidate\" with truly large drives. According to this list below, the biggest drive they list is 6TB\n\n[https://download.lenovo.com/lenovoemc/eu/en/app/answers/detail/a\\_id/33754.html](https://download.lenovo.com/lenovoemc/eu/en/app/answers/detail/a_id/33754.html)\n\nI know the NAS systems are old and EOL, but they are just local storage of downloads and videos without installed apps or services that reach outside. Usually 1 user accessing the drive, sometimes 2. The data is not \"super precious\" and while a loss would hurt, it wouldn't ruin me. I am hoping to just make one the \"mother of all data\" throwing 4x12TB or something like that at the NAS in RAID-5.\n\nQ1: Is 6TB really the limit, or is this just what was available at the time the list was release (2015). Does anyone run either of them with bigger drives than that?\n\nQ2: Can I buy new drives in stages? For example get 2x12TB running RAID-1 (I know it will only have 12TB usable), then add another 12TB and convert the RAID-1 to RAID-5 to have 24TB usable. Then add another 12TB after that to get to 36TB? This is mainly because of my available funds, because they currently run at about $200 a piece. I understand that generally you can convert RAID-1 to RAID-5 but wondering about these older systems.\n\nQ3: They currently use dedicated NAS drives (WD Red), could I use Enterprise or Surveillance disks like EXOS if they are cheaper or is this risky? I understand I can generally run whatever I want but even after googling, I am not sure if the difference between the drives should worry me for this setup\n\nQ4: Seagate Ironwolf or WD Red?", "author_fullname": "t2_5ai3u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lenovo EMC questions about max drive size", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a9cyw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673554366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long time data hoarder (though I usually refer to myself as digital hoarder), but new to the group and not a NAS/RAID/Server expert, though with decent computer knowledge.&lt;/p&gt;\n\n&lt;p&gt;I currently have 2 EOL Lenovo NAS (PX4-400D with 4x4TB and IX4-300D with 4x3TB) but would like to &amp;quot;consolidate&amp;quot; with truly large drives. According to this list below, the biggest drive they list is 6TB&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://download.lenovo.com/lenovoemc/eu/en/app/answers/detail/a_id/33754.html\"&gt;https://download.lenovo.com/lenovoemc/eu/en/app/answers/detail/a_id/33754.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I know the NAS systems are old and EOL, but they are just local storage of downloads and videos without installed apps or services that reach outside. Usually 1 user accessing the drive, sometimes 2. The data is not &amp;quot;super precious&amp;quot; and while a loss would hurt, it wouldn&amp;#39;t ruin me. I am hoping to just make one the &amp;quot;mother of all data&amp;quot; throwing 4x12TB or something like that at the NAS in RAID-5.&lt;/p&gt;\n\n&lt;p&gt;Q1: Is 6TB really the limit, or is this just what was available at the time the list was release (2015). Does anyone run either of them with bigger drives than that?&lt;/p&gt;\n\n&lt;p&gt;Q2: Can I buy new drives in stages? For example get 2x12TB running RAID-1 (I know it will only have 12TB usable), then add another 12TB and convert the RAID-1 to RAID-5 to have 24TB usable. Then add another 12TB after that to get to 36TB? This is mainly because of my available funds, because they currently run at about $200 a piece. I understand that generally you can convert RAID-1 to RAID-5 but wondering about these older systems.&lt;/p&gt;\n\n&lt;p&gt;Q3: They currently use dedicated NAS drives (WD Red), could I use Enterprise or Surveillance disks like EXOS if they are cheaper or is this risky? I understand I can generally run whatever I want but even after googling, I am not sure if the difference between the drives should worry me for this setup&lt;/p&gt;\n\n&lt;p&gt;Q4: Seagate Ironwolf or WD Red?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a9cyw", "is_robot_indexable": true, "report_reasons": null, "author": "saldridge", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a9cyw/lenovo_emc_questions_about_max_drive_size/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a9cyw/lenovo_emc_questions_about_max_drive_size/", "subreddit_subscribers": 665671, "created_utc": 1673554366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, posted this in the QNAP sub but thought it would be worth trying here as well\n\nHave a 4 bay QNAP 453BT3 connected to my Mac via thunderbolt 3. Have just upgraded it to 4 x 16tb ironwolfs and want to set it up a little differently to my previous config as I wasnt happy with the speeds I was getting previously with RAID 5, thick volume (310 write, 580 read).\n\nI was hoping Id see a bump in speed by going RAID 10, static volume, but its about the same as before (280 write, 530 read).\n\nWould be very open to any other tips as to how to get a better speed, been hoping for something closer to 600 on the write. Raid 0 is out of the question because Id like some form of protection against a drive failure and don\u2019t mind sacrificing the storage to go with RAID 10 if it ended up being a fast option.\n\nCan try and find a 10Gbe dock to give that a go, but the thunderbolt 3 connection is managing 550 on the write so it\u2019s clearly capable of that speed.\n\nFinally Im also down to try some SSD QTier or Caching but from what I\u2019ve read it doesn\u2019t sound like that\u2019s going to give me a big enough bump in speed over what I\u2019m seeing currently.\n\nAny help or hot tips about what I could be doing wrong would be very very appreciated\nThanks!", "author_fullname": "t2_248eq7zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Poor RAID speeds Thunderbolt 3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a8zsk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673556335.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673553536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, posted this in the QNAP sub but thought it would be worth trying here as well&lt;/p&gt;\n\n&lt;p&gt;Have a 4 bay QNAP 453BT3 connected to my Mac via thunderbolt 3. Have just upgraded it to 4 x 16tb ironwolfs and want to set it up a little differently to my previous config as I wasnt happy with the speeds I was getting previously with RAID 5, thick volume (310 write, 580 read).&lt;/p&gt;\n\n&lt;p&gt;I was hoping Id see a bump in speed by going RAID 10, static volume, but its about the same as before (280 write, 530 read).&lt;/p&gt;\n\n&lt;p&gt;Would be very open to any other tips as to how to get a better speed, been hoping for something closer to 600 on the write. Raid 0 is out of the question because Id like some form of protection against a drive failure and don\u2019t mind sacrificing the storage to go with RAID 10 if it ended up being a fast option.&lt;/p&gt;\n\n&lt;p&gt;Can try and find a 10Gbe dock to give that a go, but the thunderbolt 3 connection is managing 550 on the write so it\u2019s clearly capable of that speed.&lt;/p&gt;\n\n&lt;p&gt;Finally Im also down to try some SSD QTier or Caching but from what I\u2019ve read it doesn\u2019t sound like that\u2019s going to give me a big enough bump in speed over what I\u2019m seeing currently.&lt;/p&gt;\n\n&lt;p&gt;Any help or hot tips about what I could be doing wrong would be very very appreciated\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a8zsk", "is_robot_indexable": true, "report_reasons": null, "author": "doco32", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a8zsk/poor_raid_speeds_thunderbolt_3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a8zsk/poor_raid_speeds_thunderbolt_3/", "subreddit_subscribers": 665671, "created_utc": 1673553536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been trying to get myself to use the singlefile addon along with recoll to search them. That seems somewhat future proof in terms of the pages being readable and searchable in the future but I wanted to see if others perhaps had a better workflow/ideas?\n\nOne additional thing, dead links. SingleFile has a GREAT feature to save bookmarked pages, I have backups of old bookmark files but have discovered (duh i guess) there are many dead links so was wondering if there was a way to automate replacing of deadlinks with wayback machine links from the internet archive or perhaps google cache?", "author_fullname": "t2_f5t3js0e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you archive and search webpages?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a3tme", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673541025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been trying to get myself to use the singlefile addon along with recoll to search them. That seems somewhat future proof in terms of the pages being readable and searchable in the future but I wanted to see if others perhaps had a better workflow/ideas?&lt;/p&gt;\n\n&lt;p&gt;One additional thing, dead links. SingleFile has a GREAT feature to save bookmarked pages, I have backups of old bookmark files but have discovered (duh i guess) there are many dead links so was wondering if there was a way to automate replacing of deadlinks with wayback machine links from the internet archive or perhaps google cache?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a3tme", "is_robot_indexable": true, "report_reasons": null, "author": "AlfieMcLuvin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a3tme/how_do_you_archive_and_search_webpages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a3tme/how_do_you_archive_and_search_webpages/", "subreddit_subscribers": 665671, "created_utc": 1673541025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello there, I am looking into building my first NAS and am looking for advice on the kind of disk setup to go with. At this point I plan to reuse an older PC with TrueNAS and just need to get the drives. I might *possibly* end up moving to a prebuilt NAS device like from Synology in the future, depending on how I get on with my home build.\n \n \n \nMy use case is going to be primarily archive storage and a media server, used by only a couple of users at once. An external drive would be used to backup files from the NAS. \n \n \n \nMy concern now is whether I should go with 2x 6TB disks in RAID 1 (6TB usable), or 4x 2TB disks in RAID 5 / RAIDZ1 (also 6TB usable). Rookie numbers I know, but I can't justify spending 1000s on bigger drives at this point. \n\nThere is a difference of literally a couple of dollars between the two options above.\n \n \n \nResearching the different RAIDs, the cons are each one are:\n\n*RAID1:*\n\n* cost - half the potential storage is \"wasted\". -- This is the big one, essentially buying 2 things but only getting to use 1. Future upgrades are more expensive too, needing to buy pairs of larger dsks.\n\n* In the case of a failed drive, the system needs to be shut down to repair. -- This downtime is not a concern for me.\n\n*Raid 5:*\n\n* Rebuilding after a failure is a long process (days for large drives) and leaves the data vulnerable to further loss. This sounds pretty annoying. I'm not conerned with having the data accessable 24/7, so prefer the RAID1 in this respect.\n \n \n \nMaybe anoher option is no RAID at all? As I mentioned, my most important files (and likely a good chunk of whatever else) would be backed up elsewhere, so while downloading/copying stuff all over again would be a huge pain, it's not the end of the world. This would of course be half the cost, or I could get even more storage for just a bit more, but without any redundancy. Decisions decisions... I need a push in the right direction I think.", "author_fullname": "t2_3ycsbgx9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Disk / RAID Choice for First Build", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a2g8s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673538700.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673537625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there, I am looking into building my first NAS and am looking for advice on the kind of disk setup to go with. At this point I plan to reuse an older PC with TrueNAS and just need to get the drives. I might &lt;em&gt;possibly&lt;/em&gt; end up moving to a prebuilt NAS device like from Synology in the future, depending on how I get on with my home build.&lt;/p&gt;\n\n&lt;p&gt;My use case is going to be primarily archive storage and a media server, used by only a couple of users at once. An external drive would be used to backup files from the NAS. &lt;/p&gt;\n\n&lt;p&gt;My concern now is whether I should go with 2x 6TB disks in RAID 1 (6TB usable), or 4x 2TB disks in RAID 5 / RAIDZ1 (also 6TB usable). Rookie numbers I know, but I can&amp;#39;t justify spending 1000s on bigger drives at this point. &lt;/p&gt;\n\n&lt;p&gt;There is a difference of literally a couple of dollars between the two options above.&lt;/p&gt;\n\n&lt;p&gt;Researching the different RAIDs, the cons are each one are:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;RAID1:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;cost - half the potential storage is &amp;quot;wasted&amp;quot;. -- This is the big one, essentially buying 2 things but only getting to use 1. Future upgrades are more expensive too, needing to buy pairs of larger dsks.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;In the case of a failed drive, the system needs to be shut down to repair. -- This downtime is not a concern for me.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;em&gt;Raid 5:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Rebuilding after a failure is a long process (days for large drives) and leaves the data vulnerable to further loss. This sounds pretty annoying. I&amp;#39;m not conerned with having the data accessable 24/7, so prefer the RAID1 in this respect.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Maybe anoher option is no RAID at all? As I mentioned, my most important files (and likely a good chunk of whatever else) would be backed up elsewhere, so while downloading/copying stuff all over again would be a huge pain, it&amp;#39;s not the end of the world. This would of course be half the cost, or I could get even more storage for just a bit more, but without any redundancy. Decisions decisions... I need a push in the right direction I think.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a2g8s", "is_robot_indexable": true, "report_reasons": null, "author": "RogerTheBannister", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a2g8s/disk_raid_choice_for_first_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a2g8s/disk_raid_choice_for_first_build/", "subreddit_subscribers": 665671, "created_utc": 1673537625.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}