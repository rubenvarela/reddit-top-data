{"kind": "Listing", "data": {"after": "t3_10a3tme", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5z5rj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "YouTubers said they destroyed over 100 VHS tapes of an obscure 1987 movie to increase the value of their final copy. They sold it on eBay for $80,600.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_10a50y4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 1299, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 1299, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Qv6VDMJ9RIpS_qPBQ49h4TZuGnbi7vn-81cpkqngm2Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673543913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "insider.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.insider.com/youtubers-destroy-nukie-vhs-tape-collectable-ebay-sale-redlettermedia-2023-1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?auto=webp&amp;v=enabled&amp;s=eeec15dd0fc4a21f4b70116ef9efdaab0d529fca", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c58fe37eba227f4d81b2839e2086d8bf3e388eb5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22769873551441f606718b7c7af2fc76eaeba25b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4187a265dcbbb07b8e58d84e8bd6a093e2e80ee", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b27a59c96af5d6556352631ef545aaa0eb11992b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d7b3a5c0168f9b727303ef54e8c063e24841299", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/kSRLcifnH85uyd5JGm4cflV_eZCwGKODHE5RUbddi5M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be08317c9a35c9da4e5399b1c5ba6898750ecbd4", "width": 1080, "height": 540}], "variants": {}, "id": "xRZTtLBIe0INmo3ybLtCCz7LqQtFpG1yBU1egx6NDfk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a50y4", "is_robot_indexable": true, "report_reasons": null, "author": "ET2-SW", "discussion_type": null, "num_comments": 298, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a50y4/youtubers_said_they_destroyed_over_100_vhs_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.insider.com/youtubers-destroy-nukie-vhs-tape-collectable-ebay-sale-redlettermedia-2023-1", "subreddit_subscribers": 665682, "created_utc": 1673543913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Micron Unveils 9400 SSD: 30 TB Capacities With Best-In-Class Performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": false, "name": "t3_10a4nvv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 88, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 88, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/AC0ueZqYqdOMzCdm3ntxWB65qmLoxW0SUkHoeuH1Bds.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673543009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "wccftech.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://wccftech.com/micron-unveils-9400-ssd-30-tb-capacities-with-best-in-class-performance/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?auto=webp&amp;v=enabled&amp;s=c5683a71bf7f47729b0ff2c5cc693e5e54236a37", "width": 980, "height": 620}, "resolutions": [{"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f096790349c2231fec800a7e162ac38ff971e054", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c6c9f335d72366983fe496f7108b7ba824b5f74", "width": 216, "height": 136}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6be074be75dbd483261d4e50615feebc1a996a10", "width": 320, "height": 202}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14a9c87bb2670e5df83de7748e5ddaa8c8813a88", "width": 640, "height": 404}, {"url": "https://external-preview.redd.it/ytVstvPnKHj54pM1Qo75ZoZa4XzbvbMCkazABs59QHs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5652b2fb0ca0e91c36cb053db1abddf73351f32", "width": 960, "height": 607}], "variants": {}, "id": "Z0sSJbkM797eMfo6AVw0nJG4sVsDUTAEeTjiWS9-Vlc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a4nvv", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10a4nvv/micron_unveils_9400_ssd_30_tb_capacities_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://wccftech.com/micron-unveils-9400-ssd-30-tb-capacities-with-best-in-class-performance/", "subreddit_subscribers": 665682, "created_utc": 1673543009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_7l2p5s3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2010-Present: At ten years old, I started collecting the music I was listening to after finding an ipod nano next to a drunk guy sleeping on my front lawn. 11 GB isn't a lot, but I'm still proud of hauling these tracks around for 12 years straight.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10abzbp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Audio Data", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/M-t7fnovqe2WstYdUHEZWYUyII_PtebRC--jIOobsr0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673560512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/5q6309u4aoba1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/5q6309u4aoba1.png?auto=webp&amp;v=enabled&amp;s=c345d10e74ba82f3d114f97725d2229e0b7b13bc", "width": 303, "height": 313}, "resolutions": [{"url": "https://preview.redd.it/5q6309u4aoba1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d95b3105e04660a6b7e3340da5f57b32f074ab59", "width": 108, "height": 111}, {"url": "https://preview.redd.it/5q6309u4aoba1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce58d3ded722b1eb2c8754f251a3afef62b16628", "width": 216, "height": 223}], "variants": {}, "id": "_PXHrjLaqDIiAWLyr_Xmd27OGgLdjJA4oqHgm7Z9Vf0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10abzbp", "is_robot_indexable": true, "report_reasons": null, "author": "shinnith", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10abzbp/2010present_at_ten_years_old_i_started_collecting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/5q6309u4aoba1.png", "subreddit_subscribers": 665682, "created_utc": 1673560512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got about 300,000 emails going back to 1993.  I love having the whole thing indexed and searchable for practical and nostalgic purposes, but the ratio of \"good\" to \"bad\" is ridiculous.  Even with gmail doing a lot of the heavy spam filtering, it's got a lot of garbage.\n\nDoes anyone have any pointers on how to efficiently spam filter large quantities of email spread across about 30 unix mbox files?  \n\nIf I had to, I'd be OK resorting to something somewhat iterative and convoluted (somehow feed all the mbox files to a mail server running on one of my linux boxes?) but it would be nice if there were tools out there to make this as easy as possible.", "author_fullname": "t2_tthrp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "30 years of saved email - spam filtering options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a02eo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673531289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got about 300,000 emails going back to 1993.  I love having the whole thing indexed and searchable for practical and nostalgic purposes, but the ratio of &amp;quot;good&amp;quot; to &amp;quot;bad&amp;quot; is ridiculous.  Even with gmail doing a lot of the heavy spam filtering, it&amp;#39;s got a lot of garbage.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any pointers on how to efficiently spam filter large quantities of email spread across about 30 unix mbox files?  &lt;/p&gt;\n\n&lt;p&gt;If I had to, I&amp;#39;d be OK resorting to something somewhat iterative and convoluted (somehow feed all the mbox files to a mail server running on one of my linux boxes?) but it would be nice if there were tools out there to make this as easy as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a02eo", "is_robot_indexable": true, "report_reasons": null, "author": "lectures", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a02eo/30_years_of_saved_email_spam_filtering_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a02eo/30_years_of_saved_email_spam_filtering_options/", "subreddit_subscribers": 665682, "created_utc": 1673531289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got the HDDs of almost every computer I've ever owned sitting in a box. I recently built myself a NAS and would like to save the contents of the disks to the NAS. I want to keep a full copy of the disk and not just common locations such as \"My Documents\" etc.\n\nI originally tried the na\u00efve approach of copying the entire contents of the drive into a folder. This doesn't work too well for Windows boot drives, as there are millions of tiny files all over the place. Even my 128GB SSD took over 24 hours to transfer.\n\nWhat I'm looking for is:\n\n* Ideally filesystem agnostic - while most of my drives are NTFS I have several from machines running Linux and macOS. Windows-only is better than nothing though.\n* Something that creates a single file that can be opened/mounted to view files.\n* Only the size of used space - I believe this rules out most types of disk image. I don't want to store an 8TB file for a drive that only has 100GB saved on it.\n* Free and preferably open source tools available for creation and viewing.\n\nI'm not sure if there's anything that exists that meets all my requirements but I'd be interested to know of anything that comes close. I'd also be interested to know what other people do to archive their old hard drives.\n\nThanks!", "author_fullname": "t2_rf5b8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to archive contents of old HDDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10agls0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673571698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got the HDDs of almost every computer I&amp;#39;ve ever owned sitting in a box. I recently built myself a NAS and would like to save the contents of the disks to the NAS. I want to keep a full copy of the disk and not just common locations such as &amp;quot;My Documents&amp;quot; etc.&lt;/p&gt;\n\n&lt;p&gt;I originally tried the na\u00efve approach of copying the entire contents of the drive into a folder. This doesn&amp;#39;t work too well for Windows boot drives, as there are millions of tiny files all over the place. Even my 128GB SSD took over 24 hours to transfer.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m looking for is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ideally filesystem agnostic - while most of my drives are NTFS I have several from machines running Linux and macOS. Windows-only is better than nothing though.&lt;/li&gt;\n&lt;li&gt;Something that creates a single file that can be opened/mounted to view files.&lt;/li&gt;\n&lt;li&gt;Only the size of used space - I believe this rules out most types of disk image. I don&amp;#39;t want to store an 8TB file for a drive that only has 100GB saved on it.&lt;/li&gt;\n&lt;li&gt;Free and preferably open source tools available for creation and viewing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if there&amp;#39;s anything that exists that meets all my requirements but I&amp;#39;d be interested to know of anything that comes close. I&amp;#39;d also be interested to know what other people do to archive their old hard drives.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10agls0", "is_robot_indexable": true, "report_reasons": null, "author": "Nogtail", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10agls0/best_way_to_archive_contents_of_old_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10agls0/best_way_to_archive_contents_of_old_hdds/", "subreddit_subscribers": 665682, "created_utc": 1673571698.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I\u2019ve been somewhat of a DataHoarder for a bit. AKA, I download a crap ton of files and feel apart of me die inside whenever I run out of room and have to delete stuff. I\u2019ve also had my PC growing up crash 3 times and lose *everything*, including projects that I had made and never backed up anywhere. 13 year old me was devastated.\n\nI\u2019m all ready to jump head first in, but the hard part is I really don\u2019t understand any of this technical jargon. I understand the basics, as I\u2019m currently learning how to do front end work on websites, but I\u2019ve never hosted my own server or even built my own computer before.\n\nI currently have a shitty laptop with 1TB of storage, and have only ever bought an SSD for my PS4.\n\nI do have a basic understanding of how the internet *actually* works, and can grasp the ideas behind internet protocols and how servers work.\n\nCurrently, I mainly wanna hoarde music files, (I have visited the music hoarding subreddit for tips on that) and a *l o t* of niche content / files. Mainly USTs, VSQs, SVPs, some midis, etc. If you don\u2019t know what those are, they\u2019re file types that are used by some vocal synths, like VOCALOID. Think Hatsune Miku.\n\nI do want to expand to having my own media collection in general, but that\u2019s something further down the line once I am more knowledgeable and can get a server running.\n\nShould I first build my own setup? If I stick to my laptop for the time being, is cloud storage + an external backup (like USBs or something) my best bet? Should I even bother with getting an external hard drive and just wait until I can build my own setup?\n\nAre USBs even comparable to having your data on HDDs and SSDs?\n\nOr, do any of you have any good resources for absolute newbies like me?\n\nGoogle keeps sending me to unhelpful websites, and honestly? Getting some tips from people who love data hoarding seems much more reliable then whatever google will show me.\n\nEither way, I am eternally thankful for this subreddit making me realize I should embrace my hoarding.", "author_fullname": "t2_47z0zk6b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Absolute Newbie DataHoarder with limited technical experience, any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ad410", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673563083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I\u2019ve been somewhat of a DataHoarder for a bit. AKA, I download a crap ton of files and feel apart of me die inside whenever I run out of room and have to delete stuff. I\u2019ve also had my PC growing up crash 3 times and lose &lt;em&gt;everything&lt;/em&gt;, including projects that I had made and never backed up anywhere. 13 year old me was devastated.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m all ready to jump head first in, but the hard part is I really don\u2019t understand any of this technical jargon. I understand the basics, as I\u2019m currently learning how to do front end work on websites, but I\u2019ve never hosted my own server or even built my own computer before.&lt;/p&gt;\n\n&lt;p&gt;I currently have a shitty laptop with 1TB of storage, and have only ever bought an SSD for my PS4.&lt;/p&gt;\n\n&lt;p&gt;I do have a basic understanding of how the internet &lt;em&gt;actually&lt;/em&gt; works, and can grasp the ideas behind internet protocols and how servers work.&lt;/p&gt;\n\n&lt;p&gt;Currently, I mainly wanna hoarde music files, (I have visited the music hoarding subreddit for tips on that) and a &lt;em&gt;l o t&lt;/em&gt; of niche content / files. Mainly USTs, VSQs, SVPs, some midis, etc. If you don\u2019t know what those are, they\u2019re file types that are used by some vocal synths, like VOCALOID. Think Hatsune Miku.&lt;/p&gt;\n\n&lt;p&gt;I do want to expand to having my own media collection in general, but that\u2019s something further down the line once I am more knowledgeable and can get a server running.&lt;/p&gt;\n\n&lt;p&gt;Should I first build my own setup? If I stick to my laptop for the time being, is cloud storage + an external backup (like USBs or something) my best bet? Should I even bother with getting an external hard drive and just wait until I can build my own setup?&lt;/p&gt;\n\n&lt;p&gt;Are USBs even comparable to having your data on HDDs and SSDs?&lt;/p&gt;\n\n&lt;p&gt;Or, do any of you have any good resources for absolute newbies like me?&lt;/p&gt;\n\n&lt;p&gt;Google keeps sending me to unhelpful websites, and honestly? Getting some tips from people who love data hoarding seems much more reliable then whatever google will show me.&lt;/p&gt;\n\n&lt;p&gt;Either way, I am eternally thankful for this subreddit making me realize I should embrace my hoarding.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ad410", "is_robot_indexable": true, "report_reasons": null, "author": "Ellestyx", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ad410/absolute_newbie_datahoarder_with_limited/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ad410/absolute_newbie_datahoarder_with_limited/", "subreddit_subscribers": 665682, "created_utc": 1673563083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I got bunch of **14TB** EasyStores on sale to shuck later.\n\nI haven't opened them all, but the ones that I did - I noticed there are two varieties (with slightly different packaging too) of drives inside the shell. They are both marked as WD140EDGZ and both *made in Thailand*, but the r/n and company contact address are different:\n\n=========================================\n\n\\- **us7sar180**\n\n(contacts in France, made in September 2021)\n\nShows up as  **WDC WD140EDGZ-11B2DA2** in CrystalDisk.\n\nThey are significantly ***noisier***.\n\nI read that they are rebranded *Ultrastar HC550* ***18TB*** binned down to **14TB**.  \n\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- **us7sap140**\n\n(contacts in Ireland, made in August 2022)\n\nShows up as  **WDC WD140EDGZ-11B1PA0** in CrystalDisk.\n\nThey are significantly ***quieter***.\n\nI read that they are rebranded *Ultrastar HC530*\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nSpeed is the same on all - 230 MBs. I expected \"18TB\" ones to be faster.\n\n&amp;#x200B;\n\n===========================================\n\n&amp;#x200B;\n\nI would appreciate if anyone has answers to these QUESTIONS:\n\n\\- Which one would you pick?\n\nI.e., is the extra noise in **us7sar180** warranted by any added benefits? (workload rating, performance)\n\nOr should I go with a far more quiet **us7sap140** and return their noisy siblings that I haven't unboxed yet back to BestBuy?\n\n&amp;#x200B;\n\n\\- Would the \"18TB\" **us7sar180** drive be better because  it would have more available space to reallocate bad blocks (since it's originally an 18TB drive)? Or is the space above 14TB not accessible even for block reallocation? \n\n\\- Is it possible unlock the disabled extra space in the **us7sar180** firmware to make them 18TB again?\n\n\\- People mention that these are 7200 slowed down to 5400 in firmware, but they all report 230-240 MBs in CrystalDisk Mark (4 GiB sample file, 3 runs). Can 5400 yield that kind of speed? I though 540 should be around 100-150.\n\n&amp;#x200B;\n\nI plan to use them in a ZFS/TrueNAS server if that matters.\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_2r0xql9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TWO types of WD EasyStore drives. Which one is better?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10akvbd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673583406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I got bunch of &lt;strong&gt;14TB&lt;/strong&gt; EasyStores on sale to shuck later.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t opened them all, but the ones that I did - I noticed there are two varieties (with slightly different packaging too) of drives inside the shell. They are both marked as WD140EDGZ and both &lt;em&gt;made in Thailand&lt;/em&gt;, but the r/n and company contact address are different:&lt;/p&gt;\n\n&lt;h1&gt;&lt;/h1&gt;\n\n&lt;p&gt;- &lt;strong&gt;us7sar180&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;(contacts in France, made in September 2021)&lt;/p&gt;\n\n&lt;p&gt;Shows up as  &lt;strong&gt;WDC WD140EDGZ-11B2DA2&lt;/strong&gt; in CrystalDisk.&lt;/p&gt;\n\n&lt;p&gt;They are significantly &lt;strong&gt;&lt;em&gt;noisier&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;I read that they are rebranded &lt;em&gt;Ultrastar HC550&lt;/em&gt; &lt;strong&gt;&lt;em&gt;18TB&lt;/em&gt;&lt;/strong&gt; binned down to &lt;strong&gt;14TB&lt;/strong&gt;.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;us7sap140&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;(contacts in Ireland, made in August 2022)&lt;/p&gt;\n\n&lt;p&gt;Shows up as  &lt;strong&gt;WDC WD140EDGZ-11B1PA0&lt;/strong&gt; in CrystalDisk.&lt;/p&gt;\n\n&lt;p&gt;They are significantly &lt;strong&gt;&lt;em&gt;quieter&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;I read that they are rebranded &lt;em&gt;Ultrastar HC530&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Speed is the same on all - 230 MBs. I expected &amp;quot;18TB&amp;quot; ones to be faster.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;&lt;/h1&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I would appreciate if anyone has answers to these QUESTIONS:&lt;/p&gt;\n\n&lt;p&gt;- Which one would you pick?&lt;/p&gt;\n\n&lt;p&gt;I.e., is the extra noise in &lt;strong&gt;us7sar180&lt;/strong&gt; warranted by any added benefits? (workload rating, performance)&lt;/p&gt;\n\n&lt;p&gt;Or should I go with a far more quiet &lt;strong&gt;us7sap140&lt;/strong&gt; and return their noisy siblings that I haven&amp;#39;t unboxed yet back to BestBuy?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Would the &amp;quot;18TB&amp;quot; &lt;strong&gt;us7sar180&lt;/strong&gt; drive be better because  it would have more available space to reallocate bad blocks (since it&amp;#39;s originally an 18TB drive)? Or is the space above 14TB not accessible even for block reallocation? &lt;/p&gt;\n\n&lt;p&gt;- Is it possible unlock the disabled extra space in the &lt;strong&gt;us7sar180&lt;/strong&gt; firmware to make them 18TB again?&lt;/p&gt;\n\n&lt;p&gt;- People mention that these are 7200 slowed down to 5400 in firmware, but they all report 230-240 MBs in CrystalDisk Mark (4 GiB sample file, 3 runs). Can 5400 yield that kind of speed? I though 540 should be around 100-150.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I plan to use them in a ZFS/TrueNAS server if that matters.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10akvbd", "is_robot_indexable": true, "report_reasons": null, "author": "Infinite100p", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10akvbd/two_types_of_wd_easystore_drives_which_one_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10akvbd/two_types_of_wd_easystore_drives_which_one_is/", "subreddit_subscribers": 665682, "created_utc": 1673583406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for some help in finding ways to better saturate my 10Gb network. I'm currently maxing out at ~5Gbps using a 10GB zip file but I know the storage is capable of far more (internal transfers and tar'd mbuffered external transfers are far faster) and the network is fine (iperf registers 9.8Gbps both ways). Tests are done between the pools on the server and a storage NVMe on my entirely overspec'd workstation. Any help is appreciated.\n\nServer Hardware:\n\n- Box: Dell T620\n- RAM: 220GB DDR3 ECC\n- CPU: 2x E5-2643v2 - 12C/24T@3.5GHz\n- HBA: LSI 9305-16i in IT mode\n- NIC: MCX354A-FCCT in ethernet mode, using a QSFP to SFP+ adapter\n- OS: Proxmox 7.3-4\n- File server: Samba directly on Proxmox, shares through ZFS pool settings\n\nServer Storage: \n\n- ssdpool: 8x 8TB Samsung 870QVO drives in ZFS RAID0, typically tops out at 3-4Gbps.\n- hddpool: 12x 16TB Seagate Exos HDDs in ZFS RAID0 + 2x 960GB Intel Optane 905P as a mirrored special vdev, typically tops out at 3-4Gbps.\n- cache: 1x 1.92TB Samsung DCT983 NVMe on ZFS, tops out at 5Gbps.\n\n---\n\nBefore anyone asks: I'm not concerned about the large RAID0 arrays, I have backups and am looking for lots of speed and scratch space. The 870QVO pool is read-only to the services that access it. Writes are infrequent and manual (download to cache, categorize and clean, transfer to pool). The shares are based on Proxmox because every attempt at setting up a file share container results in a ~2Gbps bottleneck. Jumbo frames are on for every device in the network. Network connections are all SFP+ DACs.", "author_fullname": "t2_u4u6f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me squeeze some more speed out of my file shares", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10aezg0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673567591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for some help in finding ways to better saturate my 10Gb network. I&amp;#39;m currently maxing out at ~5Gbps using a 10GB zip file but I know the storage is capable of far more (internal transfers and tar&amp;#39;d mbuffered external transfers are far faster) and the network is fine (iperf registers 9.8Gbps both ways). Tests are done between the pools on the server and a storage NVMe on my entirely overspec&amp;#39;d workstation. Any help is appreciated.&lt;/p&gt;\n\n&lt;p&gt;Server Hardware:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Box: Dell T620&lt;/li&gt;\n&lt;li&gt;RAM: 220GB DDR3 ECC&lt;/li&gt;\n&lt;li&gt;CPU: 2x E5-2643v2 - 12C/&lt;a href=\"mailto:24T@3.5GHz\"&gt;24T@3.5GHz&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;HBA: LSI 9305-16i in IT mode&lt;/li&gt;\n&lt;li&gt;NIC: MCX354A-FCCT in ethernet mode, using a QSFP to SFP+ adapter&lt;/li&gt;\n&lt;li&gt;OS: Proxmox 7.3-4&lt;/li&gt;\n&lt;li&gt;File server: Samba directly on Proxmox, shares through ZFS pool settings&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Server Storage: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ssdpool: 8x 8TB Samsung 870QVO drives in ZFS RAID0, typically tops out at 3-4Gbps.&lt;/li&gt;\n&lt;li&gt;hddpool: 12x 16TB Seagate Exos HDDs in ZFS RAID0 + 2x 960GB Intel Optane 905P as a mirrored special vdev, typically tops out at 3-4Gbps.&lt;/li&gt;\n&lt;li&gt;cache: 1x 1.92TB Samsung DCT983 NVMe on ZFS, tops out at 5Gbps.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Before anyone asks: I&amp;#39;m not concerned about the large RAID0 arrays, I have backups and am looking for lots of speed and scratch space. The 870QVO pool is read-only to the services that access it. Writes are infrequent and manual (download to cache, categorize and clean, transfer to pool). The shares are based on Proxmox because every attempt at setting up a file share container results in a ~2Gbps bottleneck. Jumbo frames are on for every device in the network. Network connections are all SFP+ DACs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10aezg0", "is_robot_indexable": true, "report_reasons": null, "author": "certifiedintelligent", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10aezg0/help_me_squeeze_some_more_speed_out_of_my_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10aezg0/help_me_squeeze_some_more_speed_out_of_my_file/", "subreddit_subscribers": 665682, "created_utc": 1673567591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI download a lot of content for archival purposes. The files I download are typically packaged as ZIP or RAR archives which then have to be extracted.\n\nI am wondering if there is any way I can automatically add read-only attributes to all files as they are being extracted (eg. as an option inside WinRAR or 7-Zip) instead of having to manually select the folder afterwards, then right-clicking and selecting the read-only checkbox (which sometimes I forget to do)\n\nThanks in advance.", "author_fullname": "t2_fcjv96r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Windows] Is there any way to automatically add read-only attributes to all files extracted by WinRAR or 7-Zip?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a3vd9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673603262.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673541140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I download a lot of content for archival purposes. The files I download are typically packaged as ZIP or RAR archives which then have to be extracted.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if there is any way I can automatically add read-only attributes to all files as they are being extracted (eg. as an option inside WinRAR or 7-Zip) instead of having to manually select the folder afterwards, then right-clicking and selecting the read-only checkbox (which sometimes I forget to do)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a3vd9", "is_robot_indexable": true, "report_reasons": null, "author": "P650SE", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a3vd9/windows_is_there_any_way_to_automatically_add/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a3vd9/windows_is_there_any_way_to_automatically_add/", "subreddit_subscribers": 665682, "created_utc": 1673541140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't know if it's on point with the subreddit but I don't where else I could find opinions.\nI have some games and anime series backed up in HDDs but I also wanted a second backup on a storage of a different type. Bluray is too pricey for me so I went for DVD-r and DVD-dl. \n\nThe other option would have been a chain of USB but I wanted to keep a better aesthetic /s\n\nAnyway, I am now undecided on which package to use to keep the DVDs. Considering also their shelf position. \nMy library is not directly exposed to sunlight but \"diagonally\" there is some light. Normally summer is not really harsh. Maybe there is a bit of humidity problem in some months.\n\n[Plastic case] But I recently read/heard about problems coming from black pvc cases, since they would stick to the disk and may have come chemical reaction that can ruin the disk. Is it true that hot/humidity+enclosed pvc case really makes a mess?\n\n[Transparent jewel case] So I looked for the smaller cases, the ones with the flimsy hinge that always breaks. Majority of them seems to be made of PP and other non toxic materials.\n\nJewel seems the best choice, but I want opinions on the truth of the pvc/pp/other materials debate. I don't think there are many more type of rigid cases. I don't need the folder/booklet one with 4 discs, single pockets of paper/plastic are too \"nude\".\n\nAlso, for proper temperature, is it a good idea to put the cases in a second line, behind books/other stuff?\n\n\nThen there is the external HDD placement. I know that I should plug them in sometimes to check if files are corrupted/spin the disk inside.\nSo, is it a good idea to put my HDD on a shelf, plugged to a multi-port USB that routinely I connect to an external battery? So I can power them up with minimal hassle. They should have no problem with data \"safely remove hardware\" when unplugged since they only transfer energy with the battery, I suppose. \nThe hdd shelf is also easier to manage rather than dump them in a box or pile over a desk, and it's better covered from sunlight", "author_fullname": "t2_3n8go9g5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DVD and HDD:how and where to shelf/store/encase?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10adzyc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673565201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know if it&amp;#39;s on point with the subreddit but I don&amp;#39;t where else I could find opinions.\nI have some games and anime series backed up in HDDs but I also wanted a second backup on a storage of a different type. Bluray is too pricey for me so I went for DVD-r and DVD-dl. &lt;/p&gt;\n\n&lt;p&gt;The other option would have been a chain of USB but I wanted to keep a better aesthetic /s&lt;/p&gt;\n\n&lt;p&gt;Anyway, I am now undecided on which package to use to keep the DVDs. Considering also their shelf position. \nMy library is not directly exposed to sunlight but &amp;quot;diagonally&amp;quot; there is some light. Normally summer is not really harsh. Maybe there is a bit of humidity problem in some months.&lt;/p&gt;\n\n&lt;p&gt;[Plastic case] But I recently read/heard about problems coming from black pvc cases, since they would stick to the disk and may have come chemical reaction that can ruin the disk. Is it true that hot/humidity+enclosed pvc case really makes a mess?&lt;/p&gt;\n\n&lt;p&gt;[Transparent jewel case] So I looked for the smaller cases, the ones with the flimsy hinge that always breaks. Majority of them seems to be made of PP and other non toxic materials.&lt;/p&gt;\n\n&lt;p&gt;Jewel seems the best choice, but I want opinions on the truth of the pvc/pp/other materials debate. I don&amp;#39;t think there are many more type of rigid cases. I don&amp;#39;t need the folder/booklet one with 4 discs, single pockets of paper/plastic are too &amp;quot;nude&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Also, for proper temperature, is it a good idea to put the cases in a second line, behind books/other stuff?&lt;/p&gt;\n\n&lt;p&gt;Then there is the external HDD placement. I know that I should plug them in sometimes to check if files are corrupted/spin the disk inside.\nSo, is it a good idea to put my HDD on a shelf, plugged to a multi-port USB that routinely I connect to an external battery? So I can power them up with minimal hassle. They should have no problem with data &amp;quot;safely remove hardware&amp;quot; when unplugged since they only transfer energy with the battery, I suppose. \nThe hdd shelf is also easier to manage rather than dump them in a box or pile over a desk, and it&amp;#39;s better covered from sunlight&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10adzyc", "is_robot_indexable": true, "report_reasons": null, "author": "CourierLordProcione2", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10adzyc/dvd_and_hddhow_and_where_to_shelfstoreencase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10adzyc/dvd_and_hddhow_and_where_to_shelfstoreencase/", "subreddit_subscribers": 665682, "created_utc": 1673565201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys,\n\nI am not that much of a hoarder rather a storage/compute/backup specialist.\n\nLast couple of months I found that torrent trackers don't have movies in my native language. So for me the tube is best source of movies and even 4k nature videos. Can you share with me what proof to be best way for you to download YT videos/playlist both 720p and 4k?\n\nPS\n\nHere is a snip from session that will time out:\n\nuser@host:\\~$ ls -lh\n\ntotal 235M\n\n\\-rw-rw-r-- 1 user 235M Jan  4 21:02 '\u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm.part'\n\nuser@host:\\~$ rm '\u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm.part'\n\nuser@host:\\~$ youtube-dl -f 315 [https://www.youtube.com/watch?v=LCDDzLK-sys](https://www.youtube.com/watch?v=LCDDzLK-sys)\n\n\\[youtube\\] LCDDzLK-sys: Downloading webpage\n\n\\[download\\] Destination: \u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm\n\n\\[download\\]   0.9% of 2.76GiB at 70.46KiB/s ETA 11:17:32", "author_fullname": "t2_db0rv0yp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you like to dump 4k Youtube videos ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10aa20l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673556026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I am not that much of a hoarder rather a storage/compute/backup specialist.&lt;/p&gt;\n\n&lt;p&gt;Last couple of months I found that torrent trackers don&amp;#39;t have movies in my native language. So for me the tube is best source of movies and even 4k nature videos. Can you share with me what proof to be best way for you to download YT videos/playlist both 720p and 4k?&lt;/p&gt;\n\n&lt;p&gt;PS&lt;/p&gt;\n\n&lt;p&gt;Here is a snip from session that will time out:&lt;/p&gt;\n\n&lt;p&gt;user@host:~$ ls -lh&lt;/p&gt;\n\n&lt;p&gt;total 235M&lt;/p&gt;\n\n&lt;p&gt;-rw-rw-r-- 1 user 235M Jan  4 21:02 &amp;#39;\u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm.part&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;user@host:~$ rm &amp;#39;\u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm.part&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;user@host:~$ youtube-dl -f 315 &lt;a href=\"https://www.youtube.com/watch?v=LCDDzLK-sys\"&gt;https://www.youtube.com/watch?v=LCDDzLK-sys&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[youtube] LCDDzLK-sys: Downloading webpage&lt;/p&gt;\n\n&lt;p&gt;[download] Destination: \u0422\u0435\u0437\u0438 \u0433\u043b\u0435\u0434\u043a\u0438 \u0449\u0435 \u041d\u0410\u0421\u042a\u041b\u0417\u042f\u0422 \u043e\u0447\u0438\u0442\u0435 \u0432\u0438!-LCDDzLK-sys.webm&lt;/p&gt;\n\n&lt;p&gt;[download]   0.9% of 2.76GiB at 70.46KiB/s ETA 11:17:32&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B_95ltT36M0Z_42W0ff5ZhRSP8_V3dIr00yEl4_xSyg.jpg?auto=webp&amp;v=enabled&amp;s=0750adbce2c293bab5e98c3db1fc417259085eff", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/B_95ltT36M0Z_42W0ff5ZhRSP8_V3dIr00yEl4_xSyg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a42c015f4b6d258225a8ce691de9bbef630ed1ca", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/B_95ltT36M0Z_42W0ff5ZhRSP8_V3dIr00yEl4_xSyg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9cd0fe9388dd7504c6be08fa00f019a728d63f3c", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/B_95ltT36M0Z_42W0ff5ZhRSP8_V3dIr00yEl4_xSyg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a55a3af333fb19a2e81021b7da43de8dd7ea89b", "width": 320, "height": 240}], "variants": {}, "id": "UW-6yU-dqtHKKqWFY9jvmOw8xW-z6xI7jzKq6jBnulw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "9TB TerraMaster", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10aa20l", "is_robot_indexable": true, "report_reasons": null, "author": "Accomplished-Eye1673", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10aa20l/how_do_you_like_to_dump_4k_youtube_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10aa20l/how_do_you_like_to_dump_4k_youtube_videos/", "subreddit_subscribers": 665682, "created_utc": 1673556026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anybody know a service that offers large amounts of storage for very short periods of time and how much would this cost? cheers", "author_fullname": "t2_h36vzasl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to buy 2tb cloud hosted storage for a very short period of time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a7x3c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673550889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anybody know a service that offers large amounts of storage for very short periods of time and how much would this cost? cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a7x3c", "is_robot_indexable": true, "report_reasons": null, "author": "JustYourJoe_", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a7x3c/looking_to_buy_2tb_cloud_hosted_storage_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a7x3c/looking_to_buy_2tb_cloud_hosted_storage_for_a/", "subreddit_subscribers": 665682, "created_utc": 1673550889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have approximately 400-700 videos that have an annoying, redundant, 4 second intro from the site they originated from. I would like to cut this pre-roll in a single batch operation.\n\nI have ffmpeg and Linux; I'm not sure of the command to run to accomplish this.\n\nThanks for your help :)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&gt;!Yes, it's porn!&lt;", "author_fullname": "t2_v9x1ekbb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch cut the first 4 seconds from a group of video's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10aney2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673591366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have approximately 400-700 videos that have an annoying, redundant, 4 second intro from the site they originated from. I would like to cut this pre-roll in a single batch operation.&lt;/p&gt;\n\n&lt;p&gt;I have ffmpeg and Linux; I&amp;#39;m not sure of the command to run to accomplish this.&lt;/p&gt;\n\n&lt;p&gt;Thanks for your help :)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;span class=\"md-spoiler-text\"&gt;Yes, it&amp;#39;s porn&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10aney2", "is_robot_indexable": true, "report_reasons": null, "author": "asyoucommandstan", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10aney2/batch_cut_the_first_4_seconds_from_a_group_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10aney2/batch_cut_the_first_4_seconds_from_a_group_of/", "subreddit_subscribers": 665682, "created_utc": 1673591366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been scrolling through Reddit recently and I just remembered a rarely known subreddit that I was often scrolling into. It's now about more than a year since I've last visited that subreddit that when I searched it, it was banned due to being unmoderated. I searched it through unddit and I can see the sub's comments, but not the posts and when I clicked it, I get taken back to Reddit and a message pops up saying 'It looks like you aren't allowed to do that.'. I've keep reloading the page, logging out and back in on Reddit, and searched the internet on a way to view those posts, but still nothing. Any way on how to get around this message?", "author_fullname": "t2_9pox10mc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "'It looks like you aren't allowed to do that.' Message", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ajnmc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673579898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been scrolling through Reddit recently and I just remembered a rarely known subreddit that I was often scrolling into. It&amp;#39;s now about more than a year since I&amp;#39;ve last visited that subreddit that when I searched it, it was banned due to being unmoderated. I searched it through unddit and I can see the sub&amp;#39;s comments, but not the posts and when I clicked it, I get taken back to Reddit and a message pops up saying &amp;#39;It looks like you aren&amp;#39;t allowed to do that.&amp;#39;. I&amp;#39;ve keep reloading the page, logging out and back in on Reddit, and searched the internet on a way to view those posts, but still nothing. Any way on how to get around this message?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ajnmc", "is_robot_indexable": true, "report_reasons": null, "author": "CupRevolutionary5599", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ajnmc/it_looks_like_you_arent_allowed_to_do_that_message/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ajnmc/it_looks_like_you_arent_allowed_to_do_that_message/", "subreddit_subscribers": 665682, "created_utc": 1673579898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Wonder if YouTube-DL would work, however this site isn\u2019t mainstream \n\nAny ideas? \n\nThanks", "author_fullname": "t2_jp6vscfv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download from a \u201cstreaming only\u201d site", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a8exh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673552079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wonder if YouTube-DL would work, however this site isn\u2019t mainstream &lt;/p&gt;\n\n&lt;p&gt;Any ideas? &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a8exh", "is_robot_indexable": true, "report_reasons": null, "author": "StoryThrowAway916", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a8exh/download_from_a_streaming_only_site/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a8exh/download_from_a_streaming_only_site/", "subreddit_subscribers": 665682, "created_utc": 1673552079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone found a software they like for managing a library of *arbitrary* files? I have Plex for Video, Kavita for ebooks, Paperless-ng for personal documents, etc.\n\nIs there anything like these tools but for any and all file types? Basically I'd want a tool that could read metadata, potentially file contexts for text, and also allow any file to be tagged \u00e0 la Gmail labels, making searching easy. If it's something common like HTML/PDF/mp4 it could just view the file in browser, but otherwise provide a file location reference or download link.\n\nAnyone aware of anything in this vein existing?", "author_fullname": "t2_32pyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Library Management Software for Arbitrary Files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a5ttt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673545888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone found a software they like for managing a library of &lt;em&gt;arbitrary&lt;/em&gt; files? I have Plex for Video, Kavita for ebooks, Paperless-ng for personal documents, etc.&lt;/p&gt;\n\n&lt;p&gt;Is there anything like these tools but for any and all file types? Basically I&amp;#39;d want a tool that could read metadata, potentially file contexts for text, and also allow any file to be tagged \u00e0 la Gmail labels, making searching easy. If it&amp;#39;s something common like HTML/PDF/mp4 it could just view the file in browser, but otherwise provide a file location reference or download link.&lt;/p&gt;\n\n&lt;p&gt;Anyone aware of anything in this vein existing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "12TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a5ttt", "is_robot_indexable": true, "report_reasons": null, "author": "eldridgea", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10a5ttt/library_management_software_for_arbitrary_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a5ttt/library_management_software_for_arbitrary_files/", "subreddit_subscribers": 665682, "created_utc": 1673545888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought and shucked some bargain 4TB seagate barracudas about a year ago, because I had a dream of building my own personal Pi NAS. I set it all up and am now the family\u2019s designated hoarder of photos, videos etc. but moving overseas for a couple of years and don\u2019t want to bring it all with me.\n\nI still want to hang onto them because it\u2019s still 4 x 4TB of barely used drives, and when I move back home, I want to keep set it up again.\n\nProvided I store them in a cool dry place, will they be basically as good as new in 2-3 years? Should I go to the hassle of vacuum sealing them to prevent any oxidisation?\n\nI have 3:2:1 backups so the data already on there is not of great concern given this.", "author_fullname": "t2_2qazo5u3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Long term storage of unused drives. Any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10atjwc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673613531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought and shucked some bargain 4TB seagate barracudas about a year ago, because I had a dream of building my own personal Pi NAS. I set it all up and am now the family\u2019s designated hoarder of photos, videos etc. but moving overseas for a couple of years and don\u2019t want to bring it all with me.&lt;/p&gt;\n\n&lt;p&gt;I still want to hang onto them because it\u2019s still 4 x 4TB of barely used drives, and when I move back home, I want to keep set it up again.&lt;/p&gt;\n\n&lt;p&gt;Provided I store them in a cool dry place, will they be basically as good as new in 2-3 years? Should I go to the hassle of vacuum sealing them to prevent any oxidisation?&lt;/p&gt;\n\n&lt;p&gt;I have 3:2:1 backups so the data already on there is not of great concern given this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10atjwc", "is_robot_indexable": true, "report_reasons": null, "author": "Embarrassed-Book-596", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10atjwc/long_term_storage_of_unused_drives_any_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10atjwc/long_term_storage_of_unused_drives_any_tips/", "subreddit_subscribers": 665682, "created_utc": 1673613531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I made a video a couple years ago and was looking for ways to recover it.\nIt was taken down because of copyright infringement. I still have acess to the link and the title and everything, I'm just looking for a way to download it or even just watch it at least.\n\nI've tried the Wayback Machine and sadly the only archives that exist it's when the video was already deleted.\n\nI would appreciate any help and advice", "author_fullname": "t2_zk9kw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for ways to download deleted youtube video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10asxz7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673611616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made a video a couple years ago and was looking for ways to recover it.\nIt was taken down because of copyright infringement. I still have acess to the link and the title and everything, I&amp;#39;m just looking for a way to download it or even just watch it at least.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried the Wayback Machine and sadly the only archives that exist it&amp;#39;s when the video was already deleted.&lt;/p&gt;\n\n&lt;p&gt;I would appreciate any help and advice&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10asxz7", "is_robot_indexable": true, "report_reasons": null, "author": "Thomson5", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10asxz7/looking_for_ways_to_download_deleted_youtube_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10asxz7/looking_for_ways_to_download_deleted_youtube_video/", "subreddit_subscribers": 665682, "created_utc": 1673611616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I forgot to mention, it would need to be in bulk. Not manually changing as I have hundreds of videos. I have some videos that were bulk downloaded from a website, but in the order of newest to oldest, meaning the most recently posted videos are the oldest files and the first posted videos are the newest files. This means that if I want to bulk download again in the future, the order of files will be (newest, oldest, future) which is completely out of order chronologically. Is there a way for me to do a simple, mass metadata edit (it doesn't matter what the date or time is exactly), to where the current order of oldest to newest download gets switched from newest to oldest? Ex, file that has download time of 10:00pm, and another that has a download time of 10:01pm, get switched to \\[time A\\] and \\[time B which is anytime after time A\\]. Some sort of tool would do something along the lines of (in computer language): Switch file A to X time. Switch file B to X + 1 time. Switch file C to X + 2 time (with letters corresponding to the order in which the files are being changed, not just editing them in a random order like A, D, Z, B, C, etc.)", "author_fullname": "t2_7ht8cct", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Changing metadata on mp4 files to where the current order of \"oldest to newest\" date downloaded/created is changed to \"newest to oldest\", is it possible? Simply changing the sort from new to old is not enough.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10astwn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673611262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I forgot to mention, it would need to be in bulk. Not manually changing as I have hundreds of videos. I have some videos that were bulk downloaded from a website, but in the order of newest to oldest, meaning the most recently posted videos are the oldest files and the first posted videos are the newest files. This means that if I want to bulk download again in the future, the order of files will be (newest, oldest, future) which is completely out of order chronologically. Is there a way for me to do a simple, mass metadata edit (it doesn&amp;#39;t matter what the date or time is exactly), to where the current order of oldest to newest download gets switched from newest to oldest? Ex, file that has download time of 10:00pm, and another that has a download time of 10:01pm, get switched to [time A] and [time B which is anytime after time A]. Some sort of tool would do something along the lines of (in computer language): Switch file A to X time. Switch file B to X + 1 time. Switch file C to X + 2 time (with letters corresponding to the order in which the files are being changed, not just editing them in a random order like A, D, Z, B, C, etc.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10astwn", "is_robot_indexable": true, "report_reasons": null, "author": "GamingDragon27", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10astwn/changing_metadata_on_mp4_files_to_where_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10astwn/changing_metadata_on_mp4_files_to_where_the/", "subreddit_subscribers": 665682, "created_utc": 1673611262.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi \n\ni got a new setup and can\u2019t decide wich ssd to buy\n\ni have now 4 HDDS and 1 sata 860 evo ssd 250gb\n\n&amp;#x200B;\n\nthe three in the title are all the same price at 80 euro\n\nor the 980 at 75 and the 980 pro at 115 euro\n\n&amp;#x200B;\n\ni dont play alot i want just something reliable,\n\n&amp;#x200B;\n\nthe 860 evo 250gb 25to total and 9543 hours of use total\n\nit has only 4gb left now\n\nthanks", "author_fullname": "t2_mw0q8zc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Samsung 980 vs WD SN770 vs crucial P3 plus at 80 euro", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10asp6x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673610847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;/p&gt;\n\n&lt;p&gt;i got a new setup and can\u2019t decide wich ssd to buy&lt;/p&gt;\n\n&lt;p&gt;i have now 4 HDDS and 1 sata 860 evo ssd 250gb&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;the three in the title are all the same price at 80 euro&lt;/p&gt;\n\n&lt;p&gt;or the 980 at 75 and the 980 pro at 115 euro&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;i dont play alot i want just something reliable,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;the 860 evo 250gb 25to total and 9543 hours of use total&lt;/p&gt;\n\n&lt;p&gt;it has only 4gb left now&lt;/p&gt;\n\n&lt;p&gt;thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10asp6x", "is_robot_indexable": true, "report_reasons": null, "author": "fakesupplydrop", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10asp6x/samsung_980_vs_wd_sn770_vs_crucial_p3_plus_at_80/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10asp6x/samsung_980_vs_wd_sn770_vs_crucial_p3_plus_at_80/", "subreddit_subscribers": 665682, "created_utc": 1673610847.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "(I hope this question is ok here) \n\nFiguring you all have experience with failing drives, my question is 2 fold -\n\nAt what point do you pull a drive? I am running a NAS with 8 x 8TB drives. One reports 16 bad sectors, all in August, no change since then, still 16. Another drive shows 135 \"Drive Reconnection count\" all from October/November. Regardless of warranty, would you pull either/both drives and replace, or wait for some higher threshold? \n\nAnd, if under warranty, how many bad sectors before you'd return under warranty? How many 'reconnects'? Do the manufacturers have a minimum on these numbers? e.g. 'under xxx bad sectors is still a good drive'? \n\nThe NAS shows both drives as Healthy. As if these numbers aren't yet concerning.", "author_fullname": "t2_3qou2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manufacturer criteria for returns?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10asfc2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673609899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(I hope this question is ok here) &lt;/p&gt;\n\n&lt;p&gt;Figuring you all have experience with failing drives, my question is 2 fold -&lt;/p&gt;\n\n&lt;p&gt;At what point do you pull a drive? I am running a NAS with 8 x 8TB drives. One reports 16 bad sectors, all in August, no change since then, still 16. Another drive shows 135 &amp;quot;Drive Reconnection count&amp;quot; all from October/November. Regardless of warranty, would you pull either/both drives and replace, or wait for some higher threshold? &lt;/p&gt;\n\n&lt;p&gt;And, if under warranty, how many bad sectors before you&amp;#39;d return under warranty? How many &amp;#39;reconnects&amp;#39;? Do the manufacturers have a minimum on these numbers? e.g. &amp;#39;under xxx bad sectors is still a good drive&amp;#39;? &lt;/p&gt;\n\n&lt;p&gt;The NAS shows both drives as Healthy. As if these numbers aren&amp;#39;t yet concerning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10asfc2", "is_robot_indexable": true, "report_reasons": null, "author": "joetaxpayer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10asfc2/manufacturer_criteria_for_returns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10asfc2/manufacturer_criteria_for_returns/", "subreddit_subscribers": 665682, "created_utc": 1673609899.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm just thinking here. In a RAID 5 array of four drives, the data is split across each drive, and the data will be corrupted or lost if two separate drives fail at the same time, right? So in theory, all those spare drives I've got sitting around that still work but got taken out for being too small should each individually just have the equivalent of corrupted data if I just accessed one on its own, right? Am I thinking right or is there something I'm missing?\n\nI'd test this myself but at the moment I don't have any spare drive slots or enclosures to put the drive in", "author_fullname": "t2_qf6hcoj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the data on one single drive meaningless without the rest of the RAID array?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10aqank", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673602098.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just thinking here. In a RAID 5 array of four drives, the data is split across each drive, and the data will be corrupted or lost if two separate drives fail at the same time, right? So in theory, all those spare drives I&amp;#39;ve got sitting around that still work but got taken out for being too small should each individually just have the equivalent of corrupted data if I just accessed one on its own, right? Am I thinking right or is there something I&amp;#39;m missing?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d test this myself but at the moment I don&amp;#39;t have any spare drive slots or enclosures to put the drive in&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "18TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10aqank", "is_robot_indexable": true, "report_reasons": null, "author": "ian9921", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10aqank/is_the_data_on_one_single_drive_meaningless/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10aqank/is_the_data_on_one_single_drive_meaningless/", "subreddit_subscribers": 665682, "created_utc": 1673602098.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Long time data hoarder (though I usually refer to myself as digital hoarder), but new to the group and not a NAS/RAID/Server expert, though with decent computer knowledge.\n\nI currently have 2 EOL Lenovo NAS (PX4-400D with 4x4TB and IX4-300D with 4x3TB) but would like to \"consolidate\" with truly large drives. According to this list below, the biggest drive they list is 6TB\n\n[https://download.lenovo.com/lenovoemc/eu/en/app/answers/detail/a\\_id/33754.html](https://download.lenovo.com/lenovoemc/eu/en/app/answers/detail/a_id/33754.html)\n\nI know the NAS systems are old and EOL, but they are just local storage of downloads and videos without installed apps or services that reach outside. Usually 1 user accessing the drive, sometimes 2. The data is not \"super precious\" and while a loss would hurt, it wouldn't ruin me. I am hoping to just make one the \"mother of all data\" throwing 4x12TB or something like that at the NAS in RAID-5.\n\nQ1: Is 6TB really the limit, or is this just what was available at the time the list was release (2015). Does anyone run either of them with bigger drives than that?\n\nQ2: Can I buy new drives in stages? For example get 2x12TB running RAID-1 (I know it will only have 12TB usable), then add another 12TB and convert the RAID-1 to RAID-5 to have 24TB usable. Then add another 12TB after that to get to 36TB? This is mainly because of my available funds, because they currently run at about $200 a piece. I understand that generally you can convert RAID-1 to RAID-5 but wondering about these older systems.\n\nQ3: They currently use dedicated NAS drives (WD Red), could I use Enterprise or Surveillance disks like EXOS if they are cheaper or is this risky? I understand I can generally run whatever I want but even after googling, I am not sure if the difference between the drives should worry me for this setup\n\nQ4: Seagate Ironwolf or WD Red?", "author_fullname": "t2_5ai3u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lenovo EMC questions about max drive size", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a9cyw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673554366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long time data hoarder (though I usually refer to myself as digital hoarder), but new to the group and not a NAS/RAID/Server expert, though with decent computer knowledge.&lt;/p&gt;\n\n&lt;p&gt;I currently have 2 EOL Lenovo NAS (PX4-400D with 4x4TB and IX4-300D with 4x3TB) but would like to &amp;quot;consolidate&amp;quot; with truly large drives. According to this list below, the biggest drive they list is 6TB&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://download.lenovo.com/lenovoemc/eu/en/app/answers/detail/a_id/33754.html\"&gt;https://download.lenovo.com/lenovoemc/eu/en/app/answers/detail/a_id/33754.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I know the NAS systems are old and EOL, but they are just local storage of downloads and videos without installed apps or services that reach outside. Usually 1 user accessing the drive, sometimes 2. The data is not &amp;quot;super precious&amp;quot; and while a loss would hurt, it wouldn&amp;#39;t ruin me. I am hoping to just make one the &amp;quot;mother of all data&amp;quot; throwing 4x12TB or something like that at the NAS in RAID-5.&lt;/p&gt;\n\n&lt;p&gt;Q1: Is 6TB really the limit, or is this just what was available at the time the list was release (2015). Does anyone run either of them with bigger drives than that?&lt;/p&gt;\n\n&lt;p&gt;Q2: Can I buy new drives in stages? For example get 2x12TB running RAID-1 (I know it will only have 12TB usable), then add another 12TB and convert the RAID-1 to RAID-5 to have 24TB usable. Then add another 12TB after that to get to 36TB? This is mainly because of my available funds, because they currently run at about $200 a piece. I understand that generally you can convert RAID-1 to RAID-5 but wondering about these older systems.&lt;/p&gt;\n\n&lt;p&gt;Q3: They currently use dedicated NAS drives (WD Red), could I use Enterprise or Surveillance disks like EXOS if they are cheaper or is this risky? I understand I can generally run whatever I want but even after googling, I am not sure if the difference between the drives should worry me for this setup&lt;/p&gt;\n\n&lt;p&gt;Q4: Seagate Ironwolf or WD Red?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a9cyw", "is_robot_indexable": true, "report_reasons": null, "author": "saldridge", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a9cyw/lenovo_emc_questions_about_max_drive_size/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a9cyw/lenovo_emc_questions_about_max_drive_size/", "subreddit_subscribers": 665682, "created_utc": 1673554366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, posted this in the QNAP sub but thought it would be worth trying here as well\n\nHave a 4 bay QNAP 453BT3 connected to my Mac via thunderbolt 3. Have just upgraded it to 4 x 16tb ironwolfs and want to set it up a little differently to my previous config as I wasnt happy with the speeds I was getting previously with RAID 5, thick volume (310 write, 580 read).\n\nI was hoping Id see a bump in speed by going RAID 10, static volume, but its about the same as before (280 write, 530 read).\n\nWould be very open to any other tips as to how to get a better speed, been hoping for something closer to 600 on the write. Raid 0 is out of the question because Id like some form of protection against a drive failure and don\u2019t mind sacrificing the storage to go with RAID 10 if it ended up being a fast option.\n\nCan try and find a 10Gbe dock to give that a go, but the thunderbolt 3 connection is managing 550 on the write so it\u2019s clearly capable of that speed.\n\nFinally Im also down to try some SSD QTier or Caching but from what I\u2019ve read it doesn\u2019t sound like that\u2019s going to give me a big enough bump in speed over what I\u2019m seeing currently.\n\nAny help or hot tips about what I could be doing wrong would be very very appreciated\nThanks!", "author_fullname": "t2_248eq7zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Poor RAID speeds Thunderbolt 3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a8zsk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673556335.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673553536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, posted this in the QNAP sub but thought it would be worth trying here as well&lt;/p&gt;\n\n&lt;p&gt;Have a 4 bay QNAP 453BT3 connected to my Mac via thunderbolt 3. Have just upgraded it to 4 x 16tb ironwolfs and want to set it up a little differently to my previous config as I wasnt happy with the speeds I was getting previously with RAID 5, thick volume (310 write, 580 read).&lt;/p&gt;\n\n&lt;p&gt;I was hoping Id see a bump in speed by going RAID 10, static volume, but its about the same as before (280 write, 530 read).&lt;/p&gt;\n\n&lt;p&gt;Would be very open to any other tips as to how to get a better speed, been hoping for something closer to 600 on the write. Raid 0 is out of the question because Id like some form of protection against a drive failure and don\u2019t mind sacrificing the storage to go with RAID 10 if it ended up being a fast option.&lt;/p&gt;\n\n&lt;p&gt;Can try and find a 10Gbe dock to give that a go, but the thunderbolt 3 connection is managing 550 on the write so it\u2019s clearly capable of that speed.&lt;/p&gt;\n\n&lt;p&gt;Finally Im also down to try some SSD QTier or Caching but from what I\u2019ve read it doesn\u2019t sound like that\u2019s going to give me a big enough bump in speed over what I\u2019m seeing currently.&lt;/p&gt;\n\n&lt;p&gt;Any help or hot tips about what I could be doing wrong would be very very appreciated\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a8zsk", "is_robot_indexable": true, "report_reasons": null, "author": "doco32", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a8zsk/poor_raid_speeds_thunderbolt_3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a8zsk/poor_raid_speeds_thunderbolt_3/", "subreddit_subscribers": 665682, "created_utc": 1673553536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been trying to get myself to use the singlefile addon along with recoll to search them. That seems somewhat future proof in terms of the pages being readable and searchable in the future but I wanted to see if others perhaps had a better workflow/ideas?\n\nOne additional thing, dead links. SingleFile has a GREAT feature to save bookmarked pages, I have backups of old bookmark files but have discovered (duh i guess) there are many dead links so was wondering if there was a way to automate replacing of deadlinks with wayback machine links from the internet archive or perhaps google cache?", "author_fullname": "t2_f5t3js0e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you archive and search webpages?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10a3tme", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673541025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been trying to get myself to use the singlefile addon along with recoll to search them. That seems somewhat future proof in terms of the pages being readable and searchable in the future but I wanted to see if others perhaps had a better workflow/ideas?&lt;/p&gt;\n\n&lt;p&gt;One additional thing, dead links. SingleFile has a GREAT feature to save bookmarked pages, I have backups of old bookmark files but have discovered (duh i guess) there are many dead links so was wondering if there was a way to automate replacing of deadlinks with wayback machine links from the internet archive or perhaps google cache?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10a3tme", "is_robot_indexable": true, "report_reasons": null, "author": "AlfieMcLuvin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10a3tme/how_do_you_archive_and_search_webpages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10a3tme/how_do_you_archive_and_search_webpages/", "subreddit_subscribers": 665682, "created_utc": 1673541025.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}