{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Do you want to learn Data Engineering? In 2023, we start another iteration of Data Engineering Zoomcamp. It's a free, practical, 10-week long course about the main concepts in Data Engineering.\u00a0  \n\n\nJoin us to learn about:\n\n* Docker, Terraform and GCP\n* Orchestration with Prefect\n* Creating a data warehouse with BigQuery\n* Analytics engineering and dbt\n* Batch processing and Spark\n* Stream processing and Kafka\n\nIt starts on the 16th of January, 2023.\u00a0 \n\nSign up here: [https://github.com/DataTalksClub/data-engineering-zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)", "author_fullname": "t2_ipugc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Data engineering bootcamp - Data Engineering Zoomcamp - starts in 10 days", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104xlft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 250, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 250, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673020464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you want to learn Data Engineering? In 2023, we start another iteration of Data Engineering Zoomcamp. It&amp;#39;s a free, practical, 10-week long course about the main concepts in Data Engineering.\u00a0  &lt;/p&gt;\n\n&lt;p&gt;Join us to learn about:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Docker, Terraform and GCP&lt;/li&gt;\n&lt;li&gt;Orchestration with Prefect&lt;/li&gt;\n&lt;li&gt;Creating a data warehouse with BigQuery&lt;/li&gt;\n&lt;li&gt;Analytics engineering and dbt&lt;/li&gt;\n&lt;li&gt;Batch processing and Spark&lt;/li&gt;\n&lt;li&gt;Stream processing and Kafka&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It starts on the 16th of January, 2023.\u00a0 &lt;/p&gt;\n\n&lt;p&gt;Sign up here: &lt;a href=\"https://github.com/DataTalksClub/data-engineering-zoomcamp\"&gt;https://github.com/DataTalksClub/data-engineering-zoomcamp&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?auto=webp&amp;s=816cc84d06fc71a798a41729c8e59b67057a7a40", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e6c2088f704523b4111a08f9c68f79d1887dce1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=95bdc1082cd1b4f1284442c0ab1040df5fb7abee", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=166df298958f127071afac0a0bdce49711d6fe49", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=480667fc30b7edfb18a8b621ffcb16b3a90021f1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b2bc45b5c081e5724f5c73326b5ab510230aa36a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/jeISsDcmgQIazxfPYjw8ykdAr9lQfGEGsT1r2Ed5ecs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=99f073ffe961cbf90656031cd111d9f3ecafad3a", "width": 1080, "height": 540}], "variants": {}, "id": "b6oRDf9UWdoqcJ_iqebxKYsWb30ncp2wq3cvwnbvclo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "104xlft", "is_robot_indexable": true, "report_reasons": null, "author": "stolzen", "discussion_type": null, "num_comments": 58, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104xlft/free_data_engineering_bootcamp_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104xlft/free_data_engineering_bootcamp_data_engineering/", "subreddit_subscribers": 85442, "created_utc": 1673020464.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI am looking for a group or individuals interested in practicing Python and SQL coding focussed on data engineering interviews.\n\nPlease DM or comment below if anyone is interested to join or if there is already such group.\n\nEdit: Thanks for the responding guys. I have created a new WhatsApp group. DM me for the group link.", "author_fullname": "t2_7tah61j5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for leetcode partner.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105h68t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673084380.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673070223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I am looking for a group or individuals interested in practicing Python and SQL coding focussed on data engineering interviews.&lt;/p&gt;\n\n&lt;p&gt;Please DM or comment below if anyone is interested to join or if there is already such group.&lt;/p&gt;\n\n&lt;p&gt;Edit: Thanks for the responding guys. I have created a new WhatsApp group. DM me for the group link.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "105h68t", "is_robot_indexable": true, "report_reasons": null, "author": "Test_Known", "discussion_type": null, "num_comments": 91, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105h68t/looking_for_leetcode_partner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105h68t/looking_for_leetcode_partner/", "subreddit_subscribers": 85442, "created_utc": 1673070223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Need feedback from fellow data engineers on this blog:** \n\n# ETL testing \u2014 How to test your data pipelines the right way?\n\nIt is 2023! New data paradigms (or buzz words) like ELT, reverse ETL, EtLT, Data mesh, Data contracts, FinOps and modern data stack found their way into mainstream data conversations. Our data teams are still figuring out what is hype and what is not.\n\nThere may be 10 new paradigms tomorrow but some of the fundamental challenges in data engineering \u2014 like data quality \u2014 are still relevant and not solved completely (I don\u2019t think we ever will). The first step in improving data quality is to test changes to our data pipelines vigorously.\n\nLet us review the challenges involved in testing data pipelines effectively and how to build a well-rounded testing strategy for your organization.\n\n# Why is achieving data quality hard?\n\nIn software application development world, improving the quality of software meant rigorous testing. Similarly in data engineering, we need a comprehensive testing strategy to achieve high quality data in production.\n\n&gt;Most data teams are running against hard deadlines. So, data engineering culture is such that we end up building pipelines that serve data by the end of the week instead of incorporating all the best practices that are valuable in the long run.\n\n* In ETL testing, we compare huge volumes of data (say millions of records) often from different source systems. We are comparing transformed data that are a result of complex SQL queries or Spark jobs.\n* Not all data engineers (and the data leaders) are from software engineering background and are strong in SWE development principles and best practices.\n* Running automated suite of tests and automated deployment/release of data products is still not mainstream.\n\n&gt;ETL testing is a data-centric testing process. To effectively test our pipelines, we need production like data (in terms of volume, variety, and velocity).\n\n&amp;#x200B;\n\nGetting access to production like data is hard. Here is how data teams in different companies tackle the problem of getting the right data to test the data pipelines.\n\n# 1. Mock Data:\n\nPros: This approach is prevalently used by all of us data engineers because of ease of mock data creation and availability of synthetic data generation tools (such as Faker).\n\nCons: Mock data does not reflect the production data in terms of volume, variety or velocity.\n\n# 2. Sample Prod data to Test/Dev Env:\n\nPros: Easy to copy fraction of production data than to copy huge swathes of prod data.\n\nCons: Should use the right sampling strategy to ensure the sample reflects real world prod data. Tests that run successfully on sample prod data might fail on actual prod data because volume and variety is not guaranteed.\n\n# 3. Copy all of Prod data to Test Env:\n\nPros: Availability of real world production data for testing.\n\nCons: If prod contains PII data, it might lead to data privacy violations. If the prod data is constantly changing, then the copy of prod data in test/dev environment will become stale and needs to be constantly updated. Volume and variety guaranteed, but not velocity.\n\n# 4. Copy anonymized prod data to Test Env:\n\nPros: Availability of real world production data for testing. Compliance to all data privacy regulations.\n\nCons: Again, a constantly changing prod data means the data in test env becomes stale and needs to be refreshed often. PII anonymization needs to be run every time you copy data out of prod. Manually running anonymization steps every time and maintaining a long-running test data environment is error-prone and resource intensive.\n\n# 5. Using a data versioning tool to mirror prod data to Dev/Test Env:\n\nPros: Availability of real-world production data. Automated short-lived test environments that are available through git-like API.\n\nCons: Add a new tool to your existing data stack.\n\n&amp;#x200B;\n\n[Here](https://vinodhini-sd.medium.com/forget-about-the-new-data-trends-in-2023-d2756add3317) is the full blog and appreciate your feedback!", "author_fullname": "t2_msviuy1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forget about the new data trends in 2023! This fundamental data engineering challenge is still not solved.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10525tw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673031207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Need feedback from fellow data engineers on this blog:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;h1&gt;ETL testing \u2014 How to test your data pipelines the right way?&lt;/h1&gt;\n\n&lt;p&gt;It is 2023! New data paradigms (or buzz words) like ELT, reverse ETL, EtLT, Data mesh, Data contracts, FinOps and modern data stack found their way into mainstream data conversations. Our data teams are still figuring out what is hype and what is not.&lt;/p&gt;\n\n&lt;p&gt;There may be 10 new paradigms tomorrow but some of the fundamental challenges in data engineering \u2014 like data quality \u2014 are still relevant and not solved completely (I don\u2019t think we ever will). The first step in improving data quality is to test changes to our data pipelines vigorously.&lt;/p&gt;\n\n&lt;p&gt;Let us review the challenges involved in testing data pipelines effectively and how to build a well-rounded testing strategy for your organization.&lt;/p&gt;\n\n&lt;h1&gt;Why is achieving data quality hard?&lt;/h1&gt;\n\n&lt;p&gt;In software application development world, improving the quality of software meant rigorous testing. Similarly in data engineering, we need a comprehensive testing strategy to achieve high quality data in production.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Most data teams are running against hard deadlines. So, data engineering culture is such that we end up building pipelines that serve data by the end of the week instead of incorporating all the best practices that are valuable in the long run.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In ETL testing, we compare huge volumes of data (say millions of records) often from different source systems. We are comparing transformed data that are a result of complex SQL queries or Spark jobs.&lt;/li&gt;\n&lt;li&gt;Not all data engineers (and the data leaders) are from software engineering background and are strong in SWE development principles and best practices.&lt;/li&gt;\n&lt;li&gt;Running automated suite of tests and automated deployment/release of data products is still not mainstream.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;ETL testing is a data-centric testing process. To effectively test our pipelines, we need production like data (in terms of volume, variety, and velocity).&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Getting access to production like data is hard. Here is how data teams in different companies tackle the problem of getting the right data to test the data pipelines.&lt;/p&gt;\n\n&lt;h1&gt;1. Mock Data:&lt;/h1&gt;\n\n&lt;p&gt;Pros: This approach is prevalently used by all of us data engineers because of ease of mock data creation and availability of synthetic data generation tools (such as Faker).&lt;/p&gt;\n\n&lt;p&gt;Cons: Mock data does not reflect the production data in terms of volume, variety or velocity.&lt;/p&gt;\n\n&lt;h1&gt;2. Sample Prod data to Test/Dev Env:&lt;/h1&gt;\n\n&lt;p&gt;Pros: Easy to copy fraction of production data than to copy huge swathes of prod data.&lt;/p&gt;\n\n&lt;p&gt;Cons: Should use the right sampling strategy to ensure the sample reflects real world prod data. Tests that run successfully on sample prod data might fail on actual prod data because volume and variety is not guaranteed.&lt;/p&gt;\n\n&lt;h1&gt;3. Copy all of Prod data to Test Env:&lt;/h1&gt;\n\n&lt;p&gt;Pros: Availability of real world production data for testing.&lt;/p&gt;\n\n&lt;p&gt;Cons: If prod contains PII data, it might lead to data privacy violations. If the prod data is constantly changing, then the copy of prod data in test/dev environment will become stale and needs to be constantly updated. Volume and variety guaranteed, but not velocity.&lt;/p&gt;\n\n&lt;h1&gt;4. Copy anonymized prod data to Test Env:&lt;/h1&gt;\n\n&lt;p&gt;Pros: Availability of real world production data for testing. Compliance to all data privacy regulations.&lt;/p&gt;\n\n&lt;p&gt;Cons: Again, a constantly changing prod data means the data in test env becomes stale and needs to be refreshed often. PII anonymization needs to be run every time you copy data out of prod. Manually running anonymization steps every time and maintaining a long-running test data environment is error-prone and resource intensive.&lt;/p&gt;\n\n&lt;h1&gt;5. Using a data versioning tool to mirror prod data to Dev/Test Env:&lt;/h1&gt;\n\n&lt;p&gt;Pros: Availability of real-world production data. Automated short-lived test environments that are available through git-like API.&lt;/p&gt;\n\n&lt;p&gt;Cons: Add a new tool to your existing data stack.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://vinodhini-sd.medium.com/forget-about-the-new-data-trends-in-2023-d2756add3317\"&gt;Here&lt;/a&gt; is the full blog and appreciate your feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?auto=webp&amp;s=e5cbcb7cfa87dac95dba6b8ca6fe9316e6743aaa", "width": 1200, "height": 457}, "resolutions": [{"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a97a00584776b1c8d3857040195bc99d972c8e9c", "width": 108, "height": 41}, {"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db1c7466b344a3c6f90f8619927913ac7bfd49d9", "width": 216, "height": 82}, {"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=43ff0968a785930b735f52a684727234f0da8b9f", "width": 320, "height": 121}, {"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5a44bd5a70fefd17d7af0a6b7d94ff0ec4d51e0", "width": 640, "height": 243}, {"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a0b31b3c62c016354ce15cf8e754fce15787e482", "width": 960, "height": 365}, {"url": "https://external-preview.redd.it/dNEq0M2P6VLI1b4hZChmc7D6AABcSCaSZtXY2kEgYTE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc1b0446e9dcea6a2d74bdc0a16c6c5e85d2a7f2", "width": 1080, "height": 411}], "variants": {}, "id": "OO3_ymKJyfHeZYENRf5HYl6tOP--MMtRe2gGCKBfBgw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Applied Data &amp; ML Engineer | Developer Advocate", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10525tw", "is_robot_indexable": true, "report_reasons": null, "author": "vino_and_data", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10525tw/forget_about_the_new_data_trends_in_2023_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10525tw/forget_about_the_new_data_trends_in_2023_this/", "subreddit_subscribers": 85442, "created_utc": 1673031207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious on the split of approach here\n\n[View Poll](https://www.reddit.com/poll/1059l27)", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Handling surrogate keys in your facts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1059l27", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673048836.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious on the split of approach here&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1059l27\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1059l27", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1673308037024, "options": [{"text": "We generate a unique incremental in our dim and look it up when building our fact", "id": "20857241"}, {"text": "We don\u2019t use surrogate keys", "id": "20857242"}, {"text": "We generate a hash from source system keys in our dims, but still join the dims in when building facts to get the keys", "id": "20857243"}, {"text": "We generate a hash from source and do this both when generating the dims &amp; facts. We rely on post build integrity tests.", "id": "20857244"}, {"text": "Other (in the comments)", "id": "20857245"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 239, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1059l27/handling_surrogate_keys_in_your_facts/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1059l27/handling_surrogate_keys_in_your_facts/", "subreddit_subscribers": 85442, "created_utc": 1673048836.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nI am currently a computer science major about to go into my upper division classes next year and I recently came across this subreddit/topic this weekend. I currently do not know where I want to go with my degree atm; I was thinking backend dev/engineer but now as I learn more about this field I feel like this is something I would be interested going in too. Do you guys think  it's a good idea to pursue this field out of the gate or should I get familiar with more fields or topics before going into this. I've read it's common for people to go into data engineering after they've already started their career. My immediate goal is to secure a summer internship and I've noticed a lot of the internships in my area ask for more developer skills like: react, express, javascript, HTML... \n\nHonestly I have just been overwhelming myself over this winter break obsessing about the future and I have been to impatient on trying to learn one thing at a time.", "author_fullname": "t2_6gk40pxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pursing Data Engineering as a Computer Science Student", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105bgbj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673053578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I am currently a computer science major about to go into my upper division classes next year and I recently came across this subreddit/topic this weekend. I currently do not know where I want to go with my degree atm; I was thinking backend dev/engineer but now as I learn more about this field I feel like this is something I would be interested going in too. Do you guys think  it&amp;#39;s a good idea to pursue this field out of the gate or should I get familiar with more fields or topics before going into this. I&amp;#39;ve read it&amp;#39;s common for people to go into data engineering after they&amp;#39;ve already started their career. My immediate goal is to secure a summer internship and I&amp;#39;ve noticed a lot of the internships in my area ask for more developer skills like: react, express, javascript, HTML... &lt;/p&gt;\n\n&lt;p&gt;Honestly I have just been overwhelming myself over this winter break obsessing about the future and I have been to impatient on trying to learn one thing at a time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105bgbj", "is_robot_indexable": true, "report_reasons": null, "author": "hcazj", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105bgbj/pursing_data_engineering_as_a_computer_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105bgbj/pursing_data_engineering_as_a_computer_science/", "subreddit_subscribers": 85442, "created_utc": 1673053578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jp6tbwl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modern Polars: an extensive side-by-side comparison of Polars and Pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1050b8o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1673026851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kevinheavey.github.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://kevinheavey.github.io/modern-polars/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1050b8o", "is_robot_indexable": true, "report_reasons": null, "author": "caoimhin_o_h", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1050b8o/modern_polars_an_extensive_sidebyside_comparison/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://kevinheavey.github.io/modern-polars/", "subreddit_subscribers": 85442, "created_utc": 1673026851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone! As the new year starts, I'm motivated to improve my coding skills and become more proficient in Python. I've worked on a few data engineering projects at my workplace using Python, but I still feel relatively inexperienced and struggle to write efficient, best-practice code. To address this, I'm actively seeking resources to help me learn and grow as a coder. My goal is to work on 1-2 algorithms per day so that I can expand my knowledge and be better equipped to tackle any projects my team may have in the future. Something as useful e.g., a GitHub repository with a collection of algorithms for writing Python code more effectively for specific objectives would be greatly appreciated. Any recommendations for learning materials or resources would be much appreciated!\"", "author_fullname": "t2_u2710fo6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Algorithms for Python, Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105gzxa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673069679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! As the new year starts, I&amp;#39;m motivated to improve my coding skills and become more proficient in Python. I&amp;#39;ve worked on a few data engineering projects at my workplace using Python, but I still feel relatively inexperienced and struggle to write efficient, best-practice code. To address this, I&amp;#39;m actively seeking resources to help me learn and grow as a coder. My goal is to work on 1-2 algorithms per day so that I can expand my knowledge and be better equipped to tackle any projects my team may have in the future. Something as useful e.g., a GitHub repository with a collection of algorithms for writing Python code more effectively for specific objectives would be greatly appreciated. Any recommendations for learning materials or resources would be much appreciated!&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105gzxa", "is_robot_indexable": true, "report_reasons": null, "author": "dataterre", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105gzxa/algorithms_for_python_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105gzxa/algorithms_for_python_data_engineering/", "subreddit_subscribers": 85442, "created_utc": 1673069679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI'm learning data engineering and i know python and SQL .\nSo out of curiosity i got an idea to make a project.\n\nI want to collect the temperature of 10 cities for each hour every day for 30 days.\n\nSo, my project involves the below steps\n1. Request open weathermap api for temperature of 10 cities.\n2. Extract only temperatures,and city names from the received json data\n3. Write this data into a table in SQL server.\n\nI have developed a python code and I'm running it manually every hour.\n\nI just learnt some airflow basics and trying to schedule it.\n\nHow to run this whole project on cloud?\n\nIm kind of stuck.\nShould I create a database instance on cloud?\nI'm beginner to this field.", "author_fullname": "t2_apg4thop", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help in my personal project.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105162f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673028876.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m learning data engineering and i know python and SQL .\nSo out of curiosity i got an idea to make a project.&lt;/p&gt;\n\n&lt;p&gt;I want to collect the temperature of 10 cities for each hour every day for 30 days.&lt;/p&gt;\n\n&lt;p&gt;So, my project involves the below steps\n1. Request open weathermap api for temperature of 10 cities.\n2. Extract only temperatures,and city names from the received json data\n3. Write this data into a table in SQL server.&lt;/p&gt;\n\n&lt;p&gt;I have developed a python code and I&amp;#39;m running it manually every hour.&lt;/p&gt;\n\n&lt;p&gt;I just learnt some airflow basics and trying to schedule it.&lt;/p&gt;\n\n&lt;p&gt;How to run this whole project on cloud?&lt;/p&gt;\n\n&lt;p&gt;Im kind of stuck.\nShould I create a database instance on cloud?\nI&amp;#39;m beginner to this field.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105162f", "is_robot_indexable": true, "report_reasons": null, "author": "Tombraider2598", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105162f/need_help_in_my_personal_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105162f/need_help_in_my_personal_project/", "subreddit_subscribers": 85442, "created_utc": 1673028876.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI currently work for a startup (\\~50 people, 10s engineers) focusing on scientific research.\n\n**Context**:   \nDue to the nature of our work, and the fact that most coders weren't engineers, after our few years of existence we ended up in a state where we have built several services (mostly batch computations with a config file for the context) which mostly read and write in unreferenced parquet files all across our S3. Due to the lack of unified storage framework, some other teams use postgre, while others use custom solutions.  \nThe thing is, our data now tends to grow fast (TBs) and it becomes cumbersome (impossible ?) to aggregate several parquets to make joins from different services in our code. Moreover, as the company expands, we would like to build transformations and analytics (you can think of it as datamarts) on top of all the raw data we generate.\n\n**The problem**:  \nSo my goal is to create a unique (or at least by default) datalake to unify our datasets, make joins between them without having to know the physical layout of the files, etc... I'm sure you get the idea.\n\nOne of the candidates solution is to use Iceberg (or equivalent like delta lake) for both raw and future transformed data.   \nI have read a bunch of articles on how it works and how to use it and I have come to this conclusion :   \nSince we are fully on AWS, EMR (spark) seems very good to handle both transformations and explorations of dataset with studio.  \nHowever, how would one handle simple transactions (reads, writes, updates) in our services' code ? To make it clearer, we currently do `df.to_parquet(\"s3://xxx\")` or `df.write_in_postgre(table)` to produce our raw data, would it be possible/a good pattern to translate to something like `df.write_in_iceberg` and use EMR cluster capabilities for that ? Is it possible to spawn cluster when a query is done or do we need to manually ask for the start of one ? (It is out of the question to integrate spark code in our services)  \nI have also seen that Athena could be a candidate but two things scares me :\n\n1. People are complaining a lot about random bugs/failures/wait queues\n2. Paying on scanned data frightens me a bit\n\nWhat do you think of such an architecture and choice of services ? Maybe they can co-exist ?\n\nI hope that where I struggle is clear and that the problem is correctly explained.  \nApologies in advance if this isn't the right subreddit to ask and if I should go to r/aws please tell me :)  \nThank you", "author_fullname": "t2_7wiej82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to compute queries aiming at Iceberg tables ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1050aur", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673026825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I currently work for a startup (~50 people, 10s engineers) focusing on scientific research.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;:&lt;br/&gt;\nDue to the nature of our work, and the fact that most coders weren&amp;#39;t engineers, after our few years of existence we ended up in a state where we have built several services (mostly batch computations with a config file for the context) which mostly read and write in unreferenced parquet files all across our S3. Due to the lack of unified storage framework, some other teams use postgre, while others use custom solutions.&lt;br/&gt;\nThe thing is, our data now tends to grow fast (TBs) and it becomes cumbersome (impossible ?) to aggregate several parquets to make joins from different services in our code. Moreover, as the company expands, we would like to build transformations and analytics (you can think of it as datamarts) on top of all the raw data we generate.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;:&lt;br/&gt;\nSo my goal is to create a unique (or at least by default) datalake to unify our datasets, make joins between them without having to know the physical layout of the files, etc... I&amp;#39;m sure you get the idea.&lt;/p&gt;\n\n&lt;p&gt;One of the candidates solution is to use Iceberg (or equivalent like delta lake) for both raw and future transformed data.&lt;br/&gt;\nI have read a bunch of articles on how it works and how to use it and I have come to this conclusion :&lt;br/&gt;\nSince we are fully on AWS, EMR (spark) seems very good to handle both transformations and explorations of dataset with studio.&lt;br/&gt;\nHowever, how would one handle simple transactions (reads, writes, updates) in our services&amp;#39; code ? To make it clearer, we currently do &lt;code&gt;df.to_parquet(&amp;quot;s3://xxx&amp;quot;)&lt;/code&gt; or &lt;code&gt;df.write_in_postgre(table)&lt;/code&gt; to produce our raw data, would it be possible/a good pattern to translate to something like &lt;code&gt;df.write_in_iceberg&lt;/code&gt; and use EMR cluster capabilities for that ? Is it possible to spawn cluster when a query is done or do we need to manually ask for the start of one ? (It is out of the question to integrate spark code in our services)&lt;br/&gt;\nI have also seen that Athena could be a candidate but two things scares me :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;People are complaining a lot about random bugs/failures/wait queues&lt;/li&gt;\n&lt;li&gt;Paying on scanned data frightens me a bit&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What do you think of such an architecture and choice of services ? Maybe they can co-exist ?&lt;/p&gt;\n\n&lt;p&gt;I hope that where I struggle is clear and that the problem is correctly explained.&lt;br/&gt;\nApologies in advance if this isn&amp;#39;t the right subreddit to ask and if I should go to &lt;a href=\"/r/aws\"&gt;r/aws&lt;/a&gt; please tell me :)&lt;br/&gt;\nThank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1050aur", "is_robot_indexable": true, "report_reasons": null, "author": "otineb_", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1050aur/where_to_compute_queries_aiming_at_iceberg_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1050aur/where_to_compute_queries_aiming_at_iceberg_tables/", "subreddit_subscribers": 85442, "created_utc": 1673026825.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone know if this is functional using any sort of cloud DBMS? I want to have an integer type attribute increment by one every time its associated ID is accessed in a select query so my org can award performance based comp to people that provide data. Would blockchain be able to handle something like that?\n\nAn example of what I mean is suppose Ronald (an associate) added a record for a big mac recipe to McDonalds\u2019 recipe table. One person gets the recipe from that table using a select query. As a result of the select query, that big mac recipe has an int attribute named \u2018uses\u2019 that increases by 1. Then we can query the sum of uses by associate and award the top contributors (in terms of how many times their contributions are accessed) extra pay. \n\nAnyone have any ideas? Apparently logging is a solution, as is manually writing it in the backend (but wouldn\u2019t work in the case of a PowerBI dashboard, which will be a main place data is consumed).", "author_fullname": "t2_8rmuclrh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trigger to update on select query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105ednv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673061701.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone know if this is functional using any sort of cloud DBMS? I want to have an integer type attribute increment by one every time its associated ID is accessed in a select query so my org can award performance based comp to people that provide data. Would blockchain be able to handle something like that?&lt;/p&gt;\n\n&lt;p&gt;An example of what I mean is suppose Ronald (an associate) added a record for a big mac recipe to McDonalds\u2019 recipe table. One person gets the recipe from that table using a select query. As a result of the select query, that big mac recipe has an int attribute named \u2018uses\u2019 that increases by 1. Then we can query the sum of uses by associate and award the top contributors (in terms of how many times their contributions are accessed) extra pay. &lt;/p&gt;\n\n&lt;p&gt;Anyone have any ideas? Apparently logging is a solution, as is manually writing it in the backend (but wouldn\u2019t work in the case of a PowerBI dashboard, which will be a main place data is consumed).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105ednv", "is_robot_indexable": true, "report_reasons": null, "author": "Own_Neighborhood_231", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105ednv/trigger_to_update_on_select_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105ednv/trigger_to_update_on_select_query/", "subreddit_subscribers": 85442, "created_utc": 1673061701.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The article [\u2018Parse, Don\u2019t Validate\u2019](https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/) was written by Alexis King in the context of Haskell, but it\u2019s also been mentioned in [this article](https://stianlagstad.no/2022/05/parse-dont-validate-python-edition/) in the context of Python.\n\nI mainly work with **PySpark** and, apart from validation checks, there isn\u2019t really any other way to vouch for the correctness of a table\u2019s data. I was wondering if anyone might know about the notion of \u2018parse, don\u2019t validate\u2019 as applied to a dataframe context.\n\nFor me at least, one application of having a Spark dataframe take on a specific custom type is in functions that become a lot narrower in their usage space, which should hopefully make it more difficult for people to misuse them.\n\nFor instance, maybe there is a function that groups by and aggregates:\n\n    from typing import List\n    \n    def group_by_and_count(\n        input_df: DataFrame,\n        grouping_cols: List[str]\n    ) -&gt; int:\n        return (\n            input_df\n            .groupBy(*grouping_cols)\n            .count()\n        )\n\nThis is a generic function, but perhaps the problem is that it is too generic and allows people to dump whatever they want in there, with no real checks on what goes in the `grouping_cols` parameter.\n\nWhat happens if I, for example, need to return a specific aggregated dataframe that is fed into Tableau and used by data analysts for reporting, where the grouping columns are fixed? I wouldn\u2019t want *any* column to be used (e.g. both country and ice cream flavour are strings, but one would likely be more relevant to stakeholders than the other in Tableau).\n\nIs it even possible to write `group_by_and_count` such that it accepts a set of grouping columns which are of a certain type (and here, something more specific than `str` or `int` \u2013 something like `Country` or `HttpErrorStatus` respectively)? (And, again, I work with PySpark and not Scala Spark.)\n\nI reckon this is a tough problem, since this isn\u2019t usually how we think of dataframes, and validations that we apply involve running queries against a dataframe\u2019s data. But, given that we often struggle with data quality and generally discover poor data after the fact (i.e. through a query), perhaps this helps us while we\u2019re building our workflows instead of after they\u2019re put in production.\n\nOr I could just be overthinking it and there\u2019s already a simpler solution out there. (In which case I\u2019m happy to learn more about it!)", "author_fullname": "t2_a8joqf7z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any way to parse and not validate in PySpark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104w6hi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673027413.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673016974.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The article &lt;a href=\"https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/\"&gt;\u2018Parse, Don\u2019t Validate\u2019&lt;/a&gt; was written by Alexis King in the context of Haskell, but it\u2019s also been mentioned in &lt;a href=\"https://stianlagstad.no/2022/05/parse-dont-validate-python-edition/\"&gt;this article&lt;/a&gt; in the context of Python.&lt;/p&gt;\n\n&lt;p&gt;I mainly work with &lt;strong&gt;PySpark&lt;/strong&gt; and, apart from validation checks, there isn\u2019t really any other way to vouch for the correctness of a table\u2019s data. I was wondering if anyone might know about the notion of \u2018parse, don\u2019t validate\u2019 as applied to a dataframe context.&lt;/p&gt;\n\n&lt;p&gt;For me at least, one application of having a Spark dataframe take on a specific custom type is in functions that become a lot narrower in their usage space, which should hopefully make it more difficult for people to misuse them.&lt;/p&gt;\n\n&lt;p&gt;For instance, maybe there is a function that groups by and aggregates:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from typing import List\n\ndef group_by_and_count(\n    input_df: DataFrame,\n    grouping_cols: List[str]\n) -&amp;gt; int:\n    return (\n        input_df\n        .groupBy(*grouping_cols)\n        .count()\n    )\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This is a generic function, but perhaps the problem is that it is too generic and allows people to dump whatever they want in there, with no real checks on what goes in the &lt;code&gt;grouping_cols&lt;/code&gt; parameter.&lt;/p&gt;\n\n&lt;p&gt;What happens if I, for example, need to return a specific aggregated dataframe that is fed into Tableau and used by data analysts for reporting, where the grouping columns are fixed? I wouldn\u2019t want &lt;em&gt;any&lt;/em&gt; column to be used (e.g. both country and ice cream flavour are strings, but one would likely be more relevant to stakeholders than the other in Tableau).&lt;/p&gt;\n\n&lt;p&gt;Is it even possible to write &lt;code&gt;group_by_and_count&lt;/code&gt; such that it accepts a set of grouping columns which are of a certain type (and here, something more specific than &lt;code&gt;str&lt;/code&gt; or &lt;code&gt;int&lt;/code&gt; \u2013 something like &lt;code&gt;Country&lt;/code&gt; or &lt;code&gt;HttpErrorStatus&lt;/code&gt; respectively)? (And, again, I work with PySpark and not Scala Spark.)&lt;/p&gt;\n\n&lt;p&gt;I reckon this is a tough problem, since this isn\u2019t usually how we think of dataframes, and validations that we apply involve running queries against a dataframe\u2019s data. But, given that we often struggle with data quality and generally discover poor data after the fact (i.e. through a query), perhaps this helps us while we\u2019re building our workflows instead of after they\u2019re put in production.&lt;/p&gt;\n\n&lt;p&gt;Or I could just be overthinking it and there\u2019s already a simpler solution out there. (In which case I\u2019m happy to learn more about it!)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "104w6hi", "is_robot_indexable": true, "report_reasons": null, "author": "haskathon", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104w6hi/is_there_any_way_to_parse_and_not_validate_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104w6hi/is_there_any_way_to_parse_and_not_validate_in/", "subreddit_subscribers": 85442, "created_utc": 1673016974.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm curious about data security as it relates to source control.\n\nCurrently we are running databricks on a dedicated VM for data security reasons and want to integrate it with git (Azure DevOps) but there are concerns around having data checked in via notebooks or inappropriate commits which could lead to data leaking out of the system.\n\nShould this be a concern at all or have any of you done anything to mitigate this risk?", "author_fullname": "t2_lmtu1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Security around Databricks and Azure DevOps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104vmb7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673015518.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious about data security as it relates to source control.&lt;/p&gt;\n\n&lt;p&gt;Currently we are running databricks on a dedicated VM for data security reasons and want to integrate it with git (Azure DevOps) but there are concerns around having data checked in via notebooks or inappropriate commits which could lead to data leaking out of the system.&lt;/p&gt;\n\n&lt;p&gt;Should this be a concern at all or have any of you done anything to mitigate this risk?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "104vmb7", "is_robot_indexable": true, "report_reasons": null, "author": "IMisterDogI", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104vmb7/data_security_around_databricks_and_azure_devops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104vmb7/data_security_around_databricks_and_azure_devops/", "subreddit_subscribers": 85442, "created_utc": 1673015518.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI've been building data pipelines for a while now. In contrast to the availability of examples of code design patterns, &amp; data modeling techniques, there are few to none on data flow design patterns. In my experience building pipelines, using the appropriate data flow patterns increases feature delivery speed, decreases toil during pipeline failures, and builds trust with stakeholders.\n\nWith that in mind, I wrote an article that goes over the most commonly used data flow design patterns, what they do, when to use them, and, more importantly, when not to use them. It's aimed to give an overview of the typical data flow patterns and guidelines for choosing the appropriate one for your use case.\n\n[https://www.startdataengineering.com/post/design-patterns/](https://www.startdataengineering.com/post/design-patterns/)\n\nI'd love to hear about any patterns that I have missed. Any feedback is appreciated. I hope this helps someone :)", "author_fullname": "t2_5srxspj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline design patterns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_105o50o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673095202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been building data pipelines for a while now. In contrast to the availability of examples of code design patterns, &amp;amp; data modeling techniques, there are few to none on data flow design patterns. In my experience building pipelines, using the appropriate data flow patterns increases feature delivery speed, decreases toil during pipeline failures, and builds trust with stakeholders.&lt;/p&gt;\n\n&lt;p&gt;With that in mind, I wrote an article that goes over the most commonly used data flow design patterns, what they do, when to use them, and, more importantly, when not to use them. It&amp;#39;s aimed to give an overview of the typical data flow patterns and guidelines for choosing the appropriate one for your use case.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.startdataengineering.com/post/design-patterns/\"&gt;https://www.startdataengineering.com/post/design-patterns/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear about any patterns that I have missed. Any feedback is appreciated. I hope this helps someone :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?auto=webp&amp;s=07b9ceb836c7c7084c6c77d42d60531a450215a2", "width": 1271, "height": 714}, "resolutions": [{"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=466a76c8a5472f7d08537937187bbac1b6aabc95", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6315f77e68ac7f50e47af70eb7c2c066c9eb640a", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ec53a6bfc4c92b4fd99d7382ef4c31ec411f8db", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=19347f6feaf42a0f5606d336dfbf732e35c62bf3", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=483f159de78a082763f92e6c84748d6f793af2ec", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6036328d1ff9c14bb619d2010d1058d33aff423", "width": 1080, "height": 606}], "variants": {}, "id": "1NhFR4isyIRe_ddLlHMNbBEPJQqFv_etsmRqZEKagZc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "105o50o", "is_robot_indexable": true, "report_reasons": null, "author": "joseph_machado", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105o50o/data_pipeline_design_patterns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105o50o/data_pipeline_design_patterns/", "subreddit_subscribers": 85442, "created_utc": 1673095202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are looking to use snowflake as our lake house and follow the medallion architecture (Bronze -&gt; Silver &gt; Gold).  The ingestion into bronze schema will happen by either fivertran or airbyte. I 'd like to understand the options in transforming the tables further into Silver and Gold curated zones. \n\nIs it a standard practice to use snowpark, with python scripts for transformations.  Does snowpark comes with a scheduler, to trigger the transformation jobs ?\n\nAlso dbt-Snowflake seems like an established pattern. Can anybody share your feedback and gotchas in this approach. \n\nAny other patterns ? \n\nThanks for checking this.", "author_fullname": "t2_4kvf695m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestions on data transformation options in Snowflake platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1058lpu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673046435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are looking to use snowflake as our lake house and follow the medallion architecture (Bronze -&amp;gt; Silver &amp;gt; Gold).  The ingestion into bronze schema will happen by either fivertran or airbyte. I &amp;#39;d like to understand the options in transforming the tables further into Silver and Gold curated zones. &lt;/p&gt;\n\n&lt;p&gt;Is it a standard practice to use snowpark, with python scripts for transformations.  Does snowpark comes with a scheduler, to trigger the transformation jobs ?&lt;/p&gt;\n\n&lt;p&gt;Also dbt-Snowflake seems like an established pattern. Can anybody share your feedback and gotchas in this approach. &lt;/p&gt;\n\n&lt;p&gt;Any other patterns ? &lt;/p&gt;\n\n&lt;p&gt;Thanks for checking this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1058lpu", "is_robot_indexable": true, "report_reasons": null, "author": "rasviz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1058lpu/need_suggestions_on_data_transformation_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1058lpu/need_suggestions_on_data_transformation_options/", "subreddit_subscribers": 85442, "created_utc": 1673046435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is general approach when it comes to data that can directly be determined by other data:\n\nX' = f(X)  with X being the fixed data and X' the derived data while f an arbitrary function.\n\nIn particular my questions are:\n\n1. Calculate X' once and store it vs. only store X and apply f when required later on?\n2. How to handle a scenario where X' can either be derived from X or provided directly as \"fixed\" data depending on different data sources? Telemetry data as example: device #1 provides only location data over time (-&gt; velocity (X') can be derived) while device #2 provides both location and velocity data over time (-&gt; velocity (X') can optionally be derived from the location data).\n\n&amp;#x200B;", "author_fullname": "t2_vvk7h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best approach to handle data that is based on other data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1054m6v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673039935.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673037034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is general approach when it comes to data that can directly be determined by other data:&lt;/p&gt;\n\n&lt;p&gt;X&amp;#39; = f(X)  with X being the fixed data and X&amp;#39; the derived data while f an arbitrary function.&lt;/p&gt;\n\n&lt;p&gt;In particular my questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Calculate X&amp;#39; once and store it vs. only store X and apply f when required later on?&lt;/li&gt;\n&lt;li&gt;How to handle a scenario where X&amp;#39; can either be derived from X or provided directly as &amp;quot;fixed&amp;quot; data depending on different data sources? Telemetry data as example: device #1 provides only location data over time (-&amp;gt; velocity (X&amp;#39;) can be derived) while device #2 provides both location and velocity data over time (-&amp;gt; velocity (X&amp;#39;) can optionally be derived from the location data).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1054m6v", "is_robot_indexable": true, "report_reasons": null, "author": "22Maxx", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1054m6v/best_approach_to_handle_data_that_is_based_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1054m6v/best_approach_to_handle_data_that_is_based_on/", "subreddit_subscribers": 85442, "created_utc": 1673037034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking at D365 and Dataverse, it seems Microsoft is pushing for people to use the Azure Synapse Link, which synchronises data in the data lake using the common data model format.\n\nIs there any actual benefit to using this format compared to other data formats, eg. Flat files or parquet.", "author_fullname": "t2_6o5du", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the benefits (if any) of the common data model format?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104v8au", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673014502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking at D365 and Dataverse, it seems Microsoft is pushing for people to use the Azure Synapse Link, which synchronises data in the data lake using the common data model format.&lt;/p&gt;\n\n&lt;p&gt;Is there any actual benefit to using this format compared to other data formats, eg. Flat files or parquet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "104v8au", "is_robot_indexable": true, "report_reasons": null, "author": "Cypher211", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104v8au/what_are_the_benefits_if_any_of_the_common_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104v8au/what_are_the_benefits_if_any_of_the_common_data/", "subreddit_subscribers": 85442, "created_utc": 1673014502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to improve a prediction model to have better accuracy, so I have to do constant back-and-forth between EDA, feature engineering, and model training + metrics evaluation, alongside making visualizations. I lack any organization to this, I just aimless do different steps mixed-up with different changes until I get improvements.\n\nI'm currently using pandas on jupyter notebook. Midway through the feature engineering, I have to take the data to postgresql for specific expansionary operations that might be too much for pandas,  joining them to another expensive table, but ultimately aggregating them in a size small enough to go back onto python jupyter (where it has to go anyways for it to go through the model).\n\nThis sql part really slows down my EDA and thinking flow, specifically because I'd have to refactor the SQL code for the times when I have made significant changes in the feature engineering procedure. And I'm trying to find a way to substitute it, so everything can happily stay within python, on the jupyter. I undestand that I'd still have to refactor whatever replacement python code I have too, no different than SQL, but at least I can monitor everything in the same place. Do I have any options?", "author_fullname": "t2_b7eh4ujn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to be more efficient in this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105jjt4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673079039.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673078380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to improve a prediction model to have better accuracy, so I have to do constant back-and-forth between EDA, feature engineering, and model training + metrics evaluation, alongside making visualizations. I lack any organization to this, I just aimless do different steps mixed-up with different changes until I get improvements.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using pandas on jupyter notebook. Midway through the feature engineering, I have to take the data to postgresql for specific expansionary operations that might be too much for pandas,  joining them to another expensive table, but ultimately aggregating them in a size small enough to go back onto python jupyter (where it has to go anyways for it to go through the model).&lt;/p&gt;\n\n&lt;p&gt;This sql part really slows down my EDA and thinking flow, specifically because I&amp;#39;d have to refactor the SQL code for the times when I have made significant changes in the feature engineering procedure. And I&amp;#39;m trying to find a way to substitute it, so everything can happily stay within python, on the jupyter. I undestand that I&amp;#39;d still have to refactor whatever replacement python code I have too, no different than SQL, but at least I can monitor everything in the same place. Do I have any options?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105jjt4", "is_robot_indexable": true, "report_reasons": null, "author": "countlinard", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105jjt4/how_to_be_more_efficient_in_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105jjt4/how_to_be_more_efficient_in_this/", "subreddit_subscribers": 85442, "created_utc": 1673078380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Realtime pipeline: We have a CDC pipeline running from Mongo to databricks. the message broker here is Kafka.\n\nWe want to create an audit pipeline to verify if data is moving properly.\n\ncurrent audits we do is like taking total count from mongo and trying to match it with databricks table. but this is a little costly, Any suggestions", "author_fullname": "t2_1w56xjmv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Audit realtime CDC pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105hm2u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673071614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Realtime pipeline: We have a CDC pipeline running from Mongo to databricks. the message broker here is Kafka.&lt;/p&gt;\n\n&lt;p&gt;We want to create an audit pipeline to verify if data is moving properly.&lt;/p&gt;\n\n&lt;p&gt;current audits we do is like taking total count from mongo and trying to match it with databricks table. but this is a little costly, Any suggestions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105hm2u", "is_robot_indexable": true, "report_reasons": null, "author": "avish2661", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105hm2u/audit_realtime_cdc_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105hm2u/audit_realtime_cdc_pipeline/", "subreddit_subscribers": 85442, "created_utc": 1673071614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is extensive modeling and multiple stages of transformation common? Or does data mostly arrive in a form where it can directly be queried to derive metrics and reports?\n\n[View Poll](https://www.reddit.com/poll/105ekd1)", "author_fullname": "t2_7spandv9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How many stages of data modeling / transformation does your organization use from when: (a) raw data first lands in your data warehouse, and (b) the data is ready for consumption for dashboards / reports / other?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105ekd1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673062259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is extensive modeling and multiple stages of transformation common? Or does data mostly arrive in a form where it can directly be queried to derive metrics and reports?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/105ekd1\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105ekd1", "is_robot_indexable": true, "report_reasons": null, "author": "brrdprrsn", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1673407859281, "options": [{"text": "Several stages of modeling / transformation", "id": "20860446"}, {"text": "Few stages of modeling / transformation", "id": "20860447"}, {"text": "Data that is loaded is directly queried for reports, metrics", "id": "20860448"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 82, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105ekd1/how_many_stages_of_data_modeling_transformation/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/105ekd1/how_many_stages_of_data_modeling_transformation/", "subreddit_subscribers": 85442, "created_utc": 1673062259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Reverse ETL  extracts data from the warehouse, processed to meet the data formatting needs at the destination, and fed into an application so it can be used by marketing, sales, support, and other teams in the tools they use.  \n\n\n[View Poll](https://www.reddit.com/poll/105e469)", "author_fullname": "t2_8cdm03wl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which type of pipeline does your company focuses heavily on?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105e469", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673060937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reverse ETL  extracts data from the warehouse, processed to meet the data formatting needs at the destination, and fed into an application so it can be used by marketing, sales, support, and other teams in the tools they use.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/105e469\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105e469", "is_robot_indexable": true, "report_reasons": null, "author": "Bianca_di_Angelo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1673147337859, "options": [{"text": "Primarily focused on ETL only.", "id": "20860118"}, {"text": "Primarily focused on ELT only.", "id": "20860119"}, {"text": "Primarily focused on Reverse-ETL only.", "id": "20860120"}, {"text": "ETL/ELT as well as Reverse-ETL.", "id": "20860121"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 109, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105e469/which_type_of_pipeline_does_your_company_focuses/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/105e469/which_type_of_pipeline_does_your_company_focuses/", "subreddit_subscribers": 85442, "created_utc": 1673060937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone currently doing data lineage work for multiple systems prioritized by CDEs? I\u2019m hoping to wrap my head around organizing the work in scrum and planning out releases and what will be demonstrated during reviews. Thanks!", "author_fullname": "t2_52xs90bs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Lineage using Scrum framework", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10593xy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673047670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone currently doing data lineage work for multiple systems prioritized by CDEs? I\u2019m hoping to wrap my head around organizing the work in scrum and planning out releases and what will be demonstrated during reviews. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10593xy", "is_robot_indexable": true, "report_reasons": null, "author": "ram_rod24", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10593xy/data_lineage_using_scrum_framework/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10593xy/data_lineage_using_scrum_framework/", "subreddit_subscribers": 85442, "created_utc": 1673047670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Preface:  I'm more of a data engineer and less of a cloud architect.\n\nWe have a Data Factory that is using a self hosted integration runtime on a VM.  This works fine, but now we are standing up a Synapse (serverless) instance and I need it to either connect to the existing self hosted runtime (the Data Factory one) or create a new one.  You cannot install more than one Integration Runtime on the same VM, and it does not seem that you can connect other services to it.\n\nIs there any way to connect Synapse to a SHIR that is already allocated to ADF?", "author_fullname": "t2_4270ek0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Synapse Analytics AND Data Factory users here? I have a question.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_104ycyg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673022265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Preface:  I&amp;#39;m more of a data engineer and less of a cloud architect.&lt;/p&gt;\n\n&lt;p&gt;We have a Data Factory that is using a self hosted integration runtime on a VM.  This works fine, but now we are standing up a Synapse (serverless) instance and I need it to either connect to the existing self hosted runtime (the Data Factory one) or create a new one.  You cannot install more than one Integration Runtime on the same VM, and it does not seem that you can connect other services to it.&lt;/p&gt;\n\n&lt;p&gt;Is there any way to connect Synapse to a SHIR that is already allocated to ADF?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "104ycyg", "is_robot_indexable": true, "report_reasons": null, "author": "mr_electric_wizard", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/104ycyg/any_synapse_analytics_and_data_factory_users_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/104ycyg/any_synapse_analytics_and_data_factory_users_here/", "subreddit_subscribers": 85442, "created_utc": 1673022265.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}