{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI've been building data pipelines for a while now. In contrast to the availability of examples of code design patterns, &amp; data modeling techniques, there are few to none on data flow design patterns. In my experience building pipelines, using the appropriate data flow patterns increases feature delivery speed, decreases toil during pipeline failures, and builds trust with stakeholders.\n\nWith that in mind, I wrote an article that goes over the most commonly used data flow design patterns, what they do, when to use them, and, more importantly, when not to use them. It's aimed to give an overview of the typical data flow patterns and guidelines for choosing the appropriate one for your use case.\n\n[https://www.startdataengineering.com/post/design-patterns/](https://www.startdataengineering.com/post/design-patterns/)\n\nI'd love to hear about any patterns that I have missed. Any feedback is appreciated. I hope this helps someone :)", "author_fullname": "t2_5srxspj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline design patterns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105o50o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 95, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 95, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673095202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been building data pipelines for a while now. In contrast to the availability of examples of code design patterns, &amp;amp; data modeling techniques, there are few to none on data flow design patterns. In my experience building pipelines, using the appropriate data flow patterns increases feature delivery speed, decreases toil during pipeline failures, and builds trust with stakeholders.&lt;/p&gt;\n\n&lt;p&gt;With that in mind, I wrote an article that goes over the most commonly used data flow design patterns, what they do, when to use them, and, more importantly, when not to use them. It&amp;#39;s aimed to give an overview of the typical data flow patterns and guidelines for choosing the appropriate one for your use case.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.startdataengineering.com/post/design-patterns/\"&gt;https://www.startdataengineering.com/post/design-patterns/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear about any patterns that I have missed. Any feedback is appreciated. I hope this helps someone :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?auto=webp&amp;s=07b9ceb836c7c7084c6c77d42d60531a450215a2", "width": 1271, "height": 714}, "resolutions": [{"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=466a76c8a5472f7d08537937187bbac1b6aabc95", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6315f77e68ac7f50e47af70eb7c2c066c9eb640a", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ec53a6bfc4c92b4fd99d7382ef4c31ec411f8db", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=19347f6feaf42a0f5606d336dfbf732e35c62bf3", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=483f159de78a082763f92e6c84748d6f793af2ec", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6036328d1ff9c14bb619d2010d1058d33aff423", "width": 1080, "height": 606}], "variants": {}, "id": "1NhFR4isyIRe_ddLlHMNbBEPJQqFv_etsmRqZEKagZc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "105o50o", "is_robot_indexable": true, "report_reasons": null, "author": "joseph_machado", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105o50o/data_pipeline_design_patterns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105o50o/data_pipeline_design_patterns/", "subreddit_subscribers": 85499, "created_utc": 1673095202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone! As the new year starts, I'm motivated to improve my coding skills and become more proficient in Python. I've worked on a few data engineering projects at my workplace using Python, but I still feel relatively inexperienced and struggle to write efficient, best-practice code. To address this, I'm actively seeking resources to help me learn and grow as a coder. My goal is to work on 1-2 algorithms per day so that I can expand my knowledge and be better equipped to tackle any projects my team may have in the future. Something as useful e.g., a GitHub repository with a collection of algorithms for writing Python code more effectively for specific objectives would be greatly appreciated. Any recommendations for learning materials or resources would be much appreciated!\"", "author_fullname": "t2_u2710fo6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Algorithms for Python, Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105gzxa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673069679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! As the new year starts, I&amp;#39;m motivated to improve my coding skills and become more proficient in Python. I&amp;#39;ve worked on a few data engineering projects at my workplace using Python, but I still feel relatively inexperienced and struggle to write efficient, best-practice code. To address this, I&amp;#39;m actively seeking resources to help me learn and grow as a coder. My goal is to work on 1-2 algorithms per day so that I can expand my knowledge and be better equipped to tackle any projects my team may have in the future. Something as useful e.g., a GitHub repository with a collection of algorithms for writing Python code more effectively for specific objectives would be greatly appreciated. Any recommendations for learning materials or resources would be much appreciated!&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105gzxa", "is_robot_indexable": true, "report_reasons": null, "author": "dataterre", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105gzxa/algorithms_for_python_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105gzxa/algorithms_for_python_data_engineering/", "subreddit_subscribers": 85499, "created_utc": 1673069679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious on the split of approach here\n\n[View Poll](https://www.reddit.com/poll/1059l27)", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Handling surrogate keys in your facts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1059l27", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673048836.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious on the split of approach here&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1059l27\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1059l27", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1673308037024, "options": [{"text": "We generate a unique incremental in our dim and look it up when building our fact", "id": "20857241"}, {"text": "We don\u2019t use surrogate keys", "id": "20857242"}, {"text": "We generate a hash from source system keys in our dims, but still join the dims in when building facts to get the keys", "id": "20857243"}, {"text": "We generate a hash from source and do this both when generating the dims &amp; facts. We rely on post build integrity tests.", "id": "20857244"}, {"text": "Other (in the comments)", "id": "20857245"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 307, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1059l27/handling_surrogate_keys_in_your_facts/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1059l27/handling_surrogate_keys_in_your_facts/", "subreddit_subscribers": 85499, "created_utc": 1673048836.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nI am currently a computer science major about to go into my upper division classes next year and I recently came across this subreddit/topic this weekend. I currently do not know where I want to go with my degree atm; I was thinking backend dev/engineer but now as I learn more about this field I feel like this is something I would be interested going in too. Do you guys think  it's a good idea to pursue this field out of the gate or should I get familiar with more fields or topics before going into this. I've read it's common for people to go into data engineering after they've already started their career. My immediate goal is to secure a summer internship and I've noticed a lot of the internships in my area ask for more developer skills like: react, express, javascript, HTML... \n\nHonestly I have just been overwhelming myself over this winter break obsessing about the future and I have been to impatient on trying to learn one thing at a time.", "author_fullname": "t2_6gk40pxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pursing Data Engineering as a Computer Science Student", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105bgbj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673053578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I am currently a computer science major about to go into my upper division classes next year and I recently came across this subreddit/topic this weekend. I currently do not know where I want to go with my degree atm; I was thinking backend dev/engineer but now as I learn more about this field I feel like this is something I would be interested going in too. Do you guys think  it&amp;#39;s a good idea to pursue this field out of the gate or should I get familiar with more fields or topics before going into this. I&amp;#39;ve read it&amp;#39;s common for people to go into data engineering after they&amp;#39;ve already started their career. My immediate goal is to secure a summer internship and I&amp;#39;ve noticed a lot of the internships in my area ask for more developer skills like: react, express, javascript, HTML... &lt;/p&gt;\n\n&lt;p&gt;Honestly I have just been overwhelming myself over this winter break obsessing about the future and I have been to impatient on trying to learn one thing at a time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105bgbj", "is_robot_indexable": true, "report_reasons": null, "author": "hcazj", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105bgbj/pursing_data_engineering_as_a_computer_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105bgbj/pursing_data_engineering_as_a_computer_science/", "subreddit_subscribers": 85499, "created_utc": 1673053578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hey guys, what the hell is spark ?\ni am pretty new and reading about spark online just create more confusion.\n\nLets say I \n\nIngest data --&gt; transform it --&gt; throw the data into BigQuery --&gt; query data from the warehouse --&gt; analysis\n\nWhere do you use Spark during this process?\n\nFrom what I understand, SPARK is used to distribute the data handling jobs onto multiple machines. But it's still very confusing to me on how you would apply it in a real world context.\n\nIs it correct to say it's like when you need to count inventory, instead of one person spending all day to count items, you have 100 people count then adding up the number?", "author_fullname": "t2_2pby6kdm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What the hell is Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105rg45", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673104824.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey guys, what the hell is spark ?\ni am pretty new and reading about spark online just create more confusion.&lt;/p&gt;\n\n&lt;p&gt;Lets say I &lt;/p&gt;\n\n&lt;p&gt;Ingest data --&amp;gt; transform it --&amp;gt; throw the data into BigQuery --&amp;gt; query data from the warehouse --&amp;gt; analysis&lt;/p&gt;\n\n&lt;p&gt;Where do you use Spark during this process?&lt;/p&gt;\n\n&lt;p&gt;From what I understand, SPARK is used to distribute the data handling jobs onto multiple machines. But it&amp;#39;s still very confusing to me on how you would apply it in a real world context.&lt;/p&gt;\n\n&lt;p&gt;Is it correct to say it&amp;#39;s like when you need to count inventory, instead of one person spending all day to count items, you have 100 people count then adding up the number?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105rg45", "is_robot_indexable": true, "report_reasons": null, "author": "The_Bear_Baron", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105rg45/what_the_hell_is_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105rg45/what_the_hell_is_spark/", "subreddit_subscribers": 85499, "created_utc": 1673104824.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The CEO of my company has decided to embark on an \u201cIT modernization\u201d project. The rules she laid down are simple: All existing workflows must be converted to AWS software-as-a-service tools. This means updating 15+ years worth of data pipelines and reports written in the Pentaho suite to services like Glue and Lambda. I\u2019m overseeing a team of 3 data engineers to get this done (I\u2019m an engineer myself as well).\n\nComplicating matters is that this won\u2019t be merely rewriting pipelines in a new language because the source systems themselves will also be replaced at the same time (new CRM tool, new HR system, etc) to abide by this mandate.\n\nAnyone been through a similar migration and have tips to share, horror stories, or moral support to offer?", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud migration stories", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_105ybmh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673122153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The CEO of my company has decided to embark on an \u201cIT modernization\u201d project. The rules she laid down are simple: All existing workflows must be converted to AWS software-as-a-service tools. This means updating 15+ years worth of data pipelines and reports written in the Pentaho suite to services like Glue and Lambda. I\u2019m overseeing a team of 3 data engineers to get this done (I\u2019m an engineer myself as well).&lt;/p&gt;\n\n&lt;p&gt;Complicating matters is that this won\u2019t be merely rewriting pipelines in a new language because the source systems themselves will also be replaced at the same time (new CRM tool, new HR system, etc) to abide by this mandate.&lt;/p&gt;\n\n&lt;p&gt;Anyone been through a similar migration and have tips to share, horror stories, or moral support to offer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105ybmh", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105ybmh/cloud_migration_stories/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105ybmh/cloud_migration_stories/", "subreddit_subscribers": 85499, "created_utc": 1673122153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_engotadk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vimanyu Chaturvedi on LinkedIn: DATA ENGINEERING ROADMAP 2023 | 109 comments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_105veky", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BDBMjZl9M-gZmZWhHe-Hp54G3HQTtTRYdMH1M2Z4Lig.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673114848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/posts/vimanyu_data-engineering-roadmap-2023-ugcPost-7015647693871955969-hVYe", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?auto=webp&amp;s=18689041ed1ad1829e19b3f16bdf0619c3add354", "width": 1400, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd3104eb2954de61b63b4aded5889d7f481667e4", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c971a4bc289c8f96921326f627cebe12f3ac1660", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4aa8c0a35a98b8aba18c141f215320d65e017724", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b4185fb0b2f226c0a0e3e33b8be8f2575bf9ab5", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f9aebe95893f1568f8433661797ed06581b8b5b", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4632741c44875873f4766540a85113c0e703b906", "width": 1080, "height": 617}], "variants": {}, "id": "CjbMbFq2MEqSKWpNCjv-ipCLADmRBQ1ZCG3w2yy71f0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "105veky", "is_robot_indexable": true, "report_reasons": null, "author": "StrangeAd1054", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105veky/vimanyu_chaturvedi_on_linkedin_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/posts/vimanyu_data-engineering-roadmap-2023-ugcPost-7015647693871955969-hVYe", "subreddit_subscribers": 85499, "created_utc": 1673114848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone know if this is functional using any sort of cloud DBMS? I want to have an integer type attribute increment by one every time its associated ID is accessed in a select query so my org can award performance based comp to people that provide data. Would blockchain be able to handle something like that?\n\nAn example of what I mean is suppose Ronald (an associate) added a record for a big mac recipe to McDonalds\u2019 recipe table. One person gets the recipe from that table using a select query. As a result of the select query, that big mac recipe has an int attribute named \u2018uses\u2019 that increases by 1. Then we can query the sum of uses by associate and award the top contributors (in terms of how many times their contributions are accessed) extra pay. \n\nAnyone have any ideas? Apparently logging is a solution, as is manually writing it in the backend (but wouldn\u2019t work in the case of a PowerBI dashboard, which will be a main place data is consumed).", "author_fullname": "t2_8rmuclrh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trigger to update on select query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105ednv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673061701.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone know if this is functional using any sort of cloud DBMS? I want to have an integer type attribute increment by one every time its associated ID is accessed in a select query so my org can award performance based comp to people that provide data. Would blockchain be able to handle something like that?&lt;/p&gt;\n\n&lt;p&gt;An example of what I mean is suppose Ronald (an associate) added a record for a big mac recipe to McDonalds\u2019 recipe table. One person gets the recipe from that table using a select query. As a result of the select query, that big mac recipe has an int attribute named \u2018uses\u2019 that increases by 1. Then we can query the sum of uses by associate and award the top contributors (in terms of how many times their contributions are accessed) extra pay. &lt;/p&gt;\n\n&lt;p&gt;Anyone have any ideas? Apparently logging is a solution, as is manually writing it in the backend (but wouldn\u2019t work in the case of a PowerBI dashboard, which will be a main place data is consumed).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105ednv", "is_robot_indexable": true, "report_reasons": null, "author": "Own_Neighborhood_231", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105ednv/trigger_to_update_on_select_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105ednv/trigger_to_update_on_select_query/", "subreddit_subscribers": 85499, "created_utc": 1673061701.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The company that I'm working started to make a digital transformation and is starting to extract data from APIs and uploading them into a database. My boss wants to do it using pentaho (neither him or me have great experience with data engeneering), but whenever i'm studying (i'm trying to get into the DE field) I don't hear nobody talk about pentaho.\n\n  \nSo the question is: Is pentaho a good tool for this use? if not, what tool do you recommend the most?", "author_fullname": "t2_9qzkaccz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Pentaho a good tool for ETL and orchestration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105rnzc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673105397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The company that I&amp;#39;m working started to make a digital transformation and is starting to extract data from APIs and uploading them into a database. My boss wants to do it using pentaho (neither him or me have great experience with data engeneering), but whenever i&amp;#39;m studying (i&amp;#39;m trying to get into the DE field) I don&amp;#39;t hear nobody talk about pentaho.&lt;/p&gt;\n\n&lt;p&gt;So the question is: Is pentaho a good tool for this use? if not, what tool do you recommend the most?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105rnzc", "is_robot_indexable": true, "report_reasons": null, "author": "Miguel-2001", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105rnzc/is_pentaho_a_good_tool_for_etl_and_orchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105rnzc/is_pentaho_a_good_tool_for_etl_and_orchestration/", "subreddit_subscribers": 85499, "created_utc": 1673105397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are looking to use snowflake as our lake house and follow the medallion architecture (Bronze -&gt; Silver &gt; Gold).  The ingestion into bronze schema will happen by either fivertran or airbyte. I 'd like to understand the options in transforming the tables further into Silver and Gold curated zones. \n\nIs it a standard practice to use snowpark, with python scripts for transformations.  Does snowpark comes with a scheduler, to trigger the transformation jobs ?\n\nAlso dbt-Snowflake seems like an established pattern. Can anybody share your feedback and gotchas in this approach. \n\nAny other patterns ? \n\nThanks for checking this.", "author_fullname": "t2_4kvf695m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestions on data transformation options in Snowflake platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1058lpu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673046435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are looking to use snowflake as our lake house and follow the medallion architecture (Bronze -&amp;gt; Silver &amp;gt; Gold).  The ingestion into bronze schema will happen by either fivertran or airbyte. I &amp;#39;d like to understand the options in transforming the tables further into Silver and Gold curated zones. &lt;/p&gt;\n\n&lt;p&gt;Is it a standard practice to use snowpark, with python scripts for transformations.  Does snowpark comes with a scheduler, to trigger the transformation jobs ?&lt;/p&gt;\n\n&lt;p&gt;Also dbt-Snowflake seems like an established pattern. Can anybody share your feedback and gotchas in this approach. &lt;/p&gt;\n\n&lt;p&gt;Any other patterns ? &lt;/p&gt;\n\n&lt;p&gt;Thanks for checking this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1058lpu", "is_robot_indexable": true, "report_reasons": null, "author": "rasviz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1058lpu/need_suggestions_on_data_transformation_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1058lpu/need_suggestions_on_data_transformation_options/", "subreddit_subscribers": 85499, "created_utc": 1673046435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reading Delta Lake Tables into Polars DataFrames", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_105yr8s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1673123269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "delta.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://delta.io/blog/2022-12-22-reading-delta-lake-tables-polars-dataframe/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "105yr8s", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105yr8s/reading_delta_lake_tables_into_polars_dataframes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://delta.io/blog/2022-12-22-reading-delta-lake-tables-polars-dataframe/", "subreddit_subscribers": 85499, "created_utc": 1673123269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polars: Blazingly Fast DataFrames in Rust and Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_105ymls", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/kVy3-gMdViM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Polars: Blazingly Fast DataFrames in Rust and Python\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Polars: Blazingly Fast DataFrames in Rust and Python", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/kVy3-gMdViM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Polars: Blazingly Fast DataFrames in Rust and Python\"&gt;&lt;/iframe&gt;", "author_name": "Databricks", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/kVy3-gMdViM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Databricks"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/kVy3-gMdViM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Polars: Blazingly Fast DataFrames in Rust and Python\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/105ymls", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Iee1B1QE4EZcEmtj09fmA1TkNh-UscP-yhAIelGpjGg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673122931.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=kVy3-gMdViM", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/T_TB_hwSdS4S_dlQb5vrAaxQvoTKg7tnwv9MVTGOq-E.jpg?auto=webp&amp;s=2b3871c8287180aa1be49639004b8b1e9a2aca91", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/T_TB_hwSdS4S_dlQb5vrAaxQvoTKg7tnwv9MVTGOq-E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=465978812ffee79c9149a8337f0fe46543a7f69d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/T_TB_hwSdS4S_dlQb5vrAaxQvoTKg7tnwv9MVTGOq-E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ea41c92abd432b6edfdacb882b11334c7a000176", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/T_TB_hwSdS4S_dlQb5vrAaxQvoTKg7tnwv9MVTGOq-E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f360e7111ae5e41144f65c6cdefc20217a82604b", "width": 320, "height": 240}], "variants": {}, "id": "sBd7HxFyYggOhVsKo3Qrs5XPAG6AkEYBFFUCPLg-XFU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "105ymls", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105ymls/polars_blazingly_fast_dataframes_in_rust_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=kVy3-gMdViM", "subreddit_subscribers": 85499, "created_utc": 1673122931.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Polars: Blazingly Fast DataFrames in Rust and Python", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/kVy3-gMdViM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Polars: Blazingly Fast DataFrames in Rust and Python\"&gt;&lt;/iframe&gt;", "author_name": "Databricks", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/kVy3-gMdViM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Databricks"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My apologies if this isn\u2019t the place for this type of question, but I\u2019m curious to hear other\u2019s thoughts on juggling life and studying for a career in data engineering. \n\nI\u2019m currently a Data Analyst and have been for about 4 years at this point. I\u2019m very proficient in SQL and am trying to advance beyond the fundamentals of Python. My biggest concern is the idea of burnout and overwhelming myself in the pursuit of a career change. Just for context, I\u2019m a father of three so my time is definitely limited. I understand that these constraints are not unique to me whatsoever, but I\u2019d like to hear how others have effectively managed their time in their career pursuits. \n\nThoughts?", "author_fullname": "t2_yusz1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Effective Study Sessions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_105y2zn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673121556.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My apologies if this isn\u2019t the place for this type of question, but I\u2019m curious to hear other\u2019s thoughts on juggling life and studying for a career in data engineering. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently a Data Analyst and have been for about 4 years at this point. I\u2019m very proficient in SQL and am trying to advance beyond the fundamentals of Python. My biggest concern is the idea of burnout and overwhelming myself in the pursuit of a career change. Just for context, I\u2019m a father of three so my time is definitely limited. I understand that these constraints are not unique to me whatsoever, but I\u2019d like to hear how others have effectively managed their time in their career pursuits. &lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105y2zn", "is_robot_indexable": true, "report_reasons": null, "author": "dvalinhunter", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105y2zn/effective_study_sessions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105y2zn/effective_study_sessions/", "subreddit_subscribers": 85499, "created_utc": 1673121556.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The answer to this quesiton might be rather simple but I lack the experience to know.\n\nIn it's most general form, this question is about the most compact data format to store a table of values with lot's of repeated values, e.g. for n columns:\n\n|idx|1|2|3|4|...|n|\n|---|-|-|-|-|---|-|\n|idx1|1v1|2v1|3v1|4v1|...|nv1|\n|idx2|1v1|**2v2**|3v1|4v1|...|nv1|\n|idx3|**1v2**|2v2|3v1|**4v2**|...|nv1|\n|idx4|1v2|**2v3**|3v1|4v2|...|**nv2**|\n|idx5|1v2|2v3|**3v2**|4v2|...|nv2|\n|idx6|**1v3**|2v3|3v2|4v2|...|nv2|\n\nI emphasise where the values change from those in previous rows. The format of the example values here is simply that 'nvm' means the mth value ('v') of column n.\n\nSpecifically, my application is a set of n quantities that are updating over time independently of each other, and I want to store the whole history of all quantities, the index being the time, i.e. it's basically a log recording when any of the values change, but they don't typically all change at the same time, only a few change at once.\n\nA starting point might be to consider storing it as a csv with unchanged values being left blank:\n\n```\ntime,1,2,3,4,...,n\nt1,1v1,2v1,3v1,4v1,...,nv1\nt2,,2v2,,,...,\nt3,1v2,,,4v2,...,\nt4,,2v3,,,...,nv2\nt5,,,3v2,,...,\nt6,1v3,,,,...,\n```\nWhen reading the data, the empty values in a row can then be forward-filled from the last time the value of that quantity changed.\n\nThe issue here is that for large n, that's a lot (n-1) of redundant commas increasing the size of the file, and the number of commas remains the same for each new row, even if only one or two values have changed.\n\nHence the next idea is not storing it as a csv to generate a table out of, and instead just including on each new line only those quantities that had changed with their new values:\n```\ntime,1,2,3,4,...,n\nt1,1v1,2v1,3v1,4v1,...,nv1\nt2,2:2v2\nt3,1:1v2,4:4v2\nt4,2:2v3,n:nv2\nt5,3:3v2\nt6,1:1v3\n```\nthe format here being that 'x:new_value' means quantity x has changed value to new_value. Assuming all characters and the column headings are 1 byte each, ths saves a constant n-1 bytes per row and only adds 3 bytes per updated value (column header, colon, comma) in the row, so the result is smaller in size as long as the number of values changing in each row remains smaller than (n-1)/3.\n\nIs there any better and more efficient way than this? Overall, in addition to minimal file size, I'm looking for:\n- a solution with fast appending of new data on the fly\n- when reading the data, something that allows efficiently isolating both by time to see what quantities changed at that time (ideally without having to traverse all previous times to get there), and by quantity to see at what times it that quantity changed (ideally without having to traverse other quantities to get there).", "author_fullname": "t2_rvpfxld", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most efficient format for storing a table with repeated column values", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105x2r0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673119053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The answer to this quesiton might be rather simple but I lack the experience to know.&lt;/p&gt;\n\n&lt;p&gt;In it&amp;#39;s most general form, this question is about the most compact data format to store a table of values with lot&amp;#39;s of repeated values, e.g. for n columns:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;idx&lt;/th&gt;\n&lt;th&gt;1&lt;/th&gt;\n&lt;th&gt;2&lt;/th&gt;\n&lt;th&gt;3&lt;/th&gt;\n&lt;th&gt;4&lt;/th&gt;\n&lt;th&gt;...&lt;/th&gt;\n&lt;th&gt;n&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;idx1&lt;/td&gt;\n&lt;td&gt;1v1&lt;/td&gt;\n&lt;td&gt;2v1&lt;/td&gt;\n&lt;td&gt;3v1&lt;/td&gt;\n&lt;td&gt;4v1&lt;/td&gt;\n&lt;td&gt;...&lt;/td&gt;\n&lt;td&gt;nv1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;idx2&lt;/td&gt;\n&lt;td&gt;1v1&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;2v2&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;3v1&lt;/td&gt;\n&lt;td&gt;4v1&lt;/td&gt;\n&lt;td&gt;...&lt;/td&gt;\n&lt;td&gt;nv1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;idx3&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;1v2&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;2v2&lt;/td&gt;\n&lt;td&gt;3v1&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;4v2&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;...&lt;/td&gt;\n&lt;td&gt;nv1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;idx4&lt;/td&gt;\n&lt;td&gt;1v2&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;2v3&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;3v1&lt;/td&gt;\n&lt;td&gt;4v2&lt;/td&gt;\n&lt;td&gt;...&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;nv2&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;idx5&lt;/td&gt;\n&lt;td&gt;1v2&lt;/td&gt;\n&lt;td&gt;2v3&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;3v2&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;4v2&lt;/td&gt;\n&lt;td&gt;...&lt;/td&gt;\n&lt;td&gt;nv2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;idx6&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;1v3&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;2v3&lt;/td&gt;\n&lt;td&gt;3v2&lt;/td&gt;\n&lt;td&gt;4v2&lt;/td&gt;\n&lt;td&gt;...&lt;/td&gt;\n&lt;td&gt;nv2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I emphasise where the values change from those in previous rows. The format of the example values here is simply that &amp;#39;nvm&amp;#39; means the mth value (&amp;#39;v&amp;#39;) of column n.&lt;/p&gt;\n\n&lt;p&gt;Specifically, my application is a set of n quantities that are updating over time independently of each other, and I want to store the whole history of all quantities, the index being the time, i.e. it&amp;#39;s basically a log recording when any of the values change, but they don&amp;#39;t typically all change at the same time, only a few change at once.&lt;/p&gt;\n\n&lt;p&gt;A starting point might be to consider storing it as a csv with unchanged values being left blank:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\ntime,1,2,3,4,...,n\nt1,1v1,2v1,3v1,4v1,...,nv1\nt2,,2v2,,,...,\nt3,1v2,,,4v2,...,\nt4,,2v3,,,...,nv2\nt5,,,3v2,,...,\nt6,1v3,,,,...,\n&lt;/code&gt;\nWhen reading the data, the empty values in a row can then be forward-filled from the last time the value of that quantity changed.&lt;/p&gt;\n\n&lt;p&gt;The issue here is that for large n, that&amp;#39;s a lot (n-1) of redundant commas increasing the size of the file, and the number of commas remains the same for each new row, even if only one or two values have changed.&lt;/p&gt;\n\n&lt;p&gt;Hence the next idea is not storing it as a csv to generate a table out of, and instead just including on each new line only those quantities that had changed with their new values:\n&lt;code&gt;\ntime,1,2,3,4,...,n\nt1,1v1,2v1,3v1,4v1,...,nv1\nt2,2:2v2\nt3,1:1v2,4:4v2\nt4,2:2v3,n:nv2\nt5,3:3v2\nt6,1:1v3\n&lt;/code&gt;\nthe format here being that &amp;#39;x:new_value&amp;#39; means quantity x has changed value to new_value. Assuming all characters and the column headings are 1 byte each, ths saves a constant n-1 bytes per row and only adds 3 bytes per updated value (column header, colon, comma) in the row, so the result is smaller in size as long as the number of values changing in each row remains smaller than (n-1)/3.&lt;/p&gt;\n\n&lt;p&gt;Is there any better and more efficient way than this? Overall, in addition to minimal file size, I&amp;#39;m looking for:\n- a solution with fast appending of new data on the fly\n- when reading the data, something that allows efficiently isolating both by time to see what quantities changed at that time (ideally without having to traverse all previous times to get there), and by quantity to see at what times it that quantity changed (ideally without having to traverse other quantities to get there).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105x2r0", "is_robot_indexable": true, "report_reasons": null, "author": "O_I_GR", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105x2r0/most_efficient_format_for_storing_a_table_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105x2r0/most_efficient_format_for_storing_a_table_with/", "subreddit_subscribers": 85499, "created_utc": 1673119053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been doing some research on various data quality tools recently.  \nIt got me wondering, in companies that are mainly focused on 3rd party integrations, e.g data from salesforce, what is the most common cause for a data quality issue? is it bad SQL? missing / malformed data at the source ? (e.g a sales rep didn't fill a field correctly) and how do you handle these problems?\n\n&amp;#x200B;\n\nI'm adding a poll for summarization, but please share  example on the use cases you faced.\n\n[View Poll](https://www.reddit.com/poll/105v661)", "author_fullname": "t2_ayp5oyir", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data quality issues resulting in source data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105v661", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673114297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been doing some research on various data quality tools recently.&lt;br/&gt;\nIt got me wondering, in companies that are mainly focused on 3rd party integrations, e.g data from salesforce, what is the most common cause for a data quality issue? is it bad SQL? missing / malformed data at the source ? (e.g a sales rep didn&amp;#39;t fill a field correctly) and how do you handle these problems?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m adding a poll for summarization, but please share  example on the use cases you faced.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/105v661\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105v661", "is_robot_indexable": true, "report_reasons": null, "author": "Puzzleheaded_Dog_614", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1673373497333, "options": [{"text": "Data Engineer bugs", "id": "20871255"}, {"text": "Misunderstanding of requirements", "id": "20871256"}, {"text": "Missing data in source system", "id": "20871257"}, {"text": "Wrong data in source system", "id": "20871258"}, {"text": "Other", "id": "20871259"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 19, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105v661/data_quality_issues_resulting_in_source_data/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/105v661/data_quality_issues_resulting_in_source_data/", "subreddit_subscribers": 85499, "created_utc": 1673114297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to improve a prediction model to have better accuracy, so I have to do constant back-and-forth between EDA, feature engineering, and model training + metrics evaluation, alongside making visualizations. I lack any organization to this, I just aimless do different steps mixed-up with different changes until I get improvements.\n\nI'm currently using pandas on jupyter notebook. Midway through the feature engineering, I have to take the data to postgresql for specific expansionary operations that might be too much for pandas,  joining them to another expensive table, but ultimately aggregating them in a size small enough to go back onto python jupyter (where it has to go anyways for it to go through the model).\n\nThis sql part really slows down my EDA and thinking flow, specifically because I'd have to refactor the SQL code for the times when I have made significant changes in the feature engineering procedure. And I'm trying to find a way to substitute it, so everything can happily stay within python, on the jupyter. I undestand that I'd still have to refactor whatever replacement python code I have too, no different than SQL, but at least I can monitor everything in the same place. Do I have any options?", "author_fullname": "t2_b7eh4ujn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to be more efficient in this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105jjt4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673079039.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673078380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to improve a prediction model to have better accuracy, so I have to do constant back-and-forth between EDA, feature engineering, and model training + metrics evaluation, alongside making visualizations. I lack any organization to this, I just aimless do different steps mixed-up with different changes until I get improvements.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using pandas on jupyter notebook. Midway through the feature engineering, I have to take the data to postgresql for specific expansionary operations that might be too much for pandas,  joining them to another expensive table, but ultimately aggregating them in a size small enough to go back onto python jupyter (where it has to go anyways for it to go through the model).&lt;/p&gt;\n\n&lt;p&gt;This sql part really slows down my EDA and thinking flow, specifically because I&amp;#39;d have to refactor the SQL code for the times when I have made significant changes in the feature engineering procedure. And I&amp;#39;m trying to find a way to substitute it, so everything can happily stay within python, on the jupyter. I undestand that I&amp;#39;d still have to refactor whatever replacement python code I have too, no different than SQL, but at least I can monitor everything in the same place. Do I have any options?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105jjt4", "is_robot_indexable": true, "report_reasons": null, "author": "countlinard", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105jjt4/how_to_be_more_efficient_in_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105jjt4/how_to_be_more_efficient_in_this/", "subreddit_subscribers": 85499, "created_utc": 1673078380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Realtime pipeline: We have a CDC pipeline running from Mongo to databricks. the message broker here is Kafka.\n\nWe want to create an audit pipeline to verify if data is moving properly.\n\ncurrent audits we do is like taking total count from mongo and trying to match it with databricks table. but this is a little costly, Any suggestions", "author_fullname": "t2_1w56xjmv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Audit realtime CDC pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105hm2u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673071614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Realtime pipeline: We have a CDC pipeline running from Mongo to databricks. the message broker here is Kafka.&lt;/p&gt;\n\n&lt;p&gt;We want to create an audit pipeline to verify if data is moving properly.&lt;/p&gt;\n\n&lt;p&gt;current audits we do is like taking total count from mongo and trying to match it with databricks table. but this is a little costly, Any suggestions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105hm2u", "is_robot_indexable": true, "report_reasons": null, "author": "avish2661", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105hm2u/audit_realtime_cdc_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105hm2u/audit_realtime_cdc_pipeline/", "subreddit_subscribers": 85499, "created_utc": 1673071614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is extensive modeling and multiple stages of transformation common? Or does data mostly arrive in a form where it can directly be queried to derive metrics and reports?\n\n[View Poll](https://www.reddit.com/poll/105ekd1)", "author_fullname": "t2_7spandv9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How many stages of data modeling / transformation does your organization use from when: (a) raw data first lands in your data warehouse, and (b) the data is ready for consumption for dashboards / reports / other?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105ekd1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673062259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is extensive modeling and multiple stages of transformation common? Or does data mostly arrive in a form where it can directly be queried to derive metrics and reports?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/105ekd1\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105ekd1", "is_robot_indexable": true, "report_reasons": null, "author": "brrdprrsn", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1673407859281, "options": [{"text": "Several stages of modeling / transformation", "id": "20860446"}, {"text": "Few stages of modeling / transformation", "id": "20860447"}, {"text": "Data that is loaded is directly queried for reports, metrics", "id": "20860448"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 101, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105ekd1/how_many_stages_of_data_modeling_transformation/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/105ekd1/how_many_stages_of_data_modeling_transformation/", "subreddit_subscribers": 85499, "created_utc": 1673062259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Reverse ETL  extracts data from the warehouse, processed to meet the data formatting needs at the destination, and fed into an application so it can be used by marketing, sales, support, and other teams in the tools they use.  \n\n\n[View Poll](https://www.reddit.com/poll/105e469)", "author_fullname": "t2_8cdm03wl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which type of pipeline does your company focuses heavily on?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105e469", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673060937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reverse ETL  extracts data from the warehouse, processed to meet the data formatting needs at the destination, and fed into an application so it can be used by marketing, sales, support, and other teams in the tools they use.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/105e469\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105e469", "is_robot_indexable": true, "report_reasons": null, "author": "Bianca_di_Angelo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1673147337859, "options": [{"text": "Primarily focused on ETL only.", "id": "20860118"}, {"text": "Primarily focused on ELT only.", "id": "20860119"}, {"text": "Primarily focused on Reverse-ETL only.", "id": "20860120"}, {"text": "ETL/ELT as well as Reverse-ETL.", "id": "20860121"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 136, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105e469/which_type_of_pipeline_does_your_company_focuses/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/105e469/which_type_of_pipeline_does_your_company_focuses/", "subreddit_subscribers": 85499, "created_utc": 1673060937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone currently doing data lineage work for multiple systems prioritized by CDEs? I\u2019m hoping to wrap my head around organizing the work in scrum and planning out releases and what will be demonstrated during reviews. Thanks!", "author_fullname": "t2_52xs90bs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Lineage using Scrum framework", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10593xy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673047670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone currently doing data lineage work for multiple systems prioritized by CDEs? I\u2019m hoping to wrap my head around organizing the work in scrum and planning out releases and what will be demonstrated during reviews. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10593xy", "is_robot_indexable": true, "report_reasons": null, "author": "ram_rod24", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10593xy/data_lineage_using_scrum_framework/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10593xy/data_lineage_using_scrum_framework/", "subreddit_subscribers": 85499, "created_utc": 1673047670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious on how people go about structuring the logic of their SQL joins to ensure both performance and readability.\n\n[View Poll](https://www.reddit.com/poll/105wzzl)", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you structure complex SQL joins", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105wzzl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673118858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious on how people go about structuring the logic of their SQL joins to ensure both performance and readability.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/105wzzl\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105wzzl", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1673378058723, "options": [{"text": "One big query, with sub-queries throughout", "id": "20872576"}, {"text": "Each step broken down into a procedural set of CTEs with a \u2018SELECT * FROM final\u2019 at the end", "id": "20872577"}, {"text": "A series of SQL statements, each leveraging the persisted table created by the previous query", "id": "20872578"}, {"text": "Something else : in the comments", "id": "20872579"}, {"text": "It\u2019s a secret. Just show me what everyone else does.", "id": "20872580"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 73, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105wzzl/how_do_you_structure_complex_sql_joins/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/105wzzl/how_do_you_structure_complex_sql_joins/", "subreddit_subscribers": 85499, "created_utc": 1673118858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does AWS glue that represent ETL functions do that work with No Code like Azure Data Factory?", "author_fullname": "t2_63im4vf3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question to Data Engineers:", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105tmf1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.17, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673110445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does AWS glue that represent ETL functions do that work with No Code like Azure Data Factory?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105tmf1", "is_robot_indexable": true, "report_reasons": null, "author": "AmrBayoumy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105tmf1/question_to_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105tmf1/question_to_data_engineers/", "subreddit_subscribers": 85499, "created_utc": 1673110445.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}