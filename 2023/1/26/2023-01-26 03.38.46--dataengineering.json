{"kind": "Listing", "data": {"after": "t3_10lhg07", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8wtp2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Follow up on that Google Drive question...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "name": "t3_10leclf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 82, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 82, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9E8-72aRx6kITb95ac6W2WAyjc2bOevHwkrWNlfhPwI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674692397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/mkhpod7r6aea1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/mkhpod7r6aea1.jpg?auto=webp&amp;v=enabled&amp;s=6e5f60e59e304631eab891981f46cbacf2ebe344", "width": 636, "height": 499}, "resolutions": [{"url": "https://preview.redd.it/mkhpod7r6aea1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86805916e7678b8aaaab5f79769289d27605eeeb", "width": 108, "height": 84}, {"url": "https://preview.redd.it/mkhpod7r6aea1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2473e5d377a8b5834dc4a96da7bc1f1632a2b9d0", "width": 216, "height": 169}, {"url": "https://preview.redd.it/mkhpod7r6aea1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c2f663c061a90e5df3aa90fb46321df236dc169", "width": 320, "height": 251}], "variants": {}, "id": "TFStNsn4aE4w9PrRvR22lmEoCFONpr0c-GLVLpqSj7I"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "10leclf", "is_robot_indexable": true, "report_reasons": null, "author": "bartosaq", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10leclf/follow_up_on_that_google_drive_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/mkhpod7r6aea1.jpg", "subreddit_subscribers": 87472, "created_utc": 1674692397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AWS defines a data lake as 'a centralised repository that allows you to store all your structure and unstructured data at any scale'. \n\nIf I put a bunch of files in Google Drive, have I created a data lake or is there something special about S3? I'm still struggling to pin down all the terminology for different types of data store.", "author_fullname": "t2_djdhkrg6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Google Drive a data lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10l1kxb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674661332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AWS defines a data lake as &amp;#39;a centralised repository that allows you to store all your structure and unstructured data at any scale&amp;#39;. &lt;/p&gt;\n\n&lt;p&gt;If I put a bunch of files in Google Drive, have I created a data lake or is there something special about S3? I&amp;#39;m still struggling to pin down all the terminology for different types of data store.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10l1kxb", "is_robot_indexable": true, "report_reasons": null, "author": "user192034", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10l1kxb/is_google_drive_a_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10l1kxb/is_google_drive_a_data_lake/", "subreddit_subscribers": 87472, "created_utc": 1674661332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Right now I\u2019m an Analyst but for the past year or so at my company I\u2019ve been doing data engineering work like extracting data from different types of sources (API endpoints, files), writing code in python/sql to build data pipelines, use tools like airflow and AWS, deploying code to GitHub using git, refactoring the codebase, and collaborating with data engineers to do the previously stated.\n\nShould I ask for a title change to include \u2018engineer\u2019? I know data analysts do this work somewhat but I spend like 90% of my time doing the above stuff.\n\nI\u2019m concerned about when I go to look for a new job they won\u2019t sersiously consider me for a data engineering role.\n\nIf so, I am not sure how to go about that conversation in terms of potentially dealing with pushback from mamangement, so any advice/pointers on that would be greatly appreciated.", "author_fullname": "t2_f2plcmjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I ask for \u2018Engineer\u2019 in my title?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10l17ui", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674660658.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674660410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Right now I\u2019m an Analyst but for the past year or so at my company I\u2019ve been doing data engineering work like extracting data from different types of sources (API endpoints, files), writing code in python/sql to build data pipelines, use tools like airflow and AWS, deploying code to GitHub using git, refactoring the codebase, and collaborating with data engineers to do the previously stated.&lt;/p&gt;\n\n&lt;p&gt;Should I ask for a title change to include \u2018engineer\u2019? I know data analysts do this work somewhat but I spend like 90% of my time doing the above stuff.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m concerned about when I go to look for a new job they won\u2019t sersiously consider me for a data engineering role.&lt;/p&gt;\n\n&lt;p&gt;If so, I am not sure how to go about that conversation in terms of potentially dealing with pushback from mamangement, so any advice/pointers on that would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10l17ui", "is_robot_indexable": true, "report_reasons": null, "author": "Tough_Bag_458", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10l17ui/should_i_ask_for_engineer_in_my_title/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10l17ui/should_i_ask_for_engineer_in_my_title/", "subreddit_subscribers": 87472, "created_utc": 1674660410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This morning I called in a plumber to fix my toilet that was clogged yet again (I live in an old building, infrastructure is pretty bad or deteriorated). The whole process took less than an hour. I watched him equip very long service gloves and shove a pressure spray down the toilet. He said if it didn't work he would use a professional toilet snake. It did work, so I paid for 4 \"uses of the pressure tool\" (wtf is this pricing model) and he left. \n\nLet me tell you, he charged a copious amount of money to do it. And it is the average price for this service in my area. I did the math and, if he visited one client per day, he would be making FOUR TIMES more than me, working as a junior data engineer.\n\nSo I'm thinking that I average around 10h per day in working my actual job and keeping up with new de technologies, coding exercises, solving bugs in personal projects etc. Also worrying about if my company will fire me in the next quarter or whether AI will halve job openings in the next 5 years.\n\nMeanwhile this guy can work 1 hour per day, and make four times my salary.\n\nI'm the real clown here.", "author_fullname": "t2_1fice0pt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thinking of start fixing (real) pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kz9ag", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674655214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This morning I called in a plumber to fix my toilet that was clogged yet again (I live in an old building, infrastructure is pretty bad or deteriorated). The whole process took less than an hour. I watched him equip very long service gloves and shove a pressure spray down the toilet. He said if it didn&amp;#39;t work he would use a professional toilet snake. It did work, so I paid for 4 &amp;quot;uses of the pressure tool&amp;quot; (wtf is this pricing model) and he left. &lt;/p&gt;\n\n&lt;p&gt;Let me tell you, he charged a copious amount of money to do it. And it is the average price for this service in my area. I did the math and, if he visited one client per day, he would be making FOUR TIMES more than me, working as a junior data engineer.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m thinking that I average around 10h per day in working my actual job and keeping up with new de technologies, coding exercises, solving bugs in personal projects etc. Also worrying about if my company will fire me in the next quarter or whether AI will halve job openings in the next 5 years.&lt;/p&gt;\n\n&lt;p&gt;Meanwhile this guy can work 1 hour per day, and make four times my salary.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m the real clown here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10kz9ag", "is_robot_indexable": true, "report_reasons": null, "author": "brotherkaramasov", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10kz9ag/thinking_of_start_fixing_real_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10kz9ag/thinking_of_start_fixing_real_pipelines/", "subreddit_subscribers": 87472, "created_utc": 1674655214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There was a post recently about breaking up functions into smaller chunks. It got me thinking: how do you unit test an Airflow Dag? It\u2019s basically a daisy chain of pipeline steps. Take for example a pipeline that pulls data from an S3 bucket, validates the data, cleans it, and loads it into a landing table in a database. \n\nWhat are some of the approaches/tools the community uses? Would PyTest handle this?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unit Testing an Airflow Dag", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10l68qc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674672537.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There was a post recently about breaking up functions into smaller chunks. It got me thinking: how do you unit test an Airflow Dag? It\u2019s basically a daisy chain of pipeline steps. Take for example a pipeline that pulls data from an S3 bucket, validates the data, cleans it, and loads it into a landing table in a database. &lt;/p&gt;\n\n&lt;p&gt;What are some of the approaches/tools the community uses? Would PyTest handle this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10l68qc", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10l68qc/unit_testing_an_airflow_dag/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10l68qc/unit_testing_an_airflow_dag/", "subreddit_subscribers": 87472, "created_utc": 1674672537.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello\n I have to work on a project implemented with Azure Synapse Analytics Dedicated SQL. \nDo you have a book teaching all the best practices to put in place for the project?\n An example of an end to end datalake project developed with best practices?", "author_fullname": "t2_j68sac68", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "End to end project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kvmxa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674642547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello\n I have to work on a project implemented with Azure Synapse Analytics Dedicated SQL. \nDo you have a book teaching all the best practices to put in place for the project?\n An example of an end to end datalake project developed with best practices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10kvmxa", "is_robot_indexable": true, "report_reasons": null, "author": "Playful-Sprinkles-27", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10kvmxa/end_to_end_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10kvmxa/end_to_end_project/", "subreddit_subscribers": 87472, "created_utc": 1674642547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If it were up to me, I would say \u201c2 sprints every quarter to work on tech debt\u201d for the data teams. \n\nWishful thinking- I know!\n\nWhat would be your team\u2019s?", "author_fullname": "t2_msviuy1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you are to come up with one new year resolution for your data team, what would it be?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kuct1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674637159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If it were up to me, I would say \u201c2 sprints every quarter to work on tech debt\u201d for the data teams. &lt;/p&gt;\n\n&lt;p&gt;Wishful thinking- I know!&lt;/p&gt;\n\n&lt;p&gt;What would be your team\u2019s?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Applied Data &amp; ML Engineer | Developer Advocate", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10kuct1", "is_robot_indexable": true, "report_reasons": null, "author": "vino_and_data", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10kuct1/if_you_are_to_come_up_with_one_new_year/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10kuct1/if_you_are_to_come_up_with_one_new_year/", "subreddit_subscribers": 87472, "created_utc": 1674637159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I get asked these questions a lot by prospects inquiring about Materialize, and I wanted to share a nice writeup that includes a nifty interactive diagram. I hope you enjoy, and I look forward to any follow-on questions you might have!\n\n- https://materialize.com/guides/streaming-database/\n\nps: I genuinely enjoy these kind of data architecture discussions and I like the tech my company sells. Please keep me honest if my posts feel too \u201cself-promote-y\u201d. Materialize is not the only streaming database, so I\u2019m down to discuss the broader approach.", "author_fullname": "t2_5p00kusf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a Streaming Database? When would I use one? When would I use a data warehouse instead?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10l22ku", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1674662546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I get asked these questions a lot by prospects inquiring about Materialize, and I wanted to share a nice writeup that includes a nifty interactive diagram. I hope you enjoy, and I look forward to any follow-on questions you might have!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://materialize.com/guides/streaming-database/\"&gt;https://materialize.com/guides/streaming-database/&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;ps: I genuinely enjoy these kind of data architecture discussions and I like the tech my company sells. Please keep me honest if my posts feel too \u201cself-promote-y\u201d. Materialize is not the only streaming database, so I\u2019m down to discuss the broader approach.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LiDJ9PkviuuQkUbeVlrTNFpxVsqOBjF3okg1teYxg20.jpg?auto=webp&amp;v=enabled&amp;s=40e1d7827c1f41c748842c4edb067120d4c0e3b7", "width": 1636, "height": 655}, "resolutions": [{"url": "https://external-preview.redd.it/LiDJ9PkviuuQkUbeVlrTNFpxVsqOBjF3okg1teYxg20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12bb7a41e0ff8c5ef406e4cbe219cb91bfcf33da", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/LiDJ9PkviuuQkUbeVlrTNFpxVsqOBjF3okg1teYxg20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93c910c6ced6adac0f7afa5b37921b7015226440", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/LiDJ9PkviuuQkUbeVlrTNFpxVsqOBjF3okg1teYxg20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=207d1ac9f0dd7b83882224f64eb7368849966b57", "width": 320, "height": 128}, {"url": "https://external-preview.redd.it/LiDJ9PkviuuQkUbeVlrTNFpxVsqOBjF3okg1teYxg20.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3878ba477aa0e814b91e7cb04a91e0a97fb65008", "width": 640, "height": 256}, {"url": "https://external-preview.redd.it/LiDJ9PkviuuQkUbeVlrTNFpxVsqOBjF3okg1teYxg20.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35dcc539f73f59941578e866906e5a2df3ea8ea9", "width": 960, "height": 384}, {"url": "https://external-preview.redd.it/LiDJ9PkviuuQkUbeVlrTNFpxVsqOBjF3okg1teYxg20.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b575979e39651709e02748d4678d1ea3df058cf7", "width": 1080, "height": 432}], "variants": {}, "id": "nER1gh_5MePRyoU3PJ4PPG59EB1QJLfOZ0p2ljidJPk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10l22ku", "is_robot_indexable": true, "report_reasons": null, "author": "Chuck-Alt-Delete", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10l22ku/what_is_a_streaming_database_when_would_i_use_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10l22ku/what_is_a_streaming_database_when_would_i_use_one/", "subreddit_subscribers": 87472, "created_utc": 1674662546.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to build a service that is always up, consumes messages from Kafka, writes files to blob storage, and produces events to a different Kafka topic. And in addition, serves a \"health\" endpoint for Kubernetes health checks. \n\nDo you know of any open-source project or an example of how one would structure this?", "author_fullname": "t2_u8nde1tz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "example project structure for Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kv2tt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674640234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to build a service that is always up, consumes messages from Kafka, writes files to blob storage, and produces events to a different Kafka topic. And in addition, serves a &amp;quot;health&amp;quot; endpoint for Kubernetes health checks. &lt;/p&gt;\n\n&lt;p&gt;Do you know of any open-source project or an example of how one would structure this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10kv2tt", "is_robot_indexable": true, "report_reasons": null, "author": "topdownAC", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10kv2tt/example_project_structure_for_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10kv2tt/example_project_structure_for_kafka/", "subreddit_subscribers": 87472, "created_utc": 1674640234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi.\n\nSuppose we have the data lake (all on prem) with spark and all the needed tools to get whatever we want.\n\nNow, we need to be able to quickly create dashboards and automatically update visualizations.\n\nWhat are the scheduling and underlying aggregated databases of your choice? AirFlow+Postgres is a simple choice, let's think of something different.", "author_fullname": "t2_4clu4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reporting Visualization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10l6wxv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1674703630.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674674096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi.&lt;/p&gt;\n\n&lt;p&gt;Suppose we have the data lake (all on prem) with spark and all the needed tools to get whatever we want.&lt;/p&gt;\n\n&lt;p&gt;Now, we need to be able to quickly create dashboards and automatically update visualizations.&lt;/p&gt;\n\n&lt;p&gt;What are the scheduling and underlying aggregated databases of your choice? AirFlow+Postgres is a simple choice, let&amp;#39;s think of something different.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10l6wxv", "is_robot_indexable": true, "report_reasons": null, "author": "inteloid", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10l6wxv/reporting_visualization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10l6wxv/reporting_visualization/", "subreddit_subscribers": 87472, "created_utc": 1674674096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you unit test SQL transformations? Except for using dbt? What are possibilities? It is probably not very easy to do so I suppose.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL transformations unit testing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10l9pgg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674680751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you unit test SQL transformations? Except for using dbt? What are possibilities? It is probably not very easy to do so I suppose.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10l9pgg", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10l9pgg/sql_transformations_unit_testing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10l9pgg/sql_transformations_unit_testing/", "subreddit_subscribers": 87472, "created_utc": 1674680751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We ingest geospatial data from the internet in a lot of different sources.\n\nData is bad in two ways\n\n1. Geospatially incorrect: coordinates or elevation value is incorrect. Some of this just requires expertise.\n2. Metadata incorrect.\n\nThis geospatial data is transformed to one common coordinate system, other things are done to it, and it becomes compiled with other geospatial data to create singular products over a given region before it goes out of the door. In that compiling, it is effecting other geospatial data because we need to decide which of all the geospatial data at any given location is selected.\n\nThe whole process is a bit longer and more complicated.\n\nWhat I've found issues with is that we have manual review but at any given point, there's so much data that things are going to be missed. We have compliance checks in code but things come in so many different ways that things could be missed. And finally we end up having done many things to the data but at any given point in the pipeline we likely end up finding out that something in one of the geospatial sources is incorrect. At this point it's in our database records, it's locally stored, it's been manipulated, it's even effected other data during the compiling process.\n\nJust wanted to know if you guys have any advice, experience, or wisdom to deal with this type of scenario as I'm generally pretty green and learning as I go.", "author_fullname": "t2_vp6b3ibw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for creating pipelines when there's high amounts of bad data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10l1tca", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674661930.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We ingest geospatial data from the internet in a lot of different sources.&lt;/p&gt;\n\n&lt;p&gt;Data is bad in two ways&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Geospatially incorrect: coordinates or elevation value is incorrect. Some of this just requires expertise.&lt;/li&gt;\n&lt;li&gt;Metadata incorrect.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This geospatial data is transformed to one common coordinate system, other things are done to it, and it becomes compiled with other geospatial data to create singular products over a given region before it goes out of the door. In that compiling, it is effecting other geospatial data because we need to decide which of all the geospatial data at any given location is selected.&lt;/p&gt;\n\n&lt;p&gt;The whole process is a bit longer and more complicated.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;ve found issues with is that we have manual review but at any given point, there&amp;#39;s so much data that things are going to be missed. We have compliance checks in code but things come in so many different ways that things could be missed. And finally we end up having done many things to the data but at any given point in the pipeline we likely end up finding out that something in one of the geospatial sources is incorrect. At this point it&amp;#39;s in our database records, it&amp;#39;s locally stored, it&amp;#39;s been manipulated, it&amp;#39;s even effected other data during the compiling process.&lt;/p&gt;\n\n&lt;p&gt;Just wanted to know if you guys have any advice, experience, or wisdom to deal with this type of scenario as I&amp;#39;m generally pretty green and learning as I go.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10l1tca", "is_robot_indexable": true, "report_reasons": null, "author": "PossibleMine1247", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10l1tca/advice_for_creating_pipelines_when_theres_high/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10l1tca/advice_for_creating_pipelines_when_theres_high/", "subreddit_subscribers": 87472, "created_utc": 1674661930.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to OCR and make searchable 6 million documents that are of varying complexity and am looking for a resource with deep experience in this area.  We need to make some specific data searchable and creating a simple full text index by image isn't going to cut it. \n\nBefore designing and implementing the pipeline, I\u2019m interested in determining feasibility of obtaining the results we need by running some sample images.\n\nI'm looking for commentary on experience, to be pointed in the right direction, or perhaps even to engage on a consultative basis, the possibility of our ask.\n\nSo far, I know that a significant amount of docs will require de-skew as well as layout analysis (zoning). Because of varying formats I\u2019m convinced that zoning, with a significant amount of search logic (to produce a useful index) executed per zone, will be needed to get satisfactory results.  The block output of the Google Vision AI output looks promising.\n\nI\u2019m aware the OCR API\u2019s from AWS and Google are perhaps my best bet. It would be wonderful if there were an API that was already trained on documents of this type. Perhaps there is.\n\n&amp;#x200B;\n\n**See key points and examples below...**\n\n* Images are public courthouse documents related to mineral rights\n* Documents are of many, many, many different formats spanning decades.\n* Approximately 6 million images, between 4 and 8 TB\n* Goal is to OCR these documents and make searchable some specific pieces of information.\n* Specifically, there are 4 categories of data we want to search for and find in these documents. These 4 types are 1.Clerk Stamp Data, 2.Names, 3.STR, and 4.Document Type.\n\n&amp;#x200B;\n\n1. **Data From Clerk Stamp (this is literally a stamp on the paper)\u2026**\n\n* Book #\n* Page #\n* Page # Sequence\n* County\n* State\n\n**2. Names designated as following\u2026**\n\n* Assignor / Assignee\n* Seller / Buyer\n* Seller / Purchaser\n* Grantor / Grantee\n* Lessor / Lessee\n\n**3. STR (section, township, range) (challenge here is the many formats)**\n\n* [https://en.wikipedia.org/wiki/Section\\_(United\\_States\\_land\\_surveying)](https://en.wikipedia.org/wiki/Section_(United_States_land_surveying))\n* See below different 10 versions of specifying Section 6, Township 5S, Range 6W. There are at least 16 other versions.\n* S-6 T-5S R-6W\n* 6 5south 6West\n* 06 05South 06West\n* S06 05S 06W\n* S6 5S 6W\n* 6-5S-6W\n* 06-05-06\n* 6-5-6\n* Section 06 Township 05S Range 06W\n* Section 6 Township 5S Range 6W\n\n**4. Document Type (almost all documents can be identified as the following)**\n\n* Oil &amp; Gas Lease\n* Mineral Deed\n* Assignment\n* Bill of Sale\n* Division Order\n* Probate\n* Pay Stub or Revenue Statement\n* Affidavit\n* Affidavit of Death\n* Affidavit of Heirship\n* Affidavit of Pooling\n* Exhibit (A, B, C, etc.)\n* Quit Claim Deed\n* Assignment of Mortgage\n* Warranty Deed\n* Wellbore Assignment\n\n&amp;#x200B;\n\nHere I\u2019m going to reference some example documents and describe the challenge.  Example documents can be found here...\n\n[https://www.dropbox.com/t/e1Nc8Eg3eNpRLcwU](https://www.dropbox.com/t/e1Nc8Eg3eNpRLcwU)\n\n&amp;#x200B;\n\n**An \\*easy\\* example...**\n\nMineral Deed-Atoka-1 pg.pdf\n\n&amp;#x200B;\n\n**Two Documents with many STR...**\n\nAgreement to Purchase Minerals-Coal County-Book 0712-Page 874-878.pdf\n\nWellbore Assignment-Logan County-Book 3006-Page 371-383.pdf\n\n&amp;#x200B;\n\n**Document with many STR that are sideways, and the STR is broken into columns**\n\nAssignment and Bill of Sale-Grady County-Book 6012-Page 486-503.pdf\n\n&amp;#x200B;\n\n**Document that lists Township and Range, then lists a bunch of sections below. We need to make this searchable by one of the recognized complete STR formats above.**\n\nAffidavit Coal\\_Book719\\_page423.JPG", "author_fullname": "t2_tacue4my", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sophisticated OCR Post Processing of Courthouse Images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ldij2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674690193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to OCR and make searchable 6 million documents that are of varying complexity and am looking for a resource with deep experience in this area.  We need to make some specific data searchable and creating a simple full text index by image isn&amp;#39;t going to cut it. &lt;/p&gt;\n\n&lt;p&gt;Before designing and implementing the pipeline, I\u2019m interested in determining feasibility of obtaining the results we need by running some sample images.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for commentary on experience, to be pointed in the right direction, or perhaps even to engage on a consultative basis, the possibility of our ask.&lt;/p&gt;\n\n&lt;p&gt;So far, I know that a significant amount of docs will require de-skew as well as layout analysis (zoning). Because of varying formats I\u2019m convinced that zoning, with a significant amount of search logic (to produce a useful index) executed per zone, will be needed to get satisfactory results.  The block output of the Google Vision AI output looks promising.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m aware the OCR API\u2019s from AWS and Google are perhaps my best bet. It would be wonderful if there were an API that was already trained on documents of this type. Perhaps there is.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;See key points and examples below...&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Images are public courthouse documents related to mineral rights&lt;/li&gt;\n&lt;li&gt;Documents are of many, many, many different formats spanning decades.&lt;/li&gt;\n&lt;li&gt;Approximately 6 million images, between 4 and 8 TB&lt;/li&gt;\n&lt;li&gt;Goal is to OCR these documents and make searchable some specific pieces of information.&lt;/li&gt;\n&lt;li&gt;Specifically, there are 4 categories of data we want to search for and find in these documents. These 4 types are 1.Clerk Stamp Data, 2.Names, 3.STR, and 4.Document Type.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Data From Clerk Stamp (this is literally a stamp on the paper)\u2026&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Book #&lt;/li&gt;\n&lt;li&gt;Page #&lt;/li&gt;\n&lt;li&gt;Page # Sequence&lt;/li&gt;\n&lt;li&gt;County&lt;/li&gt;\n&lt;li&gt;State&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Names designated as following\u2026&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Assignor / Assignee&lt;/li&gt;\n&lt;li&gt;Seller / Buyer&lt;/li&gt;\n&lt;li&gt;Seller / Purchaser&lt;/li&gt;\n&lt;li&gt;Grantor / Grantee&lt;/li&gt;\n&lt;li&gt;Lessor / Lessee&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;3. STR (section, township, range) (challenge here is the many formats)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://en.wikipedia.org/wiki/Section_(United_States_land_surveying\"&gt;https://en.wikipedia.org/wiki/Section_(United_States_land_surveying)&lt;/a&gt;)&lt;/li&gt;\n&lt;li&gt;See below different 10 versions of specifying Section 6, Township 5S, Range 6W. There are at least 16 other versions.&lt;/li&gt;\n&lt;li&gt;S-6 T-5S R-6W&lt;/li&gt;\n&lt;li&gt;6 5south 6West&lt;/li&gt;\n&lt;li&gt;06 05South 06West&lt;/li&gt;\n&lt;li&gt;S06 05S 06W&lt;/li&gt;\n&lt;li&gt;S6 5S 6W&lt;/li&gt;\n&lt;li&gt;6-5S-6W&lt;/li&gt;\n&lt;li&gt;06-05-06&lt;/li&gt;\n&lt;li&gt;6-5-6&lt;/li&gt;\n&lt;li&gt;Section 06 Township 05S Range 06W&lt;/li&gt;\n&lt;li&gt;Section 6 Township 5S Range 6W&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;4. Document Type (almost all documents can be identified as the following)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Oil &amp;amp; Gas Lease&lt;/li&gt;\n&lt;li&gt;Mineral Deed&lt;/li&gt;\n&lt;li&gt;Assignment&lt;/li&gt;\n&lt;li&gt;Bill of Sale&lt;/li&gt;\n&lt;li&gt;Division Order&lt;/li&gt;\n&lt;li&gt;Probate&lt;/li&gt;\n&lt;li&gt;Pay Stub or Revenue Statement&lt;/li&gt;\n&lt;li&gt;Affidavit&lt;/li&gt;\n&lt;li&gt;Affidavit of Death&lt;/li&gt;\n&lt;li&gt;Affidavit of Heirship&lt;/li&gt;\n&lt;li&gt;Affidavit of Pooling&lt;/li&gt;\n&lt;li&gt;Exhibit (A, B, C, etc.)&lt;/li&gt;\n&lt;li&gt;Quit Claim Deed&lt;/li&gt;\n&lt;li&gt;Assignment of Mortgage&lt;/li&gt;\n&lt;li&gt;Warranty Deed&lt;/li&gt;\n&lt;li&gt;Wellbore Assignment&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here I\u2019m going to reference some example documents and describe the challenge.  Example documents can be found here...&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.dropbox.com/t/e1Nc8Eg3eNpRLcwU\"&gt;https://www.dropbox.com/t/e1Nc8Eg3eNpRLcwU&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;An *easy* example...&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Mineral Deed-Atoka-1 pg.pdf&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Two Documents with many STR...&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Agreement to Purchase Minerals-Coal County-Book 0712-Page 874-878.pdf&lt;/p&gt;\n\n&lt;p&gt;Wellbore Assignment-Logan County-Book 3006-Page 371-383.pdf&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Document with many STR that are sideways, and the STR is broken into columns&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Assignment and Bill of Sale-Grady County-Book 6012-Page 486-503.pdf&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Document that lists Township and Range, then lists a bunch of sections below. We need to make this searchable by one of the recognized complete STR formats above.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Affidavit Coal_Book719_page423.JPG&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ldij2", "is_robot_indexable": true, "report_reasons": null, "author": "theoffshoot2", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ldij2/sophisticated_ocr_post_processing_of_courthouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ldij2/sophisticated_ocr_post_processing_of_courthouse/", "subreddit_subscribers": 87472, "created_utc": 1674690193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently had an interview with heavy focus on spark. There were two questions that were troubling to me, I was not able to understand on how to answer them. Putting them out here, wanting to know how would you approach them.  \n1. How would you delete data of one customer from your system. So imagine you have few PBs of data regarding your customer many interactions with your product, now a customer asks to have everything on him erased (GDPR compliance), so how do you delete the data for this customer.   \n\\&gt;&gt;Scanning all the data doesn't make sense. I asked about how data is strucuted, partitioned, any columns on which I can identify the customer etc but I was not able to come up with a good answer for this.  \n\n\n2. You read a 50 GB file in spark - How many job/stages/partitions would be created? No other info regarding anything was provided, when I asked about config and such, I was asked to assume everything as default.", "author_fullname": "t2_cu6opso3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10lbxlk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674686190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently had an interview with heavy focus on spark. There were two questions that were troubling to me, I was not able to understand on how to answer them. Putting them out here, wanting to know how would you approach them.&lt;br/&gt;\n1. How would you delete data of one customer from your system. So imagine you have few PBs of data regarding your customer many interactions with your product, now a customer asks to have everything on him erased (GDPR compliance), so how do you delete the data for this customer.&lt;br/&gt;\n&amp;gt;&amp;gt;Scanning all the data doesn&amp;#39;t make sense. I asked about how data is strucuted, partitioned, any columns on which I can identify the customer etc but I was not able to come up with a good answer for this.  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;You read a 50 GB file in spark - How many job/stages/partitions would be created? No other info regarding anything was provided, when I asked about config and such, I was asked to assume everything as default.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10lbxlk", "is_robot_indexable": true, "report_reasons": null, "author": "bha159", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10lbxlk/interview_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10lbxlk/interview_experience/", "subreddit_subscribers": 87472, "created_utc": 1674686190.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_12li6zgs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "#TrueDataOps Podcast with Kent Graziano and Cynthia Meyersohn", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 64, "top_awarded_type": null, "hide_score": false, "name": "t3_10l26y4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 64, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/8t7rKfLuWoPw2lsEIluUc7_LIF5aAiixmBHG8kUrlm8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674662832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/video/event/urn:li:ugcPost:7022215454584705024/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5ZI7oL3JTPPt59G0vTfOaQMHvka17QCAdFnF87leUeA.jpg?auto=webp&amp;v=enabled&amp;s=751b05e77b1c50dfc8477f4c599cb33affc7e2fc", "width": 64, "height": 64}, "resolutions": [], "variants": {}, "id": "QqSY3F9i2BgB-OdT_JpQr1vBqr2oq4spYNzkghHXwCM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10l26y4", "is_robot_indexable": true, "report_reasons": null, "author": "Dkreig", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10l26y4/truedataops_podcast_with_kent_graziano_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/video/event/urn:li:ugcPost:7022215454584705024/", "subreddit_subscribers": 87472, "created_utc": 1674662832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nSo I'm evaluating replacing some of our ingestion tech with airbyte.  Working through some local testing with it, however I'm hoping to get anyone who has experience with the tool's take.  Thanks in advance.  \n\n\n1. What's your overall impression on usability, particularly I need it for simple extract from source and sink to s3.\n2. How alpha are the alpha connectors?  Some of our big sources only have alpha connectors so curious if they're extremely buggy or its more like a public preview and has some quirks but is overall stable.", "author_fullname": "t2_pg4xpqjk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte/ Airbyte Alpha connectors", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kzzn4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674657208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m evaluating replacing some of our ingestion tech with airbyte.  Working through some local testing with it, however I&amp;#39;m hoping to get anyone who has experience with the tool&amp;#39;s take.  Thanks in advance.  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What&amp;#39;s your overall impression on usability, particularly I need it for simple extract from source and sink to s3.&lt;/li&gt;\n&lt;li&gt;How alpha are the alpha connectors?  Some of our big sources only have alpha connectors so curious if they&amp;#39;re extremely buggy or its more like a public preview and has some quirks but is overall stable.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10kzzn4", "is_robot_indexable": true, "report_reasons": null, "author": "LowOwl2591", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10kzzn4/airbyte_airbyte_alpha_connectors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10kzzn4/airbyte_airbyte_alpha_connectors/", "subreddit_subscribers": 87472, "created_utc": 1674657208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any recommendations for a browser-based visual query builder tool? \n\nWe\u2019re looking for a tool to allow business users to query our customers table and put together custom email lists (ie \u201creturn all customers in Albuquerque\u201d) without having to learn SQL. Standard BI tools tend to fall apart when exporting &gt; 100k rows since they\u2019re designed for analytics, not data dumps.\n\nAny suggestions?", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visual query builder tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10lb32i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674684096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any recommendations for a browser-based visual query builder tool? &lt;/p&gt;\n\n&lt;p&gt;We\u2019re looking for a tool to allow business users to query our customers table and put together custom email lists (ie \u201creturn all customers in Albuquerque\u201d) without having to learn SQL. Standard BI tools tend to fall apart when exporting &amp;gt; 100k rows since they\u2019re designed for analytics, not data dumps.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10lb32i", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10lb32i/visual_query_builder_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10lb32i/visual_query_builder_tool/", "subreddit_subscribers": 87472, "created_utc": 1674684096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For AIRFLOW installed on an EC2 machine. What would be a good way to setup GIT based deployment of DAGs in the DAG folder?", "author_fullname": "t2_6lp7aig4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AIRFLOW", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10l5i5z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674670814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For AIRFLOW installed on an EC2 machine. What would be a good way to setup GIT based deployment of DAGs in the DAG folder?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10l5i5z", "is_robot_indexable": true, "report_reasons": null, "author": "ask_can", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10l5i5z/airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10l5i5z/airflow/", "subreddit_subscribers": 87472, "created_utc": 1674670814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ce2ldlob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EP 3 - Migrating from Delta Lake to Iceberg", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10l58ez", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F1q80PoBn46DPeJrfoIzl6F%2Fvideo%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F1q80PoBn46DPeJrfoIzl6F&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1fe0c101f090886a328931ca5e&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"600\" height=\"338\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 600, "scrolling": false, "height": 338}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "open.spotify.com", "oembed": {"provider_url": "https://spotify.com", "description": "Listen to this episode from Gnarly Data Waves by Dremio on Spotify. Iceberg has been gaining wide adoption in the industry as the defacto open standard for data lakehouse table formats. Join us as we help you learn the options and strategies you can employ when migrating tables from Delta Lake to Apache Iceberg.", "title": "EP 3 - Migrating from Delta Lake to Iceberg", "type": "video", "thumbnail_width": 300, "height": 338, "width": 600, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F1q80PoBn46DPeJrfoIzl6F%2Fvideo%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F1q80PoBn46DPeJrfoIzl6F&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1fe0c101f090886a328931ca5e&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"600\" height=\"338\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Spotify", "thumbnail_url": "https://i.scdn.co/image/ab67656300005f1fe0c101f090886a328931ca5e", "thumbnail_height": 300}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F1q80PoBn46DPeJrfoIzl6F%2Fvideo%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F1q80PoBn46DPeJrfoIzl6F&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1fe0c101f090886a328931ca5e&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"600\" height=\"338\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 600, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/10l58ez", "height": 338}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2Gv3dBqGNIu_JRYNqOi3_QYk0tIVPAIz7qNSSq80orI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1674670178.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.spotify.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.spotify.com/episode/1q80PoBn46DPeJrfoIzl6F?si=eduyW-bdQL-0fnjINhtptA", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FKmWnPRFKnUQon_nsuTRduPAqHmBmEx7cm_dfhr9bas.jpg?auto=webp&amp;v=enabled&amp;s=f1e86b6f17cbe610313d58716c956c15507d065d", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/FKmWnPRFKnUQon_nsuTRduPAqHmBmEx7cm_dfhr9bas.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c71a49f983cece5e49c77096d4390f395789674e", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/FKmWnPRFKnUQon_nsuTRduPAqHmBmEx7cm_dfhr9bas.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af5a4df3d45b9773725a32b48e7d2c7a09b3802a", "width": 216, "height": 216}], "variants": {}, "id": "RGUnRCYv_Tq1o1pYkDmpI9ZxNBBAR87rLdnvKeqoOhg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10l58ez", "is_robot_indexable": true, "report_reasons": null, "author": "amdatalakehouse", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10l58ez/ep_3_migrating_from_delta_lake_to_iceberg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.spotify.com/episode/1q80PoBn46DPeJrfoIzl6F?si=eduyW-bdQL-0fnjINhtptA", "subreddit_subscribers": 87472, "created_utc": 1674670178.0, "num_crossposts": 0, "media": {"type": "open.spotify.com", "oembed": {"provider_url": "https://spotify.com", "description": "Listen to this episode from Gnarly Data Waves by Dremio on Spotify. Iceberg has been gaining wide adoption in the industry as the defacto open standard for data lakehouse table formats. Join us as we help you learn the options and strategies you can employ when migrating tables from Delta Lake to Apache Iceberg.", "title": "EP 3 - Migrating from Delta Lake to Iceberg", "type": "video", "thumbnail_width": 300, "height": 338, "width": 600, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F1q80PoBn46DPeJrfoIzl6F%2Fvideo%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F1q80PoBn46DPeJrfoIzl6F&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1fe0c101f090886a328931ca5e&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"600\" height=\"338\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Spotify", "thumbnail_url": "https://i.scdn.co/image/ab67656300005f1fe0c101f090886a328931ca5e", "thumbnail_height": 300}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there I'm a new data engineer and this is my first task at my new job so please bear with me if it sounds dumb. \n\nI am tasked to build a test pipeline in Synapse which should copy historical data from an Azure SQL DB and then load it into my test container in ADLS2. The historical data has to be stored in a YYYY/YYYYMM/YYYYMMDD format in the ADLS2 container as Json files. I have a column in the table called the datekey which is an integer in YYYYMMDD format. They already have an incremental pipeline run that takes care of all current data from December 2022 so they want me to export all data before December 2022. \n\nI am only familiar with Blob storage so any pointers to resources or tips would be appreciated. Thank you.", "author_fullname": "t2_mtxnq3u8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Synapse question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10l3s2y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674666712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there I&amp;#39;m a new data engineer and this is my first task at my new job so please bear with me if it sounds dumb. &lt;/p&gt;\n\n&lt;p&gt;I am tasked to build a test pipeline in Synapse which should copy historical data from an Azure SQL DB and then load it into my test container in ADLS2. The historical data has to be stored in a YYYY/YYYYMM/YYYYMMDD format in the ADLS2 container as Json files. I have a column in the table called the datekey which is an integer in YYYYMMDD format. They already have an incremental pipeline run that takes care of all current data from December 2022 so they want me to export all data before December 2022. &lt;/p&gt;\n\n&lt;p&gt;I am only familiar with Blob storage so any pointers to resources or tips would be appreciated. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10l3s2y", "is_robot_indexable": true, "report_reasons": null, "author": "1000gratitudepunches", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10l3s2y/azure_synapse_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10l3s2y/azure_synapse_question/", "subreddit_subscribers": 87472, "created_utc": 1674666712.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi! The product is a serverless platform to easily build and distribute continuous insights as streams (to power real-time actions and live analytics).\n\nThe rationale is that it\u2019s been a nightmare for teams to easily deliver on stream processing use cases, mainly due to the lack of good/serverless tooling and we\u2019re trying to close that gap. We\u2019ve been running a private beta for some time and the platform is now looking very stable so I want to open it up to more people - especially individuals with projects (mostly been testing with enterprise users). Would love to hear your thoughts and am happy to open an instance if you have something you want to try.", "author_fullname": "t2_jmkgo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for beta testers for an event-driven analytics platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10kyjwf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674653097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! The product is a serverless platform to easily build and distribute continuous insights as streams (to power real-time actions and live analytics).&lt;/p&gt;\n\n&lt;p&gt;The rationale is that it\u2019s been a nightmare for teams to easily deliver on stream processing use cases, mainly due to the lack of good/serverless tooling and we\u2019re trying to close that gap. We\u2019ve been running a private beta for some time and the platform is now looking very stable so I want to open it up to more people - especially individuals with projects (mostly been testing with enterprise users). Would love to hear your thoughts and am happy to open an instance if you have something you want to try.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10kyjwf", "is_robot_indexable": true, "report_reasons": null, "author": "n0user", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10kyjwf/looking_for_beta_testers_for_an_eventdriven/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10kyjwf/looking_for_beta_testers_for_an_eventdriven/", "subreddit_subscribers": 87472, "created_utc": 1674653097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm currently building a database on prem with data from many sources. Some exported data are quite sensitive and needs to be transformed before written into the database, like aggregation, grouping, and deleting columns. I'm coming from an Azure background and there are various tools like Data factory, databricks or just an azure function to do these tasks. But what would you recommend for on prem? I don't want to vendor lock into a pay to use tool since I also have to justify it with my stakeholders.", "author_fullname": "t2_bgbrbly9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Transformation Tools on prem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ktja1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674633650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently building a database on prem with data from many sources. Some exported data are quite sensitive and needs to be transformed before written into the database, like aggregation, grouping, and deleting columns. I&amp;#39;m coming from an Azure background and there are various tools like Data factory, databricks or just an azure function to do these tasks. But what would you recommend for on prem? I don&amp;#39;t want to vendor lock into a pay to use tool since I also have to justify it with my stakeholders.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ktja1", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Inspection3886", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ktja1/data_transformation_tools_on_prem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ktja1/data_transformation_tools_on_prem/", "subreddit_subscribers": 87472, "created_utc": 1674633650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm creating a data pipeline for IoT data and need to limit the amount of data being stored for cost reasons. Is there some way to delete old data in a delta lake while reading in new data? Like limit to 1 day old data or a certain number of rows?\n\nOnce the raw data is processed I have no need for it anymore, thoughts?", "author_fullname": "t2_f0vap1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there some way to limit the amount of allowed rows in a delta table with structured streaming?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10lhrd9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674702161.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m creating a data pipeline for IoT data and need to limit the amount of data being stored for cost reasons. Is there some way to delete old data in a delta lake while reading in new data? Like limit to 1 day old data or a certain number of rows?&lt;/p&gt;\n\n&lt;p&gt;Once the raw data is processed I have no need for it anymore, thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10lhrd9", "is_robot_indexable": true, "report_reasons": null, "author": "Reddit_Account_C-137", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10lhrd9/is_there_some_way_to_limit_the_amount_of_allowed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10lhrd9/is_there_some_way_to_limit_the_amount_of_allowed/", "subreddit_subscribers": 87472, "created_utc": 1674702161.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am interviewing right now, and am constantly asked for experience with tools like Hadoop, glue, databricks, spark, Kafka, datafactory, etc. \n\nIn my roles, I have used Python, C#, SQL (many variations), and MongoDB, among a few other niche languages. Now I have a nice system where I orchestrate scripts w/ airflow, and i have also built windows services / daemons for different things. All for data pipeline / monitoring purposes, both streaming and batched data. I have hosted things either on-prem working with IT departments to spin up vms or have deployed on Linux using AWS EC2.\n\nMost of my experience is at mid market companies and I\u2019m largely self taught, but I have been doing this work for 5 years now after a short stint in management consulting and make a nice living with a 9-5 and 2 regular clients, using the skills listed\n\nI guess what I\u2019m asking is, what am I missing and am I missing out on anything with these tools? I have never had a need for anything other than what I listed, and am curious what I am missing", "author_fullname": "t2_lfd04s6w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the benefits of etl and big data tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10lhgo5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674701296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am interviewing right now, and am constantly asked for experience with tools like Hadoop, glue, databricks, spark, Kafka, datafactory, etc. &lt;/p&gt;\n\n&lt;p&gt;In my roles, I have used Python, C#, SQL (many variations), and MongoDB, among a few other niche languages. Now I have a nice system where I orchestrate scripts w/ airflow, and i have also built windows services / daemons for different things. All for data pipeline / monitoring purposes, both streaming and batched data. I have hosted things either on-prem working with IT departments to spin up vms or have deployed on Linux using AWS EC2.&lt;/p&gt;\n\n&lt;p&gt;Most of my experience is at mid market companies and I\u2019m largely self taught, but I have been doing this work for 5 years now after a short stint in management consulting and make a nice living with a 9-5 and 2 regular clients, using the skills listed&lt;/p&gt;\n\n&lt;p&gt;I guess what I\u2019m asking is, what am I missing and am I missing out on anything with these tools? I have never had a need for anything other than what I listed, and am curious what I am missing&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10lhgo5", "is_robot_indexable": true, "report_reasons": null, "author": "No_Caterpillar_7258", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10lhgo5/what_are_the_benefits_of_etl_and_big_data_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10lhgo5/what_are_the_benefits_of_etl_and_big_data_tools/", "subreddit_subscribers": 87472, "created_utc": 1674701296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys, I have been working with as an ETL developer for almost 10 years. I have been using tools like SSIS and Azure data factory. I want to learn DataBricks and Kafka as well. I have done some online course on databricks and Kafka and know the basic like maipulation with dataframe usine SQL and Pyspark and creating producer and consumer in a local machine for kafka. However, I haven't worked on real project with industry level. Anyone of you working on these stacks and need extra hand I would like to be part of it for my learning.\n\nThanks!!", "author_fullname": "t2_rhr52nlz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Want to learn industrial level of Databricks/Spark/Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10lhg07", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1674701241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys, I have been working with as an ETL developer for almost 10 years. I have been using tools like SSIS and Azure data factory. I want to learn DataBricks and Kafka as well. I have done some online course on databricks and Kafka and know the basic like maipulation with dataframe usine SQL and Pyspark and creating producer and consumer in a local machine for kafka. However, I haven&amp;#39;t worked on real project with industry level. Anyone of you working on these stacks and need extra hand I would like to be part of it for my learning.&lt;/p&gt;\n\n&lt;p&gt;Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10lhg07", "is_robot_indexable": true, "report_reasons": null, "author": "Brilliant-Seat-3013", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10lhg07/want_to_learn_industrial_level_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10lhg07/want_to_learn_industrial_level_of/", "subreddit_subscribers": 87472, "created_utc": 1674701241.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}