{"kind": "Listing", "data": {"after": "t3_1075852", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Constructive feedback very much appreciated.\n\nHere is the guide:\n\nhttps://medium.com/@goughgough/the-best-way-for-microsoft-teams-users-without-administrator-rights-to-save-export-print-copy-8212aa9e5f11\n\n__TL;DR:__\nTo export Teams chat messages without Microsoft Teams admin rights, download Gildas Lormeau's (GL) browser extension at https://github.com/gildas-lormeau/single-file-export-chat.\n\nBy the way, this extension is based on their excellent Singlefile browser extension.\n\n__Assumptions:__\nYou are not very tech-savvy.\n\nYou can log into Microsoft Teams in a browser at https://teams.microsoft.com/\n\nIn Teams, you do not have admin rights for a group chat. Nevertheless, you still need to export the messages from that specific group chat.\n\nYou have multiple days, months, and even years worth of Teams messages to export and you have no time for useless advice such as manual copying and pasting them one page at a time.\n\nYou want to use noncommercial software to export for free.\n\nYou want to export messages from the Chat section (in Microsoft Teams left column). NOT the Team section (in Microsoft Teams left column).\n\nYou wish to export Teams messages in their entirety, including any body text that contains clickable links.\n\nYou want to export Teams messages to a searchable final output rather than an image file.\n\nYou do not want to waste time manually copying and pasting individual Teams messages, which is one of the techniques in quite a few of the online guides. This manual copying and pasting makes sense if you only have a few Teams messages to export.\n\nYou do not want to use the GoFullPage browser extension. Even though it is not as effective as GL\u2019s solutions, it does let you export Teams messages as images (e.g., a non-searchable PDF file). Before I came across GL\u2019s methods, the GoFullPage browser extension was the best method I tried. Unfortunately, the final product is not searchable due to its image format.", "author_fullname": "t2_ipdh111g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just published my guide for Microsoft Teams users (without administrator rights) to save, export, print, copy, archive, back up, or migrate Teams conversation threads, messages, chat history. Hope you like it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106fwt4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 209, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 209, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673246654.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673173191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Constructive feedback very much appreciated.&lt;/p&gt;\n\n&lt;p&gt;Here is the guide:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@goughgough/the-best-way-for-microsoft-teams-users-without-administrator-rights-to-save-export-print-copy-8212aa9e5f11\"&gt;https://medium.com/@goughgough/the-best-way-for-microsoft-teams-users-without-administrator-rights-to-save-export-print-copy-8212aa9e5f11&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;\nTo export Teams chat messages without Microsoft Teams admin rights, download Gildas Lormeau&amp;#39;s (GL) browser extension at &lt;a href=\"https://github.com/gildas-lormeau/single-file-export-chat\"&gt;https://github.com/gildas-lormeau/single-file-export-chat&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;By the way, this extension is based on their excellent Singlefile browser extension.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;\nYou are not very tech-savvy.&lt;/p&gt;\n\n&lt;p&gt;You can log into Microsoft Teams in a browser at &lt;a href=\"https://teams.microsoft.com/\"&gt;https://teams.microsoft.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In Teams, you do not have admin rights for a group chat. Nevertheless, you still need to export the messages from that specific group chat.&lt;/p&gt;\n\n&lt;p&gt;You have multiple days, months, and even years worth of Teams messages to export and you have no time for useless advice such as manual copying and pasting them one page at a time.&lt;/p&gt;\n\n&lt;p&gt;You want to use noncommercial software to export for free.&lt;/p&gt;\n\n&lt;p&gt;You want to export messages from the Chat section (in Microsoft Teams left column). NOT the Team section (in Microsoft Teams left column).&lt;/p&gt;\n\n&lt;p&gt;You wish to export Teams messages in their entirety, including any body text that contains clickable links.&lt;/p&gt;\n\n&lt;p&gt;You want to export Teams messages to a searchable final output rather than an image file.&lt;/p&gt;\n\n&lt;p&gt;You do not want to waste time manually copying and pasting individual Teams messages, which is one of the techniques in quite a few of the online guides. This manual copying and pasting makes sense if you only have a few Teams messages to export.&lt;/p&gt;\n\n&lt;p&gt;You do not want to use the GoFullPage browser extension. Even though it is not as effective as GL\u2019s solutions, it does let you export Teams messages as images (e.g., a non-searchable PDF file). Before I came across GL\u2019s methods, the GoFullPage browser extension was the best method I tried. Unfortunately, the final product is not searchable due to its image format.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 150, "id": "award_f44611f1-b89e-46dc-97fe-892280b13b82", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Thank you stranger. Shows the award.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Helpful", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106fwt4", "is_robot_indexable": true, "report_reasons": null, "author": "cashpayer", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106fwt4/just_published_my_guide_for_microsoft_teams_users/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106fwt4/just_published_my_guide_for_microsoft_teams_users/", "subreddit_subscribers": 664864, "created_utc": 1673173191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm wondering if I'm weird or not... but I enjoy watching my downloads go and mental place bets on which download will finish first. Does anyone else do this, or am I just... weird?\n\n&amp;#x200B;\n\nEDIT: Wow, thank you generous Redditor for the Award! \ud83e\udd29", "author_fullname": "t2_8zyu4htp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone else watch their downloads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10701td", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 194, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 194, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673243399.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673225968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering if I&amp;#39;m weird or not... but I enjoy watching my downloads go and mental place bets on which download will finish first. Does anyone else do this, or am I just... weird?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EDIT: Wow, thank you generous Redditor for the Award! \ud83e\udd29&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "award_2ae56630-cfe0-424e-b810-4945b9145358", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/Animated_Helpful_128.png", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Thank you stranger. Gives %{coin_symbol}100 Coins to both the author and the community.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 100, "count": 1, "static_icon_height": 2048, "name": "Helpful (Pro)", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=57278b329d19fd1d345888bfff68a51528777538", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=db7b3f20402a8a6820a4ffebf35160d2557986e2", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=0100d8da8f4dae0dc81d430733aa622d752c268c", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=1029c080a179f45b6d83a51ed79dfd57997ae266", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=50e7f8a870f79df7bc38bedb8a12e01137df5a77", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_q0gj4/zkc9cw88c8361_ChristmasHelpful.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "70TB usable, 48TB backup, 70TB cloud backup", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10701td", "is_robot_indexable": true, "report_reasons": null, "author": "ComputingElephant", "discussion_type": null, "num_comments": 78, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10701td/does_anyone_else_watch_their_downloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10701td/does_anyone_else_watch_their_downloads/", "subreddit_subscribers": 664864, "created_utc": 1673225968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need help finding duplicates of my JAV collection, the problem is that some files have their titles in different format like:\n\na) GVG-001.mp4\n\nb) GVG001.mp4\n\nc) GVG001 - additional movie title\n\nWhat software would you recommend to find dupes? Duplicate files might have different lengths so CRC search will not work. I need some way to analyze the first few letters of a filename keeping in mind that there may or may not be \"-\" sing in there. Can you help me out?\n\nI have total commander but I'm not sure if its search function is as powerful.", "author_fullname": "t2_tmiw5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for finding duplicate files based on a similar filename.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106g20g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673173717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help finding duplicates of my JAV collection, the problem is that some files have their titles in different format like:&lt;/p&gt;\n\n&lt;p&gt;a) GVG-001.mp4&lt;/p&gt;\n\n&lt;p&gt;b) GVG001.mp4&lt;/p&gt;\n\n&lt;p&gt;c) GVG001 - additional movie title&lt;/p&gt;\n\n&lt;p&gt;What software would you recommend to find dupes? Duplicate files might have different lengths so CRC search will not work. I need some way to analyze the first few letters of a filename keeping in mind that there may or may not be &amp;quot;-&amp;quot; sing in there. Can you help me out?&lt;/p&gt;\n\n&lt;p&gt;I have total commander but I&amp;#39;m not sure if its search function is as powerful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106g20g", "is_robot_indexable": true, "report_reasons": null, "author": "wooshaq", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106g20g/software_for_finding_duplicate_files_based_on_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106g20g/software_for_finding_duplicate_files_based_on_a/", "subreddit_subscribers": 664864, "created_utc": 1673173717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a a 1TB SanDisk 520s and I am looking for a solution to expand my storage capacity. Obviously I could just get another 1TB, but then I would then have multiple drives I need to eject when I move my MacBook about.\n\nLooking at YouTube videos it does look like it has an M2 NVME drive installed, so this got me thinking. For the same price of another 1TB sandisk I can get a 2tb M2 drive.\n\nIs there some enclose preferably unpowered (or type c) that can hold multiple M2s? I suppose two drives would be ok, but would prefer four. I have looked on Amazon and they either seem to be exposed, too big and also need power.\n\nRaid capability isn't necessary. Portability is fairly important though.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/j2bssghsbvaa1.jpg?width=1500&amp;format=pjpg&amp;auto=webp&amp;s=41d07a33f39622a5df81dac42d95e4fed58059a9", "author_fullname": "t2_1gm1x1ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a solution for a multi M2 drive storage with a single cable. Have a 1TB SanDisk and want to expand.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 136, "top_awarded_type": null, "hide_score": false, "media_metadata": {"j2bssghsbvaa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 105, "x": 108, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=77213af351033a331e12698f8fd978e66d31f5b6"}, {"y": 210, "x": 216, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8883baee2817b6e67db79fedb328247d0b8d9bab"}, {"y": 312, "x": 320, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ff67dcc4cc07c268e4297d71b4100401dce0ac5"}, {"y": 624, "x": 640, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1ed99c8890df4ea17367f49af5feac416f0526e7"}, {"y": 936, "x": 960, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4a24605ba408fd541bb62c311a5b6cbe1669d182"}, {"y": 1053, "x": 1080, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8580e7afd77f36be18428e754b33d33b762f5f2d"}], "s": {"y": 1463, "x": 1500, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=1500&amp;format=pjpg&amp;auto=webp&amp;s=41d07a33f39622a5df81dac42d95e4fed58059a9"}, "id": "j2bssghsbvaa1"}}, "name": "t3_106fh0r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/38krtbUG4CznLJGNjfsGTJkU05UacwwdYt4J-2n4Rss.jpg", "edited": 1673204933.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673171633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a a 1TB SanDisk 520s and I am looking for a solution to expand my storage capacity. Obviously I could just get another 1TB, but then I would then have multiple drives I need to eject when I move my MacBook about.&lt;/p&gt;\n\n&lt;p&gt;Looking at YouTube videos it does look like it has an M2 NVME drive installed, so this got me thinking. For the same price of another 1TB sandisk I can get a 2tb M2 drive.&lt;/p&gt;\n\n&lt;p&gt;Is there some enclose preferably unpowered (or type c) that can hold multiple M2s? I suppose two drives would be ok, but would prefer four. I have looked on Amazon and they either seem to be exposed, too big and also need power.&lt;/p&gt;\n\n&lt;p&gt;Raid capability isn&amp;#39;t necessary. Portability is fairly important though.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/j2bssghsbvaa1.jpg?width=1500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=41d07a33f39622a5df81dac42d95e4fed58059a9\"&gt;https://preview.redd.it/j2bssghsbvaa1.jpg?width=1500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=41d07a33f39622a5df81dac42d95e4fed58059a9&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106fh0r", "is_robot_indexable": true, "report_reasons": null, "author": "matmah", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106fh0r/looking_for_a_solution_for_a_multi_m2_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106fh0r/looking_for_a_solution_for_a_multi_m2_drive/", "subreddit_subscribers": 664864, "created_utc": 1673171633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have quite a collection of MP3 audio books that I have collected over the decades.  Some are divided by chapter.  Some are even divided by chapter in different CD folders.  I am looking for a good Windows app that will take a folder of MP3s (with even subfolders) and convert them into a single M4B.  OpenAudible lets me import the MP3s, but I can't figure out how to combine and convert them using that, so I'm looking for suggestions.", "author_fullname": "t2_6qsou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Windows app for combining multiple MP3s into single M4B", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106m2yz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673192233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have quite a collection of MP3 audio books that I have collected over the decades.  Some are divided by chapter.  Some are even divided by chapter in different CD folders.  I am looking for a good Windows app that will take a folder of MP3s (with even subfolders) and convert them into a single M4B.  OpenAudible lets me import the MP3s, but I can&amp;#39;t figure out how to combine and convert them using that, so I&amp;#39;m looking for suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106m2yz", "is_robot_indexable": true, "report_reasons": null, "author": "djeaton", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106m2yz/best_windows_app_for_combining_multiple_mp3s_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106m2yz/best_windows_app_for_combining_multiple_mp3s_into/", "subreddit_subscribers": 664864, "created_utc": 1673192233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "  \nIt seems to be that the tool is for windows --&gt; linux, there is an issue asking for linux -&gt; linux support.   \n\n\nI wonder can we incorporate the learnings from this repo to make rsync faster?  \n\n\n[https://github.com/google/cdc-file-transfer](https://github.com/google/cdc-file-transfer)", "author_fullname": "t2_10swnr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google made a tool like rsync which is 3x faster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1070uod", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673228097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems to be that the tool is for windows --&amp;gt; linux, there is an issue asking for linux -&amp;gt; linux support.   &lt;/p&gt;\n\n&lt;p&gt;I wonder can we incorporate the learnings from this repo to make rsync faster?  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/google/cdc-file-transfer\"&gt;https://github.com/google/cdc-file-transfer&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/F3gS0keU5u6APy4XFmDVh3rKuENeV73WiCiMVb2dv9c.jpg?auto=webp&amp;s=883bef74661df66b12c436dddd0f17cb1d2cffb2", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/F3gS0keU5u6APy4XFmDVh3rKuENeV73WiCiMVb2dv9c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=549bc42e11f60fb8e167274a48a305f5fb79d2e0", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/F3gS0keU5u6APy4XFmDVh3rKuENeV73WiCiMVb2dv9c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=66e8525078641c398516baeef50a272d1a75f5e4", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/F3gS0keU5u6APy4XFmDVh3rKuENeV73WiCiMVb2dv9c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6756142e99cbbfd2dd7a790ed458b247ce78d9c4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/F3gS0keU5u6APy4XFmDVh3rKuENeV73WiCiMVb2dv9c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=599480085122faa0760e6ca95430262de46f3414", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/F3gS0keU5u6APy4XFmDVh3rKuENeV73WiCiMVb2dv9c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fca795667d9d17ef7424a70c68d51dfd0b926360", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/F3gS0keU5u6APy4XFmDVh3rKuENeV73WiCiMVb2dv9c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=65a635ebac776d18eed85c14c15493d3ab2fac6d", "width": 1080, "height": 540}], "variants": {}, "id": "kGsARX3xzB05E0JSMq50t6awGzdJuHpNghARJ63YZ1A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1070uod", "is_robot_indexable": true, "report_reasons": null, "author": "umbcorp", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1070uod/google_made_a_tool_like_rsync_which_is_3x_faster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1070uod/google_made_a_tool_like_rsync_which_is_3x_faster/", "subreddit_subscribers": 664864, "created_utc": 1673228097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\nThe HDDs will be part of my self built NAS, which is housed in an old Fractal Design Define R6 - a case that comes with some amount of soundproofing. Unfortunately, in my apartment I don't have any space where noise absolutely does not matter. For now, it will be in the working room, where I and the gf spend significant amounts of time trying to be productive. Because of this, I want to buy HDDs that are the most likely to not annoy us too much. Read/Write performance is a secondary concern, so long as they make full use of my gigabit ethernet connection.\n\nI've done a bunch of research on this; both in this sub and beyond, and unfortunately there is no drive for which I don't find three different people making five contradicting claims about their noise levels.\n\nFor instance, I've heard a bunch of good things about the Ultrastar DC HC550 drives; but other sources say they produce a thudding noise every five seconds or so, which may or may not be quite annoying.\n\nI also recently got a good deal on two Toshiba MG08ACA16TE drives which are sitting here unused so far, as I'm considering returning them. I've heard people claim they're rather quiet or really loud.\n\nIt would be helpful to hear from people who have tried different HDDs from different companies and thus can give some sort of comparison. I am also totally open to other models or to shucking, if that's still a thing.\n\nOS Wise I am likely going with True NAS Core for the NAS, if this has any bearing on the question.", "author_fullname": "t2_pvd4p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quiet HDDs for NAS in working room?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106tcb2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673209736.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;The HDDs will be part of my self built NAS, which is housed in an old Fractal Design Define R6 - a case that comes with some amount of soundproofing. Unfortunately, in my apartment I don&amp;#39;t have any space where noise absolutely does not matter. For now, it will be in the working room, where I and the gf spend significant amounts of time trying to be productive. Because of this, I want to buy HDDs that are the most likely to not annoy us too much. Read/Write performance is a secondary concern, so long as they make full use of my gigabit ethernet connection.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done a bunch of research on this; both in this sub and beyond, and unfortunately there is no drive for which I don&amp;#39;t find three different people making five contradicting claims about their noise levels.&lt;/p&gt;\n\n&lt;p&gt;For instance, I&amp;#39;ve heard a bunch of good things about the Ultrastar DC HC550 drives; but other sources say they produce a thudding noise every five seconds or so, which may or may not be quite annoying.&lt;/p&gt;\n\n&lt;p&gt;I also recently got a good deal on two Toshiba MG08ACA16TE drives which are sitting here unused so far, as I&amp;#39;m considering returning them. I&amp;#39;ve heard people claim they&amp;#39;re rather quiet or really loud.&lt;/p&gt;\n\n&lt;p&gt;It would be helpful to hear from people who have tried different HDDs from different companies and thus can give some sort of comparison. I am also totally open to other models or to shucking, if that&amp;#39;s still a thing.&lt;/p&gt;\n\n&lt;p&gt;OS Wise I am likely going with True NAS Core for the NAS, if this has any bearing on the question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106tcb2", "is_robot_indexable": true, "report_reasons": null, "author": "SWHH", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106tcb2/quiet_hdds_for_nas_in_working_room/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106tcb2/quiet_hdds_for_nas_in_working_room/", "subreddit_subscribers": 664864, "created_utc": 1673209736.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I seem to have Rclone setup properly, but Im a bit confused on how I would clone an entire folder of my external drive(connected to my mac) into a specific folder in my Google drive account. My Rclone account name will be NAME for the purpose of this post\n\nThe folder that I'm looking to copy is in this chain \"4tb&gt;offloaded footage&gt;categorized&gt;DJI&gt;2021\n\nand I'm looking to copy this to my google drive \"My Drive&gt;Categorized footage&gt;DJI&gt;2021\n\nHere is the terminal command that I'm using\n\n    rclone -v sync /Volumes/4TB\\ Offloaded\\ Footage/Categorized/DJI/2021 NAME:\n\nThen what do I put after this to get the contents copied into \n\n&gt;My Drive&gt;Categorized footage&gt;DJI&gt;2021", "author_fullname": "t2_y7xmw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using RClone to copy folder to folder?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106m302", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673192235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I seem to have Rclone setup properly, but Im a bit confused on how I would clone an entire folder of my external drive(connected to my mac) into a specific folder in my Google drive account. My Rclone account name will be NAME for the purpose of this post&lt;/p&gt;\n\n&lt;p&gt;The folder that I&amp;#39;m looking to copy is in this chain &amp;quot;4tb&amp;gt;offloaded footage&amp;gt;categorized&amp;gt;DJI&amp;gt;2021&lt;/p&gt;\n\n&lt;p&gt;and I&amp;#39;m looking to copy this to my google drive &amp;quot;My Drive&amp;gt;Categorized footage&amp;gt;DJI&amp;gt;2021&lt;/p&gt;\n\n&lt;p&gt;Here is the terminal command that I&amp;#39;m using&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;rclone -v sync /Volumes/4TB\\ Offloaded\\ Footage/Categorized/DJI/2021 NAME:\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then what do I put after this to get the contents copied into &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;My Drive&amp;gt;Categorized footage&amp;gt;DJI&amp;gt;2021&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106m302", "is_robot_indexable": true, "report_reasons": null, "author": "AllAboutGadgets", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106m302/using_rclone_to_copy_folder_to_folder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106m302/using_rclone_to_copy_folder_to_folder/", "subreddit_subscribers": 664864, "created_utc": 1673192235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just had an external hard drive become corrupted and unreadable. I thought this would be a good opportunity to upgrade my setup, but I have some questions.\n\n&amp;#x200B;\n\n1. I was thinking of getting regular hard drives and a hard drive toaster rather than external drives. Is there a consensus as to which setup is better?\n\n&amp;#x200B;\n\n2. I need to transfer data between linux and windows computers. I was using the ExFat file system for this. Is this a good idea or is there a better file system?\n\n&amp;#x200B;\n\n3. I was told recently that drives larger than 2TB are far more susceptible to corruption. Is this true? \n\n&amp;#x200B;\n\n4. What programs would you recommend for data recovery on a hard drive?\n\n&amp;#x200B;\n\nThank you", "author_fullname": "t2_x9tme", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me upgrade my storage setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106ojil", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673198293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just had an external hard drive become corrupted and unreadable. I thought this would be a good opportunity to upgrade my setup, but I have some questions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I was thinking of getting regular hard drives and a hard drive toaster rather than external drives. Is there a consensus as to which setup is better?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I need to transfer data between linux and windows computers. I was using the ExFat file system for this. Is this a good idea or is there a better file system?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I was told recently that drives larger than 2TB are far more susceptible to corruption. Is this true? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What programs would you recommend for data recovery on a hard drive?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106ojil", "is_robot_indexable": true, "report_reasons": null, "author": "archmage24601", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106ojil/help_me_upgrade_my_storage_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106ojil/help_me_upgrade_my_storage_setup/", "subreddit_subscribers": 664864, "created_utc": 1673198293.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Considering we are data hoarders, file and directory organization is paramount, so I ask you, the Data Hoarders, how do you organize and classify you data?\n\nLooking forward to your insights.", "author_fullname": "t2_11dj0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you organize your data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106yq0m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673222507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Considering we are data hoarders, file and directory organization is paramount, so I ask you, the Data Hoarders, how do you organize and classify you data?&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your insights.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "30TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "106yq0m", "is_robot_indexable": true, "report_reasons": null, "author": "nando1969", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/106yq0m/how_do_you_organize_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106yq0m/how_do_you_organize_your_data/", "subreddit_subscribers": 664864, "created_utc": 1673222507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi folks,\n\nNot sure if this is the right place to post this question, but here goes.\n\nI have boxes and boxes of photography slides that I'd like to digitise. Currently, my solution is to buy something like [https://www.amazon.co.uk/dp/B0074H6NTO](https://www.amazon.co.uk/dp/B0074H6NTO?tag=georiot-trd-21&amp;th=1&amp;ascsubtag=dcw-gb-8778368215956837000-21&amp;geniuslink=true), but that involves me having to fill the trays with 4 slides at a time. Obviously not totally feasible with thousands of slides.\n\nDoes anyone have any experience with a project like this? Or ideas on how to proceed?\n\nCheers.", "author_fullname": "t2_3ued9syv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digitising thousands of 35mm photo slides", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106j8ia", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673184407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;Not sure if this is the right place to post this question, but here goes.&lt;/p&gt;\n\n&lt;p&gt;I have boxes and boxes of photography slides that I&amp;#39;d like to digitise. Currently, my solution is to buy something like &lt;a href=\"https://www.amazon.co.uk/dp/B0074H6NTO?tag=georiot-trd-21&amp;amp;th=1&amp;amp;ascsubtag=dcw-gb-8778368215956837000-21&amp;amp;geniuslink=true\"&gt;https://www.amazon.co.uk/dp/B0074H6NTO&lt;/a&gt;, but that involves me having to fill the trays with 4 slides at a time. Obviously not totally feasible with thousands of slides.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience with a project like this? Or ideas on how to proceed?&lt;/p&gt;\n\n&lt;p&gt;Cheers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106j8ia", "is_robot_indexable": true, "report_reasons": null, "author": "thisismyfirsttime123", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106j8ia/digitising_thousands_of_35mm_photo_slides/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106j8ia/digitising_thousands_of_35mm_photo_slides/", "subreddit_subscribers": 664864, "created_utc": 1673184407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There's a book I own on military uniforms, and I'm trying to get a non-grainy image of [this fella](https://i.imgur.com/yr541Sn.jpg) uploaded to my computer. What is the best machine/method of getting this image from the book to the computer as accurately as possible? My alternative is trying to PS the grain out of the image I posted, but I don't even know if that's possible.", "author_fullname": "t2_n982b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the easiest way to scan an image from a book as clearly as possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1075tsf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673242157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a book I own on military uniforms, and I&amp;#39;m trying to get a non-grainy image of &lt;a href=\"https://i.imgur.com/yr541Sn.jpg\"&gt;this fella&lt;/a&gt; uploaded to my computer. What is the best machine/method of getting this image from the book to the computer as accurately as possible? My alternative is trying to PS the grain out of the image I posted, but I don&amp;#39;t even know if that&amp;#39;s possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yg5pV5bD6535dEMfmUemqSAUIE69Xty_4dW1Y5Xx0mc.jpg?auto=webp&amp;s=cc70d250e2eb503e7cf500c1f972684e4347e329", "width": 771, "height": 1600}, "resolutions": [{"url": "https://external-preview.redd.it/yg5pV5bD6535dEMfmUemqSAUIE69Xty_4dW1Y5Xx0mc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=26f2b05b8367cb1db3aa95c3bb146e55ab151306", "width": 108, "height": 216}, {"url": "https://external-preview.redd.it/yg5pV5bD6535dEMfmUemqSAUIE69Xty_4dW1Y5Xx0mc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cda2ccec79e18bda17617134f5f9ae355736644e", "width": 216, "height": 432}, {"url": "https://external-preview.redd.it/yg5pV5bD6535dEMfmUemqSAUIE69Xty_4dW1Y5Xx0mc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba2785f625bf427b58469f738837dffff2bccd3d", "width": 320, "height": 640}, {"url": "https://external-preview.redd.it/yg5pV5bD6535dEMfmUemqSAUIE69Xty_4dW1Y5Xx0mc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=864741d8ab1e3c55e4f86fa94be0446d947e70ce", "width": 640, "height": 1280}], "variants": {}, "id": "mN1LaWXmgj3fQa6ps5x19PV9Wvn108QOga1J8H2LyUA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1075tsf", "is_robot_indexable": true, "report_reasons": null, "author": "Burnnoticelover", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1075tsf/what_is_the_easiest_way_to_scan_an_image_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1075tsf/what_is_the_easiest_way_to_scan_an_image_from_a/", "subreddit_subscribers": 664864, "created_utc": 1673242157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm interested in setting up a CCTV system, 1 security camera only.\n\nWhere I'm at in the middle of nowhere (Australia) there isn't fixed line, and there is barely any data signal with most carriers.\n\nWhat I'd like is to have the security camera recording to a local system, let's say a simple computer setup with some HDDs that constantly records with a set retention.\n\nBut I'd also want to be able to remotely view what's happening when I am not at the location, and being limited with internet options (cellular plans only) the cost of 24/7 streaming online would not be viable. So, I thought about a system that: records locally, then every 1 hour (more or less) uploads 1 photo to a cloud service, let's say back blaze (from a portable wifi router connected via SIM). That way, I can look to check to see if there is anything gone occasionally without having to livestream which would use a huge amount of data, and if something did happen, I can go check the local storage and see what exactly happened.\n\nIdeally, the only monthly costs would be:\n\nSIM data plan\n\nBackblaze\n\nIf anyone has some knowledge on what cameras are the best option or how I could achieve this, if i'm on the right track etc please feel free to share!", "author_fullname": "t2_vgleq1ng", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Outback CCTV system with limited internet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106elia", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673168431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m interested in setting up a CCTV system, 1 security camera only.&lt;/p&gt;\n\n&lt;p&gt;Where I&amp;#39;m at in the middle of nowhere (Australia) there isn&amp;#39;t fixed line, and there is barely any data signal with most carriers.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;d like is to have the security camera recording to a local system, let&amp;#39;s say a simple computer setup with some HDDs that constantly records with a set retention.&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;d also want to be able to remotely view what&amp;#39;s happening when I am not at the location, and being limited with internet options (cellular plans only) the cost of 24/7 streaming online would not be viable. So, I thought about a system that: records locally, then every 1 hour (more or less) uploads 1 photo to a cloud service, let&amp;#39;s say back blaze (from a portable wifi router connected via SIM). That way, I can look to check to see if there is anything gone occasionally without having to livestream which would use a huge amount of data, and if something did happen, I can go check the local storage and see what exactly happened.&lt;/p&gt;\n\n&lt;p&gt;Ideally, the only monthly costs would be:&lt;/p&gt;\n\n&lt;p&gt;SIM data plan&lt;/p&gt;\n\n&lt;p&gt;Backblaze&lt;/p&gt;\n\n&lt;p&gt;If anyone has some knowledge on what cameras are the best option or how I could achieve this, if i&amp;#39;m on the right track etc please feel free to share!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106elia", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Wing9364", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106elia/outback_cctv_system_with_limited_internet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106elia/outback_cctv_system_with_limited_internet/", "subreddit_subscribers": 664864, "created_utc": 1673168431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for an image host (like imagur) that does not compress the image I upload. 100% lossless. I don't really care about features other than I would need to be able to see it from anywhere so would other people. Thanks.", "author_fullname": "t2_usun3qeu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lossless Image Hosting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106d1dr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673162829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for an image host (like imagur) that does not compress the image I upload. 100% lossless. I don&amp;#39;t really care about features other than I would need to be able to see it from anywhere so would other people. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106d1dr", "is_robot_indexable": true, "report_reasons": null, "author": "PuzzleheadedTennis23", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106d1dr/lossless_image_hosting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106d1dr/lossless_image_hosting/", "subreddit_subscribers": 664864, "created_utc": 1673162829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What I want is a copy program that runs overnight / in a certain time frame, so I can copy a NAS to a backup when the network is not in use.  It runs from 11pm to 6am &amp; at 6am it stops transferring until that night.  So it doesn't have to be restarted, but pickups up where it left off?\n\nI know I could use robocopy for starters, then schedule a task to kill it in the morning?\n\nAnything fanicier?  This would be using a Windows host with the NAS shares mapped on the Win box.\n\nIs there something I could use on UnRAID that would run on the server?", "author_fullname": "t2_d1ic8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time frame copier", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106tvqy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673211209.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673211024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What I want is a copy program that runs overnight / in a certain time frame, so I can copy a NAS to a backup when the network is not in use.  It runs from 11pm to 6am &amp;amp; at 6am it stops transferring until that night.  So it doesn&amp;#39;t have to be restarted, but pickups up where it left off?&lt;/p&gt;\n\n&lt;p&gt;I know I could use robocopy for starters, then schedule a task to kill it in the morning?&lt;/p&gt;\n\n&lt;p&gt;Anything fanicier?  This would be using a Windows host with the NAS shares mapped on the Win box.&lt;/p&gt;\n\n&lt;p&gt;Is there something I could use on UnRAID that would run on the server?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "128TB UnRAID &amp; 72TB Synology", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106tvqy", "is_robot_indexable": true, "report_reasons": null, "author": "hacnstein", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/106tvqy/time_frame_copier/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106tvqy/time_frame_copier/", "subreddit_subscribers": 664864, "created_utc": 1673211024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI am having issues with my freshly built NAS using TRUENAS Scale. I had an Intel core i7 12700K cpu from a previous build, so I chose to build around that. The other key components are as follows:\n\n1. MSI MAG B660M MORTAR WIFI DDR4\n2. Corsair Vengeance LPX 32GB (2 X 16GB) DDR4 3600\n\nI successfully installed the iso using a usb stick. I\u2019m at the stage where I should be able to access the web GUI. However, the link/ip address is not there and instead reads \u201c**The web interface could not be accessed. Please check network configuration**\u201d.\n\nHere are the other options:\n1. Configure network interface\n2. Configure network settings\n3. Configure static routes\n4. Change local administrator password\n5. Reset configuration to defaults\n6. Open TRUENAS CLI Shell\n7. Open Linux Shell\n8. Reboot\n9. Shutdown\n\nFrankly, I am not very technologically inclined and feel I\u2019ve made a mistake. I\u2019m not sure what I can do. I\u2019ve attempted looking up solutions and they tend to say it\u2019s a problem with the built in Wi-Fi (2.5G LAN and Intel Wi-Fi 6E Solution). I\u2019m in around 5 hours of research and all I\u2019m seeing is possibly getting a Intel NIC. That would be fine, but my home does not have Ethernet. \n\nThe only thing I can really think of is:\n\nA) Update the motherboard bios (not sure how to do this given it\u2019s on a NAS os)\n\nB) Buy a extremely expensive server board with w680 chipset. This would mean spending about 2x the amount of money I already have.\n\nC) Scrap it and just buy a synology which is still a net negative but at least I know it\u2019ll work. \n\nI wanted to be able to mess around with Linux and it\u2019s capabilities but this is beyond frustrating and frankly a bit demoralizing being my first experience with it.", "author_fullname": "t2_rwbgmvw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TRUENAS scale error: The web interface could not be accessed. New to NAS and could use some advice.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106t4gb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673209207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I am having issues with my freshly built NAS using TRUENAS Scale. I had an Intel core i7 12700K cpu from a previous build, so I chose to build around that. The other key components are as follows:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;MSI MAG B660M MORTAR WIFI DDR4&lt;/li&gt;\n&lt;li&gt;Corsair Vengeance LPX 32GB (2 X 16GB) DDR4 3600&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I successfully installed the iso using a usb stick. I\u2019m at the stage where I should be able to access the web GUI. However, the link/ip address is not there and instead reads \u201c&lt;strong&gt;The web interface could not be accessed. Please check network configuration&lt;/strong&gt;\u201d.&lt;/p&gt;\n\n&lt;p&gt;Here are the other options:\n1. Configure network interface\n2. Configure network settings\n3. Configure static routes\n4. Change local administrator password\n5. Reset configuration to defaults\n6. Open TRUENAS CLI Shell\n7. Open Linux Shell\n8. Reboot\n9. Shutdown&lt;/p&gt;\n\n&lt;p&gt;Frankly, I am not very technologically inclined and feel I\u2019ve made a mistake. I\u2019m not sure what I can do. I\u2019ve attempted looking up solutions and they tend to say it\u2019s a problem with the built in Wi-Fi (2.5G LAN and Intel Wi-Fi 6E Solution). I\u2019m in around 5 hours of research and all I\u2019m seeing is possibly getting a Intel NIC. That would be fine, but my home does not have Ethernet. &lt;/p&gt;\n\n&lt;p&gt;The only thing I can really think of is:&lt;/p&gt;\n\n&lt;p&gt;A) Update the motherboard bios (not sure how to do this given it\u2019s on a NAS os)&lt;/p&gt;\n\n&lt;p&gt;B) Buy a extremely expensive server board with w680 chipset. This would mean spending about 2x the amount of money I already have.&lt;/p&gt;\n\n&lt;p&gt;C) Scrap it and just buy a synology which is still a net negative but at least I know it\u2019ll work. &lt;/p&gt;\n\n&lt;p&gt;I wanted to be able to mess around with Linux and it\u2019s capabilities but this is beyond frustrating and frankly a bit demoralizing being my first experience with it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106t4gb", "is_robot_indexable": true, "report_reasons": null, "author": "Karizmology", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106t4gb/truenas_scale_error_the_web_interface_could_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106t4gb/truenas_scale_error_the_web_interface_could_not/", "subreddit_subscribers": 664864, "created_utc": 1673209207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, I wanted to buy a new HDD to replace my WD Elements 4TB that lately gives me some problems. I am undecided whether to buy the same model or an alternative model in which case I would not know which one. Would someone be able to advise me on valid alternatives :)", "author_fullname": "t2_2ee66xxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on which HDD to buy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106rn2p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673205667.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I wanted to buy a new HDD to replace my WD Elements 4TB that lately gives me some problems. I am undecided whether to buy the same model or an alternative model in which case I would not know which one. Would someone be able to advise me on valid alternatives :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106rn2p", "is_robot_indexable": true, "report_reasons": null, "author": "xXx_F3D3_xXx", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106rn2p/advice_on_which_hdd_to_buy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106rn2p/advice_on_which_hdd_to_buy/", "subreddit_subscribers": 664864, "created_utc": 1673205667.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently at a point in my journey where I'm looking to start planning out my storage config for my first homelab. This isn't a post asking EXACTLY what I should do but moreso trying to get a feel for what first timers don't really think about. What are common regrets you've heard or experienced when setting up the storage for homelab for the first time? I'm still deciding how much extra storage I plan on having(I know I need more than 10TB). Still deciding on a file system(I'm definitely leaning towards ZFS). Deciding whether I want to virtualize my NAS or not. For context, I plan on using this lab as part \"production\"(things like Plex, pihole, Home Assistant and a steam cache) and part testing to get more familiar with all kinds of things, from setting up a Windows domain controller to familiarizing myself with kubernetes.", "author_fullname": "t2_7sqku996", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for insight from those who've been there.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106exgz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673169614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently at a point in my journey where I&amp;#39;m looking to start planning out my storage config for my first homelab. This isn&amp;#39;t a post asking EXACTLY what I should do but moreso trying to get a feel for what first timers don&amp;#39;t really think about. What are common regrets you&amp;#39;ve heard or experienced when setting up the storage for homelab for the first time? I&amp;#39;m still deciding how much extra storage I plan on having(I know I need more than 10TB). Still deciding on a file system(I&amp;#39;m definitely leaning towards ZFS). Deciding whether I want to virtualize my NAS or not. For context, I plan on using this lab as part &amp;quot;production&amp;quot;(things like Plex, pihole, Home Assistant and a steam cache) and part testing to get more familiar with all kinds of things, from setting up a Windows domain controller to familiarizing myself with kubernetes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106exgz", "is_robot_indexable": true, "report_reasons": null, "author": "RiggedyWreckt", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106exgz/looking_for_insight_from_those_whove_been_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106exgz/looking_for_insight_from_those_whove_been_there/", "subreddit_subscribers": 664864, "created_utc": 1673169614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I've been backing up all day to google drive, using Rclone (or so I thought)\n\nEvery once in a while terminal on my mac pushes out an update, which up until this point I thought was showing progress. Looking closer it shows \n\n&gt;2023/01/08 21:16:24 ERROR : 11:18:21 - **REDACTED**: Failed to copy: googleapi: Error 403: User rate limit exceeded., userRateLimitExceeded\n2023/01/08 21:17:04 INFO  : \nTransferred:   \t  788.472 GiB / 2.121 TiB, 36%, 0 B/s, ETA -1s\nErrors:                 5 (retrying may help)\nTransferred:          290 / 775, 37%\nElapsed time:    9h27m0.7s\nTransferring:\n\nAny ideas?", "author_fullname": "t2_y7xmw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snag in backing up to Google drive with RClone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1071v85", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673230811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve been backing up all day to google drive, using Rclone (or so I thought)&lt;/p&gt;\n\n&lt;p&gt;Every once in a while terminal on my mac pushes out an update, which up until this point I thought was showing progress. Looking closer it shows &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;2023/01/08 21:16:24 ERROR : 11:18:21 - &lt;strong&gt;REDACTED&lt;/strong&gt;: Failed to copy: googleapi: Error 403: User rate limit exceeded., userRateLimitExceeded\n2023/01/08 21:17:04 INFO  : \nTransferred:      788.472 GiB / 2.121 TiB, 36%, 0 B/s, ETA -1s\nErrors:                 5 (retrying may help)\nTransferred:          290 / 775, 37%\nElapsed time:    9h27m0.7s\nTransferring:&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1071v85", "is_robot_indexable": true, "report_reasons": null, "author": "AllAboutGadgets", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1071v85/snag_in_backing_up_to_google_drive_with_rclone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1071v85/snag_in_backing_up_to_google_drive_with_rclone/", "subreddit_subscribers": 664864, "created_utc": 1673230811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had thought this would be easier since it's an Android device, but my Windows 10 PC does not seem to recognize the phone. I have installed USB drivers from the LG website, and have enabled USB debugging on the phone, but no dice. Are there any utilities that might work or alternative ways to get the files off the phone?", "author_fullname": "t2_108wws", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backing up all data (ideally root folder) from old Android phone (LG P500H)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106zino", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673224564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had thought this would be easier since it&amp;#39;s an Android device, but my Windows 10 PC does not seem to recognize the phone. I have installed USB drivers from the LG website, and have enabled USB debugging on the phone, but no dice. Are there any utilities that might work or alternative ways to get the files off the phone?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106zino", "is_robot_indexable": true, "report_reasons": null, "author": "jehube", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106zino/backing_up_all_data_ideally_root_folder_from_old/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106zino/backing_up_all_data_ideally_root_folder_from_old/", "subreddit_subscribers": 664864, "created_utc": 1673224564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I tried recovering a lost video using the Wayback Machine, but I get the error \"Attempts to archive this video failed.\".  Somehow, I'm still able to see slides if I hover over the bar that shows how much of the video has passed. What can I do?\n\nThis is the video:\n\n[https://web.archive.org/web/20200618135626/https://www.youtube.com/watch?v=LMlFwHpqEpU](https://web.archive.org/web/20200618135626/https://www.youtube.com/watch?v=LMlFwHpqEpU)", "author_fullname": "t2_mxw50928", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wayback Machine Error", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106oxb7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673199208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried recovering a lost video using the Wayback Machine, but I get the error &amp;quot;Attempts to archive this video failed.&amp;quot;.  Somehow, I&amp;#39;m still able to see slides if I hover over the bar that shows how much of the video has passed. What can I do?&lt;/p&gt;\n\n&lt;p&gt;This is the video:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://web.archive.org/web/20200618135626/https://www.youtube.com/watch?v=LMlFwHpqEpU\"&gt;https://web.archive.org/web/20200618135626/https://www.youtube.com/watch?v=LMlFwHpqEpU&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106oxb7", "is_robot_indexable": true, "report_reasons": null, "author": "Adriana_Istrate", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106oxb7/wayback_machine_error/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106oxb7/wayback_machine_error/", "subreddit_subscribers": 664864, "created_utc": 1673199208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have an external hard drive with a LOT of folders and subfolders and would like to be able to collectively and simultaneously view all of the files and folders in one of the main folders (for the purposes of sorting by date). Is that possible?", "author_fullname": "t2_i1z51n31", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about viewing files in folder/subfolder tree all at once", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106ovpx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673199106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an external hard drive with a LOT of folders and subfolders and would like to be able to collectively and simultaneously view all of the files and folders in one of the main folders (for the purposes of sorting by date). Is that possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106ovpx", "is_robot_indexable": true, "report_reasons": null, "author": "tucson_throwaway1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106ovpx/question_about_viewing_files_in_foldersubfolder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106ovpx/question_about_viewing_files_in_foldersubfolder/", "subreddit_subscribers": 664864, "created_utc": 1673199106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to convert my deceased parent's DVD library. A lot of the DVDs are not made anymore or available on stream. I have a Plex server up and running. I promised I'd do it.\n\nI feel like I'm spending half my life doing DVD ripping. There are hundreds, and I've already done at least 200. Are there any services? I managed to find someone for the CDs but not the DVDs.\n\nADDS: I'd deliver it by car, and DVDs can be thrown away after. I provide the hard drive to upload it to and then can pick-up the hard drive or have it Fedexed at the end.\n\nRight now, I download what I can and then deal with DVDs when there are no rips or no perfect rips. He did really manage to create a very obscure collection; history series that are extremely obscure and mostly unwanted by people who upload/download. A lot of the stuff available for download when compared to the DVDs has an episode missing or the like.   \n\n\nAnd I still have to digitize the books... oh the books... on the positive, I have a fantastic library at the end.", "author_fullname": "t2_pk1rztla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Service to convert DVD library to digital?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1076ne9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673249626.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673244699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to convert my deceased parent&amp;#39;s DVD library. A lot of the DVDs are not made anymore or available on stream. I have a Plex server up and running. I promised I&amp;#39;d do it.&lt;/p&gt;\n\n&lt;p&gt;I feel like I&amp;#39;m spending half my life doing DVD ripping. There are hundreds, and I&amp;#39;ve already done at least 200. Are there any services? I managed to find someone for the CDs but not the DVDs.&lt;/p&gt;\n\n&lt;p&gt;ADDS: I&amp;#39;d deliver it by car, and DVDs can be thrown away after. I provide the hard drive to upload it to and then can pick-up the hard drive or have it Fedexed at the end.&lt;/p&gt;\n\n&lt;p&gt;Right now, I download what I can and then deal with DVDs when there are no rips or no perfect rips. He did really manage to create a very obscure collection; history series that are extremely obscure and mostly unwanted by people who upload/download. A lot of the stuff available for download when compared to the DVDs has an episode missing or the like.   &lt;/p&gt;\n\n&lt;p&gt;And I still have to digitize the books... oh the books... on the positive, I have a fantastic library at the end.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1076ne9", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Cricket-861", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1076ne9/service_to_convert_dvd_library_to_digital/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1076ne9/service_to_convert_dvd_library_to_digital/", "subreddit_subscribers": 664864, "created_utc": 1673244699.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I can\u2019t find any answer online. I have a ton of old DVD\u2019s that I want to rip, and I know I need a blue ray player to rip blue ray disks, but does the same apply for 4K since you need a 4K player to play the full 4K resolution. I can find no mention of it online.\n\nI need to buy a player so I can connect it to my pc, and 4K blue ray players get expensive fast.", "author_fullname": "t2_attayt20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I need a 4K dvd player to rip 4K DVD\u2019s?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1075bdv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673240614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can\u2019t find any answer online. I have a ton of old DVD\u2019s that I want to rip, and I know I need a blue ray player to rip blue ray disks, but does the same apply for 4K since you need a 4K player to play the full 4K resolution. I can find no mention of it online.&lt;/p&gt;\n\n&lt;p&gt;I need to buy a player so I can connect it to my pc, and 4K blue ray players get expensive fast.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1075bdv", "is_robot_indexable": true, "report_reasons": null, "author": "AlternateWitness", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1075bdv/do_i_need_a_4k_dvd_player_to_rip_4k_dvds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1075bdv/do_i_need_a_4k_dvd_player_to_rip_4k_dvds/", "subreddit_subscribers": 664864, "created_utc": 1673240614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello Guys, \nWhat is you recommendation for a cloud storage to store and automatically sync my local 4 TB of data ? I would simply need just auto sync features with a valid app from the provider, and possibly a good price balance for it. \nI would mainly use it to store personal files, my music collection and audio libraries that I use for music production.\nAt the moment I just checked Google Drive that is 99\u20ac per year where I live and I was wondering if anything better exists.", "author_fullname": "t2_u634f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Cloud Storage for 4 TB data (EU - IT) MAC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1075852", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673240380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Guys, \nWhat is you recommendation for a cloud storage to store and automatically sync my local 4 TB of data ? I would simply need just auto sync features with a valid app from the provider, and possibly a good price balance for it. \nI would mainly use it to store personal files, my music collection and audio libraries that I use for music production.\nAt the moment I just checked Google Drive that is 99\u20ac per year where I live and I was wondering if anything better exists.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1075852", "is_robot_indexable": true, "report_reasons": null, "author": "Kronical_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1075852/best_cloud_storage_for_4_tb_data_eu_it_mac/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1075852/best_cloud_storage_for_4_tb_data_eu_it_mac/", "subreddit_subscribers": 664864, "created_utc": 1673240380.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}