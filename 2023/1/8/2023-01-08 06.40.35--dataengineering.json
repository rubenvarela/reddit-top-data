{"kind": "Listing", "data": {"after": null, "dist": 19, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI've been building data pipelines for a while now. In contrast to the availability of examples of code design patterns, &amp; data modeling techniques, there are few to none on data flow design patterns. In my experience building pipelines, using the appropriate data flow patterns increases feature delivery speed, decreases toil during pipeline failures, and builds trust with stakeholders.\n\nWith that in mind, I wrote an article that goes over the most commonly used data flow design patterns, what they do, when to use them, and, more importantly, when not to use them. It's aimed to give an overview of the typical data flow patterns and guidelines for choosing the appropriate one for your use case.\n\n[https://www.startdataengineering.com/post/design-patterns/](https://www.startdataengineering.com/post/design-patterns/)\n\nI'd love to hear about any patterns that I have missed. Any feedback is appreciated. I hope this helps someone :)", "author_fullname": "t2_5srxspj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline design patterns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105o50o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 175, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 175, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673095202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been building data pipelines for a while now. In contrast to the availability of examples of code design patterns, &amp;amp; data modeling techniques, there are few to none on data flow design patterns. In my experience building pipelines, using the appropriate data flow patterns increases feature delivery speed, decreases toil during pipeline failures, and builds trust with stakeholders.&lt;/p&gt;\n\n&lt;p&gt;With that in mind, I wrote an article that goes over the most commonly used data flow design patterns, what they do, when to use them, and, more importantly, when not to use them. It&amp;#39;s aimed to give an overview of the typical data flow patterns and guidelines for choosing the appropriate one for your use case.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.startdataengineering.com/post/design-patterns/\"&gt;https://www.startdataengineering.com/post/design-patterns/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear about any patterns that I have missed. Any feedback is appreciated. I hope this helps someone :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?auto=webp&amp;s=07b9ceb836c7c7084c6c77d42d60531a450215a2", "width": 1271, "height": 714}, "resolutions": [{"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=466a76c8a5472f7d08537937187bbac1b6aabc95", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6315f77e68ac7f50e47af70eb7c2c066c9eb640a", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ec53a6bfc4c92b4fd99d7382ef4c31ec411f8db", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=19347f6feaf42a0f5606d336dfbf732e35c62bf3", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=483f159de78a082763f92e6c84748d6f793af2ec", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/LLoCR9CVFgkSmBRkevUxUh9U33VSmKWzpxennMnSJ1c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6036328d1ff9c14bb619d2010d1058d33aff423", "width": 1080, "height": 606}], "variants": {}, "id": "1NhFR4isyIRe_ddLlHMNbBEPJQqFv_etsmRqZEKagZc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "105o50o", "is_robot_indexable": true, "report_reasons": null, "author": "joseph_machado", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105o50o/data_pipeline_design_patterns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105o50o/data_pipeline_design_patterns/", "subreddit_subscribers": 85559, "created_utc": 1673095202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hey guys, what the hell is spark ?\ni am pretty new and reading about spark online just create more confusion.\n\nLets say I \n\nIngest data --&gt; transform it --&gt; throw the data into BigQuery --&gt; query data from the warehouse --&gt; analysis\n\nWhere do you use Spark during this process?\n\nFrom what I understand, SPARK is used to distribute the data handling jobs onto multiple machines. But it's still very confusing to me on how you would apply it in a real world context.\n\nIs it correct to say it's like when you need to count inventory, instead of one person spending all day to count items, you have 100 people count then adding up the number?", "author_fullname": "t2_2pby6kdm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What the hell is Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105rg45", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673104824.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey guys, what the hell is spark ?\ni am pretty new and reading about spark online just create more confusion.&lt;/p&gt;\n\n&lt;p&gt;Lets say I &lt;/p&gt;\n\n&lt;p&gt;Ingest data --&amp;gt; transform it --&amp;gt; throw the data into BigQuery --&amp;gt; query data from the warehouse --&amp;gt; analysis&lt;/p&gt;\n\n&lt;p&gt;Where do you use Spark during this process?&lt;/p&gt;\n\n&lt;p&gt;From what I understand, SPARK is used to distribute the data handling jobs onto multiple machines. But it&amp;#39;s still very confusing to me on how you would apply it in a real world context.&lt;/p&gt;\n\n&lt;p&gt;Is it correct to say it&amp;#39;s like when you need to count inventory, instead of one person spending all day to count items, you have 100 people count then adding up the number?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105rg45", "is_robot_indexable": true, "report_reasons": null, "author": "The_Bear_Baron", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105rg45/what_the_hell_is_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105rg45/what_the_hell_is_spark/", "subreddit_subscribers": 85559, "created_utc": 1673104824.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The CEO of my company has decided to embark on an \u201cIT modernization\u201d project. The rules she laid down are simple: All existing workflows must be converted to AWS software-as-a-service tools. This means updating 15+ years worth of data pipelines and reports written in the Pentaho suite to services like Glue and Lambda. I\u2019m overseeing a team of 3 data engineers to get this done (I\u2019m an engineer myself as well).\n\nComplicating matters is that this won\u2019t be merely rewriting pipelines in a new language because the source systems themselves will also be replaced at the same time (new CRM tool, new HR system, etc) to abide by this mandate.\n\nAnyone been through a similar migration and have tips to share, horror stories, or moral support to offer?", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud migration stories", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105ybmh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673122153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The CEO of my company has decided to embark on an \u201cIT modernization\u201d project. The rules she laid down are simple: All existing workflows must be converted to AWS software-as-a-service tools. This means updating 15+ years worth of data pipelines and reports written in the Pentaho suite to services like Glue and Lambda. I\u2019m overseeing a team of 3 data engineers to get this done (I\u2019m an engineer myself as well).&lt;/p&gt;\n\n&lt;p&gt;Complicating matters is that this won\u2019t be merely rewriting pipelines in a new language because the source systems themselves will also be replaced at the same time (new CRM tool, new HR system, etc) to abide by this mandate.&lt;/p&gt;\n\n&lt;p&gt;Anyone been through a similar migration and have tips to share, horror stories, or moral support to offer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105ybmh", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105ybmh/cloud_migration_stories/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105ybmh/cloud_migration_stories/", "subreddit_subscribers": 85559, "created_utc": 1673122153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polars: Blazingly Fast DataFrames in Rust and Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_105ymls", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/kVy3-gMdViM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Polars: Blazingly Fast DataFrames in Rust and Python\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Polars: Blazingly Fast DataFrames in Rust and Python", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/kVy3-gMdViM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Polars: Blazingly Fast DataFrames in Rust and Python\"&gt;&lt;/iframe&gt;", "author_name": "Databricks", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/kVy3-gMdViM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Databricks"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/kVy3-gMdViM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Polars: Blazingly Fast DataFrames in Rust and Python\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/105ymls", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Iee1B1QE4EZcEmtj09fmA1TkNh-UscP-yhAIelGpjGg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673122931.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=kVy3-gMdViM", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/T_TB_hwSdS4S_dlQb5vrAaxQvoTKg7tnwv9MVTGOq-E.jpg?auto=webp&amp;s=2b3871c8287180aa1be49639004b8b1e9a2aca91", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/T_TB_hwSdS4S_dlQb5vrAaxQvoTKg7tnwv9MVTGOq-E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=465978812ffee79c9149a8337f0fe46543a7f69d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/T_TB_hwSdS4S_dlQb5vrAaxQvoTKg7tnwv9MVTGOq-E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ea41c92abd432b6edfdacb882b11334c7a000176", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/T_TB_hwSdS4S_dlQb5vrAaxQvoTKg7tnwv9MVTGOq-E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f360e7111ae5e41144f65c6cdefc20217a82604b", "width": 320, "height": 240}], "variants": {}, "id": "sBd7HxFyYggOhVsKo3Qrs5XPAG6AkEYBFFUCPLg-XFU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "105ymls", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105ymls/polars_blazingly_fast_dataframes_in_rust_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=kVy3-gMdViM", "subreddit_subscribers": 85559, "created_utc": 1673122931.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Polars: Blazingly Fast DataFrames in Rust and Python", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/kVy3-gMdViM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Polars: Blazingly Fast DataFrames in Rust and Python\"&gt;&lt;/iframe&gt;", "author_name": "Databricks", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/kVy3-gMdViM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Databricks"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_engotadk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vimanyu Chaturvedi on LinkedIn: DATA ENGINEERING ROADMAP 2023 | 109 comments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_105veky", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BDBMjZl9M-gZmZWhHe-Hp54G3HQTtTRYdMH1M2Z4Lig.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1673114848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/posts/vimanyu_data-engineering-roadmap-2023-ugcPost-7015647693871955969-hVYe", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?auto=webp&amp;s=18689041ed1ad1829e19b3f16bdf0619c3add354", "width": 1400, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd3104eb2954de61b63b4aded5889d7f481667e4", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c971a4bc289c8f96921326f627cebe12f3ac1660", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4aa8c0a35a98b8aba18c141f215320d65e017724", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b4185fb0b2f226c0a0e3e33b8be8f2575bf9ab5", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f9aebe95893f1568f8433661797ed06581b8b5b", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4632741c44875873f4766540a85113c0e703b906", "width": 1080, "height": 617}], "variants": {}, "id": "CjbMbFq2MEqSKWpNCjv-ipCLADmRBQ1ZCG3w2yy71f0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "105veky", "is_robot_indexable": true, "report_reasons": null, "author": "StrangeAd1054", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105veky/vimanyu_chaturvedi_on_linkedin_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/posts/vimanyu_data-engineering-roadmap-2023-ugcPost-7015647693871955969-hVYe", "subreddit_subscribers": 85559, "created_utc": 1673114848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Virtual peer-to-peer data engineering sessions January 25th: \n\n&amp;#x200B;\n\n* Becoming a data team lead - Matthew Weingarten (Disney)\n* Asset-based orchestration - Jonathan Neo (Canva)\n* Building data quality as a product - Alejandra Cabrera (Clearbit) and Mona Rokibe (Telmai)\n* Driving data culture change with data contracts - Andrew Jones (GoCardless)\n* Going from DevOps to DataOps - Ali Khalid (Emirates)\n* What habits do successful data engineers have? - Panel discussion\n* Going from IC to a manager - Panel discussion\n* Fireside chat about FinOps with Thiago Gil (FinOps Foundation) and Clinton Ford (Unravel)\n* Building the best data team is not rocket science Nirmal Budhathoki (Microsoft)", "author_fullname": "t2_ff7f8okm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "excited for this www.datateamssummit.com", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1064xv7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673138759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Virtual peer-to-peer data engineering sessions January 25th: &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Becoming a data team lead - Matthew Weingarten (Disney)&lt;/li&gt;\n&lt;li&gt;Asset-based orchestration - Jonathan Neo (Canva)&lt;/li&gt;\n&lt;li&gt;Building data quality as a product - Alejandra Cabrera (Clearbit) and Mona Rokibe (Telmai)&lt;/li&gt;\n&lt;li&gt;Driving data culture change with data contracts - Andrew Jones (GoCardless)&lt;/li&gt;\n&lt;li&gt;Going from DevOps to DataOps - Ali Khalid (Emirates)&lt;/li&gt;\n&lt;li&gt;What habits do successful data engineers have? - Panel discussion&lt;/li&gt;\n&lt;li&gt;Going from IC to a manager - Panel discussion&lt;/li&gt;\n&lt;li&gt;Fireside chat about FinOps with Thiago Gil (FinOps Foundation) and Clinton Ford (Unravel)&lt;/li&gt;\n&lt;li&gt;Building the best data team is not rocket science Nirmal Budhathoki (Microsoft)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 50, "id": "award_02d9ab2c-162e-4c01-8438-317a016ed3d9", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=16&amp;height=16&amp;auto=webp&amp;s=10034f3fdf8214c8377134bb60c5b832d4bbf588", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=32&amp;height=32&amp;auto=webp&amp;s=100f785bf261fa9452a5d82ee0ef0793369dbfa5", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=48&amp;height=48&amp;auto=webp&amp;s=b15d030fdfbbe4af4a5b34ab9dc90a174df40a23", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=64&amp;height=64&amp;auto=webp&amp;s=601c75be6ee30dc4b47a5c65d64dea9a185502a1", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=128&amp;height=128&amp;auto=webp&amp;s=540f36e65c0e2f1347fe32020e4a1565e3680437", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "I'm in this with you.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Take My Energy", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=16&amp;height=16&amp;auto=webp&amp;s=045db73f47a9513c44823d132b4c393ab9241b6a", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=32&amp;height=32&amp;auto=webp&amp;s=298a02e0edbb5b5e293087eeede63802cbe1d2c7", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=48&amp;height=48&amp;auto=webp&amp;s=7d06d606eb23dbcd6dbe39ee0e60588c5eb89065", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=64&amp;height=64&amp;auto=webp&amp;s=ecd9854b14104a36a210028c43420f0dababd96b", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=128&amp;height=128&amp;auto=webp&amp;s=0d5d7b92c1d66aff435f2ad32e6330ca2b971f6d", "width": 128, "height": 128}], "icon_format": "PNG", "icon_height": 2048, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1064xv7", "is_robot_indexable": true, "report_reasons": null, "author": "hesanastronaut", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1064xv7/excited_for_this_wwwdatateamssummitcom/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1064xv7/excited_for_this_wwwdatateamssummitcom/", "subreddit_subscribers": 85559, "created_utc": 1673138759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The answer to this quesiton might be rather simple but I lack the experience to know.\n\nIn it's most general form, this question is about the most compact data format to store a table of values with lot's of repeated values, e.g. for n columns:\n\n|idx|1|2|3|4|...|n|\n|---|-|-|-|-|---|-|\n|idx1|1v1|2v1|3v1|4v1|...|nv1|\n|idx2|1v1|**2v2**|3v1|4v1|...|nv1|\n|idx3|**1v2**|2v2|3v1|**4v2**|...|nv1|\n|idx4|1v2|**2v3**|3v1|4v2|...|**nv2**|\n|idx5|1v2|2v3|**3v2**|4v2|...|nv2|\n|idx6|**1v3**|2v3|3v2|4v2|...|nv2|\n\nI emphasise where the values change from those in previous rows. The format of the example values here is simply that 'nvm' means the mth value ('v') of column n.\n\nSpecifically, my application is a set of n quantities that are updating over time independently of each other, and I want to store the whole history of all quantities, the index being the time, i.e. it's basically a log recording when any of the values change, but they don't typically all change at the same time, only a few change at once.\n\nA starting point might be to consider storing it as a csv with unchanged values being left blank:\n\n```\ntime,1,2,3,4,...,n\nt1,1v1,2v1,3v1,4v1,...,nv1\nt2,,2v2,,,...,\nt3,1v2,,,4v2,...,\nt4,,2v3,,,...,nv2\nt5,,,3v2,,...,\nt6,1v3,,,,...,\n```\nWhen reading the data, the empty values in a row can then be forward-filled from the last time the value of that quantity changed.\n\nThe issue here is that for large n, that's a lot (n-1) of redundant commas increasing the size of the file, and the number of commas remains the same for each new row, even if only one or two values have changed.\n\nHence the next idea is not storing it as a csv to generate a table out of, and instead just including on each new line only those quantities that had changed with their new values:\n```\ntime,1,2,3,4,...,n\nt1,1v1,2v1,3v1,4v1,...,nv1\nt2,2:2v2\nt3,1:1v2,4:4v2\nt4,2:2v3,n:nv2\nt5,3:3v2\nt6,1:1v3\n```\nthe format here being that 'x:new_value' means quantity x has changed value to new_value. Assuming all characters and the column headings are 1 byte each, ths saves a constant n-1 bytes per row and only adds 3 bytes per updated value (column header, colon, comma) in the row, so the result is smaller in size as long as the number of values changing in each row remains smaller than (n-1)/3.\n\nIs there any better and more efficient way than this? Overall, in addition to minimal file size, I'm looking for:\n- a solution with fast inserting of new data on the fly\n- when reading the data, something that allows efficiently isolating both by time to see what quantities changed at that time (ideally without having to traverse all previous times to get there), and by quantity to see at what times it that quantity changed (ideally without having to traverse other quantities to get there).", "author_fullname": "t2_rvpfxld", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most efficient format for storing a table with repeated column values", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105x2r0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673129075.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673119053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The answer to this quesiton might be rather simple but I lack the experience to know.&lt;/p&gt;\n\n&lt;p&gt;In it&amp;#39;s most general form, this question is about the most compact data format to store a table of values with lot&amp;#39;s of repeated values, e.g. for n columns:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;idx&lt;/th&gt;\n&lt;th&gt;1&lt;/th&gt;\n&lt;th&gt;2&lt;/th&gt;\n&lt;th&gt;3&lt;/th&gt;\n&lt;th&gt;4&lt;/th&gt;\n&lt;th&gt;...&lt;/th&gt;\n&lt;th&gt;n&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;idx1&lt;/td&gt;\n&lt;td&gt;1v1&lt;/td&gt;\n&lt;td&gt;2v1&lt;/td&gt;\n&lt;td&gt;3v1&lt;/td&gt;\n&lt;td&gt;4v1&lt;/td&gt;\n&lt;td&gt;...&lt;/td&gt;\n&lt;td&gt;nv1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;idx2&lt;/td&gt;\n&lt;td&gt;1v1&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;2v2&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;3v1&lt;/td&gt;\n&lt;td&gt;4v1&lt;/td&gt;\n&lt;td&gt;...&lt;/td&gt;\n&lt;td&gt;nv1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;idx3&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;1v2&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;2v2&lt;/td&gt;\n&lt;td&gt;3v1&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;4v2&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;...&lt;/td&gt;\n&lt;td&gt;nv1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;idx4&lt;/td&gt;\n&lt;td&gt;1v2&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;2v3&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;3v1&lt;/td&gt;\n&lt;td&gt;4v2&lt;/td&gt;\n&lt;td&gt;...&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;nv2&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;idx5&lt;/td&gt;\n&lt;td&gt;1v2&lt;/td&gt;\n&lt;td&gt;2v3&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;3v2&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;4v2&lt;/td&gt;\n&lt;td&gt;...&lt;/td&gt;\n&lt;td&gt;nv2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;idx6&lt;/td&gt;\n&lt;td&gt;&lt;strong&gt;1v3&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;2v3&lt;/td&gt;\n&lt;td&gt;3v2&lt;/td&gt;\n&lt;td&gt;4v2&lt;/td&gt;\n&lt;td&gt;...&lt;/td&gt;\n&lt;td&gt;nv2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I emphasise where the values change from those in previous rows. The format of the example values here is simply that &amp;#39;nvm&amp;#39; means the mth value (&amp;#39;v&amp;#39;) of column n.&lt;/p&gt;\n\n&lt;p&gt;Specifically, my application is a set of n quantities that are updating over time independently of each other, and I want to store the whole history of all quantities, the index being the time, i.e. it&amp;#39;s basically a log recording when any of the values change, but they don&amp;#39;t typically all change at the same time, only a few change at once.&lt;/p&gt;\n\n&lt;p&gt;A starting point might be to consider storing it as a csv with unchanged values being left blank:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\ntime,1,2,3,4,...,n\nt1,1v1,2v1,3v1,4v1,...,nv1\nt2,,2v2,,,...,\nt3,1v2,,,4v2,...,\nt4,,2v3,,,...,nv2\nt5,,,3v2,,...,\nt6,1v3,,,,...,\n&lt;/code&gt;\nWhen reading the data, the empty values in a row can then be forward-filled from the last time the value of that quantity changed.&lt;/p&gt;\n\n&lt;p&gt;The issue here is that for large n, that&amp;#39;s a lot (n-1) of redundant commas increasing the size of the file, and the number of commas remains the same for each new row, even if only one or two values have changed.&lt;/p&gt;\n\n&lt;p&gt;Hence the next idea is not storing it as a csv to generate a table out of, and instead just including on each new line only those quantities that had changed with their new values:\n&lt;code&gt;\ntime,1,2,3,4,...,n\nt1,1v1,2v1,3v1,4v1,...,nv1\nt2,2:2v2\nt3,1:1v2,4:4v2\nt4,2:2v3,n:nv2\nt5,3:3v2\nt6,1:1v3\n&lt;/code&gt;\nthe format here being that &amp;#39;x:new_value&amp;#39; means quantity x has changed value to new_value. Assuming all characters and the column headings are 1 byte each, ths saves a constant n-1 bytes per row and only adds 3 bytes per updated value (column header, colon, comma) in the row, so the result is smaller in size as long as the number of values changing in each row remains smaller than (n-1)/3.&lt;/p&gt;\n\n&lt;p&gt;Is there any better and more efficient way than this? Overall, in addition to minimal file size, I&amp;#39;m looking for:\n- a solution with fast inserting of new data on the fly\n- when reading the data, something that allows efficiently isolating both by time to see what quantities changed at that time (ideally without having to traverse all previous times to get there), and by quantity to see at what times it that quantity changed (ideally without having to traverse other quantities to get there).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105x2r0", "is_robot_indexable": true, "report_reasons": null, "author": "O_I_GR", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105x2r0/most_efficient_format_for_storing_a_table_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105x2r0/most_efficient_format_for_storing_a_table_with/", "subreddit_subscribers": 85559, "created_utc": 1673119053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The company that I'm working started to make a digital transformation and is starting to extract data from APIs and uploading them into a database. My boss wants to do it using pentaho (neither him or me have great experience with data engeneering), but whenever i'm studying (i'm trying to get into the DE field) I don't hear nobody talk about pentaho.\n\n  \nSo the question is: Is pentaho a good tool for this use? if not, what tool do you recommend the most?", "author_fullname": "t2_9qzkaccz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Pentaho a good tool for ETL and orchestration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105rnzc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673105397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The company that I&amp;#39;m working started to make a digital transformation and is starting to extract data from APIs and uploading them into a database. My boss wants to do it using pentaho (neither him or me have great experience with data engeneering), but whenever i&amp;#39;m studying (i&amp;#39;m trying to get into the DE field) I don&amp;#39;t hear nobody talk about pentaho.&lt;/p&gt;\n\n&lt;p&gt;So the question is: Is pentaho a good tool for this use? if not, what tool do you recommend the most?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105rnzc", "is_robot_indexable": true, "report_reasons": null, "author": "Miguel-2001", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105rnzc/is_pentaho_a_good_tool_for_etl_and_orchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105rnzc/is_pentaho_a_good_tool_for_etl_and_orchestration/", "subreddit_subscribers": 85559, "created_utc": 1673105397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, long time lurker, first time poster here. As the title says. I\u2019ve never come across one professionally but have been reading about them lately. I\u2019ve been trying to figure out what i want to do to advance my career and have been intrigued by this position. My background - working in the IT domain for 20 years (help desk, sysadmin, net admin with CCNA, app developer, now data engineer). Been in sql dev/data engineering for the past 7 years, during that time got my masters in data analytics. Basically I\u2019d like to leverage my prior IT experience and tack it on to my current data experience. Now whether that\u2019s with my current firm (our firm is relatively small with &gt;75 employees), or moving to another one is another question, but I guess I\u2019d like to know if being a CDO is an ideal career progression or if I should stick with the technical side of things and be a SME on a particular technology (currently working with Azure in our org). Thanks in advance.", "author_fullname": "t2_4dmi1k8m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone here either a Chief Data Officer or know/work with one?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_106ai0p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673154554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, long time lurker, first time poster here. As the title says. I\u2019ve never come across one professionally but have been reading about them lately. I\u2019ve been trying to figure out what i want to do to advance my career and have been intrigued by this position. My background - working in the IT domain for 20 years (help desk, sysadmin, net admin with CCNA, app developer, now data engineer). Been in sql dev/data engineering for the past 7 years, during that time got my masters in data analytics. Basically I\u2019d like to leverage my prior IT experience and tack it on to my current data experience. Now whether that\u2019s with my current firm (our firm is relatively small with &amp;gt;75 employees), or moving to another one is another question, but I guess I\u2019d like to know if being a CDO is an ideal career progression or if I should stick with the technical side of things and be a SME on a particular technology (currently working with Azure in our org). Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "106ai0p", "is_robot_indexable": true, "report_reasons": null, "author": "never_say_frisco", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/106ai0p/is_anyone_here_either_a_chief_data_officer_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/106ai0p/is_anyone_here_either_a_chief_data_officer_or/", "subreddit_subscribers": 85559, "created_utc": 1673154554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apologies in advance for the level of detail provided, but this is the best I can do in a public forum.\n\n**Background**:  We are a small, lean startup that is in the process of selecting a database solution that will accommodate high speed/performance read queries for an analytics database.   The data will likely all be structured.   We currently use Postgres as our main database in the product.\n\nWe are basically looking to implement the ability to perform a query across the entire dataset which is primarily financial transaction / holdings data with some additional structured data tied to each client in the dataset.   Currently, these queries across the entire dataset run batch and take hours in Postgres on a one VM implementation (Like I said, cost is an overriding concern). \n\nWe want to give users the ability to run a similar query **in near real time** as part of a new analytics tool.   It still needs to look across the entire dataset but the batch process needs to run it for 30+ different scenarios across the entire dataset.   The user would just be running it for 1 scenario at a time.\n\nTo give a very rough idea of potential data size as a starting point (not considering future growth that is expected), we are talking about 200,000 clients with 100,000,000 historical transactions, and other related data.    I know this isn't very specific, but just trying to give some very rough idea of where we are.   The total current Postgres DB in the existing product is between 150-200GB but will continue to grow.   We don't currently use replicas or scaling as we are a small operation and the overnight batch process works for the existing project.\n\nWe are envisioning creating a separate analytics database that is easily scalable and focused on reads not writes to provide users with a query tool with the performance we need.\n\nI've looked at Snowflake but am concerned about getting locked into a proprietary platform where prices might get away from us.   We don't have a lot of financial resources to burn right now, and we need to prove out that we can get clients for the new application to bring in enough revenue.\n\nTaking Snowflake off the table, I am trying to find a scalable, open source, cloud agnostic alternative (we may be installing this analytics database both in client's VPCs and our own cloud infrastructure sitting on one of the big 3.\n\nWe have been looking at Cassandra since it is open source and scales horizontally, but I'm not sure if it is the best solution for our needs.   I was hoping to get some input from people with Cassandra experience on whether or not we are barking up the right tree.   I'm somewhat familiar with NoSQL solutions as we used to run MongoDB for our less structured data.   A lot of the queries will be based on date ranges of data, combined with looking for other criteria such as transactions above a certain amount, or in a range.\n\nI'm also open to any other suggestions that ideally are open source, where we have full control over deployment, are cloud agnostic, and importantly where we can easily control cost and tweak resources.   We want something that we can grow with, but need to start conservatively in terms of financial outlays.\n\nIf Hydra was more mature, it seems like it could be an interesting alternative that would let us build on top of Postgres which we're already using, but it looks too young to me for us to take a risk on.\n\nThanks for any input!", "author_fullname": "t2_o1obd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for some database solution selection advice...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1062vj7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673133520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies in advance for the level of detail provided, but this is the best I can do in a public forum.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;:  We are a small, lean startup that is in the process of selecting a database solution that will accommodate high speed/performance read queries for an analytics database.   The data will likely all be structured.   We currently use Postgres as our main database in the product.&lt;/p&gt;\n\n&lt;p&gt;We are basically looking to implement the ability to perform a query across the entire dataset which is primarily financial transaction / holdings data with some additional structured data tied to each client in the dataset.   Currently, these queries across the entire dataset run batch and take hours in Postgres on a one VM implementation (Like I said, cost is an overriding concern). &lt;/p&gt;\n\n&lt;p&gt;We want to give users the ability to run a similar query &lt;strong&gt;in near real time&lt;/strong&gt; as part of a new analytics tool.   It still needs to look across the entire dataset but the batch process needs to run it for 30+ different scenarios across the entire dataset.   The user would just be running it for 1 scenario at a time.&lt;/p&gt;\n\n&lt;p&gt;To give a very rough idea of potential data size as a starting point (not considering future growth that is expected), we are talking about 200,000 clients with 100,000,000 historical transactions, and other related data.    I know this isn&amp;#39;t very specific, but just trying to give some very rough idea of where we are.   The total current Postgres DB in the existing product is between 150-200GB but will continue to grow.   We don&amp;#39;t currently use replicas or scaling as we are a small operation and the overnight batch process works for the existing project.&lt;/p&gt;\n\n&lt;p&gt;We are envisioning creating a separate analytics database that is easily scalable and focused on reads not writes to provide users with a query tool with the performance we need.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked at Snowflake but am concerned about getting locked into a proprietary platform where prices might get away from us.   We don&amp;#39;t have a lot of financial resources to burn right now, and we need to prove out that we can get clients for the new application to bring in enough revenue.&lt;/p&gt;\n\n&lt;p&gt;Taking Snowflake off the table, I am trying to find a scalable, open source, cloud agnostic alternative (we may be installing this analytics database both in client&amp;#39;s VPCs and our own cloud infrastructure sitting on one of the big 3.&lt;/p&gt;\n\n&lt;p&gt;We have been looking at Cassandra since it is open source and scales horizontally, but I&amp;#39;m not sure if it is the best solution for our needs.   I was hoping to get some input from people with Cassandra experience on whether or not we are barking up the right tree.   I&amp;#39;m somewhat familiar with NoSQL solutions as we used to run MongoDB for our less structured data.   A lot of the queries will be based on date ranges of data, combined with looking for other criteria such as transactions above a certain amount, or in a range.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also open to any other suggestions that ideally are open source, where we have full control over deployment, are cloud agnostic, and importantly where we can easily control cost and tweak resources.   We want something that we can grow with, but need to start conservatively in terms of financial outlays.&lt;/p&gt;\n\n&lt;p&gt;If Hydra was more mature, it seems like it could be an interesting alternative that would let us build on top of Postgres which we&amp;#39;re already using, but it looks too young to me for us to take a risk on.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any input!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1062vj7", "is_robot_indexable": true, "report_reasons": null, "author": "williamlee666", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1062vj7/looking_for_some_database_solution_selection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1062vj7/looking_for_some_database_solution_selection/", "subreddit_subscribers": 85559, "created_utc": 1673133520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m not sure I fully understand this, so reaching out for clarification.\n\nMy understanding of whether you use ETL or ELT is about where the transform occurs in relation to the warehouse \u2019server\u2019. So taking Snowflake as example, if you extract from source and load straight to variant columns in Snowflake, then transform through to a fact and dim model, this would be ELT. This makes sense.\n\nBut what about when there isn\u2019t a warehouse? What about when you simply persist the data in S3, with Hudi, Iceberg or Delta Lake and view through Athena or Presto?\n\nIn the above solution (once data warehouses are out of the picture) I\u2019m very unclear as to the difference between the two. Does it simply become a naming convention with no real practical difference, or is there still something fundamentally different about the two?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELT &amp; ETL in respect of a Lakehouse architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1061shh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673130848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m not sure I fully understand this, so reaching out for clarification.&lt;/p&gt;\n\n&lt;p&gt;My understanding of whether you use ETL or ELT is about where the transform occurs in relation to the warehouse \u2019server\u2019. So taking Snowflake as example, if you extract from source and load straight to variant columns in Snowflake, then transform through to a fact and dim model, this would be ELT. This makes sense.&lt;/p&gt;\n\n&lt;p&gt;But what about when there isn\u2019t a warehouse? What about when you simply persist the data in S3, with Hudi, Iceberg or Delta Lake and view through Athena or Presto?&lt;/p&gt;\n\n&lt;p&gt;In the above solution (once data warehouses are out of the picture) I\u2019m very unclear as to the difference between the two. Does it simply become a naming convention with no real practical difference, or is there still something fundamentally different about the two?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1061shh", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1061shh/elt_etl_in_respect_of_a_lakehouse_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1061shh/elt_etl_in_respect_of_a_lakehouse_architecture/", "subreddit_subscribers": 85559, "created_utc": 1673130848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have many slack channels where people ask questions and the questions are answered. We\u2018d like a way to create these Q&amp;As into a knowledge base. Has anyone done something like this?  we could create an integration to a confluence page but this wouldn\u2019t exactly make things readable and user friendly. any thoughts or examples of how others have done would be much appreciate.", "author_fullname": "t2_r7qxfuaq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slack messages into knowledge base?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1060y3b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673128770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have many slack channels where people ask questions and the questions are answered. We\u2018d like a way to create these Q&amp;amp;As into a knowledge base. Has anyone done something like this?  we could create an integration to a confluence page but this wouldn\u2019t exactly make things readable and user friendly. any thoughts or examples of how others have done would be much appreciate.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1060y3b", "is_robot_indexable": true, "report_reasons": null, "author": "JerryReceiver", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1060y3b/slack_messages_into_knowledge_base/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1060y3b/slack_messages_into_knowledge_base/", "subreddit_subscribers": 85559, "created_utc": 1673128770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My apologies if this isn\u2019t the place for this type of question, but I\u2019m curious to hear other\u2019s thoughts on juggling life and studying for a career in data engineering. \n\nI\u2019m currently a Data Analyst and have been for about 4 years at this point. I\u2019m very proficient in SQL and am trying to advance beyond the fundamentals of Python. My biggest concern is the idea of burnout and overwhelming myself in the pursuit of a career change. Just for context, I\u2019m a father of three so my time is definitely limited. I understand that these constraints are not unique to me whatsoever, but I\u2019d like to hear how others have effectively managed their time in their career pursuits. \n\nThoughts?", "author_fullname": "t2_yusz1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Effective Study Sessions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105y2zn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673121556.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My apologies if this isn\u2019t the place for this type of question, but I\u2019m curious to hear other\u2019s thoughts on juggling life and studying for a career in data engineering. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently a Data Analyst and have been for about 4 years at this point. I\u2019m very proficient in SQL and am trying to advance beyond the fundamentals of Python. My biggest concern is the idea of burnout and overwhelming myself in the pursuit of a career change. Just for context, I\u2019m a father of three so my time is definitely limited. I understand that these constraints are not unique to me whatsoever, but I\u2019d like to hear how others have effectively managed their time in their career pursuits. &lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105y2zn", "is_robot_indexable": true, "report_reasons": null, "author": "dvalinhunter", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105y2zn/effective_study_sessions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105y2zn/effective_study_sessions/", "subreddit_subscribers": 85559, "created_utc": 1673121556.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been doing some research on various data quality tools recently.  \nIt got me wondering, in companies that are mainly focused on 3rd party integrations, e.g data from salesforce, what is the most common cause for a data quality issue? is it bad SQL? missing / malformed data at the source ? (e.g a sales rep didn't fill a field correctly) and how do you handle these problems?\n\n&amp;#x200B;\n\nI'm adding a poll for summarization, but please share  example on the use cases you faced.\n\n[View Poll](https://www.reddit.com/poll/105v661)", "author_fullname": "t2_ayp5oyir", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data quality issues resulting in source data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105v661", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673114297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been doing some research on various data quality tools recently.&lt;br/&gt;\nIt got me wondering, in companies that are mainly focused on 3rd party integrations, e.g data from salesforce, what is the most common cause for a data quality issue? is it bad SQL? missing / malformed data at the source ? (e.g a sales rep didn&amp;#39;t fill a field correctly) and how do you handle these problems?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m adding a poll for summarization, but please share  example on the use cases you faced.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/105v661\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105v661", "is_robot_indexable": true, "report_reasons": null, "author": "Puzzleheaded_Dog_614", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1673373497333, "options": [{"text": "Data Engineer bugs", "id": "20871255"}, {"text": "Misunderstanding of requirements", "id": "20871256"}, {"text": "Missing data in source system", "id": "20871257"}, {"text": "Wrong data in source system", "id": "20871258"}, {"text": "Other", "id": "20871259"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 39, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105v661/data_quality_issues_resulting_in_source_data/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/105v661/data_quality_issues_resulting_in_source_data/", "subreddit_subscribers": 85559, "created_utc": 1673114297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to improve a prediction model to have better accuracy, so I have to do constant back-and-forth between EDA, feature engineering, and model training + metrics evaluation, alongside making visualizations. I lack any organization to this, I just aimless do different steps mixed-up with different changes until I get improvements.\n\nI'm currently using pandas on jupyter notebook. Midway through the feature engineering, I have to take the data to postgresql for specific expansionary operations that might be too much for pandas,  joining them to another expensive table, but ultimately aggregating them in a size small enough to go back onto python jupyter (where it has to go anyways for it to go through the model).\n\nThis sql part really slows down my EDA and thinking flow, specifically because I'd have to refactor the SQL code for the times when I have made significant changes in the feature engineering procedure. And I'm trying to find a way to substitute it, so everything can happily stay within python, on the jupyter. I undestand that I'd still have to refactor whatever replacement python code I have too, no different than SQL, but at least I can monitor everything in the same place. Do I have any options?", "author_fullname": "t2_b7eh4ujn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to be more efficient in this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105jjt4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673079039.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673078380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to improve a prediction model to have better accuracy, so I have to do constant back-and-forth between EDA, feature engineering, and model training + metrics evaluation, alongside making visualizations. I lack any organization to this, I just aimless do different steps mixed-up with different changes until I get improvements.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using pandas on jupyter notebook. Midway through the feature engineering, I have to take the data to postgresql for specific expansionary operations that might be too much for pandas,  joining them to another expensive table, but ultimately aggregating them in a size small enough to go back onto python jupyter (where it has to go anyways for it to go through the model).&lt;/p&gt;\n\n&lt;p&gt;This sql part really slows down my EDA and thinking flow, specifically because I&amp;#39;d have to refactor the SQL code for the times when I have made significant changes in the feature engineering procedure. And I&amp;#39;m trying to find a way to substitute it, so everything can happily stay within python, on the jupyter. I undestand that I&amp;#39;d still have to refactor whatever replacement python code I have too, no different than SQL, but at least I can monitor everything in the same place. Do I have any options?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105jjt4", "is_robot_indexable": true, "report_reasons": null, "author": "countlinard", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105jjt4/how_to_be_more_efficient_in_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105jjt4/how_to_be_more_efficient_in_this/", "subreddit_subscribers": 85559, "created_utc": 1673078380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Realtime pipeline: We have a CDC pipeline running from Mongo to databricks. the message broker here is Kafka.\n\nWe want to create an audit pipeline to verify if data is moving properly.\n\ncurrent audits we do is like taking total count from mongo and trying to match it with databricks table. but this is a little costly, Any suggestions", "author_fullname": "t2_1w56xjmv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Audit realtime CDC pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105hm2u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673071614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Realtime pipeline: We have a CDC pipeline running from Mongo to databricks. the message broker here is Kafka.&lt;/p&gt;\n\n&lt;p&gt;We want to create an audit pipeline to verify if data is moving properly.&lt;/p&gt;\n\n&lt;p&gt;current audits we do is like taking total count from mongo and trying to match it with databricks table. but this is a little costly, Any suggestions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "105hm2u", "is_robot_indexable": true, "report_reasons": null, "author": "avish2661", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105hm2u/audit_realtime_cdc_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105hm2u/audit_realtime_cdc_pipeline/", "subreddit_subscribers": 85559, "created_utc": 1673071614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey! For a few days now I've been trying to create a small database in which I could store thousands of records/rows from a JSON file. The problem is that I don't know exactly how to migrate this data into my already existed multiple relational tables so that every relation is preserved (for example how to insert data into many-to-many relations tables). Searching for answers, I came across many solutions which ultimately made me only more confused: it can be done using stage/temporary tables, you can just use python with SQL, there was something about ORM option, converting json to csv... Quite confusing.  \n\nI should also mention that it's not a one-time migration but rather I plan to do such a thing (downloading JSON file and inserting its data into database tables) systematically, say 2x a week. There is also an issue of the potential duplicates (some of the data in JSON file will duplicate with data I'll already have in database).\n\nTo sum up, I'd be glad if someone told me in some pretty clear way what options I have and how the process looks like. And if you think that storing JSON data in relational tables is a bad idea then I'm open minded to alternatives. \n\nI'm using SQL Server but I don't mind reading answers that refer to PostgreSQL.\n\nAnd one more thing: Since I want to practice coding I prefer to use python/ SQL rather than some data migration tools.", "author_fullname": "t2_14fkd3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to move data from JSON file into multiple relational tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10612h1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673129039.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! For a few days now I&amp;#39;ve been trying to create a small database in which I could store thousands of records/rows from a JSON file. The problem is that I don&amp;#39;t know exactly how to migrate this data into my already existed multiple relational tables so that every relation is preserved (for example how to insert data into many-to-many relations tables). Searching for answers, I came across many solutions which ultimately made me only more confused: it can be done using stage/temporary tables, you can just use python with SQL, there was something about ORM option, converting json to csv... Quite confusing.  &lt;/p&gt;\n\n&lt;p&gt;I should also mention that it&amp;#39;s not a one-time migration but rather I plan to do such a thing (downloading JSON file and inserting its data into database tables) systematically, say 2x a week. There is also an issue of the potential duplicates (some of the data in JSON file will duplicate with data I&amp;#39;ll already have in database).&lt;/p&gt;\n\n&lt;p&gt;To sum up, I&amp;#39;d be glad if someone told me in some pretty clear way what options I have and how the process looks like. And if you think that storing JSON data in relational tables is a bad idea then I&amp;#39;m open minded to alternatives. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using SQL Server but I don&amp;#39;t mind reading answers that refer to PostgreSQL.&lt;/p&gt;\n\n&lt;p&gt;And one more thing: Since I want to practice coding I prefer to use python/ SQL rather than some data migration tools.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10612h1", "is_robot_indexable": true, "report_reasons": null, "author": "salvazac", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10612h1/how_to_move_data_from_json_file_into_multiple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10612h1/how_to_move_data_from_json_file_into_multiple/", "subreddit_subscribers": 85559, "created_utc": 1673129039.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious on how people go about structuring the logic of their SQL joins to ensure both performance and readability.\n\n[View Poll](https://www.reddit.com/poll/105wzzl)", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you structure complex SQL joins", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105wzzl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673118858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious on how people go about structuring the logic of their SQL joins to ensure both performance and readability.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/105wzzl\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105wzzl", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1673378058723, "options": [{"text": "One big query, with sub-queries throughout", "id": "20872576"}, {"text": "Each step broken down into a procedural set of CTEs with a \u2018SELECT * FROM final\u2019 at the end", "id": "20872577"}, {"text": "A series of SQL statements, each leveraging the persisted table created by the previous query", "id": "20872578"}, {"text": "Something else : in the comments", "id": "20872579"}, {"text": "It\u2019s a secret. Just show me what everyone else does.", "id": "20872580"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 237, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105wzzl/how_do_you_structure_complex_sql_joins/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/105wzzl/how_do_you_structure_complex_sql_joins/", "subreddit_subscribers": 85559, "created_utc": 1673118858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does AWS glue that represent ETL functions do that work with No Code like Azure Data Factory?", "author_fullname": "t2_63im4vf3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question to Data Engineers:", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105tmf1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.17, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673110445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does AWS glue that represent ETL functions do that work with No Code like Azure Data Factory?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "105tmf1", "is_robot_indexable": true, "report_reasons": null, "author": "AmrBayoumy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/105tmf1/question_to_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/105tmf1/question_to_data_engineers/", "subreddit_subscribers": 85559, "created_utc": 1673110445.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}