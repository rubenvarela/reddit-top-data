{"kind": "Listing", "data": {"after": "t3_106exgz", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One of the oldest known version of RuneScape has officially been found", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105txiy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 795, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_nixsez0f", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 795, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "runescape", "selftext": "RuneScape has been around for a long time, but the developers didn't keep regular backups of the files until 2013. According to this post they had almost nothing prior to 2005. In the comments a Redditor uploaded a copy of the files they'd made and the developers stated it was the oldest version they've found to date.", "author_fullname": "t2_nixsez0f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One of the oldest known version of RuneScape has officially been found", "link_flair_richtext": [{"e": "text", "t": "Appreciation"}], "subreddit_name_prefixed": "r/runescape", "hidden": false, "pwls": 6, "link_flair_css_class": "appreciation", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105txb5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 382, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Appreciation", "can_mod_post": false, "score": 382, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1673111180.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;RuneScape has been around for a long time, but the developers didn&amp;#39;t keep regular backups of the files until 2013. According to this post they had almost nothing prior to 2005. In the comments a Redditor uploaded a copy of the files they&amp;#39;d made and the developers stated it was the oldest version they&amp;#39;ve found to date.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/r/pcgaming/comments/105r0pd/-/j3cklw2", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "edb97aae-60be-11eb-8047-0e64d8f4fc71", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qwxl", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a9afb4", "id": "105txb5", "is_robot_indexable": true, "report_reasons": null, "author": "AnApexBread", "discussion_type": null, "num_comments": 82, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/runescape/comments/105txb5/one_of_the_oldest_known_version_of_runescape_has/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/pcgaming/comments/105r0pd/-/j3cklw2", "subreddit_subscribers": 321333, "created_utc": 1673111180.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1673111197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/r/pcgaming/comments/105r0pd/-/j3cklw2", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "105txiy", "is_robot_indexable": true, "report_reasons": null, "author": "AnApexBread", "discussion_type": null, "num_comments": 60, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_105txb5", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/105txiy/one_of_the_oldest_known_version_of_runescape_has/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/pcgaming/comments/105r0pd/-/j3cklw2", "subreddit_subscribers": 664754, "created_utc": 1673111197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a a 1TB SanDisk 520s and I am looking for a solution to expand my storage capacity. Obviously I could just get another 1TB, but then I would then have multiple drives I need to eject when I move my MacBook about.\n\nLooking at YouTube videos it does look like it has an M2 NVME drive installed, so this got me thinking. For the same price of another 1TB sandisk I can get a 2tb M2 drive.\n\nIs there some enclose preferably unpowered (or type c) that can hold multiple M2s? I suppose two drives would be ok, but would prefer four. I have looked on Amazon and they either seem to be exposed, too big and also need power.\n\nRaid capability isn't necessary. Portability is fairly important though.", "author_fullname": "t2_1gm1x1ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a solution for a multi M2 drive storage with a single cable. Have a 1TB SanDisk and want to expand.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106fh0r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673171633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a a 1TB SanDisk 520s and I am looking for a solution to expand my storage capacity. Obviously I could just get another 1TB, but then I would then have multiple drives I need to eject when I move my MacBook about.&lt;/p&gt;\n\n&lt;p&gt;Looking at YouTube videos it does look like it has an M2 NVME drive installed, so this got me thinking. For the same price of another 1TB sandisk I can get a 2tb M2 drive.&lt;/p&gt;\n\n&lt;p&gt;Is there some enclose preferably unpowered (or type c) that can hold multiple M2s? I suppose two drives would be ok, but would prefer four. I have looked on Amazon and they either seem to be exposed, too big and also need power.&lt;/p&gt;\n\n&lt;p&gt;Raid capability isn&amp;#39;t necessary. Portability is fairly important though.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106fh0r", "is_robot_indexable": true, "report_reasons": null, "author": "matmah", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106fh0r/looking_for_a_solution_for_a_multi_m2_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106fh0r/looking_for_a_solution_for_a_multi_m2_drive/", "subreddit_subscribers": 664754, "created_utc": 1673171633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there fellow DataHoarders! I searched both Google and this sub for an answer to this quandary, but couldn't find anything. Maybe what I want to do is impossible and I should just accept it. \n\nI like to game on one monitor and halfheartedly watch videos on the other in the evenings, and I wanted to see if there was a way I could consolidate all 389 GB of my locally stored video files into a single playlist that can play them randomly.\n\nI made the attempt with VLC, but it chokes on it about halfway through adding the files to the playlist. I don't know if there is a numerical limit, or if it goes by file size, not sure. Is there another application, extension, or maybe something out there in GitHub world that you know of that can accomplish this?\n\nI really appreciate any help, and thank you in advance!", "author_fullname": "t2_iwgp7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating a Huge Adult Swim Playlist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10607ro", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673126915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there fellow DataHoarders! I searched both Google and this sub for an answer to this quandary, but couldn&amp;#39;t find anything. Maybe what I want to do is impossible and I should just accept it. &lt;/p&gt;\n\n&lt;p&gt;I like to game on one monitor and halfheartedly watch videos on the other in the evenings, and I wanted to see if there was a way I could consolidate all 389 GB of my locally stored video files into a single playlist that can play them randomly.&lt;/p&gt;\n\n&lt;p&gt;I made the attempt with VLC, but it chokes on it about halfway through adding the files to the playlist. I don&amp;#39;t know if there is a numerical limit, or if it goes by file size, not sure. Is there another application, extension, or maybe something out there in GitHub world that you know of that can accomplish this?&lt;/p&gt;\n\n&lt;p&gt;I really appreciate any help, and thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10607ro", "is_robot_indexable": true, "report_reasons": null, "author": "TropicalDruid", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10607ro/creating_a_huge_adult_swim_playlist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10607ro/creating_a_huge_adult_swim_playlist/", "subreddit_subscribers": 664754, "created_utc": 1673126915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Entire question fit in the title of this post.", "author_fullname": "t2_8hlee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In a 2-drive raid 1 array, is it a viable backup strategy to pull one drive for cold storage and then replace it (rebuilding the array)? Could one then just rotate the third drive through periodically in this manner when a new backup is desired?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1067x0r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673146956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Entire question fit in the title of this post.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1067x0r", "is_robot_indexable": true, "report_reasons": null, "author": "nouvie", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1067x0r/in_a_2drive_raid_1_array_is_it_a_viable_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1067x0r/in_a_2drive_raid_1_array_is_it_a_viable_backup/", "subreddit_subscribers": 664754, "created_utc": 1673146956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "When searching for a general keyword in the search bar, usually some 30 or 50k books show up. However, only about 1k can actually be seen. That is more or less until page 100. Past that... no more pages and thousands of books we can't acess. I thought this was the place to post the question because the results are surely in their system, it shows the numbers. Tried on different browsers and platforms, so it's definitely an intentional feature. ", "author_fullname": "t2_btlsftp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to see all results in Amazon search?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105ya4l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673125624.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673122042.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When searching for a general keyword in the search bar, usually some 30 or 50k books show up. However, only about 1k can actually be seen. That is more or less until page 100. Past that... no more pages and thousands of books we can&amp;#39;t acess. I thought this was the place to post the question because the results are surely in their system, it shows the numbers. Tried on different browsers and platforms, so it&amp;#39;s definitely an intentional feature. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "105ya4l", "is_robot_indexable": true, "report_reasons": null, "author": "Freibetto", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/105ya4l/how_to_see_all_results_in_amazon_search/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/105ya4l/how_to_see_all_results_in_amazon_search/", "subreddit_subscribers": 664754, "created_utc": 1673122042.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need help finding duplicates of my JAV collection, the problem is that some files have their titles in different format like:\n\na) GVG-001.mp4\n\nb) GVG001.mp4\n\nc) GVG001 - additional movie title\n\nWhat software would you recommend to find dupes? Duplicate files might have different lengths so CRC search will not work. I need some way to analyze the first few letters of a filename keeping in mind that there may or may not be \"-\" sing in there. Can you help me out?\n\nI have total commander but I'm not sure if its search function is as powerful.", "author_fullname": "t2_tmiw5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for finding duplicate files based on a similar filename.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106g20g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673173717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help finding duplicates of my JAV collection, the problem is that some files have their titles in different format like:&lt;/p&gt;\n\n&lt;p&gt;a) GVG-001.mp4&lt;/p&gt;\n\n&lt;p&gt;b) GVG001.mp4&lt;/p&gt;\n\n&lt;p&gt;c) GVG001 - additional movie title&lt;/p&gt;\n\n&lt;p&gt;What software would you recommend to find dupes? Duplicate files might have different lengths so CRC search will not work. I need some way to analyze the first few letters of a filename keeping in mind that there may or may not be &amp;quot;-&amp;quot; sing in there. Can you help me out?&lt;/p&gt;\n\n&lt;p&gt;I have total commander but I&amp;#39;m not sure if its search function is as powerful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106g20g", "is_robot_indexable": true, "report_reasons": null, "author": "wooshaq", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106g20g/software_for_finding_duplicate_files_based_on_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106g20g/software_for_finding_duplicate_files_based_on_a/", "subreddit_subscribers": 664754, "created_utc": 1673173717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, im quite newbie to datahoarding, so basically i am mass downloading bunch of image albums for my little archive but some of these images have useful comments that would also be handy to have in txt file for further reference. \n\nI learned to use JDownloader2 for image batch downloading, I was wondering if there is something similar that I could use for comments/text on webpage?", "author_fullname": "t2_ur3sbbk7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there some easy way to extract comments from website into txt/docx files or such?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1061efc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673129851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, im quite newbie to datahoarding, so basically i am mass downloading bunch of image albums for my little archive but some of these images have useful comments that would also be handy to have in txt file for further reference. &lt;/p&gt;\n\n&lt;p&gt;I learned to use JDownloader2 for image batch downloading, I was wondering if there is something similar that I could use for comments/text on webpage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1061efc", "is_robot_indexable": true, "report_reasons": null, "author": "Kuznetsov063", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1061efc/is_there_some_easy_way_to_extract_comments_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1061efc/is_there_some_easy_way_to_extract_comments_from/", "subreddit_subscribers": 664754, "created_utc": 1673129851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm interested in setting up a CCTV system, 1 security camera only.\n\nWhere I'm at in the middle of nowhere (Australia) there isn't fixed line, and there is barely any data signal with most carriers.\n\nWhat I'd like is to have the security camera recording to a local system, let's say a simple computer setup with some HDDs that constantly records with a set retention.\n\nBut I'd also want to be able to remotely view what's happening when I am not at the location, and being limited with internet options (cellular plans only) the cost of 24/7 streaming online would not be viable. So, I thought about a system that: records locally, then every 1 hour (more or less) uploads 1 photo to a cloud service, let's say back blaze (from a portable wifi router connected via SIM). That way, I can look to check to see if there is anything gone occasionally without having to livestream which would use a huge amount of data, and if something did happen, I can go check the local storage and see what exactly happened.\n\nIdeally, the only monthly costs would be:\n\nSIM data plan\n\nBackblaze\n\nIf anyone has some knowledge on what cameras are the best option or how I could achieve this, if i'm on the right track etc please feel free to share!", "author_fullname": "t2_vgleq1ng", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Outback CCTV system with limited internet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106elia", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673168431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m interested in setting up a CCTV system, 1 security camera only.&lt;/p&gt;\n\n&lt;p&gt;Where I&amp;#39;m at in the middle of nowhere (Australia) there isn&amp;#39;t fixed line, and there is barely any data signal with most carriers.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;d like is to have the security camera recording to a local system, let&amp;#39;s say a simple computer setup with some HDDs that constantly records with a set retention.&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;d also want to be able to remotely view what&amp;#39;s happening when I am not at the location, and being limited with internet options (cellular plans only) the cost of 24/7 streaming online would not be viable. So, I thought about a system that: records locally, then every 1 hour (more or less) uploads 1 photo to a cloud service, let&amp;#39;s say back blaze (from a portable wifi router connected via SIM). That way, I can look to check to see if there is anything gone occasionally without having to livestream which would use a huge amount of data, and if something did happen, I can go check the local storage and see what exactly happened.&lt;/p&gt;\n\n&lt;p&gt;Ideally, the only monthly costs would be:&lt;/p&gt;\n\n&lt;p&gt;SIM data plan&lt;/p&gt;\n\n&lt;p&gt;Backblaze&lt;/p&gt;\n\n&lt;p&gt;If anyone has some knowledge on what cameras are the best option or how I could achieve this, if i&amp;#39;m on the right track etc please feel free to share!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106elia", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Wing9364", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106elia/outback_cctv_system_with_limited_internet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106elia/outback_cctv_system_with_limited_internet/", "subreddit_subscribers": 664754, "created_utc": 1673168431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't run raid or NAS, at least not yet, just a simple DAS(?) setup with two drives (19tb) and a single backup drive (13tb). In the future I plan to get more drives and may top out at a total of 5-6 drives.\n\nWhat's the ideal solution for this? Get a large PC case like the Meshify 2 or a smaller case and get like a 5-8 bay enclosure?", "author_fullname": "t2_hrdo3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Large PC case vs SFF with external HDD enclosure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1067u94", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673146768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t run raid or NAS, at least not yet, just a simple DAS(?) setup with two drives (19tb) and a single backup drive (13tb). In the future I plan to get more drives and may top out at a total of 5-6 drives.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the ideal solution for this? Get a large PC case like the Meshify 2 or a smaller case and get like a 5-8 bay enclosure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1067u94", "is_robot_indexable": true, "report_reasons": null, "author": "c9898", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1067u94/large_pc_case_vs_sff_with_external_hdd_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1067u94/large_pc_case_vs_sff_with_external_hdd_enclosure/", "subreddit_subscribers": 664754, "created_utc": 1673146768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After reading some of the opinions of folks in this subreddit, I decided to take a chance on some refurbed Exos X18 12TB drives for my NAS, ordered from Server Part Deals. I've swapped in two of them so far, and the SMART data looks good, but only show 25 and 15 power on hours and a start/stop count of 2. Assuming I trust the seller, does anyone know if Seagate resets SMART data when they refurbish drives? I know a lot of these drives may have never seen much or any actual use before they were rejected for one reason or another, but I'm curious if these \\*really\\* are low mileage or if Seagate reset the stats.\n\nBy the way, I have to say Server Part Deals was great to deal with. They really do ship these things well, and they shipped them fast. Thanks for the recommendation!", "author_fullname": "t2_5iea7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Seagate reset SMART data on manufacturer refurbs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105wtcx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673118396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After reading some of the opinions of folks in this subreddit, I decided to take a chance on some refurbed Exos X18 12TB drives for my NAS, ordered from Server Part Deals. I&amp;#39;ve swapped in two of them so far, and the SMART data looks good, but only show 25 and 15 power on hours and a start/stop count of 2. Assuming I trust the seller, does anyone know if Seagate resets SMART data when they refurbish drives? I know a lot of these drives may have never seen much or any actual use before they were rejected for one reason or another, but I&amp;#39;m curious if these *really* are low mileage or if Seagate reset the stats.&lt;/p&gt;\n\n&lt;p&gt;By the way, I have to say Server Part Deals was great to deal with. They really do ship these things well, and they shipped them fast. Thanks for the recommendation!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "105wtcx", "is_robot_indexable": true, "report_reasons": null, "author": "compulov", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/105wtcx/does_seagate_reset_smart_data_on_manufacturer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/105wtcx/does_seagate_reset_smart_data_on_manufacturer/", "subreddit_subscribers": 664754, "created_utc": 1673118396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have quite a collection of MP3 audio books that I have collected over the decades.  Some are divided by chapter.  Some are even divided by chapter in different CD folders.  I am looking for a good Windows app that will take a folder of MP3s (with even subfolders) and convert them into a single M4B.  OpenAudible lets me import the MP3s, but I can't figure out how to combine and convert them using that, so I'm looking for suggestions.", "author_fullname": "t2_6qsou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Windows app for combining multiple MP3s into single M4B", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_106m2yz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673192233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have quite a collection of MP3 audio books that I have collected over the decades.  Some are divided by chapter.  Some are even divided by chapter in different CD folders.  I am looking for a good Windows app that will take a folder of MP3s (with even subfolders) and convert them into a single M4B.  OpenAudible lets me import the MP3s, but I can&amp;#39;t figure out how to combine and convert them using that, so I&amp;#39;m looking for suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106m2yz", "is_robot_indexable": true, "report_reasons": null, "author": "djeaton", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106m2yz/best_windows_app_for_combining_multiple_mp3s_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106m2yz/best_windows_app_for_combining_multiple_mp3s_into/", "subreddit_subscribers": 664754, "created_utc": 1673192233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Constructive feedback very much appreciated.\n\nHere is the guide:\n\nhttps://medium.com/@goughgough/the-best-way-for-microsoft-teams-users-without-administrator-rights-to-save-export-print-copy-8212aa9e5f11\n\n__TL;DR:__\nTo export Teams chat messages without Microsoft Teams admin rights, download Gildas Lormeau's (GL) browser extension at https://github.com/gildas-lormeau/single-file-export-chat.\n\n\n__Assumptions:__\nYou are not very tech savvy.\n\nYou can log into Microsoft Teams in a brower at https://teams.microsoft.com/\n\nIn Teams, you do not have admin rights for a group chat. Nevertheless, you still need to export the messages from that specific group chat.\n\nYou want to use non commercial software and do the exporting for free.\n\nYou want to export the Chat section's messages (in Microsoft Teams left column). NOT the Team section messages (in Microsoft Teams left column).\n\nYou wish to export Teams messages in their entirety, including any body text that contains clickable links.\n\nYou want to export Teams messages to a searchable final output rather than an image file.\n\nYou do not want to waste time manually copying and pasting individual Teams messages, which is one of the techniques in quite a few of the online guides. This manual copying and pasting makes sense if you only have a few Teams messages to export.\n\nYou do not want to use the GoFullPage browser extension, even though it is not as effective as GL's solutions because it lets you export Teams messages as images (e.g. non serchable PDF file).\nBefore I came across GL's methods, GoFullPage browser extension was the best method I tried. Unfortunately, the final roduct is not searchable due to its image format.", "author_fullname": "t2_ipdh111g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just published my guide for Microsoft Teams users (without administrator rights) to save, export, print, copy, archive, back up, or migrate Teams conversation threads, messages, chat history. Hope you like it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106fwt4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673186189.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673173191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Constructive feedback very much appreciated.&lt;/p&gt;\n\n&lt;p&gt;Here is the guide:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@goughgough/the-best-way-for-microsoft-teams-users-without-administrator-rights-to-save-export-print-copy-8212aa9e5f11\"&gt;https://medium.com/@goughgough/the-best-way-for-microsoft-teams-users-without-administrator-rights-to-save-export-print-copy-8212aa9e5f11&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;\nTo export Teams chat messages without Microsoft Teams admin rights, download Gildas Lormeau&amp;#39;s (GL) browser extension at &lt;a href=\"https://github.com/gildas-lormeau/single-file-export-chat\"&gt;https://github.com/gildas-lormeau/single-file-export-chat&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;\nYou are not very tech savvy.&lt;/p&gt;\n\n&lt;p&gt;You can log into Microsoft Teams in a brower at &lt;a href=\"https://teams.microsoft.com/\"&gt;https://teams.microsoft.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In Teams, you do not have admin rights for a group chat. Nevertheless, you still need to export the messages from that specific group chat.&lt;/p&gt;\n\n&lt;p&gt;You want to use non commercial software and do the exporting for free.&lt;/p&gt;\n\n&lt;p&gt;You want to export the Chat section&amp;#39;s messages (in Microsoft Teams left column). NOT the Team section messages (in Microsoft Teams left column).&lt;/p&gt;\n\n&lt;p&gt;You wish to export Teams messages in their entirety, including any body text that contains clickable links.&lt;/p&gt;\n\n&lt;p&gt;You want to export Teams messages to a searchable final output rather than an image file.&lt;/p&gt;\n\n&lt;p&gt;You do not want to waste time manually copying and pasting individual Teams messages, which is one of the techniques in quite a few of the online guides. This manual copying and pasting makes sense if you only have a few Teams messages to export.&lt;/p&gt;\n\n&lt;p&gt;You do not want to use the GoFullPage browser extension, even though it is not as effective as GL&amp;#39;s solutions because it lets you export Teams messages as images (e.g. non serchable PDF file).\nBefore I came across GL&amp;#39;s methods, GoFullPage browser extension was the best method I tried. Unfortunately, the final roduct is not searchable due to its image format.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106fwt4", "is_robot_indexable": true, "report_reasons": null, "author": "cashpayer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106fwt4/just_published_my_guide_for_microsoft_teams_users/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106fwt4/just_published_my_guide_for_microsoft_teams_users/", "subreddit_subscribers": 664754, "created_utc": 1673173191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for an image host (like imagur) that does not compress the image I upload. 100% lossless. I don't really care about features other than I would need to be able to see it from anywhere so would other people. Thanks.", "author_fullname": "t2_usun3qeu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lossless Image Hosting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106d1dr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673162829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for an image host (like imagur) that does not compress the image I upload. 100% lossless. I don&amp;#39;t really care about features other than I would need to be able to see it from anywhere so would other people. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106d1dr", "is_robot_indexable": true, "report_reasons": null, "author": "PuzzleheadedTennis23", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106d1dr/lossless_image_hosting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106d1dr/lossless_image_hosting/", "subreddit_subscribers": 664754, "created_utc": 1673162829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Happy new year!\n\nIs any of you aware of a maximum file size limit on Hetzner storage boxes?\n\nI have tried to upload a  ~100G disk backup to my storage box over the holidays and that failed with some non-descript error. After the fact I found out that I had a local HDD failure that might have caused this...\n\nbefore I commit to uploading again (my ISD outgoing speed is quite low) I thought I'd check with the wisdom in this subreddit...", "author_fullname": "t2_14oq692k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "maximum file size on Hetzner storagebox?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10605yt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673126777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Happy new year!&lt;/p&gt;\n\n&lt;p&gt;Is any of you aware of a maximum file size limit on Hetzner storage boxes?&lt;/p&gt;\n\n&lt;p&gt;I have tried to upload a  ~100G disk backup to my storage box over the holidays and that failed with some non-descript error. After the fact I found out that I had a local HDD failure that might have caused this...&lt;/p&gt;\n\n&lt;p&gt;before I commit to uploading again (my ISD outgoing speed is quite low) I thought I&amp;#39;d check with the wisdom in this subreddit...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10605yt", "is_robot_indexable": true, "report_reasons": null, "author": "biochronox", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10605yt/maximum_file_size_on_hetzner_storagebox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10605yt/maximum_file_size_on_hetzner_storagebox/", "subreddit_subscribers": 664754, "created_utc": 1673126777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All! I want to move away from xpenology shr to something else but have no intermediary server to store data. Any tips on a cloud storage with 10tb to keep my stuff there for a month?", "author_fullname": "t2_gexty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "temporary cheap 10tb cloud storage for migration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105sjex", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673107669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All! I want to move away from xpenology shr to something else but have no intermediary server to store data. Any tips on a cloud storage with 10tb to keep my stuff there for a month?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "105sjex", "is_robot_indexable": true, "report_reasons": null, "author": "nightrave", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/105sjex/temporary_cheap_10tb_cloud_storage_for_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/105sjex/temporary_cheap_10tb_cloud_storage_for_migration/", "subreddit_subscribers": 664754, "created_utc": 1673107669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi folks,\n\nNot sure if this is the right place to post this question, but here goes.\n\nI have boxes and boxes of photography slides that I'd like to digitise. Currently, my solution is to buy something like [https://www.amazon.co.uk/dp/B0074H6NTO](https://www.amazon.co.uk/dp/B0074H6NTO?tag=georiot-trd-21&amp;th=1&amp;ascsubtag=dcw-gb-8778368215956837000-21&amp;geniuslink=true), but that involves me having to fill the trays with 4 slides at a time. Obviously not totally feasible with thousands of slides.\n\nDoes anyone have any experience with a project like this? Or ideas on how to proceed?\n\nCheers.", "author_fullname": "t2_3ued9syv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digitising thousands of 35mm photo slides", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106j8ia", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673184407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;Not sure if this is the right place to post this question, but here goes.&lt;/p&gt;\n\n&lt;p&gt;I have boxes and boxes of photography slides that I&amp;#39;d like to digitise. Currently, my solution is to buy something like &lt;a href=\"https://www.amazon.co.uk/dp/B0074H6NTO?tag=georiot-trd-21&amp;amp;th=1&amp;amp;ascsubtag=dcw-gb-8778368215956837000-21&amp;amp;geniuslink=true\"&gt;https://www.amazon.co.uk/dp/B0074H6NTO&lt;/a&gt;, but that involves me having to fill the trays with 4 slides at a time. Obviously not totally feasible with thousands of slides.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience with a project like this? Or ideas on how to proceed?&lt;/p&gt;\n\n&lt;p&gt;Cheers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106j8ia", "is_robot_indexable": true, "report_reasons": null, "author": "thisismyfirsttime123", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106j8ia/digitising_thousands_of_35mm_photo_slides/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106j8ia/digitising_thousands_of_35mm_photo_slides/", "subreddit_subscribers": 664754, "created_utc": 1673184407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got around 500 GB of data including Music, Photos, Software, Project Files, Backup Files, etc. Currently, it\u2019s all stored in Google Drive and I regularly create Backup archives of the local synced copy on external volumes. In total, there are about 80.000 files.\n\nMy goal is to manage my data more locally, since I don\u2019t want to trust the Google drive sync client. With a lot of simultaneous changes to the files, it messed up more than once, leading to inconsistent sync states between Gdrive and local storage, which have to be fixed manually. In its defense, the software just not made for that amount of data. Currently, I got the Google Drive Client running inside a Server VM, since I don\u2019t want it to go over 80k+ files every time I boot up my rig. Yes, I know about disabling auto start, however, in Hyper-V, my data is inside an extra VHDX drive which makes it easier to manage.\n\nWhat I thought was to buy a NAS and work from that. However, there are a few features that I want to use: \n\n- Incremental Backups (take Veeam for example)\n- Functionality of Volume Shadow Copy (Windows Fileserver)\n- Sync folders on a incremental basis (merge changes from the local folder to the Google drive synced folder for cloud backups, to prevent copying the whole archive over and over)\n\nMy idea was to mount the NAS storage via iCSCI on a windows server and enable volume shadow copy on that drive in order to have previous versions of files. I find that feature to be very import for me. \n\nIs there any better way to do this?", "author_fullname": "t2_2za0laa3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need new way of storing private data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106hk10", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673179141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got around 500 GB of data including Music, Photos, Software, Project Files, Backup Files, etc. Currently, it\u2019s all stored in Google Drive and I regularly create Backup archives of the local synced copy on external volumes. In total, there are about 80.000 files.&lt;/p&gt;\n\n&lt;p&gt;My goal is to manage my data more locally, since I don\u2019t want to trust the Google drive sync client. With a lot of simultaneous changes to the files, it messed up more than once, leading to inconsistent sync states between Gdrive and local storage, which have to be fixed manually. In its defense, the software just not made for that amount of data. Currently, I got the Google Drive Client running inside a Server VM, since I don\u2019t want it to go over 80k+ files every time I boot up my rig. Yes, I know about disabling auto start, however, in Hyper-V, my data is inside an extra VHDX drive which makes it easier to manage.&lt;/p&gt;\n\n&lt;p&gt;What I thought was to buy a NAS and work from that. However, there are a few features that I want to use: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Incremental Backups (take Veeam for example)&lt;/li&gt;\n&lt;li&gt;Functionality of Volume Shadow Copy (Windows Fileserver)&lt;/li&gt;\n&lt;li&gt;Sync folders on a incremental basis (merge changes from the local folder to the Google drive synced folder for cloud backups, to prevent copying the whole archive over and over)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My idea was to mount the NAS storage via iCSCI on a windows server and enable volume shadow copy on that drive in order to have previous versions of files. I find that feature to be very import for me. &lt;/p&gt;\n\n&lt;p&gt;Is there any better way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106hk10", "is_robot_indexable": true, "report_reasons": null, "author": "sim_koo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106hk10/need_new_way_of_storing_private_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106hk10/need_new_way_of_storing_private_data/", "subreddit_subscribers": 664754, "created_utc": 1673179141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Maybe I'm incredibly dense, but I can't seem to figure out how to remove the dust filter from the front of the case to give it a wash.\n\nManual says nothing, and my google-fu has failed me.", "author_fullname": "t2_7b4xf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rosewill rsv-l4500u filter replacement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10632g5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673134018.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Maybe I&amp;#39;m incredibly dense, but I can&amp;#39;t seem to figure out how to remove the dust filter from the front of the case to give it a wash.&lt;/p&gt;\n\n&lt;p&gt;Manual says nothing, and my google-fu has failed me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "90TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10632g5", "is_robot_indexable": true, "report_reasons": null, "author": "djtodd242", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10632g5/rosewill_rsvl4500u_filter_replacement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10632g5/rosewill_rsvl4500u_filter_replacement/", "subreddit_subscribers": 664754, "created_utc": 1673134018.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The disks should be fine, but the NAS itself seems to have failed. I'm curious if anyone knows an ideally free way to mount the disks on Linux or Windows?\n\nIt's 5 disks, single parity, so 12GB usable.", "author_fullname": "t2_dj2oi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to read a 15TB Drobo \"BeyondRAID\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106260d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673131796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The disks should be fine, but the NAS itself seems to have failed. I&amp;#39;m curious if anyone knows an ideally free way to mount the disks on Linux or Windows?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s 5 disks, single parity, so 12GB usable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106260d", "is_robot_indexable": true, "report_reasons": null, "author": "Krutonium", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106260d/need_to_read_a_15tb_drobo_beyondraid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106260d/need_to_read_a_15tb_drobo_beyondraid/", "subreddit_subscribers": 664754, "created_utc": 1673131796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've encountered an issue that I don't know how to troubleshoot, and have come here (what I suspect to be the highest concentration of Supermicro 846 owners on the internet) in the hope that one of you has seen something similar. When I try to connect a new drive to my Supermicro BPN-SAS2-846EL2, it spins up but does not appear to actually connect to anything. I see the following in `dmesg` on my Openmediavault 6 system:\n\n    [  756.914588] mpt2sas_cm0: handle(0x45) sas_address(0x5003048001215814) port_type(0x1)\n    [  757.461716] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n    [  757.529208] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n    [  757.597212] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n    [  757.721305] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n    [  757.721436]  end_device-0:5:24: add: handle(0x0045), sas_addr(0x5003048001215814)\n    [  757.721901] mpt2sas_cm0: mpt3sas_transport_port_remove: removed: sas_addr(0x5003048001215814)\n    [  757.721908] mpt2sas_cm0: removing handle(0x0045), sas_addr(0x5003048001215814)\n    [  757.721912] mpt2sas_cm0: enclosure logical id(0x500304800121583f), slot(8)\n\nIn contrast, this is when a drive is properly detected:\n\n    [101052.228678] mpt2sas_cm0: handle(0x2e) sas_address(0x500304800121580e) port_type(0x1)\n    [101053.231518] scsi 0:0:59:0: Direct-Access     ATA      ST18000NE000-3G6 EN01 PQ: 0 ANSI: 6\n    [101053.231544] scsi 0:0:59:0: SATA: handle(0x002e), sas_addr(0x500304800121580e), phy(14), device_name(0x0000000000000000)\n    [101053.231549] scsi 0:0:59:0: enclosure logical id (0x500304800121583f), slot(2) \n    [101053.231642] scsi 0:0:59:0: atapi(n), ncq(y), asyn_notify(n), smart(y), fua(y), sw_preserve(y)\n    [101053.231647] scsi 0:0:59:0: qdepth(32), tagged(1), scsi_level(7), cmd_que(1)\n    [101053.239770] sd 0:0:59:0: Power-on or device reset occurred\n    [101053.240360] sd 0:0:59:0: [sdab] 35156656128 512-byte logical blocks: (18.0 TB/16.4 TiB)\n    [101053.240366] sd 0:0:59:0: [sdab] 4096-byte physical blocks\n    [101053.241381] sd 0:0:59:0: [sdab] Write Protect is off\n    [101053.241384] sd 0:0:59:0: [sdab] Mode Sense: 7f 00 10 08\n    [101053.242361] sd 0:0:59:0: [sdab] Write cache: enabled, read cache: enabled, supports DPO and FUA\n    [101053.257517] sd 0:0:59:0: Attached scsi generic sg30 type 0\n    [101053.257627]  end_device-0:5:29: add: handle(0x002e), sas_addr(0x500304800121580e)\n    [101053.318871] sd 0:0:59:0: [sdab] Attached SCSI disk\n\nIt would seem that the drive is being detected, but immediately dropped by `mpt2sas`. The error code, according to [this post](https://serverfault.com/questions/876750/mdadm-marks-hdd-faulty-even-though-its-in-pristine-health), relates to a timeout:\n\n    Value           31110101h\n    Type:           30000000h       SAS\n    Origin:         01000000h       PL\n    Code:           00110000h       PL_LOGINFO_CODE_RESET See Sub-Codes below (PL_LOGINFO_SUB_CODE)\n    Sub Code:       00000100h       PL_LOGINFO_SUB_CODE_OPEN_FAILURE\n    SubSub Code:    00000001h       PL_LOGINFO_SUB_CODE_OPEN_FAILURE_NO_DEST_TIMEOUT\n\nThis error allegedly means \"Failed to open connection with error Open Reject (No Destination). Retried for 50milliseconds\" but I have yet to track down the 2009 LSI PDF referenced. The solution to the user's problem in that post doesn't work in my situation as the `/dev/sdX` device is never created and so I cannot adjust its timeout.\n   \nI first noticed this when I tried to add a new Seagate Ironwolf Pro 18TB, and thought this could be some 3.3V pin weirdness. But the backplane is powered by only molex connectors which don't provide 3.3V, and I've now confirmed this issue affects known working drives - if I pull a working drive from one slot and insert it into an affected slot, it fails to connect.\n\nAfter playing musical hard drives I've confirmed this appears to affects every slot in one of my BPN-SAS2-846EL2. I have two chassis, both 847s, with one having the 2U rear motherboard tray. All backplanes are connected via dual links cascading from a single LSI2008-based controller, the 9207-8i. Testing these drives on all other backplanes results in the expected recognition of the drive - because of this I suspect the problem is with the backplane, not the controller. I'll need to confirm the cabling to see if this is the first backplane in sequence, in case that makes a difference, but from memory it goes HBA=826EL2=**846EL2**=(external)=847EL2=846EL2.\n\nBut it gets weirder: after playing around with rebooting I noticed that drives which are connected at the time the backplane powers on are recognized and their slots can be hot-swapped as usual; any empty slot at power-up does not allow hot-swapping. Merely rebooting the server is not enough: it has to power down and then back on, which I suspect is necessary to perform a power cycle of the backplane. Because of this, and that it affects multiple slots across different rows and columns, I'm fairly confident this isn't an issue with the power supplies or molex connectors.\n\nPoking around with `lsiutil` doesn't reveal any obviously useful options - expert mode lists an option for accessing the SAS Expander UART Console but I'm unfamiliar with that and google has been of no help. This console also shows nothing when I plug or unplug a drive. Making things more challenging is I'm unsure when the problem began - I can't recall a time I ran this backplane without all the bays populated until now, so it's entirely possible this behavior has been present since I got it on eBay a few years ago. I have also just made some hardware changes, adding the second 847 chassis and so shuffled drives around, flashing the HBA firmware to 20.00.07.00 from 16.00.01.00, and upgrading from OMV 5 to 6 a few months ago. I have tried rolling back the linux kernel from 6.0.0 to 5.19.0 (the `linux-image` deb I had cached) to no avail.\n\nThat the backplane otherwise works, and that no other backplane I have exhibit this behavior, almost makes it seem like some kind of setting needs to be changed. I was under the impression these expanders don't have \"settings\" that can be changed. A quick review of the [backplane manual](https://www.supermicro.com/manuals/other/BPN-SAS2-846EL.pdf) lists a few jumpers described as \"factory setting, do not change\" or \"debug, for Supermicro internal use only\" - I'll need to take things apart to confirm jumper positions on my two 846 backplanes but have no idea if these could be impacting it.\n\nSo after this wall of text I have a few questions:\n\n* Has anyone seen this behavior before?\n* How do you go about debugging problems with a Supermicro backplane?\n* Do backplanes have \"settings\"?\n* What do these jumpers do?\n* What else should I be checking?\n\nMy next steps I think are to disconnect the other backplanes in case there's some weirdness with cascading them, and perhaps try a bootable ISO of another distro to see if it is also affected.\n\nThanks in advance for any advice.", "author_fullname": "t2_dzp6l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hot-swap problem on Supermicro backplane", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105yeet", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673122349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve encountered an issue that I don&amp;#39;t know how to troubleshoot, and have come here (what I suspect to be the highest concentration of Supermicro 846 owners on the internet) in the hope that one of you has seen something similar. When I try to connect a new drive to my Supermicro BPN-SAS2-846EL2, it spins up but does not appear to actually connect to anything. I see the following in &lt;code&gt;dmesg&lt;/code&gt; on my Openmediavault 6 system:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[  756.914588] mpt2sas_cm0: handle(0x45) sas_address(0x5003048001215814) port_type(0x1)\n[  757.461716] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n[  757.529208] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n[  757.597212] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n[  757.721305] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n[  757.721436]  end_device-0:5:24: add: handle(0x0045), sas_addr(0x5003048001215814)\n[  757.721901] mpt2sas_cm0: mpt3sas_transport_port_remove: removed: sas_addr(0x5003048001215814)\n[  757.721908] mpt2sas_cm0: removing handle(0x0045), sas_addr(0x5003048001215814)\n[  757.721912] mpt2sas_cm0: enclosure logical id(0x500304800121583f), slot(8)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;In contrast, this is when a drive is properly detected:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[101052.228678] mpt2sas_cm0: handle(0x2e) sas_address(0x500304800121580e) port_type(0x1)\n[101053.231518] scsi 0:0:59:0: Direct-Access     ATA      ST18000NE000-3G6 EN01 PQ: 0 ANSI: 6\n[101053.231544] scsi 0:0:59:0: SATA: handle(0x002e), sas_addr(0x500304800121580e), phy(14), device_name(0x0000000000000000)\n[101053.231549] scsi 0:0:59:0: enclosure logical id (0x500304800121583f), slot(2) \n[101053.231642] scsi 0:0:59:0: atapi(n), ncq(y), asyn_notify(n), smart(y), fua(y), sw_preserve(y)\n[101053.231647] scsi 0:0:59:0: qdepth(32), tagged(1), scsi_level(7), cmd_que(1)\n[101053.239770] sd 0:0:59:0: Power-on or device reset occurred\n[101053.240360] sd 0:0:59:0: [sdab] 35156656128 512-byte logical blocks: (18.0 TB/16.4 TiB)\n[101053.240366] sd 0:0:59:0: [sdab] 4096-byte physical blocks\n[101053.241381] sd 0:0:59:0: [sdab] Write Protect is off\n[101053.241384] sd 0:0:59:0: [sdab] Mode Sense: 7f 00 10 08\n[101053.242361] sd 0:0:59:0: [sdab] Write cache: enabled, read cache: enabled, supports DPO and FUA\n[101053.257517] sd 0:0:59:0: Attached scsi generic sg30 type 0\n[101053.257627]  end_device-0:5:29: add: handle(0x002e), sas_addr(0x500304800121580e)\n[101053.318871] sd 0:0:59:0: [sdab] Attached SCSI disk\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;It would seem that the drive is being detected, but immediately dropped by &lt;code&gt;mpt2sas&lt;/code&gt;. The error code, according to &lt;a href=\"https://serverfault.com/questions/876750/mdadm-marks-hdd-faulty-even-though-its-in-pristine-health\"&gt;this post&lt;/a&gt;, relates to a timeout:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Value           31110101h\nType:           30000000h       SAS\nOrigin:         01000000h       PL\nCode:           00110000h       PL_LOGINFO_CODE_RESET See Sub-Codes below (PL_LOGINFO_SUB_CODE)\nSub Code:       00000100h       PL_LOGINFO_SUB_CODE_OPEN_FAILURE\nSubSub Code:    00000001h       PL_LOGINFO_SUB_CODE_OPEN_FAILURE_NO_DEST_TIMEOUT\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This error allegedly means &amp;quot;Failed to open connection with error Open Reject (No Destination). Retried for 50milliseconds&amp;quot; but I have yet to track down the 2009 LSI PDF referenced. The solution to the user&amp;#39;s problem in that post doesn&amp;#39;t work in my situation as the &lt;code&gt;/dev/sdX&lt;/code&gt; device is never created and so I cannot adjust its timeout.&lt;/p&gt;\n\n&lt;p&gt;I first noticed this when I tried to add a new Seagate Ironwolf Pro 18TB, and thought this could be some 3.3V pin weirdness. But the backplane is powered by only molex connectors which don&amp;#39;t provide 3.3V, and I&amp;#39;ve now confirmed this issue affects known working drives - if I pull a working drive from one slot and insert it into an affected slot, it fails to connect.&lt;/p&gt;\n\n&lt;p&gt;After playing musical hard drives I&amp;#39;ve confirmed this appears to affects every slot in one of my BPN-SAS2-846EL2. I have two chassis, both 847s, with one having the 2U rear motherboard tray. All backplanes are connected via dual links cascading from a single LSI2008-based controller, the 9207-8i. Testing these drives on all other backplanes results in the expected recognition of the drive - because of this I suspect the problem is with the backplane, not the controller. I&amp;#39;ll need to confirm the cabling to see if this is the first backplane in sequence, in case that makes a difference, but from memory it goes HBA=826EL2=&lt;strong&gt;846EL2&lt;/strong&gt;=(external)=847EL2=846EL2.&lt;/p&gt;\n\n&lt;p&gt;But it gets weirder: after playing around with rebooting I noticed that drives which are connected at the time the backplane powers on are recognized and their slots can be hot-swapped as usual; any empty slot at power-up does not allow hot-swapping. Merely rebooting the server is not enough: it has to power down and then back on, which I suspect is necessary to perform a power cycle of the backplane. Because of this, and that it affects multiple slots across different rows and columns, I&amp;#39;m fairly confident this isn&amp;#39;t an issue with the power supplies or molex connectors.&lt;/p&gt;\n\n&lt;p&gt;Poking around with &lt;code&gt;lsiutil&lt;/code&gt; doesn&amp;#39;t reveal any obviously useful options - expert mode lists an option for accessing the SAS Expander UART Console but I&amp;#39;m unfamiliar with that and google has been of no help. This console also shows nothing when I plug or unplug a drive. Making things more challenging is I&amp;#39;m unsure when the problem began - I can&amp;#39;t recall a time I ran this backplane without all the bays populated until now, so it&amp;#39;s entirely possible this behavior has been present since I got it on eBay a few years ago. I have also just made some hardware changes, adding the second 847 chassis and so shuffled drives around, flashing the HBA firmware to 20.00.07.00 from 16.00.01.00, and upgrading from OMV 5 to 6 a few months ago. I have tried rolling back the linux kernel from 6.0.0 to 5.19.0 (the &lt;code&gt;linux-image&lt;/code&gt; deb I had cached) to no avail.&lt;/p&gt;\n\n&lt;p&gt;That the backplane otherwise works, and that no other backplane I have exhibit this behavior, almost makes it seem like some kind of setting needs to be changed. I was under the impression these expanders don&amp;#39;t have &amp;quot;settings&amp;quot; that can be changed. A quick review of the &lt;a href=\"https://www.supermicro.com/manuals/other/BPN-SAS2-846EL.pdf\"&gt;backplane manual&lt;/a&gt; lists a few jumpers described as &amp;quot;factory setting, do not change&amp;quot; or &amp;quot;debug, for Supermicro internal use only&amp;quot; - I&amp;#39;ll need to take things apart to confirm jumper positions on my two 846 backplanes but have no idea if these could be impacting it.&lt;/p&gt;\n\n&lt;p&gt;So after this wall of text I have a few questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Has anyone seen this behavior before?&lt;/li&gt;\n&lt;li&gt;How do you go about debugging problems with a Supermicro backplane?&lt;/li&gt;\n&lt;li&gt;Do backplanes have &amp;quot;settings&amp;quot;?&lt;/li&gt;\n&lt;li&gt;What do these jumpers do?&lt;/li&gt;\n&lt;li&gt;What else should I be checking?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My next steps I think are to disconnect the other backplanes in case there&amp;#39;s some weirdness with cascading them, and perhaps try a bootable ISO of another distro to see if it is also affected.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UiIPF2mqA_DFQCLEDWvkDDxPFwNwPdql7LOx5xV-euw.jpg?auto=webp&amp;s=bbfd64870f33bdb05e624063cdefbb5b9d58a33a", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/UiIPF2mqA_DFQCLEDWvkDDxPFwNwPdql7LOx5xV-euw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ff79c60078c0653520e4624a396864ea6a12989", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/UiIPF2mqA_DFQCLEDWvkDDxPFwNwPdql7LOx5xV-euw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=52c100d94355d301313024053a9066e4bd64d2ca", "width": 216, "height": 216}], "variants": {}, "id": "GzZqARtKWMuXLpXwl8oHMIC2I_bySGgdOaO3Xww_I9Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "510TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "105yeet", "is_robot_indexable": true, "report_reasons": null, "author": "Adarnof", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/105yeet/hotswap_problem_on_supermicro_backplane/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/105yeet/hotswap_problem_on_supermicro_backplane/", "subreddit_subscribers": 664754, "created_utc": 1673122349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone I'm having issues exporting about 6.5TB of data from my DS214 to an external drive. I have a 14TB drive I partitioned to 7TB for NTFS and the other 7TB to Mac OS Extended. I was using the USB Copy app since rsync was going to take to long but after getting to about the final 300GB or so it ran out of space. Now realizing I should have just given myself more space to work with is there any way to resume where the data transfer left off?", "author_fullname": "t2_93rkx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Diskstation USB Copy Issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105v27f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673114029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone I&amp;#39;m having issues exporting about 6.5TB of data from my DS214 to an external drive. I have a 14TB drive I partitioned to 7TB for NTFS and the other 7TB to Mac OS Extended. I was using the USB Copy app since rsync was going to take to long but after getting to about the final 300GB or so it ran out of space. Now realizing I should have just given myself more space to work with is there any way to resume where the data transfer left off?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "105v27f", "is_robot_indexable": true, "report_reasons": null, "author": "klnadler", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/105v27f/diskstation_usb_copy_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/105v27f/diskstation_usb_copy_issue/", "subreddit_subscribers": 664754, "created_utc": 1673114029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I wanted to make use of my old q9400+gtx750 setup which I rebuilt as a NAS and I was wondering if there was any viable way to virtualize windowsx64 since last time a tried you coulnd't do it on a cpu whithout either VT-x or SSE4 tech(don't remember which was the problem), at least in kvm or vbox/vmware. Been some time since then so I was wondering if it was possible or I should just forget about it, I don't mind the performance too much, it's just gonna be for testing.\n\nEdit: Turns out the cpu wasnt the problem but probably my old stock motherboard which I replaced some time ago with an asus p5ql-pro. In that case which software do you recommend to virtualize and connect remotely?", "author_fullname": "t2_kbxyj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about virtualization software", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_106mmas", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673195390.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673193613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to make use of my old q9400+gtx750 setup which I rebuilt as a NAS and I was wondering if there was any viable way to virtualize windowsx64 since last time a tried you coulnd&amp;#39;t do it on a cpu whithout either VT-x or SSE4 tech(don&amp;#39;t remember which was the problem), at least in kvm or vbox/vmware. Been some time since then so I was wondering if it was possible or I should just forget about it, I don&amp;#39;t mind the performance too much, it&amp;#39;s just gonna be for testing.&lt;/p&gt;\n\n&lt;p&gt;Edit: Turns out the cpu wasnt the problem but probably my old stock motherboard which I replaced some time ago with an asus p5ql-pro. In that case which software do you recommend to virtualize and connect remotely?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106mmas", "is_robot_indexable": true, "report_reasons": null, "author": "nengon", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106mmas/question_about_virtualization_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106mmas/question_about_virtualization_software/", "subreddit_subscribers": 664754, "created_utc": 1673193613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I seem to have Rclone setup properly, but Im a bit confused on how I would clone an entire folder of my external drive(connected to my mac) into a specific folder in my Google drive account. My Rclone account name will be NAME for the purpose of this post\n\nThe folder that I'm looking to copy is in this chain \"4tb&gt;offloaded footage&gt;categorized&gt;DJI&gt;2021\n\nand I'm looking to copy this to my google drive \"My Drive&gt;Categorized footage&gt;DJI&gt;2021\n\nHere is the terminal command that I'm using\n\n    rclone -v sync /Volumes/4TB\\ Offloaded\\ Footage/Categorized/DJI/2021 NAME:\n\nThen what do I put after this to get the contents copied into \n\n&gt;My Drive&gt;Categorized footage&gt;DJI&gt;2021", "author_fullname": "t2_y7xmw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using RClone to copy folder to folder?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_106m302", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673192235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I seem to have Rclone setup properly, but Im a bit confused on how I would clone an entire folder of my external drive(connected to my mac) into a specific folder in my Google drive account. My Rclone account name will be NAME for the purpose of this post&lt;/p&gt;\n\n&lt;p&gt;The folder that I&amp;#39;m looking to copy is in this chain &amp;quot;4tb&amp;gt;offloaded footage&amp;gt;categorized&amp;gt;DJI&amp;gt;2021&lt;/p&gt;\n\n&lt;p&gt;and I&amp;#39;m looking to copy this to my google drive &amp;quot;My Drive&amp;gt;Categorized footage&amp;gt;DJI&amp;gt;2021&lt;/p&gt;\n\n&lt;p&gt;Here is the terminal command that I&amp;#39;m using&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;rclone -v sync /Volumes/4TB\\ Offloaded\\ Footage/Categorized/DJI/2021 NAME:\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then what do I put after this to get the contents copied into &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;My Drive&amp;gt;Categorized footage&amp;gt;DJI&amp;gt;2021&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106m302", "is_robot_indexable": true, "report_reasons": null, "author": "AllAboutGadgets", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106m302/using_rclone_to_copy_folder_to_folder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106m302/using_rclone_to_copy_folder_to_folder/", "subreddit_subscribers": 664754, "created_utc": 1673192235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Theres a tumblr page of a musician who uploaded music every once in a while to it but either the page got hacked or the page was deleted. there are quite a few archives of the page on archive.org but for the most part no archives of the audio posted. the audio was old enough to use flash to play. ive tried installing flash emulator and that didnt work. is there anyway to do this ? thanks.", "author_fullname": "t2_8klngbgy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Way to download audio from deleted tumblr page?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106gd15", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673174837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Theres a tumblr page of a musician who uploaded music every once in a while to it but either the page got hacked or the page was deleted. there are quite a few archives of the page on archive.org but for the most part no archives of the audio posted. the audio was old enough to use flash to play. ive tried installing flash emulator and that didnt work. is there anyway to do this ? thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106gd15", "is_robot_indexable": true, "report_reasons": null, "author": "z-gyke-1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106gd15/way_to_download_audio_from_deleted_tumblr_page/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106gd15/way_to_download_audio_from_deleted_tumblr_page/", "subreddit_subscribers": 664754, "created_utc": 1673174837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently at a point in my journey where I'm looking to start planning out my storage config for my first homelab. This isn't a post asking EXACTLY what I should do but moreso trying to get a feel for what first timers don't really think about. What are common regrets you've heard or experienced when setting up the storage for homelab for the first time? I'm still deciding how much extra storage I plan on having(I know I need more than 10TB). Still deciding on a file system(I'm definitely leaning towards ZFS). Deciding whether I want to virtualize my NAS or not. For context, I plan on using this lab as part \"production\"(things like Plex, pihole, Home Assistant and a steam cache) and part testing to get more familiar with all kinds of things, from setting up a Windows domain controller to familiarizing myself with kubernetes.", "author_fullname": "t2_7sqku996", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for insight from those who've been there.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106exgz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673169614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently at a point in my journey where I&amp;#39;m looking to start planning out my storage config for my first homelab. This isn&amp;#39;t a post asking EXACTLY what I should do but moreso trying to get a feel for what first timers don&amp;#39;t really think about. What are common regrets you&amp;#39;ve heard or experienced when setting up the storage for homelab for the first time? I&amp;#39;m still deciding how much extra storage I plan on having(I know I need more than 10TB). Still deciding on a file system(I&amp;#39;m definitely leaning towards ZFS). Deciding whether I want to virtualize my NAS or not. For context, I plan on using this lab as part &amp;quot;production&amp;quot;(things like Plex, pihole, Home Assistant and a steam cache) and part testing to get more familiar with all kinds of things, from setting up a Windows domain controller to familiarizing myself with kubernetes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106exgz", "is_robot_indexable": true, "report_reasons": null, "author": "RiggedyWreckt", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106exgz/looking_for_insight_from_those_whove_been_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106exgz/looking_for_insight_from_those_whove_been_there/", "subreddit_subscribers": 664754, "created_utc": 1673169614.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}