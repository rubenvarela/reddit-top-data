{"kind": "Listing", "data": {"after": "t3_1068wnc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One of the oldest known version of RuneScape has officially been found", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105txiy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 717, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_nixsez0f", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 717, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "runescape", "selftext": "RuneScape has been around for a long time, but the developers didn't keep regular backups of the files until 2013. According to this post they had almost nothing prior to 2005. In the comments a Redditor uploaded a copy of the files they'd made and the developers stated it was the oldest version they've found to date.", "author_fullname": "t2_nixsez0f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One of the oldest known version of RuneScape has officially been found", "link_flair_richtext": [{"e": "text", "t": "Appreciation"}], "subreddit_name_prefixed": "r/runescape", "hidden": false, "pwls": 6, "link_flair_css_class": "appreciation", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105txb5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 365, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Appreciation", "can_mod_post": false, "score": 365, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1673111180.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;RuneScape has been around for a long time, but the developers didn&amp;#39;t keep regular backups of the files until 2013. According to this post they had almost nothing prior to 2005. In the comments a Redditor uploaded a copy of the files they&amp;#39;d made and the developers stated it was the oldest version they&amp;#39;ve found to date.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/r/pcgaming/comments/105r0pd/-/j3cklw2", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "edb97aae-60be-11eb-8047-0e64d8f4fc71", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qwxl", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a9afb4", "id": "105txb5", "is_robot_indexable": true, "report_reasons": null, "author": "AnApexBread", "discussion_type": null, "num_comments": 68, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/runescape/comments/105txb5/one_of_the_oldest_known_version_of_runescape_has/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/pcgaming/comments/105r0pd/-/j3cklw2", "subreddit_subscribers": 321320, "created_utc": 1673111180.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1673111197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/r/pcgaming/comments/105r0pd/-/j3cklw2", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "105txiy", "is_robot_indexable": true, "report_reasons": null, "author": "AnApexBread", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_105txb5", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/105txiy/one_of_the_oldest_known_version_of_runescape_has/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/pcgaming/comments/105r0pd/-/j3cklw2", "subreddit_subscribers": 664735, "created_utc": 1673111197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a a 1TB SanDisk 520s and I am looking for a solution to expand my storage capacity. Obviously I could just get another 1TB, but then I would then have multiple drives I need to eject when I move my MacBook about.\n\nLooking at YouTube videos it does look like it has an M2 NVME drive installed, so this got me thinking. For the same price of another 1TB sandisk I can get a 2tb M2 drive.\n\nIs there some enclose preferably unpowered (or type c) that can hold multiple M2s? I suppose two drives would be ok, but would prefer four. I have looked on Amazon and they either seem to be exposed, too big and also need power.\n\nRaid capability isn't necessary. Portability is fairly important though.", "author_fullname": "t2_1gm1x1ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a solution for a multi M2 drive storage with a single cable. Have a 1TB SanDisk and want to expand.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106fh0r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673171633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a a 1TB SanDisk 520s and I am looking for a solution to expand my storage capacity. Obviously I could just get another 1TB, but then I would then have multiple drives I need to eject when I move my MacBook about.&lt;/p&gt;\n\n&lt;p&gt;Looking at YouTube videos it does look like it has an M2 NVME drive installed, so this got me thinking. For the same price of another 1TB sandisk I can get a 2tb M2 drive.&lt;/p&gt;\n\n&lt;p&gt;Is there some enclose preferably unpowered (or type c) that can hold multiple M2s? I suppose two drives would be ok, but would prefer four. I have looked on Amazon and they either seem to be exposed, too big and also need power.&lt;/p&gt;\n\n&lt;p&gt;Raid capability isn&amp;#39;t necessary. Portability is fairly important though.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106fh0r", "is_robot_indexable": true, "report_reasons": null, "author": "matmah", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106fh0r/looking_for_a_solution_for_a_multi_m2_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106fh0r/looking_for_a_solution_for_a_multi_m2_drive/", "subreddit_subscribers": 664735, "created_utc": 1673171633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there fellow DataHoarders! I searched both Google and this sub for an answer to this quandary, but couldn't find anything. Maybe what I want to do is impossible and I should just accept it. \n\nI like to game on one monitor and halfheartedly watch videos on the other in the evenings, and I wanted to see if there was a way I could consolidate all 389 GB of my locally stored video files into a single playlist that can play them randomly.\n\nI made the attempt with VLC, but it chokes on it about halfway through adding the files to the playlist. I don't know if there is a numerical limit, or if it goes by file size, not sure. Is there another application, extension, or maybe something out there in GitHub world that you know of that can accomplish this?\n\nI really appreciate any help, and thank you in advance!", "author_fullname": "t2_iwgp7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating a Huge Adult Swim Playlist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10607ro", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673126915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there fellow DataHoarders! I searched both Google and this sub for an answer to this quandary, but couldn&amp;#39;t find anything. Maybe what I want to do is impossible and I should just accept it. &lt;/p&gt;\n\n&lt;p&gt;I like to game on one monitor and halfheartedly watch videos on the other in the evenings, and I wanted to see if there was a way I could consolidate all 389 GB of my locally stored video files into a single playlist that can play them randomly.&lt;/p&gt;\n\n&lt;p&gt;I made the attempt with VLC, but it chokes on it about halfway through adding the files to the playlist. I don&amp;#39;t know if there is a numerical limit, or if it goes by file size, not sure. Is there another application, extension, or maybe something out there in GitHub world that you know of that can accomplish this?&lt;/p&gt;\n\n&lt;p&gt;I really appreciate any help, and thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10607ro", "is_robot_indexable": true, "report_reasons": null, "author": "TropicalDruid", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10607ro/creating_a_huge_adult_swim_playlist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10607ro/creating_a_huge_adult_swim_playlist/", "subreddit_subscribers": 664735, "created_utc": 1673126915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "When searching for a general keyword in the search bar, usually some 30 or 50k books show up. However, only about 1k can actually be seen. That is more or less until page 100. Past that... no more pages and thousands of books we can't acess. I thought this was the place to post the question because the results are surely in their system, it shows the numbers. Tried on different browsers and platforms, so it's definitely an intentional feature. ", "author_fullname": "t2_btlsftp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to see all results in Amazon search?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105ya4l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673125624.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673122042.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When searching for a general keyword in the search bar, usually some 30 or 50k books show up. However, only about 1k can actually be seen. That is more or less until page 100. Past that... no more pages and thousands of books we can&amp;#39;t acess. I thought this was the place to post the question because the results are surely in their system, it shows the numbers. Tried on different browsers and platforms, so it&amp;#39;s definitely an intentional feature. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "105ya4l", "is_robot_indexable": true, "report_reasons": null, "author": "Freibetto", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/105ya4l/how_to_see_all_results_in_amazon_search/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/105ya4l/how_to_see_all_results_in_amazon_search/", "subreddit_subscribers": 664735, "created_utc": 1673122042.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Entire question fit in the title of this post.", "author_fullname": "t2_8hlee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In a 2-drive raid 1 array, is it a viable backup strategy to pull one drive for cold storage and then replace it (rebuilding the array)? Could one then just rotate the third drive through periodically in this manner when a new backup is desired?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1067x0r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673146956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Entire question fit in the title of this post.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1067x0r", "is_robot_indexable": true, "report_reasons": null, "author": "nouvie", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1067x0r/in_a_2drive_raid_1_array_is_it_a_viable_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1067x0r/in_a_2drive_raid_1_array_is_it_a_viable_backup/", "subreddit_subscribers": 664735, "created_utc": 1673146956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, im quite newbie to datahoarding, so basically i am mass downloading bunch of image albums for my little archive but some of these images have useful comments that would also be handy to have in txt file for further reference. \n\nI learned to use JDownloader2 for image batch downloading, I was wondering if there is something similar that I could use for comments/text on webpage?", "author_fullname": "t2_ur3sbbk7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there some easy way to extract comments from website into txt/docx files or such?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1061efc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673129851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, im quite newbie to datahoarding, so basically i am mass downloading bunch of image albums for my little archive but some of these images have useful comments that would also be handy to have in txt file for further reference. &lt;/p&gt;\n\n&lt;p&gt;I learned to use JDownloader2 for image batch downloading, I was wondering if there is something similar that I could use for comments/text on webpage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1061efc", "is_robot_indexable": true, "report_reasons": null, "author": "Kuznetsov063", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1061efc/is_there_some_easy_way_to_extract_comments_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1061efc/is_there_some_easy_way_to_extract_comments_from/", "subreddit_subscribers": 664735, "created_utc": 1673129851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After reading some of the opinions of folks in this subreddit, I decided to take a chance on some refurbed Exos X18 12TB drives for my NAS, ordered from Server Part Deals. I've swapped in two of them so far, and the SMART data looks good, but only show 25 and 15 power on hours and a start/stop count of 2. Assuming I trust the seller, does anyone know if Seagate resets SMART data when they refurbish drives? I know a lot of these drives may have never seen much or any actual use before they were rejected for one reason or another, but I'm curious if these \\*really\\* are low mileage or if Seagate reset the stats.\n\nBy the way, I have to say Server Part Deals was great to deal with. They really do ship these things well, and they shipped them fast. Thanks for the recommendation!", "author_fullname": "t2_5iea7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Seagate reset SMART data on manufacturer refurbs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105wtcx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673118396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After reading some of the opinions of folks in this subreddit, I decided to take a chance on some refurbed Exos X18 12TB drives for my NAS, ordered from Server Part Deals. I&amp;#39;ve swapped in two of them so far, and the SMART data looks good, but only show 25 and 15 power on hours and a start/stop count of 2. Assuming I trust the seller, does anyone know if Seagate resets SMART data when they refurbish drives? I know a lot of these drives may have never seen much or any actual use before they were rejected for one reason or another, but I&amp;#39;m curious if these *really* are low mileage or if Seagate reset the stats.&lt;/p&gt;\n\n&lt;p&gt;By the way, I have to say Server Part Deals was great to deal with. They really do ship these things well, and they shipped them fast. Thanks for the recommendation!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "105wtcx", "is_robot_indexable": true, "report_reasons": null, "author": "compulov", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/105wtcx/does_seagate_reset_smart_data_on_manufacturer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/105wtcx/does_seagate_reset_smart_data_on_manufacturer/", "subreddit_subscribers": 664735, "created_utc": 1673118396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for an image host (like imagur) that does not compress the image I upload. 100% lossless. I don't really care about features other than I would need to be able to see it from anywhere so would other people. Thanks.", "author_fullname": "t2_usun3qeu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lossless Image Hosting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106d1dr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673162829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for an image host (like imagur) that does not compress the image I upload. 100% lossless. I don&amp;#39;t really care about features other than I would need to be able to see it from anywhere so would other people. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106d1dr", "is_robot_indexable": true, "report_reasons": null, "author": "PuzzleheadedTennis23", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106d1dr/lossless_image_hosting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106d1dr/lossless_image_hosting/", "subreddit_subscribers": 664735, "created_utc": 1673162829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't run raid or NAS, at least not yet, just a simple DAS(?) setup with two drives (19tb) and a single backup drive (13tb). In the future I plan to get more drives and may top out at a total of 5-6 drives.\n\nWhat's the ideal solution for this? Get a large PC case like the Meshify 2 or a smaller case and get like a 5-8 bay enclosure?", "author_fullname": "t2_hrdo3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Large PC case vs SFF with external HDD enclosure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1067u94", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673146768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t run raid or NAS, at least not yet, just a simple DAS(?) setup with two drives (19tb) and a single backup drive (13tb). In the future I plan to get more drives and may top out at a total of 5-6 drives.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the ideal solution for this? Get a large PC case like the Meshify 2 or a smaller case and get like a 5-8 bay enclosure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1067u94", "is_robot_indexable": true, "report_reasons": null, "author": "c9898", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1067u94/large_pc_case_vs_sff_with_external_hdd_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1067u94/large_pc_case_vs_sff_with_external_hdd_enclosure/", "subreddit_subscribers": 664735, "created_utc": 1673146768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Happy new year!\n\nIs any of you aware of a maximum file size limit on Hetzner storage boxes?\n\nI have tried to upload a  ~100G disk backup to my storage box over the holidays and that failed with some non-descript error. After the fact I found out that I had a local HDD failure that might have caused this...\n\nbefore I commit to uploading again (my ISD outgoing speed is quite low) I thought I'd check with the wisdom in this subreddit...", "author_fullname": "t2_14oq692k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "maximum file size on Hetzner storagebox?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10605yt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673126777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Happy new year!&lt;/p&gt;\n\n&lt;p&gt;Is any of you aware of a maximum file size limit on Hetzner storage boxes?&lt;/p&gt;\n\n&lt;p&gt;I have tried to upload a  ~100G disk backup to my storage box over the holidays and that failed with some non-descript error. After the fact I found out that I had a local HDD failure that might have caused this...&lt;/p&gt;\n\n&lt;p&gt;before I commit to uploading again (my ISD outgoing speed is quite low) I thought I&amp;#39;d check with the wisdom in this subreddit...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10605yt", "is_robot_indexable": true, "report_reasons": null, "author": "biochronox", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10605yt/maximum_file_size_on_hetzner_storagebox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10605yt/maximum_file_size_on_hetzner_storagebox/", "subreddit_subscribers": 664735, "created_utc": 1673126777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All! I want to move away from xpenology shr to something else but have no intermediary server to store data. Any tips on a cloud storage with 10tb to keep my stuff there for a month?", "author_fullname": "t2_gexty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "temporary cheap 10tb cloud storage for migration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105sjex", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673107669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All! I want to move away from xpenology shr to something else but have no intermediary server to store data. Any tips on a cloud storage with 10tb to keep my stuff there for a month?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "105sjex", "is_robot_indexable": true, "report_reasons": null, "author": "nightrave", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/105sjex/temporary_cheap_10tb_cloud_storage_for_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/105sjex/temporary_cheap_10tb_cloud_storage_for_migration/", "subreddit_subscribers": 664735, "created_utc": 1673107669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need help finding duplicates of my JAV collection, the problem is that some files have their titles in different format like:\n\na) GVG-001.mp4\n\nb) GVG001.mp4\n\nc) GVG001 - additional movie title\n\nWhat software would you recommend to find dupes? Duplicate files might have different lengths so CRC search will not work. I need some way to analyze the first few letters of a filename keeping in mind that there may or may not be \"-\" sing in there. Can you help me out?\n\nI have total commander but I'm not sure if its search function is as powerful.", "author_fullname": "t2_tmiw5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for finding duplicate files based on a similar filename.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106g20g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673173717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help finding duplicates of my JAV collection, the problem is that some files have their titles in different format like:&lt;/p&gt;\n\n&lt;p&gt;a) GVG-001.mp4&lt;/p&gt;\n\n&lt;p&gt;b) GVG001.mp4&lt;/p&gt;\n\n&lt;p&gt;c) GVG001 - additional movie title&lt;/p&gt;\n\n&lt;p&gt;What software would you recommend to find dupes? Duplicate files might have different lengths so CRC search will not work. I need some way to analyze the first few letters of a filename keeping in mind that there may or may not be &amp;quot;-&amp;quot; sing in there. Can you help me out?&lt;/p&gt;\n\n&lt;p&gt;I have total commander but I&amp;#39;m not sure if its search function is as powerful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106g20g", "is_robot_indexable": true, "report_reasons": null, "author": "wooshaq", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106g20g/software_for_finding_duplicate_files_based_on_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106g20g/software_for_finding_duplicate_files_based_on_a/", "subreddit_subscribers": 664735, "created_utc": 1673173717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Maybe I'm incredibly dense, but I can't seem to figure out how to remove the dust filter from the front of the case to give it a wash.\n\nManual says nothing, and my google-fu has failed me.", "author_fullname": "t2_7b4xf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rosewill rsv-l4500u filter replacement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10632g5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673134018.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Maybe I&amp;#39;m incredibly dense, but I can&amp;#39;t seem to figure out how to remove the dust filter from the front of the case to give it a wash.&lt;/p&gt;\n\n&lt;p&gt;Manual says nothing, and my google-fu has failed me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "90TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10632g5", "is_robot_indexable": true, "report_reasons": null, "author": "djtodd242", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10632g5/rosewill_rsvl4500u_filter_replacement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10632g5/rosewill_rsvl4500u_filter_replacement/", "subreddit_subscribers": 664735, "created_utc": 1673134018.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The disks should be fine, but the NAS itself seems to have failed. I'm curious if anyone knows an ideally free way to mount the disks on Linux or Windows?\n\nIt's 5 disks, single parity, so 12GB usable.", "author_fullname": "t2_dj2oi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to read a 15TB Drobo \"BeyondRAID\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106260d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673131796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The disks should be fine, but the NAS itself seems to have failed. I&amp;#39;m curious if anyone knows an ideally free way to mount the disks on Linux or Windows?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s 5 disks, single parity, so 12GB usable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106260d", "is_robot_indexable": true, "report_reasons": null, "author": "Krutonium", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106260d/need_to_read_a_15tb_drobo_beyondraid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106260d/need_to_read_a_15tb_drobo_beyondraid/", "subreddit_subscribers": 664735, "created_utc": 1673131796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've encountered an issue that I don't know how to troubleshoot, and have come here (what I suspect to be the highest concentration of Supermicro 846 owners on the internet) in the hope that one of you has seen something similar. When I try to connect a new drive to my Supermicro BPN-SAS2-846EL2, it spins up but does not appear to actually connect to anything. I see the following in `dmesg` on my Openmediavault 6 system:\n\n    [  756.914588] mpt2sas_cm0: handle(0x45) sas_address(0x5003048001215814) port_type(0x1)\n    [  757.461716] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n    [  757.529208] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n    [  757.597212] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n    [  757.721305] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n    [  757.721436]  end_device-0:5:24: add: handle(0x0045), sas_addr(0x5003048001215814)\n    [  757.721901] mpt2sas_cm0: mpt3sas_transport_port_remove: removed: sas_addr(0x5003048001215814)\n    [  757.721908] mpt2sas_cm0: removing handle(0x0045), sas_addr(0x5003048001215814)\n    [  757.721912] mpt2sas_cm0: enclosure logical id(0x500304800121583f), slot(8)\n\nIn contrast, this is when a drive is properly detected:\n\n    [101052.228678] mpt2sas_cm0: handle(0x2e) sas_address(0x500304800121580e) port_type(0x1)\n    [101053.231518] scsi 0:0:59:0: Direct-Access     ATA      ST18000NE000-3G6 EN01 PQ: 0 ANSI: 6\n    [101053.231544] scsi 0:0:59:0: SATA: handle(0x002e), sas_addr(0x500304800121580e), phy(14), device_name(0x0000000000000000)\n    [101053.231549] scsi 0:0:59:0: enclosure logical id (0x500304800121583f), slot(2) \n    [101053.231642] scsi 0:0:59:0: atapi(n), ncq(y), asyn_notify(n), smart(y), fua(y), sw_preserve(y)\n    [101053.231647] scsi 0:0:59:0: qdepth(32), tagged(1), scsi_level(7), cmd_que(1)\n    [101053.239770] sd 0:0:59:0: Power-on or device reset occurred\n    [101053.240360] sd 0:0:59:0: [sdab] 35156656128 512-byte logical blocks: (18.0 TB/16.4 TiB)\n    [101053.240366] sd 0:0:59:0: [sdab] 4096-byte physical blocks\n    [101053.241381] sd 0:0:59:0: [sdab] Write Protect is off\n    [101053.241384] sd 0:0:59:0: [sdab] Mode Sense: 7f 00 10 08\n    [101053.242361] sd 0:0:59:0: [sdab] Write cache: enabled, read cache: enabled, supports DPO and FUA\n    [101053.257517] sd 0:0:59:0: Attached scsi generic sg30 type 0\n    [101053.257627]  end_device-0:5:29: add: handle(0x002e), sas_addr(0x500304800121580e)\n    [101053.318871] sd 0:0:59:0: [sdab] Attached SCSI disk\n\nIt would seem that the drive is being detected, but immediately dropped by `mpt2sas`. The error code, according to [this post](https://serverfault.com/questions/876750/mdadm-marks-hdd-faulty-even-though-its-in-pristine-health), relates to a timeout:\n\n    Value           31110101h\n    Type:           30000000h       SAS\n    Origin:         01000000h       PL\n    Code:           00110000h       PL_LOGINFO_CODE_RESET See Sub-Codes below (PL_LOGINFO_SUB_CODE)\n    Sub Code:       00000100h       PL_LOGINFO_SUB_CODE_OPEN_FAILURE\n    SubSub Code:    00000001h       PL_LOGINFO_SUB_CODE_OPEN_FAILURE_NO_DEST_TIMEOUT\n\nThis error allegedly means \"Failed to open connection with error Open Reject (No Destination). Retried for 50milliseconds\" but I have yet to track down the 2009 LSI PDF referenced. The solution to the user's problem in that post doesn't work in my situation as the `/dev/sdX` device is never created and so I cannot adjust its timeout.\n   \nI first noticed this when I tried to add a new Seagate Ironwolf Pro 18TB, and thought this could be some 3.3V pin weirdness. But the backplane is powered by only molex connectors which don't provide 3.3V, and I've now confirmed this issue affects known working drives - if I pull a working drive from one slot and insert it into an affected slot, it fails to connect.\n\nAfter playing musical hard drives I've confirmed this appears to affects every slot in one of my BPN-SAS2-846EL2. I have two chassis, both 847s, with one having the 2U rear motherboard tray. All backplanes are connected via dual links cascading from a single LSI2008-based controller, the 9207-8i. Testing these drives on all other backplanes results in the expected recognition of the drive - because of this I suspect the problem is with the backplane, not the controller. I'll need to confirm the cabling to see if this is the first backplane in sequence, in case that makes a difference, but from memory it goes HBA=826EL2=**846EL2**=(external)=847EL2=846EL2.\n\nBut it gets weirder: after playing around with rebooting I noticed that drives which are connected at the time the backplane powers on are recognized and their slots can be hot-swapped as usual; any empty slot at power-up does not allow hot-swapping. Merely rebooting the server is not enough: it has to power down and then back on, which I suspect is necessary to perform a power cycle of the backplane. Because of this, and that it affects multiple slots across different rows and columns, I'm fairly confident this isn't an issue with the power supplies or molex connectors.\n\nPoking around with `lsiutil` doesn't reveal any obviously useful options - expert mode lists an option for accessing the SAS Expander UART Console but I'm unfamiliar with that and google has been of no help. This console also shows nothing when I plug or unplug a drive. Making things more challenging is I'm unsure when the problem began - I can't recall a time I ran this backplane without all the bays populated until now, so it's entirely possible this behavior has been present since I got it on eBay a few years ago. I have also just made some hardware changes, adding the second 847 chassis and so shuffled drives around, flashing the HBA firmware to 20.00.07.00 from 16.00.01.00, and upgrading from OMV 5 to 6 a few months ago. I have tried rolling back the linux kernel from 6.0.0 to 5.19.0 (the `linux-image` deb I had cached) to no avail.\n\nThat the backplane otherwise works, and that no other backplane I have exhibit this behavior, almost makes it seem like some kind of setting needs to be changed. I was under the impression these expanders don't have \"settings\" that can be changed. A quick review of the [backplane manual](https://www.supermicro.com/manuals/other/BPN-SAS2-846EL.pdf) lists a few jumpers described as \"factory setting, do not change\" or \"debug, for Supermicro internal use only\" - I'll need to take things apart to confirm jumper positions on my two 846 backplanes but have no idea if these could be impacting it.\n\nSo after this wall of text I have a few questions:\n\n* Has anyone seen this behavior before?\n* How do you go about debugging problems with a Supermicro backplane?\n* Do backplanes have \"settings\"?\n* What do these jumpers do?\n* What else should I be checking?\n\nMy next steps I think are to disconnect the other backplanes in case there's some weirdness with cascading them, and perhaps try a bootable ISO of another distro to see if it is also affected.\n\nThanks in advance for any advice.", "author_fullname": "t2_dzp6l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hot-swap problem on Supermicro backplane", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105yeet", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673122349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve encountered an issue that I don&amp;#39;t know how to troubleshoot, and have come here (what I suspect to be the highest concentration of Supermicro 846 owners on the internet) in the hope that one of you has seen something similar. When I try to connect a new drive to my Supermicro BPN-SAS2-846EL2, it spins up but does not appear to actually connect to anything. I see the following in &lt;code&gt;dmesg&lt;/code&gt; on my Openmediavault 6 system:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[  756.914588] mpt2sas_cm0: handle(0x45) sas_address(0x5003048001215814) port_type(0x1)\n[  757.461716] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n[  757.529208] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n[  757.597212] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n[  757.721305] mpt2sas_cm0: log_info(0x31110101): originator(PL), code(0x11), sub_code(0x0101)\n[  757.721436]  end_device-0:5:24: add: handle(0x0045), sas_addr(0x5003048001215814)\n[  757.721901] mpt2sas_cm0: mpt3sas_transport_port_remove: removed: sas_addr(0x5003048001215814)\n[  757.721908] mpt2sas_cm0: removing handle(0x0045), sas_addr(0x5003048001215814)\n[  757.721912] mpt2sas_cm0: enclosure logical id(0x500304800121583f), slot(8)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;In contrast, this is when a drive is properly detected:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[101052.228678] mpt2sas_cm0: handle(0x2e) sas_address(0x500304800121580e) port_type(0x1)\n[101053.231518] scsi 0:0:59:0: Direct-Access     ATA      ST18000NE000-3G6 EN01 PQ: 0 ANSI: 6\n[101053.231544] scsi 0:0:59:0: SATA: handle(0x002e), sas_addr(0x500304800121580e), phy(14), device_name(0x0000000000000000)\n[101053.231549] scsi 0:0:59:0: enclosure logical id (0x500304800121583f), slot(2) \n[101053.231642] scsi 0:0:59:0: atapi(n), ncq(y), asyn_notify(n), smart(y), fua(y), sw_preserve(y)\n[101053.231647] scsi 0:0:59:0: qdepth(32), tagged(1), scsi_level(7), cmd_que(1)\n[101053.239770] sd 0:0:59:0: Power-on or device reset occurred\n[101053.240360] sd 0:0:59:0: [sdab] 35156656128 512-byte logical blocks: (18.0 TB/16.4 TiB)\n[101053.240366] sd 0:0:59:0: [sdab] 4096-byte physical blocks\n[101053.241381] sd 0:0:59:0: [sdab] Write Protect is off\n[101053.241384] sd 0:0:59:0: [sdab] Mode Sense: 7f 00 10 08\n[101053.242361] sd 0:0:59:0: [sdab] Write cache: enabled, read cache: enabled, supports DPO and FUA\n[101053.257517] sd 0:0:59:0: Attached scsi generic sg30 type 0\n[101053.257627]  end_device-0:5:29: add: handle(0x002e), sas_addr(0x500304800121580e)\n[101053.318871] sd 0:0:59:0: [sdab] Attached SCSI disk\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;It would seem that the drive is being detected, but immediately dropped by &lt;code&gt;mpt2sas&lt;/code&gt;. The error code, according to &lt;a href=\"https://serverfault.com/questions/876750/mdadm-marks-hdd-faulty-even-though-its-in-pristine-health\"&gt;this post&lt;/a&gt;, relates to a timeout:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Value           31110101h\nType:           30000000h       SAS\nOrigin:         01000000h       PL\nCode:           00110000h       PL_LOGINFO_CODE_RESET See Sub-Codes below (PL_LOGINFO_SUB_CODE)\nSub Code:       00000100h       PL_LOGINFO_SUB_CODE_OPEN_FAILURE\nSubSub Code:    00000001h       PL_LOGINFO_SUB_CODE_OPEN_FAILURE_NO_DEST_TIMEOUT\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This error allegedly means &amp;quot;Failed to open connection with error Open Reject (No Destination). Retried for 50milliseconds&amp;quot; but I have yet to track down the 2009 LSI PDF referenced. The solution to the user&amp;#39;s problem in that post doesn&amp;#39;t work in my situation as the &lt;code&gt;/dev/sdX&lt;/code&gt; device is never created and so I cannot adjust its timeout.&lt;/p&gt;\n\n&lt;p&gt;I first noticed this when I tried to add a new Seagate Ironwolf Pro 18TB, and thought this could be some 3.3V pin weirdness. But the backplane is powered by only molex connectors which don&amp;#39;t provide 3.3V, and I&amp;#39;ve now confirmed this issue affects known working drives - if I pull a working drive from one slot and insert it into an affected slot, it fails to connect.&lt;/p&gt;\n\n&lt;p&gt;After playing musical hard drives I&amp;#39;ve confirmed this appears to affects every slot in one of my BPN-SAS2-846EL2. I have two chassis, both 847s, with one having the 2U rear motherboard tray. All backplanes are connected via dual links cascading from a single LSI2008-based controller, the 9207-8i. Testing these drives on all other backplanes results in the expected recognition of the drive - because of this I suspect the problem is with the backplane, not the controller. I&amp;#39;ll need to confirm the cabling to see if this is the first backplane in sequence, in case that makes a difference, but from memory it goes HBA=826EL2=&lt;strong&gt;846EL2&lt;/strong&gt;=(external)=847EL2=846EL2.&lt;/p&gt;\n\n&lt;p&gt;But it gets weirder: after playing around with rebooting I noticed that drives which are connected at the time the backplane powers on are recognized and their slots can be hot-swapped as usual; any empty slot at power-up does not allow hot-swapping. Merely rebooting the server is not enough: it has to power down and then back on, which I suspect is necessary to perform a power cycle of the backplane. Because of this, and that it affects multiple slots across different rows and columns, I&amp;#39;m fairly confident this isn&amp;#39;t an issue with the power supplies or molex connectors.&lt;/p&gt;\n\n&lt;p&gt;Poking around with &lt;code&gt;lsiutil&lt;/code&gt; doesn&amp;#39;t reveal any obviously useful options - expert mode lists an option for accessing the SAS Expander UART Console but I&amp;#39;m unfamiliar with that and google has been of no help. This console also shows nothing when I plug or unplug a drive. Making things more challenging is I&amp;#39;m unsure when the problem began - I can&amp;#39;t recall a time I ran this backplane without all the bays populated until now, so it&amp;#39;s entirely possible this behavior has been present since I got it on eBay a few years ago. I have also just made some hardware changes, adding the second 847 chassis and so shuffled drives around, flashing the HBA firmware to 20.00.07.00 from 16.00.01.00, and upgrading from OMV 5 to 6 a few months ago. I have tried rolling back the linux kernel from 6.0.0 to 5.19.0 (the &lt;code&gt;linux-image&lt;/code&gt; deb I had cached) to no avail.&lt;/p&gt;\n\n&lt;p&gt;That the backplane otherwise works, and that no other backplane I have exhibit this behavior, almost makes it seem like some kind of setting needs to be changed. I was under the impression these expanders don&amp;#39;t have &amp;quot;settings&amp;quot; that can be changed. A quick review of the &lt;a href=\"https://www.supermicro.com/manuals/other/BPN-SAS2-846EL.pdf\"&gt;backplane manual&lt;/a&gt; lists a few jumpers described as &amp;quot;factory setting, do not change&amp;quot; or &amp;quot;debug, for Supermicro internal use only&amp;quot; - I&amp;#39;ll need to take things apart to confirm jumper positions on my two 846 backplanes but have no idea if these could be impacting it.&lt;/p&gt;\n\n&lt;p&gt;So after this wall of text I have a few questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Has anyone seen this behavior before?&lt;/li&gt;\n&lt;li&gt;How do you go about debugging problems with a Supermicro backplane?&lt;/li&gt;\n&lt;li&gt;Do backplanes have &amp;quot;settings&amp;quot;?&lt;/li&gt;\n&lt;li&gt;What do these jumpers do?&lt;/li&gt;\n&lt;li&gt;What else should I be checking?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My next steps I think are to disconnect the other backplanes in case there&amp;#39;s some weirdness with cascading them, and perhaps try a bootable ISO of another distro to see if it is also affected.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UiIPF2mqA_DFQCLEDWvkDDxPFwNwPdql7LOx5xV-euw.jpg?auto=webp&amp;s=bbfd64870f33bdb05e624063cdefbb5b9d58a33a", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/UiIPF2mqA_DFQCLEDWvkDDxPFwNwPdql7LOx5xV-euw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ff79c60078c0653520e4624a396864ea6a12989", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/UiIPF2mqA_DFQCLEDWvkDDxPFwNwPdql7LOx5xV-euw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=52c100d94355d301313024053a9066e4bd64d2ca", "width": 216, "height": 216}], "variants": {}, "id": "GzZqARtKWMuXLpXwl8oHMIC2I_bySGgdOaO3Xww_I9Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "510TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "105yeet", "is_robot_indexable": true, "report_reasons": null, "author": "Adarnof", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/105yeet/hotswap_problem_on_supermicro_backplane/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/105yeet/hotswap_problem_on_supermicro_backplane/", "subreddit_subscribers": 664735, "created_utc": 1673122349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone I'm having issues exporting about 6.5TB of data from my DS214 to an external drive. I have a 14TB drive I partitioned to 7TB for NTFS and the other 7TB to Mac OS Extended. I was using the USB Copy app since rsync was going to take to long but after getting to about the final 300GB or so it ran out of space. Now realizing I should have just given myself more space to work with is there any way to resume where the data transfer left off?", "author_fullname": "t2_93rkx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Diskstation USB Copy Issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105v27f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673114029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone I&amp;#39;m having issues exporting about 6.5TB of data from my DS214 to an external drive. I have a 14TB drive I partitioned to 7TB for NTFS and the other 7TB to Mac OS Extended. I was using the USB Copy app since rsync was going to take to long but after getting to about the final 300GB or so it ran out of space. Now realizing I should have just given myself more space to work with is there any way to resume where the data transfer left off?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "105v27f", "is_robot_indexable": true, "report_reasons": null, "author": "klnadler", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/105v27f/diskstation_usb_copy_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/105v27f/diskstation_usb_copy_issue/", "subreddit_subscribers": 664735, "created_utc": 1673114029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The highest number of bays I can find readily available to purchase in Europe is the Inter-Tech 4U-4736 with 36 x 3.5\" bays, but ideally I would like a top loaded chasis (think Backblaze/45 drives) like the Chenbro RM43348 or NR40700 (48 bays) or AIC RSC-4H or RSC-4H1 (60 bays).\n\nI have scoured eBay for months without luck.\n\nDoes anyone know where 48-60 bay enclosures can be bought in Europe?", "author_fullname": "t2_dyxta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "High density chasis in Europe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105oplm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673097082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The highest number of bays I can find readily available to purchase in Europe is the Inter-Tech 4U-4736 with 36 x 3.5&amp;quot; bays, but ideally I would like a top loaded chasis (think Backblaze/45 drives) like the Chenbro RM43348 or NR40700 (48 bays) or AIC RSC-4H or RSC-4H1 (60 bays).&lt;/p&gt;\n\n&lt;p&gt;I have scoured eBay for months without luck.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know where 48-60 bay enclosures can be bought in Europe?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "105oplm", "is_robot_indexable": true, "report_reasons": null, "author": "Redspeed93", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/105oplm/high_density_chasis_in_europe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/105oplm/high_density_chasis_in_europe/", "subreddit_subscribers": 664735, "created_utc": 1673097082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Theres a tumblr page of a musician who uploaded music every once in a while to it but either the page got hacked or the page was deleted. there are quite a few archives of the page on archive.org but for the most part no archives of the audio posted. the audio was old enough to use flash to play. ive tried installing flash emulator and that didnt work. is there anyway to do this ? thanks.", "author_fullname": "t2_8klngbgy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Way to download audio from deleted tumblr page?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106gd15", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673174837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Theres a tumblr page of a musician who uploaded music every once in a while to it but either the page got hacked or the page was deleted. there are quite a few archives of the page on archive.org but for the most part no archives of the audio posted. the audio was old enough to use flash to play. ive tried installing flash emulator and that didnt work. is there anyway to do this ? thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106gd15", "is_robot_indexable": true, "report_reasons": null, "author": "z-gyke-1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106gd15/way_to_download_audio_from_deleted_tumblr_page/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106gd15/way_to_download_audio_from_deleted_tumblr_page/", "subreddit_subscribers": 664735, "created_utc": 1673174837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently at a point in my journey where I'm looking to start planning out my storage config for my first homelab. This isn't a post asking EXACTLY what I should do but moreso trying to get a feel for what first timers don't really think about. What are common regrets you've heard or experienced when setting up the storage for homelab for the first time? I'm still deciding how much extra storage I plan on having(I know I need more than 10TB). Still deciding on a file system(I'm definitely leaning towards ZFS). Deciding whether I want to virtualize my NAS or not. For context, I plan on using this lab as part \"production\"(things like Plex, pihole, Home Assistant and a steam cache) and part testing to get more familiar with all kinds of things, from setting up a Windows domain controller to familiarizing myself with kubernetes.", "author_fullname": "t2_7sqku996", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for insight from those who've been there.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106exgz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673169614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently at a point in my journey where I&amp;#39;m looking to start planning out my storage config for my first homelab. This isn&amp;#39;t a post asking EXACTLY what I should do but moreso trying to get a feel for what first timers don&amp;#39;t really think about. What are common regrets you&amp;#39;ve heard or experienced when setting up the storage for homelab for the first time? I&amp;#39;m still deciding how much extra storage I plan on having(I know I need more than 10TB). Still deciding on a file system(I&amp;#39;m definitely leaning towards ZFS). Deciding whether I want to virtualize my NAS or not. For context, I plan on using this lab as part &amp;quot;production&amp;quot;(things like Plex, pihole, Home Assistant and a steam cache) and part testing to get more familiar with all kinds of things, from setting up a Windows domain controller to familiarizing myself with kubernetes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106exgz", "is_robot_indexable": true, "report_reasons": null, "author": "RiggedyWreckt", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106exgz/looking_for_insight_from_those_whove_been_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106exgz/looking_for_insight_from_those_whove_been_there/", "subreddit_subscribers": 664735, "created_utc": 1673169614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there,\n\nIs there an app out there that would allow me to download all the HTTP Live Stream vids within a certain depth on a website ( on which I have to be logged in to have access to the vids ) ?\n\nI already use hls downloader plugin but you have to access manually each page.\n\nthanks !", "author_fullname": "t2_50eeh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HLS Crawling a website ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106djsq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673164680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;Is there an app out there that would allow me to download all the HTTP Live Stream vids within a certain depth on a website ( on which I have to be logged in to have access to the vids ) ?&lt;/p&gt;\n\n&lt;p&gt;I already use hls downloader plugin but you have to access manually each page.&lt;/p&gt;\n\n&lt;p&gt;thanks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106djsq", "is_robot_indexable": true, "report_reasons": null, "author": "krpt", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106djsq/hls_crawling_a_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106djsq/hls_crawling_a_website/", "subreddit_subscribers": 664735, "created_utc": 1673164680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I screwed a script and  deleted 4000+ files, about 1800 of which were MP4/MKV of various sizes.\n\nUsing the admin console I requested a data restore which produces a message like, the data will be restored shortly. \n\nIt's getting on 2 days with no data restored. Does anyone have any experience with this, and how long it might take?\n\n(The deleted files are not in the trash).", "author_fullname": "t2_e5fbi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time to recover Gsuite files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1065slv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673141040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I screwed a script and  deleted 4000+ files, about 1800 of which were MP4/MKV of various sizes.&lt;/p&gt;\n\n&lt;p&gt;Using the admin console I requested a data restore which produces a message like, the data will be restored shortly. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s getting on 2 days with no data restored. Does anyone have any experience with this, and how long it might take?&lt;/p&gt;\n\n&lt;p&gt;(The deleted files are not in the trash).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1065slv", "is_robot_indexable": true, "report_reasons": null, "author": "cn8fly", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1065slv/time_to_recover_gsuite_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1065slv/time_to_recover_gsuite_files/", "subreddit_subscribers": 664735, "created_utc": 1673141040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, everyone!\n\nI have created a bootable Strelec USB. On the same USB stick, I have a second partition just for ISOs. When I boot into the Strelec PE and open Macrium and try to select an ISO, none of the ISOs are visible. I downloaded two XP ISOs and made a Win10 ISO with the media creation tool. \n\nWhat am I doing wrong?\n\nThank you!", "author_fullname": "t2_rvykcumf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Macrium wkthin WinPE doesn't see ISOs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1064xy6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673138765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, everyone!&lt;/p&gt;\n\n&lt;p&gt;I have created a bootable Strelec USB. On the same USB stick, I have a second partition just for ISOs. When I boot into the Strelec PE and open Macrium and try to select an ISO, none of the ISOs are visible. I downloaded two XP ISOs and made a Win10 ISO with the media creation tool. &lt;/p&gt;\n\n&lt;p&gt;What am I doing wrong?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1064xy6", "is_robot_indexable": true, "report_reasons": null, "author": "UnRealSmoky", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1064xy6/macrium_wkthin_winpe_doesnt_see_isos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1064xy6/macrium_wkthin_winpe_doesnt_see_isos/", "subreddit_subscribers": 664735, "created_utc": 1673138765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "$60 for a 2TB seagate. im getting roughly 60mb/s and im backing up 1.6TB itll be a while. does drive speed dictate reliability any? it's slow as shit but the cheapest option for a backup.", "author_fullname": "t2_2o8t3n2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "bought a cheap drive for a backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105uk8j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673112783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;$60 for a 2TB seagate. im getting roughly 60mb/s and im backing up 1.6TB itll be a while. does drive speed dictate reliability any? it&amp;#39;s slow as shit but the cheapest option for a backup.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "105uk8j", "is_robot_indexable": true, "report_reasons": null, "author": "QualitySound96", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/105uk8j/bought_a_cheap_drive_for_a_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/105uk8j/bought_a_cheap_drive_for_a_backup/", "subreddit_subscribers": 664735, "created_utc": 1673112783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I purchased a drive from them almost a month ago and all I've gotten is the confirmation email.\n\nI've purchased from these guys plenty of times. They even sent me the wrong drive and fixed it right away.\n\ncontacting is leading nowhere since nobody is responding to emails or the chat bot.\n\nwas hoping someone knew if something was up before i just issued a chargeback and went somewhere else.", "author_fullname": "t2_w59k4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone know what going on with bitdeals?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_105s4n5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673106606.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I purchased a drive from them almost a month ago and all I&amp;#39;ve gotten is the confirmation email.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve purchased from these guys plenty of times. They even sent me the wrong drive and fixed it right away.&lt;/p&gt;\n\n&lt;p&gt;contacting is leading nowhere since nobody is responding to emails or the chat bot.&lt;/p&gt;\n\n&lt;p&gt;was hoping someone knew if something was up before i just issued a chargeback and went somewhere else.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "107TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "105s4n5", "is_robot_indexable": true, "report_reasons": null, "author": "Hairless_Human", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/105s4n5/anyone_know_what_going_on_with_bitdeals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/105s4n5/anyone_know_what_going_on_with_bitdeals/", "subreddit_subscribers": 664735, "created_utc": 1673106606.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Apparently none of the usual scrapers I typically use seem to be able to scrape from sites running on this beta, I've tried looking. If anyone could provide a way to scrape these sites it would be much appreciated.\n\nThe sites are: [https://joi.booru.org](https://joi.booru.org) and [https://captions.booru.org/](https://captions.booru.org/)", "author_fullname": "t2_7uanw9ay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a solution scraping booru sites running on", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1068wnc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673149799.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apparently none of the usual scrapers I typically use seem to be able to scrape from sites running on this beta, I&amp;#39;ve tried looking. If anyone could provide a way to scrape these sites it would be much appreciated.&lt;/p&gt;\n\n&lt;p&gt;The sites are: &lt;a href=\"https://joi.booru.org\"&gt;https://joi.booru.org&lt;/a&gt; and &lt;a href=\"https://captions.booru.org/\"&gt;https://captions.booru.org/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1068wnc", "is_robot_indexable": true, "report_reasons": null, "author": "TheUncourage", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1068wnc/looking_for_a_solution_scraping_booru_sites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1068wnc/looking_for_a_solution_scraping_booru_sites/", "subreddit_subscribers": 664735, "created_utc": 1673149799.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}