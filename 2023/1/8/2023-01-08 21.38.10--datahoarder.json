{"kind": "Listing", "data": {"after": "t3_106hqqq", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a a 1TB SanDisk 520s and I am looking for a solution to expand my storage capacity. Obviously I could just get another 1TB, but then I would then have multiple drives I need to eject when I move my MacBook about.\n\nLooking at YouTube videos it does look like it has an M2 NVME drive installed, so this got me thinking. For the same price of another 1TB sandisk I can get a 2tb M2 drive.\n\nIs there some enclose preferably unpowered (or type c) that can hold multiple M2s? I suppose two drives would be ok, but would prefer four. I have looked on Amazon and they either seem to be exposed, too big and also need power.\n\nRaid capability isn't necessary. Portability is fairly important though.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/j2bssghsbvaa1.jpg?width=1500&amp;format=pjpg&amp;auto=webp&amp;s=41d07a33f39622a5df81dac42d95e4fed58059a9", "author_fullname": "t2_1gm1x1ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a solution for a multi M2 drive storage with a single cable. Have a 1TB SanDisk and want to expand.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 136, "top_awarded_type": null, "hide_score": false, "media_metadata": {"j2bssghsbvaa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 105, "x": 108, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=77213af351033a331e12698f8fd978e66d31f5b6"}, {"y": 210, "x": 216, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8883baee2817b6e67db79fedb328247d0b8d9bab"}, {"y": 312, "x": 320, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ff67dcc4cc07c268e4297d71b4100401dce0ac5"}, {"y": 624, "x": 640, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1ed99c8890df4ea17367f49af5feac416f0526e7"}, {"y": 936, "x": 960, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4a24605ba408fd541bb62c311a5b6cbe1669d182"}, {"y": 1053, "x": 1080, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8580e7afd77f36be18428e754b33d33b762f5f2d"}], "s": {"y": 1463, "x": 1500, "u": "https://preview.redd.it/j2bssghsbvaa1.jpg?width=1500&amp;format=pjpg&amp;auto=webp&amp;s=41d07a33f39622a5df81dac42d95e4fed58059a9"}, "id": "j2bssghsbvaa1"}}, "name": "t3_106fh0r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/38krtbUG4CznLJGNjfsGTJkU05UacwwdYt4J-2n4Rss.jpg", "edited": 1673204933.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673171633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a a 1TB SanDisk 520s and I am looking for a solution to expand my storage capacity. Obviously I could just get another 1TB, but then I would then have multiple drives I need to eject when I move my MacBook about.&lt;/p&gt;\n\n&lt;p&gt;Looking at YouTube videos it does look like it has an M2 NVME drive installed, so this got me thinking. For the same price of another 1TB sandisk I can get a 2tb M2 drive.&lt;/p&gt;\n\n&lt;p&gt;Is there some enclose preferably unpowered (or type c) that can hold multiple M2s? I suppose two drives would be ok, but would prefer four. I have looked on Amazon and they either seem to be exposed, too big and also need power.&lt;/p&gt;\n\n&lt;p&gt;Raid capability isn&amp;#39;t necessary. Portability is fairly important though.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/j2bssghsbvaa1.jpg?width=1500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=41d07a33f39622a5df81dac42d95e4fed58059a9\"&gt;https://preview.redd.it/j2bssghsbvaa1.jpg?width=1500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=41d07a33f39622a5df81dac42d95e4fed58059a9&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106fh0r", "is_robot_indexable": true, "report_reasons": null, "author": "matmah", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106fh0r/looking_for_a_solution_for_a_multi_m2_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106fh0r/looking_for_a_solution_for_a_multi_m2_drive/", "subreddit_subscribers": 664795, "created_utc": 1673171633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Constructive feedback very much appreciated.\n\nHere is the guide:\n\nhttps://medium.com/@goughgough/the-best-way-for-microsoft-teams-users-without-administrator-rights-to-save-export-print-copy-8212aa9e5f11\n\n__TL;DR:__\nTo export Teams chat messages without Microsoft Teams admin rights, download Gildas Lormeau's (GL) browser extension at https://github.com/gildas-lormeau/single-file-export-chat.\n\n\n__Assumptions:__\nYou are not very tech savvy.\n\nYou can log into Microsoft Teams in a brower at https://teams.microsoft.com/\n\nIn Teams, you do not have admin rights for a group chat. Nevertheless, you still need to export the messages from that specific group chat.\n\nYou want to use non commercial software and do the exporting for free.\n\nYou want to export the Chat section's messages (in Microsoft Teams left column). NOT the Team section messages (in Microsoft Teams left column).\n\nYou wish to export Teams messages in their entirety, including any body text that contains clickable links.\n\nYou want to export Teams messages to a searchable final output rather than an image file.\n\nYou do not want to waste time manually copying and pasting individual Teams messages, which is one of the techniques in quite a few of the online guides. This manual copying and pasting makes sense if you only have a few Teams messages to export.\n\nYou do not want to use the GoFullPage browser extension, even though it is not as effective as GL's solutions because it lets you export Teams messages as images (e.g. non serchable PDF file).\nBefore I came across GL's methods, GoFullPage browser extension was the best method I tried. Unfortunately, the final roduct is not searchable due to its image format.", "author_fullname": "t2_ipdh111g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just published my guide for Microsoft Teams users (without administrator rights) to save, export, print, copy, archive, back up, or migrate Teams conversation threads, messages, chat history. Hope you like it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106fwt4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673186189.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673173191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Constructive feedback very much appreciated.&lt;/p&gt;\n\n&lt;p&gt;Here is the guide:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@goughgough/the-best-way-for-microsoft-teams-users-without-administrator-rights-to-save-export-print-copy-8212aa9e5f11\"&gt;https://medium.com/@goughgough/the-best-way-for-microsoft-teams-users-without-administrator-rights-to-save-export-print-copy-8212aa9e5f11&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;\nTo export Teams chat messages without Microsoft Teams admin rights, download Gildas Lormeau&amp;#39;s (GL) browser extension at &lt;a href=\"https://github.com/gildas-lormeau/single-file-export-chat\"&gt;https://github.com/gildas-lormeau/single-file-export-chat&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;\nYou are not very tech savvy.&lt;/p&gt;\n\n&lt;p&gt;You can log into Microsoft Teams in a brower at &lt;a href=\"https://teams.microsoft.com/\"&gt;https://teams.microsoft.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In Teams, you do not have admin rights for a group chat. Nevertheless, you still need to export the messages from that specific group chat.&lt;/p&gt;\n\n&lt;p&gt;You want to use non commercial software and do the exporting for free.&lt;/p&gt;\n\n&lt;p&gt;You want to export the Chat section&amp;#39;s messages (in Microsoft Teams left column). NOT the Team section messages (in Microsoft Teams left column).&lt;/p&gt;\n\n&lt;p&gt;You wish to export Teams messages in their entirety, including any body text that contains clickable links.&lt;/p&gt;\n\n&lt;p&gt;You want to export Teams messages to a searchable final output rather than an image file.&lt;/p&gt;\n\n&lt;p&gt;You do not want to waste time manually copying and pasting individual Teams messages, which is one of the techniques in quite a few of the online guides. This manual copying and pasting makes sense if you only have a few Teams messages to export.&lt;/p&gt;\n\n&lt;p&gt;You do not want to use the GoFullPage browser extension, even though it is not as effective as GL&amp;#39;s solutions because it lets you export Teams messages as images (e.g. non serchable PDF file).\nBefore I came across GL&amp;#39;s methods, GoFullPage browser extension was the best method I tried. Unfortunately, the final roduct is not searchable due to its image format.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106fwt4", "is_robot_indexable": true, "report_reasons": null, "author": "cashpayer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106fwt4/just_published_my_guide_for_microsoft_teams_users/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106fwt4/just_published_my_guide_for_microsoft_teams_users/", "subreddit_subscribers": 664795, "created_utc": 1673173191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need help finding duplicates of my JAV collection, the problem is that some files have their titles in different format like:\n\na) GVG-001.mp4\n\nb) GVG001.mp4\n\nc) GVG001 - additional movie title\n\nWhat software would you recommend to find dupes? Duplicate files might have different lengths so CRC search will not work. I need some way to analyze the first few letters of a filename keeping in mind that there may or may not be \"-\" sing in there. Can you help me out?\n\nI have total commander but I'm not sure if its search function is as powerful.", "author_fullname": "t2_tmiw5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for finding duplicate files based on a similar filename.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106g20g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673173717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help finding duplicates of my JAV collection, the problem is that some files have their titles in different format like:&lt;/p&gt;\n\n&lt;p&gt;a) GVG-001.mp4&lt;/p&gt;\n\n&lt;p&gt;b) GVG001.mp4&lt;/p&gt;\n\n&lt;p&gt;c) GVG001 - additional movie title&lt;/p&gt;\n\n&lt;p&gt;What software would you recommend to find dupes? Duplicate files might have different lengths so CRC search will not work. I need some way to analyze the first few letters of a filename keeping in mind that there may or may not be &amp;quot;-&amp;quot; sing in there. Can you help me out?&lt;/p&gt;\n\n&lt;p&gt;I have total commander but I&amp;#39;m not sure if its search function is as powerful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106g20g", "is_robot_indexable": true, "report_reasons": null, "author": "wooshaq", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106g20g/software_for_finding_duplicate_files_based_on_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106g20g/software_for_finding_duplicate_files_based_on_a/", "subreddit_subscribers": 664795, "created_utc": 1673173717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have quite a collection of MP3 audio books that I have collected over the decades.  Some are divided by chapter.  Some are even divided by chapter in different CD folders.  I am looking for a good Windows app that will take a folder of MP3s (with even subfolders) and convert them into a single M4B.  OpenAudible lets me import the MP3s, but I can't figure out how to combine and convert them using that, so I'm looking for suggestions.", "author_fullname": "t2_6qsou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Windows app for combining multiple MP3s into single M4B", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106m2yz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673192233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have quite a collection of MP3 audio books that I have collected over the decades.  Some are divided by chapter.  Some are even divided by chapter in different CD folders.  I am looking for a good Windows app that will take a folder of MP3s (with even subfolders) and convert them into a single M4B.  OpenAudible lets me import the MP3s, but I can&amp;#39;t figure out how to combine and convert them using that, so I&amp;#39;m looking for suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106m2yz", "is_robot_indexable": true, "report_reasons": null, "author": "djeaton", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106m2yz/best_windows_app_for_combining_multiple_mp3s_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106m2yz/best_windows_app_for_combining_multiple_mp3s_into/", "subreddit_subscribers": 664795, "created_utc": 1673192233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Entire question fit in the title of this post.", "author_fullname": "t2_8hlee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In a 2-drive raid 1 array, is it a viable backup strategy to pull one drive for cold storage and then replace it (rebuilding the array)? Could one then just rotate the third drive through periodically in this manner when a new backup is desired?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1067x0r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673146956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Entire question fit in the title of this post.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1067x0r", "is_robot_indexable": true, "report_reasons": null, "author": "nouvie", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1067x0r/in_a_2drive_raid_1_array_is_it_a_viable_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1067x0r/in_a_2drive_raid_1_array_is_it_a_viable_backup/", "subreddit_subscribers": 664795, "created_utc": 1673146956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there fellow DataHoarders! I searched both Google and this sub for an answer to this quandary, but couldn't find anything. Maybe what I want to do is impossible and I should just accept it. \n\nI like to game on one monitor and halfheartedly watch videos on the other in the evenings, and I wanted to see if there was a way I could consolidate all 389 GB of my locally stored video files into a single playlist that can play them randomly.\n\nI made the attempt with VLC, but it chokes on it about halfway through adding the files to the playlist. I don't know if there is a numerical limit, or if it goes by file size, not sure. Is there another application, extension, or maybe something out there in GitHub world that you know of that can accomplish this?\n\nI really appreciate any help, and thank you in advance!", "author_fullname": "t2_iwgp7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating a Huge Adult Swim Playlist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10607ro", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673126915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there fellow DataHoarders! I searched both Google and this sub for an answer to this quandary, but couldn&amp;#39;t find anything. Maybe what I want to do is impossible and I should just accept it. &lt;/p&gt;\n\n&lt;p&gt;I like to game on one monitor and halfheartedly watch videos on the other in the evenings, and I wanted to see if there was a way I could consolidate all 389 GB of my locally stored video files into a single playlist that can play them randomly.&lt;/p&gt;\n\n&lt;p&gt;I made the attempt with VLC, but it chokes on it about halfway through adding the files to the playlist. I don&amp;#39;t know if there is a numerical limit, or if it goes by file size, not sure. Is there another application, extension, or maybe something out there in GitHub world that you know of that can accomplish this?&lt;/p&gt;\n\n&lt;p&gt;I really appreciate any help, and thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10607ro", "is_robot_indexable": true, "report_reasons": null, "author": "TropicalDruid", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10607ro/creating_a_huge_adult_swim_playlist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10607ro/creating_a_huge_adult_swim_playlist/", "subreddit_subscribers": 664795, "created_utc": 1673126915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I seem to have Rclone setup properly, but Im a bit confused on how I would clone an entire folder of my external drive(connected to my mac) into a specific folder in my Google drive account. My Rclone account name will be NAME for the purpose of this post\n\nThe folder that I'm looking to copy is in this chain \"4tb&gt;offloaded footage&gt;categorized&gt;DJI&gt;2021\n\nand I'm looking to copy this to my google drive \"My Drive&gt;Categorized footage&gt;DJI&gt;2021\n\nHere is the terminal command that I'm using\n\n    rclone -v sync /Volumes/4TB\\ Offloaded\\ Footage/Categorized/DJI/2021 NAME:\n\nThen what do I put after this to get the contents copied into \n\n&gt;My Drive&gt;Categorized footage&gt;DJI&gt;2021", "author_fullname": "t2_y7xmw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using RClone to copy folder to folder?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106m302", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673192235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I seem to have Rclone setup properly, but Im a bit confused on how I would clone an entire folder of my external drive(connected to my mac) into a specific folder in my Google drive account. My Rclone account name will be NAME for the purpose of this post&lt;/p&gt;\n\n&lt;p&gt;The folder that I&amp;#39;m looking to copy is in this chain &amp;quot;4tb&amp;gt;offloaded footage&amp;gt;categorized&amp;gt;DJI&amp;gt;2021&lt;/p&gt;\n\n&lt;p&gt;and I&amp;#39;m looking to copy this to my google drive &amp;quot;My Drive&amp;gt;Categorized footage&amp;gt;DJI&amp;gt;2021&lt;/p&gt;\n\n&lt;p&gt;Here is the terminal command that I&amp;#39;m using&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;rclone -v sync /Volumes/4TB\\ Offloaded\\ Footage/Categorized/DJI/2021 NAME:\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then what do I put after this to get the contents copied into &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;My Drive&amp;gt;Categorized footage&amp;gt;DJI&amp;gt;2021&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106m302", "is_robot_indexable": true, "report_reasons": null, "author": "AllAboutGadgets", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106m302/using_rclone_to_copy_folder_to_folder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106m302/using_rclone_to_copy_folder_to_folder/", "subreddit_subscribers": 664795, "created_utc": 1673192235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi folks,\n\nNot sure if this is the right place to post this question, but here goes.\n\nI have boxes and boxes of photography slides that I'd like to digitise. Currently, my solution is to buy something like [https://www.amazon.co.uk/dp/B0074H6NTO](https://www.amazon.co.uk/dp/B0074H6NTO?tag=georiot-trd-21&amp;th=1&amp;ascsubtag=dcw-gb-8778368215956837000-21&amp;geniuslink=true), but that involves me having to fill the trays with 4 slides at a time. Obviously not totally feasible with thousands of slides.\n\nDoes anyone have any experience with a project like this? Or ideas on how to proceed?\n\nCheers.", "author_fullname": "t2_3ued9syv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digitising thousands of 35mm photo slides", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106j8ia", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673184407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;Not sure if this is the right place to post this question, but here goes.&lt;/p&gt;\n\n&lt;p&gt;I have boxes and boxes of photography slides that I&amp;#39;d like to digitise. Currently, my solution is to buy something like &lt;a href=\"https://www.amazon.co.uk/dp/B0074H6NTO?tag=georiot-trd-21&amp;amp;th=1&amp;amp;ascsubtag=dcw-gb-8778368215956837000-21&amp;amp;geniuslink=true\"&gt;https://www.amazon.co.uk/dp/B0074H6NTO&lt;/a&gt;, but that involves me having to fill the trays with 4 slides at a time. Obviously not totally feasible with thousands of slides.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience with a project like this? Or ideas on how to proceed?&lt;/p&gt;\n\n&lt;p&gt;Cheers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106j8ia", "is_robot_indexable": true, "report_reasons": null, "author": "thisismyfirsttime123", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106j8ia/digitising_thousands_of_35mm_photo_slides/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106j8ia/digitising_thousands_of_35mm_photo_slides/", "subreddit_subscribers": 664795, "created_utc": 1673184407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm interested in setting up a CCTV system, 1 security camera only.\n\nWhere I'm at in the middle of nowhere (Australia) there isn't fixed line, and there is barely any data signal with most carriers.\n\nWhat I'd like is to have the security camera recording to a local system, let's say a simple computer setup with some HDDs that constantly records with a set retention.\n\nBut I'd also want to be able to remotely view what's happening when I am not at the location, and being limited with internet options (cellular plans only) the cost of 24/7 streaming online would not be viable. So, I thought about a system that: records locally, then every 1 hour (more or less) uploads 1 photo to a cloud service, let's say back blaze (from a portable wifi router connected via SIM). That way, I can look to check to see if there is anything gone occasionally without having to livestream which would use a huge amount of data, and if something did happen, I can go check the local storage and see what exactly happened.\n\nIdeally, the only monthly costs would be:\n\nSIM data plan\n\nBackblaze\n\nIf anyone has some knowledge on what cameras are the best option or how I could achieve this, if i'm on the right track etc please feel free to share!", "author_fullname": "t2_vgleq1ng", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Outback CCTV system with limited internet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106elia", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673168431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m interested in setting up a CCTV system, 1 security camera only.&lt;/p&gt;\n\n&lt;p&gt;Where I&amp;#39;m at in the middle of nowhere (Australia) there isn&amp;#39;t fixed line, and there is barely any data signal with most carriers.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;d like is to have the security camera recording to a local system, let&amp;#39;s say a simple computer setup with some HDDs that constantly records with a set retention.&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;d also want to be able to remotely view what&amp;#39;s happening when I am not at the location, and being limited with internet options (cellular plans only) the cost of 24/7 streaming online would not be viable. So, I thought about a system that: records locally, then every 1 hour (more or less) uploads 1 photo to a cloud service, let&amp;#39;s say back blaze (from a portable wifi router connected via SIM). That way, I can look to check to see if there is anything gone occasionally without having to livestream which would use a huge amount of data, and if something did happen, I can go check the local storage and see what exactly happened.&lt;/p&gt;\n\n&lt;p&gt;Ideally, the only monthly costs would be:&lt;/p&gt;\n\n&lt;p&gt;SIM data plan&lt;/p&gt;\n\n&lt;p&gt;Backblaze&lt;/p&gt;\n\n&lt;p&gt;If anyone has some knowledge on what cameras are the best option or how I could achieve this, if i&amp;#39;m on the right track etc please feel free to share!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106elia", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Wing9364", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106elia/outback_cctv_system_with_limited_internet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106elia/outback_cctv_system_with_limited_internet/", "subreddit_subscribers": 664795, "created_utc": 1673168431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't run raid or NAS, at least not yet, just a simple DAS(?) setup with two drives (19tb) and a single backup drive (13tb). In the future I plan to get more drives and may top out at a total of 5-6 drives.\n\nWhat's the ideal solution for this? Get a large PC case like the Meshify 2 or a smaller case and get like a 5-8 bay enclosure?", "author_fullname": "t2_hrdo3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Large PC case vs SFF with external HDD enclosure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1067u94", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673146768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t run raid or NAS, at least not yet, just a simple DAS(?) setup with two drives (19tb) and a single backup drive (13tb). In the future I plan to get more drives and may top out at a total of 5-6 drives.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the ideal solution for this? Get a large PC case like the Meshify 2 or a smaller case and get like a 5-8 bay enclosure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1067u94", "is_robot_indexable": true, "report_reasons": null, "author": "c9898", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1067u94/large_pc_case_vs_sff_with_external_hdd_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1067u94/large_pc_case_vs_sff_with_external_hdd_enclosure/", "subreddit_subscribers": 664795, "created_utc": 1673146768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, im quite newbie to datahoarding, so basically i am mass downloading bunch of image albums for my little archive but some of these images have useful comments that would also be handy to have in txt file for further reference. \n\nI learned to use JDownloader2 for image batch downloading, I was wondering if there is something similar that I could use for comments/text on webpage?", "author_fullname": "t2_ur3sbbk7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there some easy way to extract comments from website into txt/docx files or such?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1061efc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673129851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, im quite newbie to datahoarding, so basically i am mass downloading bunch of image albums for my little archive but some of these images have useful comments that would also be handy to have in txt file for further reference. &lt;/p&gt;\n\n&lt;p&gt;I learned to use JDownloader2 for image batch downloading, I was wondering if there is something similar that I could use for comments/text on webpage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1061efc", "is_robot_indexable": true, "report_reasons": null, "author": "Kuznetsov063", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1061efc/is_there_some_easy_way_to_extract_comments_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1061efc/is_there_some_easy_way_to_extract_comments_from/", "subreddit_subscribers": 664795, "created_utc": 1673129851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\nThe HDDs will be part of my self built NAS, which is housed in an old Fractal Design Define R6 - a case that comes with some amount of soundproofing. Unfortunately, in my apartment I don't have any space where noise absolutely does not matter. For now, it will be in the working room, where I and the gf spend significant amounts of time trying to be productive. Because of this, I want to buy HDDs that are the most likely to not annoy us too much. Read/Write performance is a secondary concern, so long as they make full use of my gigabit ethernet connection.\n\nI've done a bunch of research on this; both in this sub and beyond, and unfortunately there is no drive for which I don't find three different people making five contradicting claims about their noise levels.\n\nFor instance, I've heard a bunch of good things about the Ultrastar DC HC550 drives; but other sources say they produce a thudding noise every five seconds or so, which may or may not be quite annoying.\n\nI also recently got a good deal on two Toshiba MG08ACA16TE drives which are sitting here unused so far, as I'm considering returning them. I've heard people claim they're rather quiet or really loud.\n\nIt would be helpful to hear from people who have tried different HDDs from different companies and thus can give some sort of comparison. I am also totally open to other models or to shucking, if that's still a thing.\n\nOS Wise I am likely going with True NAS Core for the NAS, if this has any bearing on the question.", "author_fullname": "t2_pvd4p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quiet HDDs for NAS in working room?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_106tcb2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673209736.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;The HDDs will be part of my self built NAS, which is housed in an old Fractal Design Define R6 - a case that comes with some amount of soundproofing. Unfortunately, in my apartment I don&amp;#39;t have any space where noise absolutely does not matter. For now, it will be in the working room, where I and the gf spend significant amounts of time trying to be productive. Because of this, I want to buy HDDs that are the most likely to not annoy us too much. Read/Write performance is a secondary concern, so long as they make full use of my gigabit ethernet connection.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done a bunch of research on this; both in this sub and beyond, and unfortunately there is no drive for which I don&amp;#39;t find three different people making five contradicting claims about their noise levels.&lt;/p&gt;\n\n&lt;p&gt;For instance, I&amp;#39;ve heard a bunch of good things about the Ultrastar DC HC550 drives; but other sources say they produce a thudding noise every five seconds or so, which may or may not be quite annoying.&lt;/p&gt;\n\n&lt;p&gt;I also recently got a good deal on two Toshiba MG08ACA16TE drives which are sitting here unused so far, as I&amp;#39;m considering returning them. I&amp;#39;ve heard people claim they&amp;#39;re rather quiet or really loud.&lt;/p&gt;\n\n&lt;p&gt;It would be helpful to hear from people who have tried different HDDs from different companies and thus can give some sort of comparison. I am also totally open to other models or to shucking, if that&amp;#39;s still a thing.&lt;/p&gt;\n\n&lt;p&gt;OS Wise I am likely going with True NAS Core for the NAS, if this has any bearing on the question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106tcb2", "is_robot_indexable": true, "report_reasons": null, "author": "SWHH", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106tcb2/quiet_hdds_for_nas_in_working_room/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106tcb2/quiet_hdds_for_nas_in_working_room/", "subreddit_subscribers": 664795, "created_utc": 1673209736.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI am having issues with my freshly built NAS using TRUENAS Scale. I had an Intel core i7 12700K cpu from a previous build, so I chose to build around that. The other key components are as follows:\n\n1. MSI MAG B660M MORTAR WIFI DDR4\n2. Corsair Vengeance LPX 32GB (2 X 16GB) DDR4 3600\n\nI successfully installed the iso using a usb stick. I\u2019m at the stage where I should be able to access the web GUI. However, the link/ip address is not there and instead reads \u201c**The web interface could not be accessed. Please check network configuration**\u201d.\n\nHere are the other options:\n1. Configure network interface\n2. Configure network settings\n3. Configure static routes\n4. Change local administrator password\n5. Reset configuration to defaults\n6. Open TRUENAS CLI Shell\n7. Open Linux Shell\n8. Reboot\n9. Shutdown\n\nFrankly, I am not very technologically inclined and feel I\u2019ve made a mistake. I\u2019m not sure what I can do. I\u2019ve attempted looking up solutions and they tend to say it\u2019s a problem with the built in Wi-Fi (2.5G LAN and Intel Wi-Fi 6E Solution). I\u2019m in around 5 hours of research and all I\u2019m seeing is possibly getting a Intel NIC. That would be fine, but my home does not have Ethernet. \n\nThe only thing I can really think of is:\n\nA) Update the motherboard bios (not sure how to do this given it\u2019s on a NAS os)\n\nB) Buy a extremely expensive server board with w680 chipset. This would mean spending about 2x the amount of money I already have.\n\nC) Scrap it and just buy a synology which is still a net negative but at least I know it\u2019ll work. \n\nI wanted to be able to mess around with Linux and it\u2019s capabilities but this is beyond frustrating and frankly a bit demoralizing being my first experience with it.", "author_fullname": "t2_rwbgmvw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TRUENAS scale error: The web interface could not be accessed. New to NAS and could use some advice.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_106t4gb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673209207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I am having issues with my freshly built NAS using TRUENAS Scale. I had an Intel core i7 12700K cpu from a previous build, so I chose to build around that. The other key components are as follows:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;MSI MAG B660M MORTAR WIFI DDR4&lt;/li&gt;\n&lt;li&gt;Corsair Vengeance LPX 32GB (2 X 16GB) DDR4 3600&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I successfully installed the iso using a usb stick. I\u2019m at the stage where I should be able to access the web GUI. However, the link/ip address is not there and instead reads \u201c&lt;strong&gt;The web interface could not be accessed. Please check network configuration&lt;/strong&gt;\u201d.&lt;/p&gt;\n\n&lt;p&gt;Here are the other options:\n1. Configure network interface\n2. Configure network settings\n3. Configure static routes\n4. Change local administrator password\n5. Reset configuration to defaults\n6. Open TRUENAS CLI Shell\n7. Open Linux Shell\n8. Reboot\n9. Shutdown&lt;/p&gt;\n\n&lt;p&gt;Frankly, I am not very technologically inclined and feel I\u2019ve made a mistake. I\u2019m not sure what I can do. I\u2019ve attempted looking up solutions and they tend to say it\u2019s a problem with the built in Wi-Fi (2.5G LAN and Intel Wi-Fi 6E Solution). I\u2019m in around 5 hours of research and all I\u2019m seeing is possibly getting a Intel NIC. That would be fine, but my home does not have Ethernet. &lt;/p&gt;\n\n&lt;p&gt;The only thing I can really think of is:&lt;/p&gt;\n\n&lt;p&gt;A) Update the motherboard bios (not sure how to do this given it\u2019s on a NAS os)&lt;/p&gt;\n\n&lt;p&gt;B) Buy a extremely expensive server board with w680 chipset. This would mean spending about 2x the amount of money I already have.&lt;/p&gt;\n\n&lt;p&gt;C) Scrap it and just buy a synology which is still a net negative but at least I know it\u2019ll work. &lt;/p&gt;\n\n&lt;p&gt;I wanted to be able to mess around with Linux and it\u2019s capabilities but this is beyond frustrating and frankly a bit demoralizing being my first experience with it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106t4gb", "is_robot_indexable": true, "report_reasons": null, "author": "Karizmology", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106t4gb/truenas_scale_error_the_web_interface_could_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106t4gb/truenas_scale_error_the_web_interface_could_not/", "subreddit_subscribers": 664795, "created_utc": 1673209207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently at a point in my journey where I'm looking to start planning out my storage config for my first homelab. This isn't a post asking EXACTLY what I should do but moreso trying to get a feel for what first timers don't really think about. What are common regrets you've heard or experienced when setting up the storage for homelab for the first time? I'm still deciding how much extra storage I plan on having(I know I need more than 10TB). Still deciding on a file system(I'm definitely leaning towards ZFS). Deciding whether I want to virtualize my NAS or not. For context, I plan on using this lab as part \"production\"(things like Plex, pihole, Home Assistant and a steam cache) and part testing to get more familiar with all kinds of things, from setting up a Windows domain controller to familiarizing myself with kubernetes.", "author_fullname": "t2_7sqku996", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for insight from those who've been there.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106exgz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673169614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently at a point in my journey where I&amp;#39;m looking to start planning out my storage config for my first homelab. This isn&amp;#39;t a post asking EXACTLY what I should do but moreso trying to get a feel for what first timers don&amp;#39;t really think about. What are common regrets you&amp;#39;ve heard or experienced when setting up the storage for homelab for the first time? I&amp;#39;m still deciding how much extra storage I plan on having(I know I need more than 10TB). Still deciding on a file system(I&amp;#39;m definitely leaning towards ZFS). Deciding whether I want to virtualize my NAS or not. For context, I plan on using this lab as part &amp;quot;production&amp;quot;(things like Plex, pihole, Home Assistant and a steam cache) and part testing to get more familiar with all kinds of things, from setting up a Windows domain controller to familiarizing myself with kubernetes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106exgz", "is_robot_indexable": true, "report_reasons": null, "author": "RiggedyWreckt", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106exgz/looking_for_insight_from_those_whove_been_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106exgz/looking_for_insight_from_those_whove_been_there/", "subreddit_subscribers": 664795, "created_utc": 1673169614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for an image host (like imagur) that does not compress the image I upload. 100% lossless. I don't really care about features other than I would need to be able to see it from anywhere so would other people. Thanks.", "author_fullname": "t2_usun3qeu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lossless Image Hosting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106d1dr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673162829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for an image host (like imagur) that does not compress the image I upload. 100% lossless. I don&amp;#39;t really care about features other than I would need to be able to see it from anywhere so would other people. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106d1dr", "is_robot_indexable": true, "report_reasons": null, "author": "PuzzleheadedTennis23", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106d1dr/lossless_image_hosting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106d1dr/lossless_image_hosting/", "subreddit_subscribers": 664795, "created_utc": 1673162829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, everyone!\n\nI have created a bootable Strelec USB. On the same USB stick, I have a second partition just for ISOs. When I boot into the Strelec PE and open Macrium and try to select an ISO, none of the ISOs are visible. I downloaded two XP ISOs and made a Win10 ISO with the media creation tool. \n\nWhat am I doing wrong?\n\nThank you!", "author_fullname": "t2_rvykcumf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Macrium wkthin WinPE doesn't see ISOs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1064xy6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673138765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, everyone!&lt;/p&gt;\n\n&lt;p&gt;I have created a bootable Strelec USB. On the same USB stick, I have a second partition just for ISOs. When I boot into the Strelec PE and open Macrium and try to select an ISO, none of the ISOs are visible. I downloaded two XP ISOs and made a Win10 ISO with the media creation tool. &lt;/p&gt;\n\n&lt;p&gt;What am I doing wrong?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1064xy6", "is_robot_indexable": true, "report_reasons": null, "author": "UnRealSmoky", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1064xy6/macrium_wkthin_winpe_doesnt_see_isos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1064xy6/macrium_wkthin_winpe_doesnt_see_isos/", "subreddit_subscribers": 664795, "created_utc": 1673138765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Maybe I'm incredibly dense, but I can't seem to figure out how to remove the dust filter from the front of the case to give it a wash.\n\nManual says nothing, and my google-fu has failed me.", "author_fullname": "t2_7b4xf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rosewill rsv-l4500u filter replacement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10632g5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673134018.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Maybe I&amp;#39;m incredibly dense, but I can&amp;#39;t seem to figure out how to remove the dust filter from the front of the case to give it a wash.&lt;/p&gt;\n\n&lt;p&gt;Manual says nothing, and my google-fu has failed me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "90TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10632g5", "is_robot_indexable": true, "report_reasons": null, "author": "djtodd242", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10632g5/rosewill_rsvl4500u_filter_replacement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10632g5/rosewill_rsvl4500u_filter_replacement/", "subreddit_subscribers": 664795, "created_utc": 1673134018.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Happy new year!\n\nIs any of you aware of a maximum file size limit on Hetzner storage boxes?\n\nI have tried to upload a  ~100G disk backup to my storage box over the holidays and that failed with some non-descript error. After the fact I found out that I had a local HDD failure that might have caused this...\n\nbefore I commit to uploading again (my ISD outgoing speed is quite low) I thought I'd check with the wisdom in this subreddit...", "author_fullname": "t2_14oq692k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "maximum file size on Hetzner storagebox?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10605yt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673126777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Happy new year!&lt;/p&gt;\n\n&lt;p&gt;Is any of you aware of a maximum file size limit on Hetzner storage boxes?&lt;/p&gt;\n\n&lt;p&gt;I have tried to upload a  ~100G disk backup to my storage box over the holidays and that failed with some non-descript error. After the fact I found out that I had a local HDD failure that might have caused this...&lt;/p&gt;\n\n&lt;p&gt;before I commit to uploading again (my ISD outgoing speed is quite low) I thought I&amp;#39;d check with the wisdom in this subreddit...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10605yt", "is_robot_indexable": true, "report_reasons": null, "author": "biochronox", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10605yt/maximum_file_size_on_hetzner_storagebox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10605yt/maximum_file_size_on_hetzner_storagebox/", "subreddit_subscribers": 664795, "created_utc": 1673126777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What I want is a copy program that runs overnight / in a certain time frame, so I can copy a NAS to a backup when the network is not in use.  It runs from 11pm to 6am &amp; at 6am it stops transferring until that night.  So it doesn't have to be restarted, but pickups up where it left off?\n\nI know I could use robocopy for starters, then schedule a task to kill it in the morning?\n\nAnything fanicier?  This would be using a Windows host with the NAS shares mapped on the Win box.\n\nIs there something I could use on UnRAID that would run on the server?", "author_fullname": "t2_d1ic8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time frame copier", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_106tvqy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1673211209.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673211024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What I want is a copy program that runs overnight / in a certain time frame, so I can copy a NAS to a backup when the network is not in use.  It runs from 11pm to 6am &amp;amp; at 6am it stops transferring until that night.  So it doesn&amp;#39;t have to be restarted, but pickups up where it left off?&lt;/p&gt;\n\n&lt;p&gt;I know I could use robocopy for starters, then schedule a task to kill it in the morning?&lt;/p&gt;\n\n&lt;p&gt;Anything fanicier?  This would be using a Windows host with the NAS shares mapped on the Win box.&lt;/p&gt;\n\n&lt;p&gt;Is there something I could use on UnRAID that would run on the server?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "128TB UnRAID &amp; 72TB Synology", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106tvqy", "is_robot_indexable": true, "report_reasons": null, "author": "hacnstein", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/106tvqy/time_frame_copier/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106tvqy/time_frame_copier/", "subreddit_subscribers": 664795, "created_utc": 1673211024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just had an external hard drive become corrupted and unreadable. I thought this would be a good opportunity to upgrade my setup, but I have some questions.\n\n&amp;#x200B;\n\n1. I was thinking of getting regular hard drives and a hard drive toaster rather than external drives. Is there a consensus as to which setup is better?\n\n&amp;#x200B;\n\n2. I need to transfer data between linux and windows computers. I was using the ExFat file system for this. Is this a good idea or is there a better file system?\n\n&amp;#x200B;\n\n3. I was told recently that drives larger than 2TB are far more susceptible to corruption. Is this true? \n\n&amp;#x200B;\n\n4. What programs would you recommend for data recovery on a hard drive?\n\n&amp;#x200B;\n\nThank you", "author_fullname": "t2_x9tme", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me upgrade my storage setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106ojil", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673198293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just had an external hard drive become corrupted and unreadable. I thought this would be a good opportunity to upgrade my setup, but I have some questions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I was thinking of getting regular hard drives and a hard drive toaster rather than external drives. Is there a consensus as to which setup is better?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I need to transfer data between linux and windows computers. I was using the ExFat file system for this. Is this a good idea or is there a better file system?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I was told recently that drives larger than 2TB are far more susceptible to corruption. Is this true? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What programs would you recommend for data recovery on a hard drive?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106ojil", "is_robot_indexable": true, "report_reasons": null, "author": "archmage24601", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106ojil/help_me_upgrade_my_storage_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106ojil/help_me_upgrade_my_storage_setup/", "subreddit_subscribers": 664795, "created_utc": 1673198293.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Apparently none of the usual scrapers I typically use seem to be able to scrape from sites running on this beta, I've tried looking. If anyone could provide a way to scrape these sites it would be much appreciated.\n\nThe sites are: [https://joi.booru.org](https://joi.booru.org) and [https://captions.booru.org/](https://captions.booru.org/)", "author_fullname": "t2_7uanw9ay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a solution scraping booru sites running on", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1068wnc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673149799.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apparently none of the usual scrapers I typically use seem to be able to scrape from sites running on this beta, I&amp;#39;ve tried looking. If anyone could provide a way to scrape these sites it would be much appreciated.&lt;/p&gt;\n\n&lt;p&gt;The sites are: &lt;a href=\"https://joi.booru.org\"&gt;https://joi.booru.org&lt;/a&gt; and &lt;a href=\"https://captions.booru.org/\"&gt;https://captions.booru.org/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1068wnc", "is_robot_indexable": true, "report_reasons": null, "author": "TheUncourage", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1068wnc/looking_for_a_solution_scraping_booru_sites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1068wnc/looking_for_a_solution_scraping_booru_sites/", "subreddit_subscribers": 664795, "created_utc": 1673149799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I screwed a script and  deleted 4000+ files, about 1800 of which were MP4/MKV of various sizes.\n\nUsing the admin console I requested a data restore which produces a message like, the data will be restored shortly. \n\nIt's getting on 2 days with no data restored. Does anyone have any experience with this, and how long it might take?\n\n(The deleted files are not in the trash).", "author_fullname": "t2_e5fbi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time to recover Gsuite files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1065slv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673141040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I screwed a script and  deleted 4000+ files, about 1800 of which were MP4/MKV of various sizes.&lt;/p&gt;\n\n&lt;p&gt;Using the admin console I requested a data restore which produces a message like, the data will be restored shortly. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s getting on 2 days with no data restored. Does anyone have any experience with this, and how long it might take?&lt;/p&gt;\n\n&lt;p&gt;(The deleted files are not in the trash).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1065slv", "is_robot_indexable": true, "report_reasons": null, "author": "cn8fly", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1065slv/time_to_recover_gsuite_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1065slv/time_to_recover_gsuite_files/", "subreddit_subscribers": 664795, "created_utc": 1673141040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The disks should be fine, but the NAS itself seems to have failed. I'm curious if anyone knows an ideally free way to mount the disks on Linux or Windows?\n\nIt's 5 disks, single parity, so 12GB usable.", "author_fullname": "t2_dj2oi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to read a 15TB Drobo \"BeyondRAID\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106260d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673131796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The disks should be fine, but the NAS itself seems to have failed. I&amp;#39;m curious if anyone knows an ideally free way to mount the disks on Linux or Windows?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s 5 disks, single parity, so 12GB usable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106260d", "is_robot_indexable": true, "report_reasons": null, "author": "Krutonium", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106260d/need_to_read_a_15tb_drobo_beyondraid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106260d/need_to_read_a_15tb_drobo_beyondraid/", "subreddit_subscribers": 664795, "created_utc": 1673131796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I tried recovering a lost video using the Wayback Machine, but I get the error \"Attempts to archive this video failed.\".  Somehow, I'm still able to see slides if I hover over the bar that shows how much of the video has passed. What can I do?\n\nThis is the video:\n\n[https://web.archive.org/web/20200618135626/https://www.youtube.com/watch?v=LMlFwHpqEpU](https://web.archive.org/web/20200618135626/https://www.youtube.com/watch?v=LMlFwHpqEpU)", "author_fullname": "t2_mxw50928", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wayback Machine Error", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106oxb7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1673199208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried recovering a lost video using the Wayback Machine, but I get the error &amp;quot;Attempts to archive this video failed.&amp;quot;.  Somehow, I&amp;#39;m still able to see slides if I hover over the bar that shows how much of the video has passed. What can I do?&lt;/p&gt;\n\n&lt;p&gt;This is the video:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://web.archive.org/web/20200618135626/https://www.youtube.com/watch?v=LMlFwHpqEpU\"&gt;https://web.archive.org/web/20200618135626/https://www.youtube.com/watch?v=LMlFwHpqEpU&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106oxb7", "is_robot_indexable": true, "report_reasons": null, "author": "Adriana_Istrate", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106oxb7/wayback_machine_error/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106oxb7/wayback_machine_error/", "subreddit_subscribers": 664795, "created_utc": 1673199208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've just started using AWS Glacier Deep Archive as a 'last line of defense' backup solution for irreplaceable files but I have difficulties calculating the S3 Entity tag (Etag) or really any type of checksum for multipart uploads.    \n\nOne archive I uploaded to standard S3 and if I calculate the checksum of my local file with [this(https://gist.github.com/rajivnarayan/1a8e5f2b6783701e0b3717dbcfd324ba) tool (chunk size 16 as I use the web console), I get the correct checksum and correct number of chunks.    \n\nHowever, when I upload the file directly to Glacier Deep Archive, the web console always shows me a different hash and a number of chunks that's always a bit lower than on my local machine. For example the web console shows me   \n\n    33[...]8c-122     \n\n...while my local output shows     \n    \n    4a[...]0c-125     \n    \nfor a 2gb archive.      \n\nIs there a way to calculate the Etag for files uploaded directly to AWS Glacier Deep Archive or is the best course of action still to upload them to regular S3 and then either manually or through life cycle rules move them to Deep Archive?", "author_fullname": "t2_v77ico20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Calculate AWS S3 Entity tag (Etag) for files uploaded to Glacier Deep Archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_106hqqq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1673179743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve just started using AWS Glacier Deep Archive as a &amp;#39;last line of defense&amp;#39; backup solution for irreplaceable files but I have difficulties calculating the S3 Entity tag (Etag) or really any type of checksum for multipart uploads.    &lt;/p&gt;\n\n&lt;p&gt;One archive I uploaded to standard S3 and if I calculate the checksum of my local file with [this(&lt;a href=\"https://gist.github.com/rajivnarayan/1a8e5f2b6783701e0b3717dbcfd324ba\"&gt;https://gist.github.com/rajivnarayan/1a8e5f2b6783701e0b3717dbcfd324ba&lt;/a&gt;) tool (chunk size 16 as I use the web console), I get the correct checksum and correct number of chunks.    &lt;/p&gt;\n\n&lt;p&gt;However, when I upload the file directly to Glacier Deep Archive, the web console always shows me a different hash and a number of chunks that&amp;#39;s always a bit lower than on my local machine. For example the web console shows me   &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;33[...]8c-122     \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;...while my local output shows     &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;4a[...]0c-125     \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;for a 2gb archive.      &lt;/p&gt;\n\n&lt;p&gt;Is there a way to calculate the Etag for files uploaded directly to AWS Glacier Deep Archive or is the best course of action still to upload them to regular S3 and then either manually or through life cycle rules move them to Deep Archive?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?auto=webp&amp;s=079a7260ec149880c73263d64811698adb22760a", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5811c5bda5fece1040636a6af8702ba790f0fd4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eee576fd4da7535eb53ceb88dd8b52f073048441", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72872d880460efa723918c000adca0ed259cf775", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3545b9335d763c9da9c16bf7bf9a3f907dbd6f6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d241ace0f1c07088fac3f8469dbad3b05d2d419", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9055f11bdc00beb0b3589e1cae5817d6070d83bc", "width": 1080, "height": 540}], "variants": {}, "id": "OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "106hqqq", "is_robot_indexable": true, "report_reasons": null, "author": "bossderrapper", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/106hqqq/calculate_aws_s3_entity_tag_etag_for_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/106hqqq/calculate_aws_s3_entity_tag_etag_for_files/", "subreddit_subscribers": 664795, "created_utc": 1673179743.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}