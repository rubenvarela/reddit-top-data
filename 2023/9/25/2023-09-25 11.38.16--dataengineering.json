{"kind": "Listing", "data": {"after": null, "dist": 11, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The questions were fairly straightforward, irritatingly more wordy that most leetcode type questions, but there was only one table per question (there were joins but self joins only)\n\nReally nothing crazy about them. 4 questions, one hour. First question I finished in about 5mins as it was more of a fill in the blanks for a stored proc that'd give you the most popular product by year.\n\nNext one was a simple select, to get sum of highest salaries - sum of lowest. (This one actually failed a 1/7 test case but was it was easy I'd come back to it later. I'd find out soon though, I'd never see this question again...\n\nQuestion 3, shit hit the fan. All I needed to do was select names of fraudulent customers based on a condition that would come from time lapsed between transactions. Except, it wasn't a select. It was a procedure. I couldn't incrementally build out the query and check the output as I'd go. I needed to write the query and run it, if it was right it'd say so. If not, it's say test case n/N failed. No context as to where it'd fail. \n\nAgain, these questions were not hard. But next thing my fucking hand starts to shake. I've still got 40mins left, calm down. Start trying to visualize the solution. Panic sets in and nothing worked after that. Ok, next question, will skip this one for now.\n\nSame bloody thing. Procedure, not select. Incredibly wordy question. Easy enough question when youve [seen similar ones](https://datalemur.com/questions/pizzas-topping-cost), sincs it was similar to that pizza one I knew it was a self join and messing around with adding columns (at the time I was thinking I'd need a window function to get the lower cost combo,  but turns out for this question it was simply just needed go get the totals with no duplicates, easy stuff. But I bombed it. Of course I opened it up on my ide afterward and solved it in &lt;10mins, because I like to torture myself like that.\n\nSeems that not having the ability to see the outputs of selects/ctes etc completely through me off. I'm now wondering if I'm the only person that does this and I've gotten into a bad habit of banging the execute button to see the outputs of intermediary results??\n\nKicking myself now as it was so damn easy and a really cool job. Only consolation was thst there would have been another 4/5 rounds of technical interviews after this and probably wouldn't have gotten through ayway", "author_fullname": "t2_1w1o79i7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bombed an easy SQL prescreening assessment - have I picked up bad habits by looking at intermediate query results?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r9swv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695591968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The questions were fairly straightforward, irritatingly more wordy that most leetcode type questions, but there was only one table per question (there were joins but self joins only)&lt;/p&gt;\n\n&lt;p&gt;Really nothing crazy about them. 4 questions, one hour. First question I finished in about 5mins as it was more of a fill in the blanks for a stored proc that&amp;#39;d give you the most popular product by year.&lt;/p&gt;\n\n&lt;p&gt;Next one was a simple select, to get sum of highest salaries - sum of lowest. (This one actually failed a 1/7 test case but was it was easy I&amp;#39;d come back to it later. I&amp;#39;d find out soon though, I&amp;#39;d never see this question again...&lt;/p&gt;\n\n&lt;p&gt;Question 3, shit hit the fan. All I needed to do was select names of fraudulent customers based on a condition that would come from time lapsed between transactions. Except, it wasn&amp;#39;t a select. It was a procedure. I couldn&amp;#39;t incrementally build out the query and check the output as I&amp;#39;d go. I needed to write the query and run it, if it was right it&amp;#39;d say so. If not, it&amp;#39;s say test case n/N failed. No context as to where it&amp;#39;d fail. &lt;/p&gt;\n\n&lt;p&gt;Again, these questions were not hard. But next thing my fucking hand starts to shake. I&amp;#39;ve still got 40mins left, calm down. Start trying to visualize the solution. Panic sets in and nothing worked after that. Ok, next question, will skip this one for now.&lt;/p&gt;\n\n&lt;p&gt;Same bloody thing. Procedure, not select. Incredibly wordy question. Easy enough question when youve &lt;a href=\"https://datalemur.com/questions/pizzas-topping-cost\"&gt;seen similar ones&lt;/a&gt;, sincs it was similar to that pizza one I knew it was a self join and messing around with adding columns (at the time I was thinking I&amp;#39;d need a window function to get the lower cost combo,  but turns out for this question it was simply just needed go get the totals with no duplicates, easy stuff. But I bombed it. Of course I opened it up on my ide afterward and solved it in &amp;lt;10mins, because I like to torture myself like that.&lt;/p&gt;\n\n&lt;p&gt;Seems that not having the ability to see the outputs of selects/ctes etc completely through me off. I&amp;#39;m now wondering if I&amp;#39;m the only person that does this and I&amp;#39;ve gotten into a bad habit of banging the execute button to see the outputs of intermediary results??&lt;/p&gt;\n\n&lt;p&gt;Kicking myself now as it was so damn easy and a really cool job. Only consolation was thst there would have been another 4/5 rounds of technical interviews after this and probably wouldn&amp;#39;t have gotten through ayway&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?auto=webp&amp;s=a6863d19648309b6e3176a9eee52cb8216efce69", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=47a9aa9e8f07567e2b88dac9ce4e3f7a6e23f9d7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e137641eb770388594fd82c4e58db9cc64dd659", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c13cb4b7d66b52a4fe3bdf986a3d141d3c1d2806", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce1afb705cfcb979ee099eab554f759c2f89ef90", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c99bb17a41bdc454fc7204dcfb3e57276e4aacbc", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=61054982f566d0873d25f1b69cfef7e2f3a2c7f4", "width": 1080, "height": 567}], "variants": {}, "id": "wPNS14r1J4_L3TGYUAsqCV8FmgGrMho9XjGoyEnYUAQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16r9swv", "is_robot_indexable": true, "report_reasons": null, "author": "tea_horse", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16r9swv/bombed_an_easy_sql_prescreening_assessment_have_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16r9swv/bombed_an_easy_sql_prescreening_assessment_have_i/", "subreddit_subscribers": 130314, "created_utc": 1695591968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Back in the day when we had to scale an analytics pipeline over a large dataset, Hadoop/spark used to be the go to options and later pyspark became popular.   \n\n\nNow if the dataset is big we have some really interesting tools which doesn't require complex setup like spinning up a hadoop cluster.   \n\n\n* We have tools like [Polars](https://www.pola.rs/) which can stream data if the memory is not enough. \n* We have [Ibis](https://ibis-project.org/) which helps you write generic analytics pipelines and it supports multiple backends and you can either run these pipeline on a query processing engine or a pyspark cluster or even on Pandas or Polars. \n* You can create your own internal datalake using a couple of parquet files and pair it with [Duckdb](https://duckdb.org/) for superfast analytics.  \n\n\nHadoop ecosystem will still be relevant but I believe for Small and Medium Business doing data engineering is getting better and you don't require big complex clusters to solve problems every time you run out of memory. These new tools are so easy to work with that any python programmer can work with them.   \n\n\nI remember how I struggled understanding this book on [MapReduceDesign Patterns](https://www.oreilly.com/library/view/mapreduce-design-patterns/9781449341954/) a couple of years back and thinking that \"*Damnn!! data engineering is going to be hard*\".\n\nRust and python combination has resulted in some amazing python libraries.   \nOne interesting thing I see is people developing different variety of query languages like [EdgeQL](https://www.edgedb.com/docs/edgeql/index) and [Malloy](https://www.malloydata.dev/)\n\nWhat are some new things in data engineering that you are excited about?", "author_fullname": "t2_q27tep12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upcoming Data Engineering Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qwc2j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695557800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Back in the day when we had to scale an analytics pipeline over a large dataset, Hadoop/spark used to be the go to options and later pyspark became popular.   &lt;/p&gt;\n\n&lt;p&gt;Now if the dataset is big we have some really interesting tools which doesn&amp;#39;t require complex setup like spinning up a hadoop cluster.   &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We have tools like &lt;a href=\"https://www.pola.rs/\"&gt;Polars&lt;/a&gt; which can stream data if the memory is not enough. &lt;/li&gt;\n&lt;li&gt;We have &lt;a href=\"https://ibis-project.org/\"&gt;Ibis&lt;/a&gt; which helps you write generic analytics pipelines and it supports multiple backends and you can either run these pipeline on a query processing engine or a pyspark cluster or even on Pandas or Polars. &lt;/li&gt;\n&lt;li&gt;You can create your own internal datalake using a couple of parquet files and pair it with &lt;a href=\"https://duckdb.org/\"&gt;Duckdb&lt;/a&gt; for superfast analytics.&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Hadoop ecosystem will still be relevant but I believe for Small and Medium Business doing data engineering is getting better and you don&amp;#39;t require big complex clusters to solve problems every time you run out of memory. These new tools are so easy to work with that any python programmer can work with them.   &lt;/p&gt;\n\n&lt;p&gt;I remember how I struggled understanding this book on &lt;a href=\"https://www.oreilly.com/library/view/mapreduce-design-patterns/9781449341954/\"&gt;MapReduceDesign Patterns&lt;/a&gt; a couple of years back and thinking that &amp;quot;&lt;em&gt;Damnn!! data engineering is going to be hard&lt;/em&gt;&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Rust and python combination has resulted in some amazing python libraries.&lt;br/&gt;\nOne interesting thing I see is people developing different variety of query languages like &lt;a href=\"https://www.edgedb.com/docs/edgeql/index\"&gt;EdgeQL&lt;/a&gt; and &lt;a href=\"https://www.malloydata.dev/\"&gt;Malloy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What are some new things in data engineering that you are excited about?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?auto=webp&amp;s=9af49f3d253de5999b00a53a34995b08b8ae88d5", "width": 628, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=340de8e22683ded39dc1414cd0f4086995405ebf", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=409134a9198329163da028f2dfe5dc2e2480919a", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df1a090552d7181c4e1b992376626a65c7bedc03", "width": 320, "height": 320}], "variants": {}, "id": "GQEQ7WaJ43xmaAcrmZznZkixlQ7IFzW9Q8Sw1L0rwqQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16qwc2j", "is_robot_indexable": true, "report_reasons": null, "author": "__albatross", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qwc2j/upcoming_data_engineering_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qwc2j/upcoming_data_engineering_tools/", "subreddit_subscribers": 130314, "created_utc": 1695557800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been applying for jobs and recently got 3 callbacks from some contractor roles in Faang. \n\nIt turned out all of them asked SQL hard level questions in the OA Hackerrank screening. \n\nThose were very tricky and unique questions, and I had no idea to solve them if I didn\u2019t encounter them before. \n\nI have done around 200+ easy/medium SQL, and some hard. \n\nIs this a norm where they ask very difficult sql in the oa, then medium level question onsite? How to grind sql hard efficiently?", "author_fullname": "t2_csk6gf7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Always encounter SQL Hard in OA, is it normal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r2lvg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695574476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been applying for jobs and recently got 3 callbacks from some contractor roles in Faang. &lt;/p&gt;\n\n&lt;p&gt;It turned out all of them asked SQL hard level questions in the OA Hackerrank screening. &lt;/p&gt;\n\n&lt;p&gt;Those were very tricky and unique questions, and I had no idea to solve them if I didn\u2019t encounter them before. &lt;/p&gt;\n\n&lt;p&gt;I have done around 200+ easy/medium SQL, and some hard. &lt;/p&gt;\n\n&lt;p&gt;Is this a norm where they ask very difficult sql in the oa, then medium level question onsite? How to grind sql hard efficiently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16r2lvg", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Astronomer-471", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16r2lvg/always_encounter_sql_hard_in_oa_is_it_normal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16r2lvg/always_encounter_sql_hard_in_oa_is_it_normal/", "subreddit_subscribers": 130314, "created_utc": 1695574476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a data warehouse that uses SQL Server 2019 full version. Just one at the moment, and no more expenses will be approved.\n\n&amp;#x200B;\n\nThere are a lot of serious costs constraints at the company I currently work for. There is a huge love-hate relationship with technology, mostly hate. They don't understand technology and don't want to pay for it. People in crucial positions have carved roles doing things manually becoming emperors of their own very basic Excel workbook systems, which senior management doesn't want to upset. Despite the fact that the data they send is old, extremely redundant, and wrong more often than not, they see that as a fault of IT and technology and not the fact it's all done with emailing Excel based off formulas and exports.\n\n&amp;#x200B;\n\nAlso they underpay and understaff any sort of IT/systems staff because they're seen as pure expenses - it brings in over half a billion in revenue every year, and there's NO IT manager, just ONE field tech who's ok with being paid very low and me (data analyst who gets paid fair value). The person who set up IT had built one website before for a non-profit and brown-nosed her way into immediately becoming the IT manager, and set up everything about as disastrously as you can expect. Quit when the company got hit by ransomware and had to find a developer to re-build everything from scratch. They saw this as even more reason for them to hate technology . The CRM was one Access file where everyone has full Admin(!), using one table that's 250 columns long with all fields stored as strings (dates, numbers, etc and people put in anything into those fields) and special characters in the column names. They actually were on the fence about whether they even needed a data analyst at all.\n\n&amp;#x200B;\n\nOn the bright side I've learned a ton where I'm at. I was brought on just to fix the Excel formulas and macros that were breaking across the company (which took about three weeks because they were so bad, mostly vlookups and hundreds of helper columns per workbooks to sum instead of countif/sumif). We've been building data pipelines and a data warehouse using mostly API and SQL/ODBC within Python on scheduled jobs. We got the data cleaned up and into proper fields, so hopefully one day 50 years from now when a data engineer or data scientist gets approved they have something clean to work with. I don't expect to be here after a couple years.\n\n&amp;#x200B;\n\nAll that to say I can't even get $10/month subscriptions to Power BI approved for non-management (for again a company of 150 that brings in $500+ mil per year). They're starting to see the big picture though - Power BI with DirectQuery to the data warehouse has been a game changer over emailing dozens (30+ spreadsheets) every morning.\n\n&amp;#x200B;\n\nI've looking around and realizing it's FUNCTIONAL but not scalable in the long term. It'll be a Ford Focus by the time it's fully complete, but everyone else is making and driving Escalades.\n\n&amp;#x200B;\n\nSorry for the long backstory - We need more than ONE data warehouse, which $1600 cost took three months to approve. Wish I was exaggerating about this. Eventually when the whole company moves to the data warehouse for information it will start to run slowly.\n\n&amp;#x200B;\n\nWe need to stay within the Microsoft ecosystem since we don't have the technical bandwidth/skill level to do otherwise. Is a handful of SQL Express servers a reasonable/\"proper\" way to balance the load? If it gets too heavy on the data warehouse, would SQL Express on a separate server work as a \"helper\" for some of the jobs/calculating?\n\n&amp;#x200B;\n\nWhat general advice can you give since probably many of you have been through similar?\n\nEDIT: Thanks all for the suggestions. I\u2019ll find a way to get PostgreSQL up and running.", "author_fullname": "t2_vgxtzjvq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it bad practice to use SQL Server Express in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qx129", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695595638.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695559920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a data warehouse that uses SQL Server 2019 full version. Just one at the moment, and no more expenses will be approved.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;There are a lot of serious costs constraints at the company I currently work for. There is a huge love-hate relationship with technology, mostly hate. They don&amp;#39;t understand technology and don&amp;#39;t want to pay for it. People in crucial positions have carved roles doing things manually becoming emperors of their own very basic Excel workbook systems, which senior management doesn&amp;#39;t want to upset. Despite the fact that the data they send is old, extremely redundant, and wrong more often than not, they see that as a fault of IT and technology and not the fact it&amp;#39;s all done with emailing Excel based off formulas and exports.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Also they underpay and understaff any sort of IT/systems staff because they&amp;#39;re seen as pure expenses - it brings in over half a billion in revenue every year, and there&amp;#39;s NO IT manager, just ONE field tech who&amp;#39;s ok with being paid very low and me (data analyst who gets paid fair value). The person who set up IT had built one website before for a non-profit and brown-nosed her way into immediately becoming the IT manager, and set up everything about as disastrously as you can expect. Quit when the company got hit by ransomware and had to find a developer to re-build everything from scratch. They saw this as even more reason for them to hate technology . The CRM was one Access file where everyone has full Admin(!), using one table that&amp;#39;s 250 columns long with all fields stored as strings (dates, numbers, etc and people put in anything into those fields) and special characters in the column names. They actually were on the fence about whether they even needed a data analyst at all.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;On the bright side I&amp;#39;ve learned a ton where I&amp;#39;m at. I was brought on just to fix the Excel formulas and macros that were breaking across the company (which took about three weeks because they were so bad, mostly vlookups and hundreds of helper columns per workbooks to sum instead of countif/sumif). We&amp;#39;ve been building data pipelines and a data warehouse using mostly API and SQL/ODBC within Python on scheduled jobs. We got the data cleaned up and into proper fields, so hopefully one day 50 years from now when a data engineer or data scientist gets approved they have something clean to work with. I don&amp;#39;t expect to be here after a couple years.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;All that to say I can&amp;#39;t even get $10/month subscriptions to Power BI approved for non-management (for again a company of 150 that brings in $500+ mil per year). They&amp;#39;re starting to see the big picture though - Power BI with DirectQuery to the data warehouse has been a game changer over emailing dozens (30+ spreadsheets) every morning.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looking around and realizing it&amp;#39;s FUNCTIONAL but not scalable in the long term. It&amp;#39;ll be a Ford Focus by the time it&amp;#39;s fully complete, but everyone else is making and driving Escalades.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Sorry for the long backstory - We need more than ONE data warehouse, which $1600 cost took three months to approve. Wish I was exaggerating about this. Eventually when the whole company moves to the data warehouse for information it will start to run slowly.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;We need to stay within the Microsoft ecosystem since we don&amp;#39;t have the technical bandwidth/skill level to do otherwise. Is a handful of SQL Express servers a reasonable/&amp;quot;proper&amp;quot; way to balance the load? If it gets too heavy on the data warehouse, would SQL Express on a separate server work as a &amp;quot;helper&amp;quot; for some of the jobs/calculating?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What general advice can you give since probably many of you have been through similar?&lt;/p&gt;\n\n&lt;p&gt;EDIT: Thanks all for the suggestions. I\u2019ll find a way to get PostgreSQL up and running.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16qx129", "is_robot_indexable": true, "report_reasons": null, "author": "BestTomatillo6197", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qx129/is_it_bad_practice_to_use_sql_server_express_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qx129/is_it_bad_practice_to_use_sql_server_express_in/", "subreddit_subscribers": 130314, "created_utc": 1695559920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm a DE with 2 YOE + 2 YOE working as a DA.\nI am confident in my skills with SQL, Python, Azure, Databricks &amp; Power BI but have next to no exposure in anything beyond that. What do you think an important concept or tool to add to this repertoire is?\n\nI've seen job postings with Git, Terraform, Kubernetes, Airflow, Docker, Airbyte, Kafka and lots more besides, so shill me your tools!\n\nThanks", "author_fullname": "t2_o88jr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to learn after these technologies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qv9v4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695554433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a DE with 2 YOE + 2 YOE working as a DA.\nI am confident in my skills with SQL, Python, Azure, Databricks &amp;amp; Power BI but have next to no exposure in anything beyond that. What do you think an important concept or tool to add to this repertoire is?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen job postings with Git, Terraform, Kubernetes, Airflow, Docker, Airbyte, Kafka and lots more besides, so shill me your tools!&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16qv9v4", "is_robot_indexable": true, "report_reasons": null, "author": "camikaze007", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qv9v4/what_to_learn_after_these_technologies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qv9v4/what_to_learn_after_these_technologies/", "subreddit_subscribers": 130314, "created_utc": 1695554433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,I'm new and trying to do POC of CDC for streaming data from mysql to Kafka via debezium. What can I do in the correct way when initialize a snapshot from MySQL? If a slave or debezium connectors are down, how can I manage the Kafka cluster to recovery debezium instances from other MySQL slave or master? Do you have any specified whole pictures of CDC in the real world?Thanks for your recommendation,\n\nhttps://preview.redd.it/hzrcstb8ibqb1.png?width=1057&amp;format=png&amp;auto=webp&amp;s=ddc4b951a7a53c5fbaa804c7c75db791df143bd8", "author_fullname": "t2_kwjgg3p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC MySQL with debezium", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 55, "top_awarded_type": null, "hide_score": false, "media_metadata": {"hzrcstb8ibqb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 42, "x": 108, "u": "https://preview.redd.it/hzrcstb8ibqb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f8fb0a779962adcfb683216cfc7339411d99f29f"}, {"y": 85, "x": 216, "u": "https://preview.redd.it/hzrcstb8ibqb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=005e10db26a7c9233c4f0d800cf36b29ffc6bda1"}, {"y": 126, "x": 320, "u": "https://preview.redd.it/hzrcstb8ibqb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39c6218748851df4499992c78693909d4b27df32"}, {"y": 253, "x": 640, "u": "https://preview.redd.it/hzrcstb8ibqb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=95f12d39a610f400ab5a0a4c98d7f3033126e3ba"}, {"y": 379, "x": 960, "u": "https://preview.redd.it/hzrcstb8ibqb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=83f6ddd8ebafb395897d525493915cd397f72422"}], "s": {"y": 418, "x": 1057, "u": "https://preview.redd.it/hzrcstb8ibqb1.png?width=1057&amp;format=png&amp;auto=webp&amp;s=ddc4b951a7a53c5fbaa804c7c75db791df143bd8"}, "id": "hzrcstb8ibqb1"}}, "name": "t3_16rgu36", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/p0onLUeK1KG8N58AlD59dqhZxHEM-uHC6adOsdvVqL0.jpg", "edited": 1695615842.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695611253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,I&amp;#39;m new and trying to do POC of CDC for streaming data from mysql to Kafka via debezium. What can I do in the correct way when initialize a snapshot from MySQL? If a slave or debezium connectors are down, how can I manage the Kafka cluster to recovery debezium instances from other MySQL slave or master? Do you have any specified whole pictures of CDC in the real world?Thanks for your recommendation,&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hzrcstb8ibqb1.png?width=1057&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddc4b951a7a53c5fbaa804c7c75db791df143bd8\"&gt;https://preview.redd.it/hzrcstb8ibqb1.png?width=1057&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddc4b951a7a53c5fbaa804c7c75db791df143bd8&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16rgu36", "is_robot_indexable": true, "report_reasons": null, "author": "phamtanvinhme", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rgu36/cdc_mysql_with_debezium/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rgu36/cdc_mysql_with_debezium/", "subreddit_subscribers": 130314, "created_utc": 1695611253.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Please find the code for my stock data streaming application [here](https://github.com/JawaharRamis/stock-price-analysis-kafka-spark-influxdb-grafana).\n\nThe aim for the project was to familiarize myself with the various components involved and having a meaningful dashboard at the end of it. Kindly share your suggestions/advices. The main thing I am concerned is if I have structured the code in the best way possible or even the deployment setup/configurations. \n\nThis subreddit has been immense during my learning journey and I hope it continues to aid me.", "author_fullname": "t2_67og6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stock market streaming application", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rb1hz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695594977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please find the code for my stock data streaming application &lt;a href=\"https://github.com/JawaharRamis/stock-price-analysis-kafka-spark-influxdb-grafana\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;The aim for the project was to familiarize myself with the various components involved and having a meaningful dashboard at the end of it. Kindly share your suggestions/advices. The main thing I am concerned is if I have structured the code in the best way possible or even the deployment setup/configurations. &lt;/p&gt;\n\n&lt;p&gt;This subreddit has been immense during my learning journey and I hope it continues to aid me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?auto=webp&amp;s=19d09cf64df95948fd765c06fdbe5575d70e25cd", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f65ccd0fb28832b7a761eaeb280d99d175171c2", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9d0ed9c9a8b0a63514689e32dae4436ac7a3164", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2523d023e49863736668af1bd4568e75587fb0a9", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3c55af923d520b5d02f6a51bf2bfed99b9cc943c", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e71295dbcac468ecadca81e9eeacce006477ed8a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=26f4bb51218822e4328b08f224f4b41c2bec4c0f", "width": 1080, "height": 540}], "variants": {}, "id": "PyAZjXFESQEfZE9E-hmQVe5jfp1OTuXS7LpwqNAImpQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "16rb1hz", "is_robot_indexable": true, "report_reasons": null, "author": "jawz96", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rb1hz/stock_market_streaming_application/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rb1hz/stock_market_streaming_application/", "subreddit_subscribers": 130314, "created_utc": 1695594977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am supposed to give an interview on this to get a project. This is very important to me. I have some basic idea on pyspark and I have done some stuff on databricks but I have no idea about complex things. Can you plz share resources that I can study and some exercises that I can do for this interview?", "author_fullname": "t2_9peiwedk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any playlist/ resources to prepare for my pyspark interview?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rkc1n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695622827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am supposed to give an interview on this to get a project. This is very important to me. I have some basic idea on pyspark and I have done some stuff on databricks but I have no idea about complex things. Can you plz share resources that I can study and some exercises that I can do for this interview?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16rkc1n", "is_robot_indexable": true, "report_reasons": null, "author": "LucaMarko", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rkc1n/any_playlist_resources_to_prepare_for_my_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rkc1n/any_playlist_resources_to_prepare_for_my_pyspark/", "subreddit_subscribers": 130314, "created_utc": 1695622827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nWe build integrations between ERP and desperate systems for exchange of data. Wondering how this service be monetised and what pricing models can be adopted to offer as a SAAS model. Building integration is not just using existing DIY ETL platforms but need subject matter expertise to understand ERP data especially during transformation. How can infra, technology, manpower, subject matter expertise be clubbed into one pricing model. Can it be data storage+ rows of data extracted and stored? Any ideas is greatly appreciated from the community.", "author_fullname": "t2_b77s3li1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pricing options for Data integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qvb2t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695554543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;We build integrations between ERP and desperate systems for exchange of data. Wondering how this service be monetised and what pricing models can be adopted to offer as a SAAS model. Building integration is not just using existing DIY ETL platforms but need subject matter expertise to understand ERP data especially during transformation. How can infra, technology, manpower, subject matter expertise be clubbed into one pricing model. Can it be data storage+ rows of data extracted and stored? Any ideas is greatly appreciated from the community.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16qvb2t", "is_robot_indexable": true, "report_reasons": null, "author": "srikon", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qvb2t/pricing_options_for_data_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qvb2t/pricing_options_for_data_integration/", "subreddit_subscribers": 130314, "created_utc": 1695554543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to build a batch processing pipeline which transforms CSVs into some common JSON schema. What are good ways of achieving that? \n\nThe pipeline needs to:\n- start a job on signal (perhaps airflow/dagster)\n- process csv file (~10GB)\n- load json into document store\n- perform other tasks eventually, like validation, diffing, notifying, etc.\n\nI\u2019ve tried Apache Beam via Dataflow, using Scala scio, and this seems like a viable option, although maybe not especially user friendly. Documentation and examples are scattered across different SDKs using different languages and if I\u2019m not mistaken, they don\u2019t implement the same functionality. I\u2019d like to use Scala, or a similar statically typed language, as the project complexity will likely grow with time, and it would be great to have it all in one language.\n\nI don\u2019t have a lot of experience with data engineering specifically, so not sure if there are better options out there. What would you recommend? \n\nThank you!", "author_fullname": "t2_hc1vem", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch processing recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r7ths", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695587279.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to build a batch processing pipeline which transforms CSVs into some common JSON schema. What are good ways of achieving that? &lt;/p&gt;\n\n&lt;p&gt;The pipeline needs to:\n- start a job on signal (perhaps airflow/dagster)\n- process csv file (~10GB)\n- load json into document store\n- perform other tasks eventually, like validation, diffing, notifying, etc.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve tried Apache Beam via Dataflow, using Scala scio, and this seems like a viable option, although maybe not especially user friendly. Documentation and examples are scattered across different SDKs using different languages and if I\u2019m not mistaken, they don\u2019t implement the same functionality. I\u2019d like to use Scala, or a similar statically typed language, as the project complexity will likely grow with time, and it would be great to have it all in one language.&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t have a lot of experience with data engineering specifically, so not sure if there are better options out there. What would you recommend? &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16r7ths", "is_robot_indexable": true, "report_reasons": null, "author": "RustinWolf", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16r7ths/batch_processing_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16r7ths/batch_processing_recommendations/", "subreddit_subscribers": 130314, "created_utc": 1695587279.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just started doing LC, but I am quite a lazy person and only have been casually doing it. Sometimes it\u2019s just hard to feel motivated especially after a long day of work. Curious how much time does everyone spend on leetcode everyday? Would love to know those who are actively looking for jobs vs those who are just casually prepping.\n\n[View Poll](https://www.reddit.com/poll/16rj1o9)", "author_fullname": "t2_1xrjwd6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Leetcode grind duration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rj1o9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695618291.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just started doing LC, but I am quite a lazy person and only have been casually doing it. Sometimes it\u2019s just hard to feel motivated especially after a long day of work. Curious how much time does everyone spend on leetcode everyday? Would love to know those who are actively looking for jobs vs those who are just casually prepping.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/16rj1o9\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16rj1o9", "is_robot_indexable": true, "report_reasons": null, "author": "Fasthandman", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1695877491567, "options": [{"text": "Less than 1 hour", "id": "24955469"}, {"text": "1-2 hours", "id": "24955470"}, {"text": "2-3hours", "id": "24955471"}, {"text": "&gt;3 hrs", "id": "24955472"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 57, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rj1o9/leetcode_grind_duration/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/16rj1o9/leetcode_grind_duration/", "subreddit_subscribers": 130314, "created_utc": 1695618291.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}