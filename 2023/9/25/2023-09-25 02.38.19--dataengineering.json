{"kind": "Listing", "data": {"after": null, "dist": 16, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I saw this post in cscareerquestionsEU. So far there is not so much data for DE job in EU. Please share and fill your data to make it transparent. I am just sharing the post and am not affiliated to anyone -\n\n&amp;#x200B;\n\nHere is the original post -\n\nHere are some sources, none of them are self-promotion nor intended as promotion. I am not being compensated for this, I just strongly believe in the value that salary transaprency brings:\n\n1. [The Trimodal nature of salaries](https://blog.pragmaticengineer.com/software-engineering-salaries-in-the-netherlands-and-europe/)\n2. [https://levels.fyi](https://levels.fyi/) \\- make sure to filter for your location\n3. [https://techpays.eu](https://techpays.eu/) \\- basically a levels clone focused on EU with a subset of companies\n\ntl;dr: Switzerland or HFT pays about US rates but has low # of companies and jobs, Netherlands can pay US rates but you have to know which companies, then you get 2nd tier US rates (regions like Austin, or companies like blue chip companies) in Munich, Berlin, London. Most other cities and countries don't have US competitive salaries. Poland is a favorite pick for US companies to have US teams so they pay around 50k (junior) to 150k (staff+) and COL is very low so savings rates can be quite high. Because they work in English, limited need to know Polish AFAIK.", "author_fullname": "t2_8dod55p8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Everyone on this subreddit should be aware of market salaries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qscvf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 114, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 114, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695544016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw this post in cscareerquestionsEU. So far there is not so much data for DE job in EU. Please share and fill your data to make it transparent. I am just sharing the post and am not affiliated to anyone -&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here is the original post -&lt;/p&gt;\n\n&lt;p&gt;Here are some sources, none of them are self-promotion nor intended as promotion. I am not being compensated for this, I just strongly believe in the value that salary transaprency brings:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://blog.pragmaticengineer.com/software-engineering-salaries-in-the-netherlands-and-europe/\"&gt;The Trimodal nature of salaries&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://levels.fyi/\"&gt;https://levels.fyi&lt;/a&gt; - make sure to filter for your location&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://techpays.eu/\"&gt;https://techpays.eu&lt;/a&gt; - basically a levels clone focused on EU with a subset of companies&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;tl;dr: Switzerland or HFT pays about US rates but has low # of companies and jobs, Netherlands can pay US rates but you have to know which companies, then you get 2nd tier US rates (regions like Austin, or companies like blue chip companies) in Munich, Berlin, London. Most other cities and countries don&amp;#39;t have US competitive salaries. Poland is a favorite pick for US companies to have US teams so they pay around 50k (junior) to 150k (staff+) and COL is very low so savings rates can be quite high. Because they work in English, limited need to know Polish AFAIK.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lrQk5fA160485C1saBJGkDTN1VXhra76pfhBwv1zH7U.jpg?auto=webp&amp;s=71830870dfbc35e922f901495ccf405d517bfb45", "width": 2000, "height": 1401}, "resolutions": [{"url": "https://external-preview.redd.it/lrQk5fA160485C1saBJGkDTN1VXhra76pfhBwv1zH7U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3df5f29d98a7a0cd9210429e34a94035c197fd70", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/lrQk5fA160485C1saBJGkDTN1VXhra76pfhBwv1zH7U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a058ad5649801c2a517fb5bf44cd2d2395ebc1b", "width": 216, "height": 151}, {"url": "https://external-preview.redd.it/lrQk5fA160485C1saBJGkDTN1VXhra76pfhBwv1zH7U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=734ea622647a28ce672d77bf8b62c9327daa5b62", "width": 320, "height": 224}, {"url": "https://external-preview.redd.it/lrQk5fA160485C1saBJGkDTN1VXhra76pfhBwv1zH7U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=727431ca4b47d8a648080079e550cf88e37cfebe", "width": 640, "height": 448}, {"url": "https://external-preview.redd.it/lrQk5fA160485C1saBJGkDTN1VXhra76pfhBwv1zH7U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f585dee323feed81e3a80601ee7213c86452198d", "width": 960, "height": 672}, {"url": "https://external-preview.redd.it/lrQk5fA160485C1saBJGkDTN1VXhra76pfhBwv1zH7U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=38aa98ab79c9e7f942dbae4874edac7edd1a8ee1", "width": 1080, "height": 756}], "variants": {}, "id": "1Byf_oZ7o--0csi5T98ked7PXNqUSVnfHZSaWfvFzq8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16qscvf", "is_robot_indexable": true, "report_reasons": null, "author": "Diligent_Fondant6761", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qscvf/everyone_on_this_subreddit_should_be_aware_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qscvf/everyone_on_this_subreddit_should_be_aware_of/", "subreddit_subscribers": 130262, "created_utc": 1695544016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Back in the day when we had to scale an analytics pipeline over a large dataset, Hadoop/spark used to be the go to options and later pyspark became popular.   \n\n\nNow if the dataset is big we have some really interesting tools which doesn't require complex setup like spinning up a hadoop cluster.   \n\n\n* We have tools like [Polars](https://www.pola.rs/) which can stream data if the memory is not enough. \n* We have [Ibis](https://ibis-project.org/) which helps you write generic analytics pipelines and it supports multiple backends and you can either run these pipeline on a query processing engine or a pyspark cluster or even on Pandas or Polars. \n* You can create your own internal datalake using a couple of parquet files and pair it with [Duckdb](https://duckdb.org/) for superfast analytics.  \n\n\nHadoop ecosystem will still be relevant but I believe for Small and Medium Business doing data engineering is getting better and you don't require big complex clusters to solve problems every time you run out of memory. These new tools are so easy to work with that any python programmer can work with them.   \n\n\nI remember how I struggled understanding this book on [MapReduceDesign Patterns](https://www.oreilly.com/library/view/mapreduce-design-patterns/9781449341954/) a couple of years back and thinking that \"*Damnn!! data engineering is going to be hard*\".\n\nRust and python combination has resulted in some amazing python libraries.   \nOne interesting thing I see is people developing different variety of query languages like [EdgeQL](https://www.edgedb.com/docs/edgeql/index) and [Malloy](https://www.malloydata.dev/)\n\nWhat are some new things in data engineering that you are excited about?", "author_fullname": "t2_q27tep12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upcoming Data Engineering Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qwc2j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695557800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Back in the day when we had to scale an analytics pipeline over a large dataset, Hadoop/spark used to be the go to options and later pyspark became popular.   &lt;/p&gt;\n\n&lt;p&gt;Now if the dataset is big we have some really interesting tools which doesn&amp;#39;t require complex setup like spinning up a hadoop cluster.   &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We have tools like &lt;a href=\"https://www.pola.rs/\"&gt;Polars&lt;/a&gt; which can stream data if the memory is not enough. &lt;/li&gt;\n&lt;li&gt;We have &lt;a href=\"https://ibis-project.org/\"&gt;Ibis&lt;/a&gt; which helps you write generic analytics pipelines and it supports multiple backends and you can either run these pipeline on a query processing engine or a pyspark cluster or even on Pandas or Polars. &lt;/li&gt;\n&lt;li&gt;You can create your own internal datalake using a couple of parquet files and pair it with &lt;a href=\"https://duckdb.org/\"&gt;Duckdb&lt;/a&gt; for superfast analytics.&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Hadoop ecosystem will still be relevant but I believe for Small and Medium Business doing data engineering is getting better and you don&amp;#39;t require big complex clusters to solve problems every time you run out of memory. These new tools are so easy to work with that any python programmer can work with them.   &lt;/p&gt;\n\n&lt;p&gt;I remember how I struggled understanding this book on &lt;a href=\"https://www.oreilly.com/library/view/mapreduce-design-patterns/9781449341954/\"&gt;MapReduceDesign Patterns&lt;/a&gt; a couple of years back and thinking that &amp;quot;&lt;em&gt;Damnn!! data engineering is going to be hard&lt;/em&gt;&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Rust and python combination has resulted in some amazing python libraries.&lt;br/&gt;\nOne interesting thing I see is people developing different variety of query languages like &lt;a href=\"https://www.edgedb.com/docs/edgeql/index\"&gt;EdgeQL&lt;/a&gt; and &lt;a href=\"https://www.malloydata.dev/\"&gt;Malloy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What are some new things in data engineering that you are excited about?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?auto=webp&amp;s=9af49f3d253de5999b00a53a34995b08b8ae88d5", "width": 628, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=340de8e22683ded39dc1414cd0f4086995405ebf", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=409134a9198329163da028f2dfe5dc2e2480919a", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df1a090552d7181c4e1b992376626a65c7bedc03", "width": 320, "height": 320}], "variants": {}, "id": "GQEQ7WaJ43xmaAcrmZznZkixlQ7IFzW9Q8Sw1L0rwqQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16qwc2j", "is_robot_indexable": true, "report_reasons": null, "author": "__albatross", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qwc2j/upcoming_data_engineering_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qwc2j/upcoming_data_engineering_tools/", "subreddit_subscribers": 130262, "created_utc": 1695557800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What's your views on using duckdb + S3 as your datalake/datalakehouse/datawarehouse.\n\ni.e. raw data sits in S3 -&gt; duckdb to clean raw data and write back to s3 -&gt; duckdb to transform cleaned data and write back to s3 -&gt; load transformed data to postgres -&gt; connect postgres to BI tool -&gt; dashboard(s).\n\n&amp;#x200B;\n\n1. Would you say this is good or not-so-good practice?\n2. How scalable is this (if at all)?\n3. Potentially too many read/writes to s3 for your liking?\n4. What would be your good-to data pipeline architecture?", "author_fullname": "t2_4gzaf8mv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Views on using duckdb + S3 as a datalake/datalakehouse/datawarehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qu2lz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695550295.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s your views on using duckdb + S3 as your datalake/datalakehouse/datawarehouse.&lt;/p&gt;\n\n&lt;p&gt;i.e. raw data sits in S3 -&amp;gt; duckdb to clean raw data and write back to s3 -&amp;gt; duckdb to transform cleaned data and write back to s3 -&amp;gt; load transformed data to postgres -&amp;gt; connect postgres to BI tool -&amp;gt; dashboard(s).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Would you say this is good or not-so-good practice?&lt;/li&gt;\n&lt;li&gt;How scalable is this (if at all)?&lt;/li&gt;\n&lt;li&gt;Potentially too many read/writes to s3 for your liking?&lt;/li&gt;\n&lt;li&gt;What would be your good-to data pipeline architecture?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16qu2lz", "is_robot_indexable": true, "report_reasons": null, "author": "theoriginalmantooth", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qu2lz/views_on_using_duckdb_s3_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qu2lz/views_on_using_duckdb_s3_as_a/", "subreddit_subscribers": 130262, "created_utc": 1695550295.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The questions were fairly straightforward, irritatingly more wordy that most leetcode type questions, but there was only one table per question (there were joins but self joins only)\n\nReally nothing crazy about them. 4 questions, one hour. First question I finished in about 5mins as it was more of a fill in the blanks for a stored proc that'd give you the most popular product by year.\n\nNext one was a simple select, to get sum of highest salaries - sum of lowest. (This one actually failed a 1/7 test case but was it was easy I'd come back to it later. I'd find out soon though, I'd never see this question again...\n\nQuestion 3, shit hit the fan. All I needed to do was select names of fraudulent customers based on a condition that would come from time lapsed between transactions. Except, it wasn't a select. It was a procedure. I couldn't incrementally build out the query and check the output as I'd go. I needed to write the query and run it, if it was right it'd say so. If not, it's say test case n/N failed. No context as to where it'd fail. \n\nAgain, these questions were not hard. But next thing my fucking hand starts to shake. I've still got 40mins left, calm down. Start trying to visualize the solution. Panic sets in and nothing worked after that. Ok, next question, will skip this one for now.\n\nSame bloody thing. Procedure, not select. Incredibly wordy question. Easy enough question when youve [seen similar ones](https://datalemur.com/questions/pizzas-topping-cost), sincs it was similar to that pizza one I knew it was a self join and messing around with adding columns (at the time I was thinking I'd need a window function to get the lower cost combo,  but turns out for this question it was simply just needed go get the totals with no duplicates, easy stuff. But I bombed it. Of course I opened it up on my ide afterward and solved it in &lt;10mins, because I like to torture myself like that.\n\nSeems that not having the ability to see the outputs of selects/ctes etc completely through me off. I'm now wondering if I'm the only person that does this and I've gotten into a bad habit of banging the execute button to see the outputs of intermediary results??\n\nKicking myself now as it was so damn easy and a really cool job. Only consolation was thst there would have been another 4/5 rounds of technical interviews after this and probably wouldn't have gotten through ayway", "author_fullname": "t2_1w1o79i7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bombed an easy SQL prescreening assessment - have I picked up bad habits by looking at intermediate query results?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r9swv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695591968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The questions were fairly straightforward, irritatingly more wordy that most leetcode type questions, but there was only one table per question (there were joins but self joins only)&lt;/p&gt;\n\n&lt;p&gt;Really nothing crazy about them. 4 questions, one hour. First question I finished in about 5mins as it was more of a fill in the blanks for a stored proc that&amp;#39;d give you the most popular product by year.&lt;/p&gt;\n\n&lt;p&gt;Next one was a simple select, to get sum of highest salaries - sum of lowest. (This one actually failed a 1/7 test case but was it was easy I&amp;#39;d come back to it later. I&amp;#39;d find out soon though, I&amp;#39;d never see this question again...&lt;/p&gt;\n\n&lt;p&gt;Question 3, shit hit the fan. All I needed to do was select names of fraudulent customers based on a condition that would come from time lapsed between transactions. Except, it wasn&amp;#39;t a select. It was a procedure. I couldn&amp;#39;t incrementally build out the query and check the output as I&amp;#39;d go. I needed to write the query and run it, if it was right it&amp;#39;d say so. If not, it&amp;#39;s say test case n/N failed. No context as to where it&amp;#39;d fail. &lt;/p&gt;\n\n&lt;p&gt;Again, these questions were not hard. But next thing my fucking hand starts to shake. I&amp;#39;ve still got 40mins left, calm down. Start trying to visualize the solution. Panic sets in and nothing worked after that. Ok, next question, will skip this one for now.&lt;/p&gt;\n\n&lt;p&gt;Same bloody thing. Procedure, not select. Incredibly wordy question. Easy enough question when youve &lt;a href=\"https://datalemur.com/questions/pizzas-topping-cost\"&gt;seen similar ones&lt;/a&gt;, sincs it was similar to that pizza one I knew it was a self join and messing around with adding columns (at the time I was thinking I&amp;#39;d need a window function to get the lower cost combo,  but turns out for this question it was simply just needed go get the totals with no duplicates, easy stuff. But I bombed it. Of course I opened it up on my ide afterward and solved it in &amp;lt;10mins, because I like to torture myself like that.&lt;/p&gt;\n\n&lt;p&gt;Seems that not having the ability to see the outputs of selects/ctes etc completely through me off. I&amp;#39;m now wondering if I&amp;#39;m the only person that does this and I&amp;#39;ve gotten into a bad habit of banging the execute button to see the outputs of intermediary results??&lt;/p&gt;\n\n&lt;p&gt;Kicking myself now as it was so damn easy and a really cool job. Only consolation was thst there would have been another 4/5 rounds of technical interviews after this and probably wouldn&amp;#39;t have gotten through ayway&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?auto=webp&amp;s=a6863d19648309b6e3176a9eee52cb8216efce69", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=47a9aa9e8f07567e2b88dac9ce4e3f7a6e23f9d7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e137641eb770388594fd82c4e58db9cc64dd659", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c13cb4b7d66b52a4fe3bdf986a3d141d3c1d2806", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce1afb705cfcb979ee099eab554f759c2f89ef90", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c99bb17a41bdc454fc7204dcfb3e57276e4aacbc", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=61054982f566d0873d25f1b69cfef7e2f3a2c7f4", "width": 1080, "height": 567}], "variants": {}, "id": "wPNS14r1J4_L3TGYUAsqCV8FmgGrMho9XjGoyEnYUAQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16r9swv", "is_robot_indexable": true, "report_reasons": null, "author": "tea_horse", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16r9swv/bombed_an_easy_sql_prescreening_assessment_have_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16r9swv/bombed_an_easy_sql_prescreening_assessment_have_i/", "subreddit_subscribers": 130262, "created_utc": 1695591968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "just curious on your thoughts/approach on this. do you create an ETL (personal project)? do you just read articles about it? do you just read some reddit discussions or something similar?", "author_fullname": "t2_96yckejj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "for those who are already working as a DE for at least a couple of years now, how do you keep up with latest tools/concepts if your DE team does not want to explore it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qmu30", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695524651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;just curious on your thoughts/approach on this. do you create an ETL (personal project)? do you just read articles about it? do you just read some reddit discussions or something similar?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16qmu30", "is_robot_indexable": true, "report_reasons": null, "author": "march-2020", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16qmu30/for_those_who_are_already_working_as_a_de_for_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qmu30/for_those_who_are_already_working_as_a_de_for_at/", "subreddit_subscribers": 130262, "created_utc": 1695524651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a data warehouse that uses SQL Server 2019 full version. Just one at the moment, and no more expenses will be approved.\n\n&amp;#x200B;\n\nThere are a lot of serious costs constraints at the company I currently work for. There is a huge love-hate relationship with technology, mostly hate. They don't understand technology and don't want to pay for it. People in crucial positions have carved roles doing things manually becoming emperors of their own very basic Excel workbook systems, which senior management doesn't want to upset. Despite the fact that the data they send is old, extremely redundant, and wrong more often than not, they see that as a fault of IT and technology and not the fact it's all done with emailing Excel based off formulas and exports.\n\n&amp;#x200B;\n\nAlso they underpay and understaff any sort of IT/systems staff because they're seen as pure expenses - it brings in over half a billion in revenue every year, and there's NO IT manager, just ONE field tech who's ok with being paid very low and me (data analyst who gets paid fair value). The person who set up IT had built one website before for a non-profit and brown-nosed her way into immediately becoming the IT manager, and set up everything about as disastrously as you can expect. Quit when the company got hit by ransomware and had to find a developer to re-build everything from scratch. They saw this as even more reason for them to hate technology . The CRM was one Access file where everyone has full Admin(!), using one table that's 250 columns long with all fields stored as strings (dates, numbers, etc and people put in anything into those fields) and special characters in the column names. They actually were on the fence about whether they even needed a data analyst at all.\n\n&amp;#x200B;\n\nOn the bright side I've learned a ton where I'm at. I was brought on just to fix the Excel formulas and macros that were breaking across the company (which took about three weeks because they were so bad, mostly vlookups and hundreds of helper columns per workbooks to sum instead of countif/sumif). We've been building data pipelines and a data warehouse using mostly API and SQL/ODBC within Python on scheduled jobs. We got the data cleaned up and into proper fields, so hopefully one day 50 years from now when a data engineer or data scientist gets approved they have something clean to work with. I don't expect to be here after a couple years.\n\n&amp;#x200B;\n\nAll that to say I can't even get $10/month subscriptions to Power BI approved for non-management (for again a company of 150 that brings in $500+ mil per year). They're starting to see the big picture though - Power BI with DirectQuery to the data warehouse has been a game changer over emailing dozens (30+ spreadsheets) every morning.\n\n&amp;#x200B;\n\nI've looking around and realizing it's FUNCTIONAL but not scalable in the long term. It'll be a Ford Focus by the time it's fully complete, but everyone else is making and driving Escalades.\n\n&amp;#x200B;\n\nSorry for the long backstory - We need more than ONE data warehouse, which $1600 cost took three months to approve. Wish I was exaggerating about this. Eventually when the whole company moves to the data warehouse for information it will start to run slowly.\n\n&amp;#x200B;\n\nWe need to stay within the Microsoft ecosystem since we don't have the technical bandwidth/skill level to do otherwise. Is a handful of SQL Express servers a reasonable/\"proper\" way to balance the load? If it gets too heavy on the data warehouse, would SQL Express on a separate server work as a \"helper\" for some of the jobs/calculating?\n\n&amp;#x200B;\n\nWhat general advice can you give since probably many of you have been through similar?\n\nEDIT: Thanks all for the suggestions. I\u2019ll find a way to get PostgreSQL up and running.", "author_fullname": "t2_vgxtzjvq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it bad practice to use SQL Server Express in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qx129", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695595638.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695559920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a data warehouse that uses SQL Server 2019 full version. Just one at the moment, and no more expenses will be approved.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;There are a lot of serious costs constraints at the company I currently work for. There is a huge love-hate relationship with technology, mostly hate. They don&amp;#39;t understand technology and don&amp;#39;t want to pay for it. People in crucial positions have carved roles doing things manually becoming emperors of their own very basic Excel workbook systems, which senior management doesn&amp;#39;t want to upset. Despite the fact that the data they send is old, extremely redundant, and wrong more often than not, they see that as a fault of IT and technology and not the fact it&amp;#39;s all done with emailing Excel based off formulas and exports.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Also they underpay and understaff any sort of IT/systems staff because they&amp;#39;re seen as pure expenses - it brings in over half a billion in revenue every year, and there&amp;#39;s NO IT manager, just ONE field tech who&amp;#39;s ok with being paid very low and me (data analyst who gets paid fair value). The person who set up IT had built one website before for a non-profit and brown-nosed her way into immediately becoming the IT manager, and set up everything about as disastrously as you can expect. Quit when the company got hit by ransomware and had to find a developer to re-build everything from scratch. They saw this as even more reason for them to hate technology . The CRM was one Access file where everyone has full Admin(!), using one table that&amp;#39;s 250 columns long with all fields stored as strings (dates, numbers, etc and people put in anything into those fields) and special characters in the column names. They actually were on the fence about whether they even needed a data analyst at all.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;On the bright side I&amp;#39;ve learned a ton where I&amp;#39;m at. I was brought on just to fix the Excel formulas and macros that were breaking across the company (which took about three weeks because they were so bad, mostly vlookups and hundreds of helper columns per workbooks to sum instead of countif/sumif). We&amp;#39;ve been building data pipelines and a data warehouse using mostly API and SQL/ODBC within Python on scheduled jobs. We got the data cleaned up and into proper fields, so hopefully one day 50 years from now when a data engineer or data scientist gets approved they have something clean to work with. I don&amp;#39;t expect to be here after a couple years.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;All that to say I can&amp;#39;t even get $10/month subscriptions to Power BI approved for non-management (for again a company of 150 that brings in $500+ mil per year). They&amp;#39;re starting to see the big picture though - Power BI with DirectQuery to the data warehouse has been a game changer over emailing dozens (30+ spreadsheets) every morning.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looking around and realizing it&amp;#39;s FUNCTIONAL but not scalable in the long term. It&amp;#39;ll be a Ford Focus by the time it&amp;#39;s fully complete, but everyone else is making and driving Escalades.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Sorry for the long backstory - We need more than ONE data warehouse, which $1600 cost took three months to approve. Wish I was exaggerating about this. Eventually when the whole company moves to the data warehouse for information it will start to run slowly.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;We need to stay within the Microsoft ecosystem since we don&amp;#39;t have the technical bandwidth/skill level to do otherwise. Is a handful of SQL Express servers a reasonable/&amp;quot;proper&amp;quot; way to balance the load? If it gets too heavy on the data warehouse, would SQL Express on a separate server work as a &amp;quot;helper&amp;quot; for some of the jobs/calculating?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What general advice can you give since probably many of you have been through similar?&lt;/p&gt;\n\n&lt;p&gt;EDIT: Thanks all for the suggestions. I\u2019ll find a way to get PostgreSQL up and running.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16qx129", "is_robot_indexable": true, "report_reasons": null, "author": "BestTomatillo6197", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qx129/is_it_bad_practice_to_use_sql_server_express_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qx129/is_it_bad_practice_to_use_sql_server_express_in/", "subreddit_subscribers": 130262, "created_utc": 1695559920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been applying for jobs and recently got 3 callbacks from some contractor roles in Faang. \n\nIt turned out all of them asked SQL hard level questions in the OA Hackerrank screening. \n\nThose were very tricky and unique questions, and I had no idea to solve them if I didn\u2019t encounter them before. \n\nI have done around 200+ easy/medium SQL, and some hard. \n\nIs this a norm where they ask very difficult sql in the oa, then medium level question onsite? How to grind sql hard efficiently?", "author_fullname": "t2_csk6gf7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Always encounter SQL Hard in OA, is it normal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r2lvg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695574476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been applying for jobs and recently got 3 callbacks from some contractor roles in Faang. &lt;/p&gt;\n\n&lt;p&gt;It turned out all of them asked SQL hard level questions in the OA Hackerrank screening. &lt;/p&gt;\n\n&lt;p&gt;Those were very tricky and unique questions, and I had no idea to solve them if I didn\u2019t encounter them before. &lt;/p&gt;\n\n&lt;p&gt;I have done around 200+ easy/medium SQL, and some hard. &lt;/p&gt;\n\n&lt;p&gt;Is this a norm where they ask very difficult sql in the oa, then medium level question onsite? How to grind sql hard efficiently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16r2lvg", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Astronomer-471", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16r2lvg/always_encounter_sql_hard_in_oa_is_it_normal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16r2lvg/always_encounter_sql_hard_in_oa_is_it_normal/", "subreddit_subscribers": 130262, "created_utc": 1695574476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My title within my company is 'Data developer', but I do data engineering for 90% of the time. The company I work for didn't have any solid data infrastructure, so this year has mostly been spent on making the infrastructure and making some projects for our in-house dashboards team.\n\nLately my manager has told me to get ready for some data science projects too (I am the only data professional within the company) since I come from a data science background. I know how to do advanced regression, classification, clustering or any advanced statistical analysis of data since my previous job involved that work heavily.\n\nFor some reason at the back of my head I feel like I'm being squeezed. I feel like another person should've been hired to do the data science stuff instead of me supporting the already existing data infrastructure and the projects on it and to do the advanced statistical analysis too. \n\nI looked at my job description and although it did say to have competence in machine learning/statistical analysis, but when I had interviewed the manager point black told me that it would be mostly data engineering only (which was not in the job description too much), but since I wanted to switch to data engineering, I had joined the company last year.\n\nAlthough I am thankful for being paid an above market salary with lots of benefits, but still I feel like something's not right and that I should negotiate a higher compensation or tease the market for what I'm actually worth.\n\nAm I wrong to feel this way?", "author_fullname": "t2_l35gwhuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Data Engineering + Data Science Skill Demand Higher Compensation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qsm7s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695544967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My title within my company is &amp;#39;Data developer&amp;#39;, but I do data engineering for 90% of the time. The company I work for didn&amp;#39;t have any solid data infrastructure, so this year has mostly been spent on making the infrastructure and making some projects for our in-house dashboards team.&lt;/p&gt;\n\n&lt;p&gt;Lately my manager has told me to get ready for some data science projects too (I am the only data professional within the company) since I come from a data science background. I know how to do advanced regression, classification, clustering or any advanced statistical analysis of data since my previous job involved that work heavily.&lt;/p&gt;\n\n&lt;p&gt;For some reason at the back of my head I feel like I&amp;#39;m being squeezed. I feel like another person should&amp;#39;ve been hired to do the data science stuff instead of me supporting the already existing data infrastructure and the projects on it and to do the advanced statistical analysis too. &lt;/p&gt;\n\n&lt;p&gt;I looked at my job description and although it did say to have competence in machine learning/statistical analysis, but when I had interviewed the manager point black told me that it would be mostly data engineering only (which was not in the job description too much), but since I wanted to switch to data engineering, I had joined the company last year.&lt;/p&gt;\n\n&lt;p&gt;Although I am thankful for being paid an above market salary with lots of benefits, but still I feel like something&amp;#39;s not right and that I should negotiate a higher compensation or tease the market for what I&amp;#39;m actually worth.&lt;/p&gt;\n\n&lt;p&gt;Am I wrong to feel this way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16qsm7s", "is_robot_indexable": true, "report_reasons": null, "author": "Senior_Anteater4688", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qsm7s/does_data_engineering_data_science_skill_demand/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qsm7s/does_data_engineering_data_science_skill_demand/", "subreddit_subscribers": 130262, "created_utc": 1695544967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm a DE with 2 YOE + 2 YOE working as a DA.\nI am confident in my skills with SQL, Python, Azure, Databricks &amp; Power BI but have next to no exposure in anything beyond that. What do you think an important concept or tool to add to this repertoire is?\n\nI've seen job postings with Git, Terraform, Kubernetes, Airflow, Docker, Airbyte, Kafka and lots more besides, so shill me your tools!\n\nThanks", "author_fullname": "t2_o88jr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to learn after these technologies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qv9v4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695554433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a DE with 2 YOE + 2 YOE working as a DA.\nI am confident in my skills with SQL, Python, Azure, Databricks &amp;amp; Power BI but have next to no exposure in anything beyond that. What do you think an important concept or tool to add to this repertoire is?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen job postings with Git, Terraform, Kubernetes, Airflow, Docker, Airbyte, Kafka and lots more besides, so shill me your tools!&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16qv9v4", "is_robot_indexable": true, "report_reasons": null, "author": "camikaze007", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qv9v4/what_to_learn_after_these_technologies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qv9v4/what_to_learn_after_these_technologies/", "subreddit_subscribers": 130262, "created_utc": 1695554433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nWe build integrations between ERP and desperate systems for exchange of data. Wondering how this service be monetised and what pricing models can be adopted to offer as a SAAS model. Building integration is not just using existing DIY ETL platforms but need subject matter expertise to understand ERP data especially during transformation. How can infra, technology, manpower, subject matter expertise be clubbed into one pricing model. Can it be data storage+ rows of data extracted and stored? Any ideas is greatly appreciated from the community.", "author_fullname": "t2_b77s3li1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pricing options for Data integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qvb2t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695554543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;We build integrations between ERP and desperate systems for exchange of data. Wondering how this service be monetised and what pricing models can be adopted to offer as a SAAS model. Building integration is not just using existing DIY ETL platforms but need subject matter expertise to understand ERP data especially during transformation. How can infra, technology, manpower, subject matter expertise be clubbed into one pricing model. Can it be data storage+ rows of data extracted and stored? Any ideas is greatly appreciated from the community.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16qvb2t", "is_robot_indexable": true, "report_reasons": null, "author": "srikon", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qvb2t/pricing_options_for_data_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qvb2t/pricing_options_for_data_integration/", "subreddit_subscribers": 130262, "created_utc": 1695554543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Please find the code for my stock data streaming application [here](https://github.com/JawaharRamis/stock-price-analysis-kafka-spark-influxdb-grafana).\n\nThe aim for the project was to familiarize myself with the various components involved and having a meaningful dashboard at the end of it. Kindly share your suggestions/advices. The main thing I am concerned is if I have structured the code in the best way possible or even the deployment setup/configurations. \n\nThis subreddit has been immense during my learning journey and I hope it continues to aid me.", "author_fullname": "t2_67og6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stock market streaming application", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rb1hz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695594977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please find the code for my stock data streaming application &lt;a href=\"https://github.com/JawaharRamis/stock-price-analysis-kafka-spark-influxdb-grafana\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;The aim for the project was to familiarize myself with the various components involved and having a meaningful dashboard at the end of it. Kindly share your suggestions/advices. The main thing I am concerned is if I have structured the code in the best way possible or even the deployment setup/configurations. &lt;/p&gt;\n\n&lt;p&gt;This subreddit has been immense during my learning journey and I hope it continues to aid me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?auto=webp&amp;s=19d09cf64df95948fd765c06fdbe5575d70e25cd", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f65ccd0fb28832b7a761eaeb280d99d175171c2", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9d0ed9c9a8b0a63514689e32dae4436ac7a3164", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2523d023e49863736668af1bd4568e75587fb0a9", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3c55af923d520b5d02f6a51bf2bfed99b9cc943c", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e71295dbcac468ecadca81e9eeacce006477ed8a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/8sxNORt14rMIsKmWvtKK-Ip8jNrYSGXvW9jO5LHY_Jo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=26f4bb51218822e4328b08f224f4b41c2bec4c0f", "width": 1080, "height": 540}], "variants": {}, "id": "PyAZjXFESQEfZE9E-hmQVe5jfp1OTuXS7LpwqNAImpQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "16rb1hz", "is_robot_indexable": true, "report_reasons": null, "author": "jawz96", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rb1hz/stock_market_streaming_application/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rb1hz/stock_market_streaming_application/", "subreddit_subscribers": 130262, "created_utc": 1695594977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to build a batch processing pipeline which transforms CSVs into some common JSON schema. What are good ways of achieving that? \n\nThe pipeline needs to:\n- start a job on signal (perhaps airflow/dagster)\n- process csv file (~10GB)\n- load json into document store\n- perform other tasks eventually, like validation, diffing, notifying, etc.\n\nI\u2019ve tried Apache Beam via Dataflow, using Scala scio, and this seems like a viable option, although maybe not especially user friendly. Documentation and examples are scattered across different SDKs using different languages and if I\u2019m not mistaken, they don\u2019t implement the same functionality. I\u2019d like to use Scala, or a similar statically typed language, as the project complexity will likely grow with time, and it would be great to have it all in one language.\n\nI don\u2019t have a lot of experience with data engineering specifically, so not sure if there are better options out there. What would you recommend? \n\nThank you!", "author_fullname": "t2_hc1vem", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch processing recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r7ths", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695587279.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to build a batch processing pipeline which transforms CSVs into some common JSON schema. What are good ways of achieving that? &lt;/p&gt;\n\n&lt;p&gt;The pipeline needs to:\n- start a job on signal (perhaps airflow/dagster)\n- process csv file (~10GB)\n- load json into document store\n- perform other tasks eventually, like validation, diffing, notifying, etc.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve tried Apache Beam via Dataflow, using Scala scio, and this seems like a viable option, although maybe not especially user friendly. Documentation and examples are scattered across different SDKs using different languages and if I\u2019m not mistaken, they don\u2019t implement the same functionality. I\u2019d like to use Scala, or a similar statically typed language, as the project complexity will likely grow with time, and it would be great to have it all in one language.&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t have a lot of experience with data engineering specifically, so not sure if there are better options out there. What would you recommend? &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16r7ths", "is_robot_indexable": true, "report_reasons": null, "author": "RustinWolf", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16r7ths/batch_processing_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16r7ths/batch_processing_recommendations/", "subreddit_subscribers": 130262, "created_utc": 1695587279.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm start learning the DBT.  One project I want to try is to use DBT Core to implement a particular table in MS SQL DB to a small SQLlite. Currently, I use pure python/pandas to do SCD2. But I found DBT has a snapshot feature that can easily do SCD2 without doing a lot of code.\n\nSome questions: is that possible to use DBT to move data from one platform (MS SQL) to another one (SQL Lite), or it has to be the same platform.\n\nI went to some tutorials but I found that in the YAML file, it only defines the parameter of the source (MSSQL), I can't find where it defines the target (SQLLite). So, I'm not sure if DBT can work with two different platform. \n\nIf you find any tutorials to implement SCD2 from MSSQL to SQLLite, please share me the link.\n\nThank you\n\n&amp;#x200B;", "author_fullname": "t2_ceq9dvcw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "try to use DBT to implement SCD2 from MS SQL to SQLLite", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qoo00", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695530794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m start learning the DBT.  One project I want to try is to use DBT Core to implement a particular table in MS SQL DB to a small SQLlite. Currently, I use pure python/pandas to do SCD2. But I found DBT has a snapshot feature that can easily do SCD2 without doing a lot of code.&lt;/p&gt;\n\n&lt;p&gt;Some questions: is that possible to use DBT to move data from one platform (MS SQL) to another one (SQL Lite), or it has to be the same platform.&lt;/p&gt;\n\n&lt;p&gt;I went to some tutorials but I found that in the YAML file, it only defines the parameter of the source (MSSQL), I can&amp;#39;t find where it defines the target (SQLLite). So, I&amp;#39;m not sure if DBT can work with two different platform. &lt;/p&gt;\n\n&lt;p&gt;If you find any tutorials to implement SCD2 from MSSQL to SQLLite, please share me the link.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16qoo00", "is_robot_indexable": true, "report_reasons": null, "author": "uniznoir", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qoo00/try_to_use_dbt_to_implement_scd2_from_ms_sql_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qoo00/try_to_use_dbt_to_implement_scd2_from_ms_sql_to/", "subreddit_subscribers": 130262, "created_utc": 1695530794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m used to solving data issues related to query tuning , infra scaling , chunking / partitions / shards ect. \n\nBut what to do if you have network issues; I could think making your availability zone closer to data source in cloud or setting up dns. Wondering if anyone else has worked thru this ?", "author_fullname": "t2_xo4dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have data latency issues that rooted back to network issues / conditions. How did you optimize ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qm59h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695522497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m used to solving data issues related to query tuning , infra scaling , chunking / partitions / shards ect. &lt;/p&gt;\n\n&lt;p&gt;But what to do if you have network issues; I could think making your availability zone closer to data source in cloud or setting up dns. Wondering if anyone else has worked thru this ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16qm59h", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qm59h/anyone_have_data_latency_issues_that_rooted_back/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qm59h/anyone_have_data_latency_issues_that_rooted_back/", "subreddit_subscribers": 130262, "created_utc": 1695522497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a 2 part interview that I would like this community's advice on.  \n\n1st part, Architecture/design/problem solving:\n\nThey mainly use batch processing for their data ingestion.  I know they work with 3rd party systems to extract data from and normalize to a schema. My experience lies in aws, lambda functions,  s3, snowflake etc. But I want to be prepared to answer outside of that if I need to.  What should I prepare for?\n\n2nd part, talk with upstream data scientist:\n\nI've never worked along side a data scientist before.  What makes a good relationship with one and how can I show that in a casual conversation with them?\n\nThanks!", "author_fullname": "t2_1afmkbx9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Architecture interview, and DS stakeholder interview prep.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rcxjg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695600008.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 2 part interview that I would like this community&amp;#39;s advice on.  &lt;/p&gt;\n\n&lt;p&gt;1st part, Architecture/design/problem solving:&lt;/p&gt;\n\n&lt;p&gt;They mainly use batch processing for their data ingestion.  I know they work with 3rd party systems to extract data from and normalize to a schema. My experience lies in aws, lambda functions,  s3, snowflake etc. But I want to be prepared to answer outside of that if I need to.  What should I prepare for?&lt;/p&gt;\n\n&lt;p&gt;2nd part, talk with upstream data scientist:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve never worked along side a data scientist before.  What makes a good relationship with one and how can I show that in a casual conversation with them?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer \u200d\u2699\ufe0f", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16rcxjg", "is_robot_indexable": true, "report_reasons": null, "author": "w_savage", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16rcxjg/architecture_interview_and_ds_stakeholder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rcxjg/architecture_interview_and_ds_stakeholder/", "subreddit_subscribers": 130262, "created_utc": 1695600008.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm currently developing event-driven data pipelines with this setup:\n\n1. Files land in an S3 bucket.\n2. A Lambda function is triggered.\n3. This Lambda function triggers a Step Function, which includes several Glue jobs.\n\nOur pipeline processes daily files, but I'm concerned about what happens if the workflow encounters a failure. How can I implement an automatic self-healing mechanism to ensure it picks up from where it left off in case of a failure?\n\nThanks!", "author_fullname": "t2_ab39evfw0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Self healing event-driven pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qqt1u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695538399.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m currently developing event-driven data pipelines with this setup:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Files land in an S3 bucket.&lt;/li&gt;\n&lt;li&gt;A Lambda function is triggered.&lt;/li&gt;\n&lt;li&gt;This Lambda function triggers a Step Function, which includes several Glue jobs.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Our pipeline processes daily files, but I&amp;#39;m concerned about what happens if the workflow encounters a failure. How can I implement an automatic self-healing mechanism to ensure it picks up from where it left off in case of a failure?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16qqt1u", "is_robot_indexable": true, "report_reasons": null, "author": "Human-Background-47", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16qqt1u/self_healing_eventdriven_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16qqt1u/self_healing_eventdriven_pipeline/", "subreddit_subscribers": 130262, "created_utc": 1695538399.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}