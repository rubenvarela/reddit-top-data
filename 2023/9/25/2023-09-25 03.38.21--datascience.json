{"kind": "Listing", "data": {"after": "t3_16r7elg", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'll be having a job interview in a few days and I think this question might come up and I personally don't know how to explain it to a layperson. Black box methods may come in handy one day, but I realized just now that I can't briefly explain how it works without making it sound like magic. What's your workaround for this? Have you been in a situation where you presented your results and you had to explain how neural networks operate in detail? Any similar experiences?", "author_fullname": "t2_bmbqthh3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For people in the industry, how do you explain the poor interpretability of some ML techniques to bosses who are not data scientists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qtywk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 82, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 82, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695549905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll be having a job interview in a few days and I think this question might come up and I personally don&amp;#39;t know how to explain it to a layperson. Black box methods may come in handy one day, but I realized just now that I can&amp;#39;t briefly explain how it works without making it sound like magic. What&amp;#39;s your workaround for this? Have you been in a situation where you presented your results and you had to explain how neural networks operate in detail? Any similar experiences?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qtywk", "is_robot_indexable": true, "report_reasons": null, "author": "krabbypatty-o-fish", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qtywk/for_people_in_the_industry_how_do_you_explain_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qtywk/for_people_in_the_industry_how_do_you_explain_the/", "subreddit_subscribers": 1059745, "created_utc": 1695549905.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Often you hear people saying that understanding the inner workings of models and algorithms is irrelevant and a waste of time. I am currently a MS student and struggle to understand some of the inner workings of things such as M-estimation for robust regression and I believe it\u2019s due to my poor statical background (CS undergrad). \n\nShould I put the time into going back and getting proper statistical credentials (I want to frankly) or is it a nice to have that isnt worth the time and money?", "author_fullname": "t2_pdclzxln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Poor statistical/Linear Algebra foundation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r1881", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695571158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Often you hear people saying that understanding the inner workings of models and algorithms is irrelevant and a waste of time. I am currently a MS student and struggle to understand some of the inner workings of things such as M-estimation for robust regression and I believe it\u2019s due to my poor statical background (CS undergrad). &lt;/p&gt;\n\n&lt;p&gt;Should I put the time into going back and getting proper statistical credentials (I want to frankly) or is it a nice to have that isnt worth the time and money?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r1881", "is_robot_indexable": true, "report_reasons": null, "author": "LongjumpingWheel11", "discussion_type": null, "num_comments": 28, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r1881/poor_statisticallinear_algebra_foundation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r1881/poor_statisticallinear_algebra_foundation/", "subreddit_subscribers": 1059745, "created_utc": 1695571158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been working in a data science Consulting startup as a data scientist. All I've done is write sql tables. I've started job hunting. I want to build AI products. What job description would that be? I know this sounds stupid but I don't want to be an analyst anymore", "author_fullname": "t2_5gr8xljt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do data scientists do anyway?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r5v0j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695582446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working in a data science Consulting startup as a data scientist. All I&amp;#39;ve done is write sql tables. I&amp;#39;ve started job hunting. I want to build AI products. What job description would that be? I know this sounds stupid but I don&amp;#39;t want to be an analyst anymore&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r5v0j", "is_robot_indexable": true, "report_reasons": null, "author": "wonko_the_sane__", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r5v0j/what_do_data_scientists_do_anyway/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r5v0j/what_do_data_scientists_do_anyway/", "subreddit_subscribers": 1059745, "created_utc": 1695582446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I've been assigned an AI/ML project, and I've identified that the data quality is not good. It's within a large organization, which makes it challenging to find a straightforward solution to the data quality problem. Personally, I'm feeling uncomfortable about proceeding further. Interestingly, my manager and other colleagues don't seem to share the same level of concern as I do. They are more inclined to continue the project and generate \"output\". Their primary worried about what to delivery to CIO. Given this situation, what would I do in my place?", "author_fullname": "t2_5fbmh3va", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you do when data quality is bad?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ra88t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695592973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been assigned an AI/ML project, and I&amp;#39;ve identified that the data quality is not good. It&amp;#39;s within a large organization, which makes it challenging to find a straightforward solution to the data quality problem. Personally, I&amp;#39;m feeling uncomfortable about proceeding further. Interestingly, my manager and other colleagues don&amp;#39;t seem to share the same level of concern as I do. They are more inclined to continue the project and generate &amp;quot;output&amp;quot;. Their primary worried about what to delivery to CIO. Given this situation, what would I do in my place?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ra88t", "is_robot_indexable": true, "report_reasons": null, "author": "Excellent_Cost170", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ra88t/what_do_you_do_when_data_quality_is_bad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ra88t/what_do_you_do_when_data_quality_is_bad/", "subreddit_subscribers": 1059745, "created_utc": 1695592973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I browse through post often and general have limited understanding of what is being discussed. I can understand the very basics but the more indepth the convo goes the less I'm able to follow.\n\nI currently work as a BI developer, use SQL quite a bit and PowerBI, power automate.\n\nMy goal is to eventually dive into Data Science. At this time I would say I am no where close to being ready. I am enrolled amd planning to start Georgia Tech OMSA this upcoming spring. My question is.. did academics prepare you adequately to where you're able converse and function well as a DS?\n\nTIA!", "author_fullname": "t2_m8a5u0h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did academics prepare you for your role, or for the DS/ML/AI field?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qxfo0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695561104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I browse through post often and general have limited understanding of what is being discussed. I can understand the very basics but the more indepth the convo goes the less I&amp;#39;m able to follow.&lt;/p&gt;\n\n&lt;p&gt;I currently work as a BI developer, use SQL quite a bit and PowerBI, power automate.&lt;/p&gt;\n\n&lt;p&gt;My goal is to eventually dive into Data Science. At this time I would say I am no where close to being ready. I am enrolled amd planning to start Georgia Tech OMSA this upcoming spring. My question is.. did academics prepare you adequately to where you&amp;#39;re able converse and function well as a DS?&lt;/p&gt;\n\n&lt;p&gt;TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qxfo0", "is_robot_indexable": true, "report_reasons": null, "author": "AwkWORD47", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qxfo0/did_academics_prepare_you_for_your_role_or_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qxfo0/did_academics_prepare_you_for_your_role_or_for/", "subreddit_subscribers": 1059745, "created_utc": 1695561104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Enhancing your data analysis performance with Python's Numexpr and Pandas' eval/query functions \n\n This article was originally published on my personal blog [Data Leads Future.](https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/) \n\n&amp;#x200B;\n\n[ Use Numexpr to help me find the most livable city. Photo Credit: Created by Author, Canva ](https://preview.redd.it/29ec8ukgr5qb1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=238da4465da17bd301b49112d574444b16d4113c)\n\n This article will introduce you to the Python library [Numexpr](https://numexpr.readthedocs.io/en/latest/intro.html?ref=dataleadsfuture.com#), a tool that boosts the computational performance of Numpy Arrays. The eval and query methods of Pandas are also based on this library.\n\n This article also includes a hands-on weather data analysis project. \n\n By reading this article, you will understand the principles of Numexpr and how to use this powerful tool to speed up your calculations in reality. \n\n# Introduction \n\n# Recalling Numpy Arrays\n\n In a previous article discussing Numpy Arrays, I used a library example to explain why Numpy's Cache Locality is so efficient: \n\n[https://www.dataleadsfuture.com/python-lists-vs-numpy-arrays-a-deep-dive-into-memory-layout-and-performance-benefits/](https://www.dataleadsfuture.com/python-lists-vs-numpy-arrays-a-deep-dive-into-memory-layout-and-performance-benefits/)\n\n Each time you go to the library to search for materials, you take out a few books related to the content and place them next to your desk. \n\n This way, you can quickly check related materials without having to run to the shelf each time you need to read a book. \n\n This method saves a lot of time, especially when you need to consult many related books. \n\n In this scenario, the shelf is like your memory, the desk is equivalent to the CPU's L1 cache, and you, the reader, are the CPU's core. \n\n&amp;#x200B;\n\n[ When the CPU accesses RAM, the cache loads the entire cache line into the high-speed cache. Image by Author ](https://preview.redd.it/3k7gdxywr5qb1.png?width=625&amp;format=png&amp;auto=webp&amp;s=5e6a2ed6620cf23638d9c3f20e5f6f46a54e0049)\n\n### The limitations of Numpy\n\n Suppose you are unfortunate enough to encounter a demanding professor who wants you to take out Shakespeare and Tolstoy's works for a cross-comparison. \n\n At this point, taking out related books in advance will not work well. \n\n First, your desk space is limited and cannot hold all the books of these two masters at the same time, not to mention the reading notes that will be generated during the comparison process. \n\n Second, you're just one person, and comparing so many works would take too long. It would be nice if you could find a few more people to help. \n\n This is the current situation when we use Numpy to deal with large amounts of data: \n\n* The number of elements in the Array is too large to fit into the CPU's L1 cache.\n* Numpy's element-level operations are single-threaded and cannot utilize the computing power of multi-core CPUs.\n\n What should we do? \n\n Don't worry. When you really encounter a problem with too much data, you can call on our protagonist today, Numexpr, to help. \n\n## Understanding Numexpr: What and Why\n\n### How it works\n\n When Numpy encounters large arrays, element-wise calculations will experience two extremes. \n\n Let me give you an example to illustrate. Suppose there are two large Numpy ndarrays: \n\n    import numpy as np \n    import numexpr as ne  \n    \n    a = np.random.rand(100_000_000) \n    b = np.random.rand(100_000_000)\n\n When calculating the result of the expression a\\*\\*5 + 2 \\* b, there are generally two methods:\n\n One way is Numpy's vectorized calculation method, which uses two temporary arrays to store the results of a\\*\\*5 and 2\\*b separately.  \n\n    In: %timeit a**5 + 2 * b\n    \n    Out:2.11 s \u00b1 31.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n At this time, you have four arrays in your memory: a, b, a\\*\\*5, and 2 \\* b. This method will cause a lot of memory waste. \n\n Moreover, since each Array's size exceeds the CPU cache's capacity, it cannot use it well. \n\n Another way is to traverse each element in two arrays and calculate them separately. \n\n    c = np.empty(100_000_000, dtype=np.uint32)\n    \n    def calcu_elements(a, b, c):\n        for i in range(0, len(a), 1):\n            c[i] = a[i] ** 5 + 2 * b[i]\n            \n    %timeit calcu_elements(a, b, c)\n    \n    \n    Out: 24.6 s \u00b1 48.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n This method performs even worse. The calculation will be very slow because it cannot use vectorized calculations and only partially utilize the CPU cache. \n\n### Numexpr's calculation\n\n Numexpr commonly uses only one evaluate method. This method will receive an expression string each time and then compile it into bytecode using Python's compile method. \n\n Numexpr also has a virtual machine program. The virtual machine contains multiple vector registers, each using a chunk size of 4096. \n\n When Numexpr starts to calculate, it sends the data in one or more registers to the CPU's L1 cache each time. This way, there won't be a situation where the memory is too slow, and the CPU waits for data. \n\n At the same time, Numexpr's virtual machine is written in C, removing Python's GIL. It can utilize the computing power of multi-core CPUs. \n\n So, Numexpr is faster when calculating large arrays than using Numpy alone. We can make a comparison: \n\n    In:  %timeit ne.evaluate('a**5 + 2 * b')\n    Out: 258 ms \u00b1 14.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n### Summary of Numexpr's working principle\n\n Let's summarize the working principle of Numexpr and see why Numexpr is so fast: \n\n **Executing bytecode through a virtual machine.** Numexpr uses bytecode to execute expressions, which can fully utilize the [branch prediction](https://en.wikipedia.org/wiki/Branch_predictor?ref=dataleadsfuture.com) ability of the CPU, which is faster than using Python expressions. \n\n **Vectorized calculation.** Numexpr will use [SIMD (Single Instruction, Multiple Data)](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data?ref=dataleadsfuture.com) technology to improve computing efficiency significantly for the same operation on the data in each register. \n\n **Multi-core parallel computing.** Numexpr's virtual machine can decompose each task into multiple subtasks. They are executed in parallel on multiple CPU cores. \n\n **Less memory usage.** Unlike Numpy, which needs to generate intermediate arrays, Numexpr only loads a small amount of data when necessary, significantly reducing memory usage. \n\n&amp;#x200B;\n\n[ Workflow diagram of Numexpr. Image by Author ](https://preview.redd.it/46plaxk6t5qb1.png?width=863&amp;format=png&amp;auto=webp&amp;s=8c0bb7311c4fc743d720c2e0cec3c28c1ff98ed3)\n\n## Numexpr and Pandas: A Powerful Combination\n\n You might be wondering: We usually do data analysis with pandas. I understand the performance improvements Numexpr offers for Numpy, but does it have the same improvement for Pandas? \n\n The answer is Yes. \n\n The eval and query methods in pandas are implemented based on Numexpr. Let's look at some examples: \n\n### Pandas.eval for Cross-DataFrame operations\n\n When you have multiple pandas DataFrames, you can use pandas.eval to perform operations between DataFrame objects, for example: \n\n    import pandas as pd\n    \n    nrows, ncols = 1_000_000, 100\n    df1, df2, df3, df4 = (pd.DataFrame(rng.random((nrows, ncols))) for i in range(4))\n\n If you calculate the sum of these DataFrames using the traditional pandas method, the time consumed is: \n\n    In:  %timeit df1+df2+df3+df4\n    Out: 1.18 s \u00b1 65.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n You can also use pandas.eval for calculation. The time consumed is: \n\n The calculation of the eval version can improve performance by 50%, and the results are precisely the same: \n\n    In:  np.allclose(df1+df2+df3+df4, pd.eval('df1+df2+df3+df4'))\n    Out: True\n\n### DataFrame.eval for column-level operations\n\n Just like pandas.eval, DataFrame also has its own eval method. We can use this method for column-level operations within DataFrame, for example: \n\n    df = pd.DataFrame(rng.random((1000, 3)), columns=['A', 'B', 'C'])\n    \n    result1 = (df['A'] + df['B']) / (df['C'] - 1)\n    result2 = df.eval('(A + B) / (C - 1)')\n\n The results of using the traditional pandas method and the eval method are precisely the same: \n\n    In:  np.allclose(result1, result2)\n    Out: True\n\n Of course, you can also directly use the eval expression to add new columns to the DataFrame, which is very convenient: \n\n    df.eval('D = (A + B) / C', inplace=True)\n    df.head()\n\n[ Directly use the eval expression to add new columns. Image by Author ](https://preview.redd.it/ykotgj0ut5qb1.png?width=495&amp;format=png&amp;auto=webp&amp;s=f005b5782e751f2f5ea90fcddd8169cfcc2c5751)\n\n### Using DataFrame.query to quickly find data\n\n If the eval method of DataFrame executes comparison expressions, the returned result is a boolean result that meets the conditions. You need to use Mask Indexing to get the desired data: \n\n    mask = df.eval('(A &lt; 0.5) &amp; (B &lt; 0.5)')\n    result1 = df[mask]\n    result\n\n[ When filtering data only with DataFrame.query, it is necessary to use a boolean mask. Image by Author ](https://preview.redd.it/izwngwizt5qb1.png?width=469&amp;format=png&amp;auto=webp&amp;s=c7f0e4806977ccdcf3c588a67a6c913255f47091)\n\n The DataFrame.query method encapsulates this process, and you can directly obtain the desired data with the query method: \n\n    In:   result2 = df.query('A &lt; 0.5 and B &lt; 0.5')\n          np.allclose(result1, result2)\n    Out:  True\n\n When you need to use scalars in expressions, you can use the @  to indicate: \n\n    In:  Cmean = df['C'].mean()\n         result1 = df[(df.A &lt; Cmean) &amp; (df.B &lt; Cmean)]\n         result2 = df.query('A &lt; @Cmean and B &lt; @Cmean')\n         np.allclose(result1, result2)\n    Out: True\n\n This article was originally published on my personal blog [Data Leads Future.](https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/) ", "author_fullname": "t2_9r8ft2a0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring Numexpr: A Powerful Engine Behind Pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ykotgj0ut5qb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 39, "x": 108, "u": "https://preview.redd.it/ykotgj0ut5qb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45b2ace70cdc23065c16c4c7ecb49451067d5824"}, {"y": 78, "x": 216, "u": "https://preview.redd.it/ykotgj0ut5qb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ae1f3d972bbe64cc07341d2a7e32e66da793a7f"}, {"y": 115, "x": 320, "u": "https://preview.redd.it/ykotgj0ut5qb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=04db3f2e444bdb6b7cb6c9558407dc42561b74f2"}], "s": {"y": 179, "x": 495, "u": "https://preview.redd.it/ykotgj0ut5qb1.png?width=495&amp;format=png&amp;auto=webp&amp;s=f005b5782e751f2f5ea90fcddd8169cfcc2c5751"}, "id": "ykotgj0ut5qb1"}, "izwngwizt5qb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/izwngwizt5qb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2dadbb151f50fa5dbfbf14272d1860c079d94f89"}, {"y": 133, "x": 216, "u": "https://preview.redd.it/izwngwizt5qb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=184931d673c0227709ad80639f3cd20da70977d4"}, {"y": 197, "x": 320, "u": "https://preview.redd.it/izwngwizt5qb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ce4336dee4ce5c9bbedc261e3a277e72d6fa47e"}], "s": {"y": 289, "x": 469, "u": "https://preview.redd.it/izwngwizt5qb1.png?width=469&amp;format=png&amp;auto=webp&amp;s=c7f0e4806977ccdcf3c588a67a6c913255f47091"}, "id": "izwngwizt5qb1"}, "3k7gdxywr5qb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 45, "x": 108, "u": "https://preview.redd.it/3k7gdxywr5qb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a502f085a43ef5ac8f1f50599161b6d8ccfe1dc5"}, {"y": 91, "x": 216, "u": "https://preview.redd.it/3k7gdxywr5qb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fedccce3dc9866fbb52b62237eb1d362917e2eb3"}, {"y": 135, "x": 320, "u": "https://preview.redd.it/3k7gdxywr5qb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=77080ec0cc0d4b7202892c1035780ef145694726"}], "s": {"y": 264, "x": 625, "u": "https://preview.redd.it/3k7gdxywr5qb1.png?width=625&amp;format=png&amp;auto=webp&amp;s=5e6a2ed6620cf23638d9c3f20e5f6f46a54e0049"}, "id": "3k7gdxywr5qb1"}, "46plaxk6t5qb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/46plaxk6t5qb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d31e4cb8f3f2da53d3c291af37ecc489c082e5d"}, {"y": 152, "x": 216, "u": "https://preview.redd.it/46plaxk6t5qb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b292a546e21eea92bccc123befff21806f751ad"}, {"y": 226, "x": 320, "u": "https://preview.redd.it/46plaxk6t5qb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e63cb3e19ed9c368e8e297b1842854b459247ac6"}, {"y": 453, "x": 640, "u": "https://preview.redd.it/46plaxk6t5qb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=059bc7da3ab967ae8670b846e24da717ccfa6bb0"}], "s": {"y": 611, "x": 863, "u": "https://preview.redd.it/46plaxk6t5qb1.png?width=863&amp;format=png&amp;auto=webp&amp;s=8c0bb7311c4fc743d720c2e0cec3c28c1ff98ed3"}, "id": "46plaxk6t5qb1"}, "29ec8ukgr5qb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=827a05e29b961a5494a0a7bef09e05406e2e2d0e"}, {"y": 143, "x": 216, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7eda37aee46f7ac0e032a1d6986c84364e634243"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c421de1631ab305d9297cb110c5bc7e6f8fa84ee"}, {"y": 426, "x": 640, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=56aac32c3c80921dcb8d1294b3284c961c3fc4f4"}, {"y": 639, "x": 960, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4bf6231e0d1b715918aa8904b0dc7a8bdae05633"}, {"y": 719, "x": 1080, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a94c2abbd9a3ade8f23648b82905877f75b5bede"}], "s": {"y": 799, "x": 1200, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=238da4465da17bd301b49112d574444b16d4113c"}, "id": "29ec8ukgr5qb1"}}, "name": "t3_16qrxs4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ARL0vsBWFf7vstunNTahhEgCBgMGr53RAUJlwDu0dAc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1695542470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Enhancing your data analysis performance with Python&amp;#39;s Numexpr and Pandas&amp;#39; eval/query functions &lt;/p&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/\"&gt;Data Leads Future.&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/29ec8ukgr5qb1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=238da4465da17bd301b49112d574444b16d4113c\"&gt; Use Numexpr to help me find the most livable city. Photo Credit: Created by Author, Canva &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This article will introduce you to the Python library &lt;a href=\"https://numexpr.readthedocs.io/en/latest/intro.html?ref=dataleadsfuture.com#\"&gt;Numexpr&lt;/a&gt;, a tool that boosts the computational performance of Numpy Arrays. The eval and query methods of Pandas are also based on this library.&lt;/p&gt;\n\n&lt;p&gt;This article also includes a hands-on weather data analysis project. &lt;/p&gt;\n\n&lt;p&gt;By reading this article, you will understand the principles of Numexpr and how to use this powerful tool to speed up your calculations in reality. &lt;/p&gt;\n\n&lt;h1&gt;Introduction&lt;/h1&gt;\n\n&lt;h1&gt;Recalling Numpy Arrays&lt;/h1&gt;\n\n&lt;p&gt;In a previous article discussing Numpy Arrays, I used a library example to explain why Numpy&amp;#39;s Cache Locality is so efficient: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.dataleadsfuture.com/python-lists-vs-numpy-arrays-a-deep-dive-into-memory-layout-and-performance-benefits/\"&gt;https://www.dataleadsfuture.com/python-lists-vs-numpy-arrays-a-deep-dive-into-memory-layout-and-performance-benefits/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Each time you go to the library to search for materials, you take out a few books related to the content and place them next to your desk. &lt;/p&gt;\n\n&lt;p&gt;This way, you can quickly check related materials without having to run to the shelf each time you need to read a book. &lt;/p&gt;\n\n&lt;p&gt;This method saves a lot of time, especially when you need to consult many related books. &lt;/p&gt;\n\n&lt;p&gt;In this scenario, the shelf is like your memory, the desk is equivalent to the CPU&amp;#39;s L1 cache, and you, the reader, are the CPU&amp;#39;s core. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3k7gdxywr5qb1.png?width=625&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e6a2ed6620cf23638d9c3f20e5f6f46a54e0049\"&gt; When the CPU accesses RAM, the cache loads the entire cache line into the high-speed cache. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;The limitations of Numpy&lt;/h3&gt;\n\n&lt;p&gt;Suppose you are unfortunate enough to encounter a demanding professor who wants you to take out Shakespeare and Tolstoy&amp;#39;s works for a cross-comparison. &lt;/p&gt;\n\n&lt;p&gt;At this point, taking out related books in advance will not work well. &lt;/p&gt;\n\n&lt;p&gt;First, your desk space is limited and cannot hold all the books of these two masters at the same time, not to mention the reading notes that will be generated during the comparison process. &lt;/p&gt;\n\n&lt;p&gt;Second, you&amp;#39;re just one person, and comparing so many works would take too long. It would be nice if you could find a few more people to help. &lt;/p&gt;\n\n&lt;p&gt;This is the current situation when we use Numpy to deal with large amounts of data: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The number of elements in the Array is too large to fit into the CPU&amp;#39;s L1 cache.&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Numpy&amp;#39;s element-level operations are single-threaded and cannot utilize the computing power of multi-core CPUs.&lt;/p&gt;\n\n&lt;p&gt;What should we do? &lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t worry. When you really encounter a problem with too much data, you can call on our protagonist today, Numexpr, to help. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Understanding Numexpr: What and Why&lt;/h2&gt;\n\n&lt;h3&gt;How it works&lt;/h3&gt;\n\n&lt;p&gt;When Numpy encounters large arrays, element-wise calculations will experience two extremes. &lt;/p&gt;\n\n&lt;p&gt;Let me give you an example to illustrate. Suppose there are two large Numpy ndarrays: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import numpy as np \nimport numexpr as ne  \n\na = np.random.rand(100_000_000) \nb = np.random.rand(100_000_000)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;When calculating the result of the expression a**5 + 2 * b, there are generally two methods:&lt;/p&gt;\n\n&lt;p&gt;One way is Numpy&amp;#39;s vectorized calculation method, which uses two temporary arrays to store the results of a**5 and 2*b separately.  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In: %timeit a**5 + 2 * b\n\nOut:2.11 s \u00b1 31.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;At this time, you have four arrays in your memory: a, b, a**5, and 2 * b. This method will cause a lot of memory waste. &lt;/p&gt;\n\n&lt;p&gt;Moreover, since each Array&amp;#39;s size exceeds the CPU cache&amp;#39;s capacity, it cannot use it well. &lt;/p&gt;\n\n&lt;p&gt;Another way is to traverse each element in two arrays and calculate them separately. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;c = np.empty(100_000_000, dtype=np.uint32)\n\ndef calcu_elements(a, b, c):\n    for i in range(0, len(a), 1):\n        c[i] = a[i] ** 5 + 2 * b[i]\n\n%timeit calcu_elements(a, b, c)\n\n\nOut: 24.6 s \u00b1 48.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This method performs even worse. The calculation will be very slow because it cannot use vectorized calculations and only partially utilize the CPU cache. &lt;/p&gt;\n\n&lt;h3&gt;Numexpr&amp;#39;s calculation&lt;/h3&gt;\n\n&lt;p&gt;Numexpr commonly uses only one evaluate method. This method will receive an expression string each time and then compile it into bytecode using Python&amp;#39;s compile method. &lt;/p&gt;\n\n&lt;p&gt;Numexpr also has a virtual machine program. The virtual machine contains multiple vector registers, each using a chunk size of 4096. &lt;/p&gt;\n\n&lt;p&gt;When Numexpr starts to calculate, it sends the data in one or more registers to the CPU&amp;#39;s L1 cache each time. This way, there won&amp;#39;t be a situation where the memory is too slow, and the CPU waits for data. &lt;/p&gt;\n\n&lt;p&gt;At the same time, Numexpr&amp;#39;s virtual machine is written in C, removing Python&amp;#39;s GIL. It can utilize the computing power of multi-core CPUs. &lt;/p&gt;\n\n&lt;p&gt;So, Numexpr is faster when calculating large arrays than using Numpy alone. We can make a comparison: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:  %timeit ne.evaluate(&amp;#39;a**5 + 2 * b&amp;#39;)\nOut: 258 ms \u00b1 14.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;Summary of Numexpr&amp;#39;s working principle&lt;/h3&gt;\n\n&lt;p&gt;Let&amp;#39;s summarize the working principle of Numexpr and see why Numexpr is so fast: &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Executing bytecode through a virtual machine.&lt;/strong&gt; Numexpr uses bytecode to execute expressions, which can fully utilize the &lt;a href=\"https://en.wikipedia.org/wiki/Branch_predictor?ref=dataleadsfuture.com\"&gt;branch prediction&lt;/a&gt; ability of the CPU, which is faster than using Python expressions. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Vectorized calculation.&lt;/strong&gt; Numexpr will use &lt;a href=\"https://en.wikipedia.org/wiki/Single_instruction,_multiple_data?ref=dataleadsfuture.com\"&gt;SIMD (Single Instruction, Multiple Data)&lt;/a&gt; technology to improve computing efficiency significantly for the same operation on the data in each register. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Multi-core parallel computing.&lt;/strong&gt; Numexpr&amp;#39;s virtual machine can decompose each task into multiple subtasks. They are executed in parallel on multiple CPU cores. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Less memory usage.&lt;/strong&gt; Unlike Numpy, which needs to generate intermediate arrays, Numexpr only loads a small amount of data when necessary, significantly reducing memory usage. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/46plaxk6t5qb1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c0bb7311c4fc743d720c2e0cec3c28c1ff98ed3\"&gt; Workflow diagram of Numexpr. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Numexpr and Pandas: A Powerful Combination&lt;/h2&gt;\n\n&lt;p&gt;You might be wondering: We usually do data analysis with pandas. I understand the performance improvements Numexpr offers for Numpy, but does it have the same improvement for Pandas? &lt;/p&gt;\n\n&lt;p&gt;The answer is Yes. &lt;/p&gt;\n\n&lt;p&gt;The eval and query methods in pandas are implemented based on Numexpr. Let&amp;#39;s look at some examples: &lt;/p&gt;\n\n&lt;h3&gt;Pandas.eval for Cross-DataFrame operations&lt;/h3&gt;\n\n&lt;p&gt;When you have multiple pandas DataFrames, you can use pandas.eval to perform operations between DataFrame objects, for example: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import pandas as pd\n\nnrows, ncols = 1_000_000, 100\ndf1, df2, df3, df4 = (pd.DataFrame(rng.random((nrows, ncols))) for i in range(4))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;If you calculate the sum of these DataFrames using the traditional pandas method, the time consumed is: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:  %timeit df1+df2+df3+df4\nOut: 1.18 s \u00b1 65.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;You can also use pandas.eval for calculation. The time consumed is: &lt;/p&gt;\n\n&lt;p&gt;The calculation of the eval version can improve performance by 50%, and the results are precisely the same: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:  np.allclose(df1+df2+df3+df4, pd.eval(&amp;#39;df1+df2+df3+df4&amp;#39;))\nOut: True\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;DataFrame.eval for column-level operations&lt;/h3&gt;\n\n&lt;p&gt;Just like pandas.eval, DataFrame also has its own eval method. We can use this method for column-level operations within DataFrame, for example: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df = pd.DataFrame(rng.random((1000, 3)), columns=[&amp;#39;A&amp;#39;, &amp;#39;B&amp;#39;, &amp;#39;C&amp;#39;])\n\nresult1 = (df[&amp;#39;A&amp;#39;] + df[&amp;#39;B&amp;#39;]) / (df[&amp;#39;C&amp;#39;] - 1)\nresult2 = df.eval(&amp;#39;(A + B) / (C - 1)&amp;#39;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The results of using the traditional pandas method and the eval method are precisely the same: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:  np.allclose(result1, result2)\nOut: True\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Of course, you can also directly use the eval expression to add new columns to the DataFrame, which is very convenient: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df.eval(&amp;#39;D = (A + B) / C&amp;#39;, inplace=True)\ndf.head()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ykotgj0ut5qb1.png?width=495&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f005b5782e751f2f5ea90fcddd8169cfcc2c5751\"&gt; Directly use the eval expression to add new columns. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;Using DataFrame.query to quickly find data&lt;/h3&gt;\n\n&lt;p&gt;If the eval method of DataFrame executes comparison expressions, the returned result is a boolean result that meets the conditions. You need to use Mask Indexing to get the desired data: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;mask = df.eval(&amp;#39;(A &amp;lt; 0.5) &amp;amp; (B &amp;lt; 0.5)&amp;#39;)\nresult1 = df[mask]\nresult\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/izwngwizt5qb1.png?width=469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7f0e4806977ccdcf3c588a67a6c913255f47091\"&gt; When filtering data only with DataFrame.query, it is necessary to use a boolean mask. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The DataFrame.query method encapsulates this process, and you can directly obtain the desired data with the query method: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:   result2 = df.query(&amp;#39;A &amp;lt; 0.5 and B &amp;lt; 0.5&amp;#39;)\n      np.allclose(result1, result2)\nOut:  True\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;When you need to use scalars in expressions, you can use the @  to indicate: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:  Cmean = df[&amp;#39;C&amp;#39;].mean()\n     result1 = df[(df.A &amp;lt; Cmean) &amp;amp; (df.B &amp;lt; Cmean)]\n     result2 = df.query(&amp;#39;A &amp;lt; @Cmean and B &amp;lt; @Cmean&amp;#39;)\n     np.allclose(result1, result2)\nOut: True\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/\"&gt;Data Leads Future.&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?auto=webp&amp;s=4f4c5a16af6b6d5e954c2bd0d6ec11d253a3f16f", "width": 1387, "height": 924}, "resolutions": [{"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3216385c481210e323283ae6e03a16e14245f2a1", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6a3bbf3191c78cddfbd87af49e76d6667ca85fd", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=202ec313e2baa2ef1670e9cfd24d6c2560d1cd21", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ff3e49cdb7d8d0746f47c590240ce1fc893bc917", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=33dc2fd6c45c6a3cc3cbd721fe9ad6a9f5ab7779", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf7b49d72586c5ef89b6b8f7dc0d9c7de09de4b8", "width": 1080, "height": 719}], "variants": {}, "id": "Bjw_Y7mZr_m-tfiX4fEkjjZrKlokqny6OjPxyctYkDg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qrxs4", "is_robot_indexable": true, "report_reasons": null, "author": "qtalen", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qrxs4/exploring_numexpr_a_powerful_engine_behind_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qrxs4/exploring_numexpr_a_powerful_engine_behind_pandas/", "subreddit_subscribers": 1059745, "created_utc": 1695542470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I find it to be a very useful tool.  However, only my current employer is using it.  If I change jobs, I won't have access to Alteryx anymore and my skill with deteriorate with non-use.     \nIt is unlike Excel where I'm sharpening my skills with it every place I work.  Any software is worth learning for your career, but we humans tend to forget knowledge what don't use.  Or I do at least, maybe it is just me.  Your thoughts?  \n", "author_fullname": "t2_duzivvbs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Alteryx a practical skill to learn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r9lfj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695591476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I find it to be a very useful tool.  However, only my current employer is using it.  If I change jobs, I won&amp;#39;t have access to Alteryx anymore and my skill with deteriorate with non-use.&lt;br/&gt;\nIt is unlike Excel where I&amp;#39;m sharpening my skills with it every place I work.  Any software is worth learning for your career, but we humans tend to forget knowledge what don&amp;#39;t use.  Or I do at least, maybe it is just me.  Your thoughts?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r9lfj", "is_robot_indexable": true, "report_reasons": null, "author": "How_Much2", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r9lfj/is_alteryx_a_practical_skill_to_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r9lfj/is_alteryx_a_practical_skill_to_learn/", "subreddit_subscribers": 1059745, "created_utc": 1695591476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello guys, I needed some assistance with Forecasting a Time Series dataset using Neural Nets.\n\nFor context, the data has an annual seasonality with daily records of upto 5 years. Previously, I had used Statistical Methods like TBATS and DHR along with GARCH to transform and forecast.\n\nNow, I have been attempting to do the same using RNNs. I started off with GRU and LSTM with a few Hyperparameter Optimization Algorithms for the optimum depth, lookback window etc. only to end up with an eggregious yield of approx 10 MAPE.\n\nThus, I'd appreciate your input as to what can I improve be it an industry practise, some other insightful articles on the same or even alternative ML approaches!\n\nThanks in advance :)", "author_fullname": "t2_j6sk2g9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time Series Forecasting using RNN", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qnryv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695527752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I needed some assistance with Forecasting a Time Series dataset using Neural Nets.&lt;/p&gt;\n\n&lt;p&gt;For context, the data has an annual seasonality with daily records of upto 5 years. Previously, I had used Statistical Methods like TBATS and DHR along with GARCH to transform and forecast.&lt;/p&gt;\n\n&lt;p&gt;Now, I have been attempting to do the same using RNNs. I started off with GRU and LSTM with a few Hyperparameter Optimization Algorithms for the optimum depth, lookback window etc. only to end up with an eggregious yield of approx 10 MAPE.&lt;/p&gt;\n\n&lt;p&gt;Thus, I&amp;#39;d appreciate your input as to what can I improve be it an industry practise, some other insightful articles on the same or even alternative ML approaches!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qnryv", "is_robot_indexable": true, "report_reasons": null, "author": "magic_groovin", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qnryv/time_series_forecasting_using_rnn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qnryv/time_series_forecasting_using_rnn/", "subreddit_subscribers": 1059745, "created_utc": 1695527752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi Folks,\n\nFor context:\n\nI've been tasked with planning capacity for our helpdesk team at work.  There is it total 4 agents and they solve \\~50 tickets a month.  The business question I have been tasked with answering is \"At what point do we hire more agents.\n\nI have access to the data from Tableau for both tickets closed and created over a 2 year period, these are however separate data sets so will need to merge them together. \n\nI'm currently now familiar with any forecasting models but from my research might select ARIMA.\n\nAny advice on how to tackle this? currently also using chatgpt for some brainstorming. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_b3b9wr8r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "capacity forecasting planning advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r00gq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695568135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks,&lt;/p&gt;\n\n&lt;p&gt;For context:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been tasked with planning capacity for our helpdesk team at work.  There is it total 4 agents and they solve ~50 tickets a month.  The business question I have been tasked with answering is &amp;quot;At what point do we hire more agents.&lt;/p&gt;\n\n&lt;p&gt;I have access to the data from Tableau for both tickets closed and created over a 2 year period, these are however separate data sets so will need to merge them together. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently now familiar with any forecasting models but from my research might select ARIMA.&lt;/p&gt;\n\n&lt;p&gt;Any advice on how to tackle this? currently also using chatgpt for some brainstorming. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r00gq", "is_robot_indexable": true, "report_reasons": null, "author": "oaklandcruser", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r00gq/capacity_forecasting_planning_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r00gq/capacity_forecasting_planning_advice/", "subreddit_subscribers": 1059745, "created_utc": 1695568135.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello everyone!\n\nI hope you're all doing well. I'm currently working on fine-tuning my CV for a data scientist/ML Engineer or MLOps (need to acquire more skills in here) position, and I would greatly appreciate your feedback and insights.\n\nI've included all the essential details, but I want to make sure it truly reflects my skills and experience in the best possible way. If you have any tips, suggestions, or even specific areas you think I should focus on, please feel free to share them in the comments below.\n\nHere's the link to my CV: [https://ibb.co/9wL6H4N](https://ibb.co/9wL6H4N) or [https://postimg.cc/LJR3qQ2N](https://postimg.cc/LJR3qQ2N)\n\nThanks a million in advance for your help! Your expertise means a lot to me, and I'm excited to hear your thoughts. \ud83d\ude0a", "author_fullname": "t2_6fp6ab8e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Feedback to Improve my Data Scientist CV", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16re6pa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695605006.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695603459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I hope you&amp;#39;re all doing well. I&amp;#39;m currently working on fine-tuning my CV for a data scientist/ML Engineer or MLOps (need to acquire more skills in here) position, and I would greatly appreciate your feedback and insights.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve included all the essential details, but I want to make sure it truly reflects my skills and experience in the best possible way. If you have any tips, suggestions, or even specific areas you think I should focus on, please feel free to share them in the comments below.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the link to my CV: &lt;a href=\"https://ibb.co/9wL6H4N\"&gt;https://ibb.co/9wL6H4N&lt;/a&gt; or &lt;a href=\"https://postimg.cc/LJR3qQ2N\"&gt;https://postimg.cc/LJR3qQ2N&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks a million in advance for your help! Your expertise means a lot to me, and I&amp;#39;m excited to hear your thoughts. \ud83d\ude0a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JgmUFSu7Phck7U0zM-k96c0WdakVCANBMlS6qjIRR8s.jpg?auto=webp&amp;s=080beac32424633bb1d709ab01ddd272933b5bfb", "width": 1700, "height": 2200}, "resolutions": [{"url": "https://external-preview.redd.it/JgmUFSu7Phck7U0zM-k96c0WdakVCANBMlS6qjIRR8s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9bacb6fc374cfe1cc6f49d6ca98030c0d2025231", "width": 108, "height": 139}, {"url": "https://external-preview.redd.it/JgmUFSu7Phck7U0zM-k96c0WdakVCANBMlS6qjIRR8s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=496fff2f9dc1a1859a69056e874ba77e3fb351b6", "width": 216, "height": 279}, {"url": "https://external-preview.redd.it/JgmUFSu7Phck7U0zM-k96c0WdakVCANBMlS6qjIRR8s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e5f39d2013a02e497f0db6d7b0373edb4d4438c", "width": 320, "height": 414}, {"url": "https://external-preview.redd.it/JgmUFSu7Phck7U0zM-k96c0WdakVCANBMlS6qjIRR8s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8deb66bca79bc2d162008c0340a714a821769ac1", "width": 640, "height": 828}, {"url": "https://external-preview.redd.it/JgmUFSu7Phck7U0zM-k96c0WdakVCANBMlS6qjIRR8s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e490ce9e2c833cc5ed2803466ef03cceb4c915d2", "width": 960, "height": 1242}, {"url": "https://external-preview.redd.it/JgmUFSu7Phck7U0zM-k96c0WdakVCANBMlS6qjIRR8s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1aa922a1ca4f2788b4ad21abe2d2d744043863b8", "width": 1080, "height": 1397}], "variants": {}, "id": "25sW4H8zj0HfTiUS9jHV7peTlZlv4aagFbGWq-76o4Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16re6pa", "is_robot_indexable": true, "report_reasons": null, "author": "BlackLands123", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16re6pa/seeking_feedback_to_improve_my_data_scientist_cv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16re6pa/seeking_feedback_to_improve_my_data_scientist_cv/", "subreddit_subscribers": 1059745, "created_utc": 1695603459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is there a way to calculate jitter or smoothness in motion capture movement, that result in numbers/values? if so, is there any standard to it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_16rgkxa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_cdt81y6f", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8TL8SLdj4BY6EtbmXIJYWQOE1dxhaZaO7zP_xwaYFMY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "computervision", "selftext": "So, i create a real time motion capture system that translate into an animated character movement and add a kalman filter to smooth the captured movement, since it's jitter/shaking so much without it.\n\nthe thing is, i can't find a way to compare the raw and filtered that result in numbers that prove the filtered movement is smoother than the raw movement. only the graphs and character animation that show it is smoother.\n\nthen i try to calculate both of their standard deviation by creating a predefined animation to be compared and it resulted in the following graphs and table.but ultimately, that only result in accuracy, which doesn't show significant difference anyway. that mean the filter did make the movement smoother but not increase it accuracy.\n\nnow my professor ask if maybe there's a standard to how smooth or jittery the movement is? maybe standard in animation, or games.\n\nso far i can't find a way to get the jitter or smoothness values, let alone the standard. can someone deny or confirm if there's way to calculate it and standard for it? or should i just proof the smoothness improvement with only the graphs?\n\nthanks\n\nhttps://preview.redd.it/xmy78l6gbbqb1.png?width=5693&amp;format=png&amp;auto=webp&amp;s=dde3f9801662a59670c867d111c5e12fd7dd96bf\n\nhttps://preview.redd.it/uveapdedcbqb1.png?width=849&amp;format=png&amp;auto=webp&amp;s=485dd8a5714987110b0b14bdc6de21e12d74cbb5\n\n&amp;#x200B;", "author_fullname": "t2_cdt81y6f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is there a way to calculate jitter or smoothness in motion capture movement, that result in numbers/values? if so, is there any standard to it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/computervision", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"xmy78l6gbbqb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 129, "x": 108, "u": "https://preview.redd.it/xmy78l6gbbqb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=efe73ddd0ffa5a8a6415394ee0412f68d0137cd8"}, {"y": 258, "x": 216, "u": "https://preview.redd.it/xmy78l6gbbqb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ee2ea2dea15893929c966edfb3a5a9feffbdea1"}, {"y": 383, "x": 320, "u": "https://preview.redd.it/xmy78l6gbbqb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d66b363ad2544fa687807ffb92723c9e812ed1e8"}, {"y": 766, "x": 640, "u": "https://preview.redd.it/xmy78l6gbbqb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4dc6bc0ebbec7c56f02a88d3153f29e8c4333e37"}, {"y": 1149, "x": 960, "u": "https://preview.redd.it/xmy78l6gbbqb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa23be98c1663a3fc30e38a43828b917de74b80d"}, {"y": 1292, "x": 1080, "u": "https://preview.redd.it/xmy78l6gbbqb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0138b36d29587aac58a730e27d0f3d81ed4b5928"}], "s": {"y": 6814, "x": 5693, "u": "https://preview.redd.it/xmy78l6gbbqb1.png?width=5693&amp;format=png&amp;auto=webp&amp;s=dde3f9801662a59670c867d111c5e12fd7dd96bf"}, "id": "xmy78l6gbbqb1"}, "uveapdedcbqb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 41, "x": 108, "u": "https://preview.redd.it/uveapdedcbqb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=50c12036aef4dd789404178bdc12623256e6ca26"}, {"y": 82, "x": 216, "u": "https://preview.redd.it/uveapdedcbqb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f8645efd54675d7d66cb5b4b0e3cc247af884c2"}, {"y": 122, "x": 320, "u": "https://preview.redd.it/uveapdedcbqb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1a7eed1103fa6504230edaef3a8b0d1f2ece6fb"}, {"y": 244, "x": 640, "u": "https://preview.redd.it/uveapdedcbqb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6ed794675dccefbe06afc84fdd9c45be52993c96"}], "s": {"y": 325, "x": 849, "u": "https://preview.redd.it/uveapdedcbqb1.png?width=849&amp;format=png&amp;auto=webp&amp;s=485dd8a5714987110b0b14bdc6de21e12d74cbb5"}, "id": "uveapdedcbqb1"}}, "name": "t3_16rgcfh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help: Project", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8TL8SLdj4BY6EtbmXIJYWQOE1dxhaZaO7zP_xwaYFMY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695609811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.computervision", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, i create a real time motion capture system that translate into an animated character movement and add a kalman filter to smooth the captured movement, since it&amp;#39;s jitter/shaking so much without it.&lt;/p&gt;\n\n&lt;p&gt;the thing is, i can&amp;#39;t find a way to compare the raw and filtered that result in numbers that prove the filtered movement is smoother than the raw movement. only the graphs and character animation that show it is smoother.&lt;/p&gt;\n\n&lt;p&gt;then i try to calculate both of their standard deviation by creating a predefined animation to be compared and it resulted in the following graphs and table.but ultimately, that only result in accuracy, which doesn&amp;#39;t show significant difference anyway. that mean the filter did make the movement smoother but not increase it accuracy.&lt;/p&gt;\n\n&lt;p&gt;now my professor ask if maybe there&amp;#39;s a standard to how smooth or jittery the movement is? maybe standard in animation, or games.&lt;/p&gt;\n\n&lt;p&gt;so far i can&amp;#39;t find a way to get the jitter or smoothness values, let alone the standard. can someone deny or confirm if there&amp;#39;s way to calculate it and standard for it? or should i just proof the smoothness improvement with only the graphs?&lt;/p&gt;\n\n&lt;p&gt;thanks&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/xmy78l6gbbqb1.png?width=5693&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dde3f9801662a59670c867d111c5e12fd7dd96bf\"&gt;https://preview.redd.it/xmy78l6gbbqb1.png?width=5693&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dde3f9801662a59670c867d111c5e12fd7dd96bf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uveapdedcbqb1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=485dd8a5714987110b0b14bdc6de21e12d74cbb5\"&gt;https://preview.redd.it/uveapdedcbqb1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=485dd8a5714987110b0b14bdc6de21e12d74cbb5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2be07b9a-850c-11eb-9ef0-0e67fd476361", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2rfzn", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#fdff99", "id": "16rgcfh", "is_robot_indexable": true, "report_reasons": null, "author": "UniversityOk378", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/computervision/comments/16rgcfh/is_there_a_way_to_calculate_jitter_or_smoothness/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/computervision/comments/16rgcfh/is_there_a_way_to_calculate_jitter_or_smoothness/", "subreddit_subscribers": 79722, "created_utc": 1695609811.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1695610517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.computervision", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/computervision/comments/16rgcfh/is_there_a_way_to_calculate_jitter_or_smoothness/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16rgkxa", "is_robot_indexable": true, "report_reasons": null, "author": "UniversityOk378", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_16rgcfh", "author_flair_text_color": null, "permalink": "/r/datascience/comments/16rgkxa/is_there_a_way_to_calculate_jitter_or_smoothness/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/computervision/comments/16rgcfh/is_there_a_way_to_calculate_jitter_or_smoothness/", "subreddit_subscribers": 1059745, "created_utc": 1695610517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_e2yfk59o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Andreas Kretz on LinkedIn: Data Science AMA. Maybe this will help you cover the basics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": true, "name": "t3_16rfm2h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2E0HZn11hcveJE4dTTSoOEh-mrgJd3TsYRT2d1LG8OI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695607656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/posts/andreas-kretz_tomorrow-im-doing-a-live-stream-with-my-activity-7111781783221719041-nvr6?utm_source=share&amp;utm_medium=member_ios", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?auto=webp&amp;s=18689041ed1ad1829e19b3f16bdf0619c3add354", "width": 1400, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd3104eb2954de61b63b4aded5889d7f481667e4", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c971a4bc289c8f96921326f627cebe12f3ac1660", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4aa8c0a35a98b8aba18c141f215320d65e017724", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b4185fb0b2f226c0a0e3e33b8be8f2575bf9ab5", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f9aebe95893f1568f8433661797ed06581b8b5b", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4632741c44875873f4766540a85113c0e703b906", "width": 1080, "height": 617}], "variants": {}, "id": "CjbMbFq2MEqSKWpNCjv-ipCLADmRBQ1ZCG3w2yy71f0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16rfm2h", "is_robot_indexable": true, "report_reasons": null, "author": "samjenkins377", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16rfm2h/andreas_kretz_on_linkedin_data_science_ama_maybe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/posts/andreas-kretz_tomorrow-im-doing-a-live-stream-with-my-activity-7111781783221719041-nvr6?utm_source=share&amp;utm_medium=member_ios", "subreddit_subscribers": 1059745, "created_utc": 1695607656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Can someone guide as to how I derive the optimal cost and gamma values in R.\n\nDo you use tune.svm() ?", "author_fullname": "t2_tir3dln2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hyperparameter Tuning for SVM, stuck!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16rfar3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695606755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can someone guide as to how I derive the optimal cost and gamma values in R.&lt;/p&gt;\n\n&lt;p&gt;Do you use tune.svm() ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16rfar3", "is_robot_indexable": true, "report_reasons": null, "author": "Fun_Elevator_814", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16rfar3/hyperparameter_tuning_for_svm_stuck/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16rfar3/hyperparameter_tuning_for_svm_stuck/", "subreddit_subscribers": 1059745, "created_utc": 1695606755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all,\n\nAfter a year of studying and interviewing I\u2019m finally leaving consulting (Big 4) and moving to a Consumer Tech startup focused on food/grocery delivery. The role is focused on marketing and user growth. \n\nI\u2019ve done work with A/B Testing and experimentation in my current role (basic) as well as built some Marketing Mix Models and Segmentation models. \n\nThe company I\u2019m joining is really going to be focused on driving profitability and retention. I\u2019m wondering if there are any courses/reading items y\u2019all recommend on causal inference, experimentation, marketing analytics, segmentation and personalization. \n\nAlso what can I do to succeed in my new role prior to joining? \n\nThanks!", "author_fullname": "t2_jrhff4f29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got a new job focused on growth and marketing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16rf8p0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695606591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;After a year of studying and interviewing I\u2019m finally leaving consulting (Big 4) and moving to a Consumer Tech startup focused on food/grocery delivery. The role is focused on marketing and user growth. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve done work with A/B Testing and experimentation in my current role (basic) as well as built some Marketing Mix Models and Segmentation models. &lt;/p&gt;\n\n&lt;p&gt;The company I\u2019m joining is really going to be focused on driving profitability and retention. I\u2019m wondering if there are any courses/reading items y\u2019all recommend on causal inference, experimentation, marketing analytics, segmentation and personalization. &lt;/p&gt;\n\n&lt;p&gt;Also what can I do to succeed in my new role prior to joining? &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16rf8p0", "is_robot_indexable": true, "report_reasons": null, "author": "Terrible-Hamster-342", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16rf8p0/got_a_new_job_focused_on_growth_and_marketing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16rf8p0/got_a_new_job_focused_on_growth_and_marketing/", "subreddit_subscribers": 1059745, "created_utc": 1695606591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\nI'm currently in a Master of Public Policy program, with a focus on data analysis, graduating in 2024. My goal was to transition to a new career. Along the way, I've caught the data science bug. I've loaded up on CS, Data Science, and quant electives, and I'm pondering whether to pursue an online MSCS or MSDS part-time after graduating.\n\nHere's the kicker: I've already got a BA in International Relations, an AA language degree from the military, and a BS in Information Technology from abroad (honestly, the curriculum was so similar to the school's computer science program). I've also had a data analyst internship at a non-profit, some undergraduate research gigs dealing with statistics, and a past job where I did coding and analysis.\n\nThe temptation to keep studying is fueled by my leftover GI Bill funds, so tuition isn't a concern. My goal? Be more competitive in the job market. But, let's be real: I'm kinda burnt out on academia.\n\nShould I pursue an MSCS/MSDS degree if I want to get into this field? Or should I have another strategy in mind? Would love to hear your takes.\n\n&amp;#x200B;", "author_fullname": "t2_4oqvwnb3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I underprepared or overdoing my education plan? Should I Go for Another Master's in Data Science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16relxw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695604730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently in a Master of Public Policy program, with a focus on data analysis, graduating in 2024. My goal was to transition to a new career. Along the way, I&amp;#39;ve caught the data science bug. I&amp;#39;ve loaded up on CS, Data Science, and quant electives, and I&amp;#39;m pondering whether to pursue an online MSCS or MSDS part-time after graduating.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the kicker: I&amp;#39;ve already got a BA in International Relations, an AA language degree from the military, and a BS in Information Technology from abroad (honestly, the curriculum was so similar to the school&amp;#39;s computer science program). I&amp;#39;ve also had a data analyst internship at a non-profit, some undergraduate research gigs dealing with statistics, and a past job where I did coding and analysis.&lt;/p&gt;\n\n&lt;p&gt;The temptation to keep studying is fueled by my leftover GI Bill funds, so tuition isn&amp;#39;t a concern. My goal? Be more competitive in the job market. But, let&amp;#39;s be real: I&amp;#39;m kinda burnt out on academia.&lt;/p&gt;\n\n&lt;p&gt;Should I pursue an MSCS/MSDS degree if I want to get into this field? Or should I have another strategy in mind? Would love to hear your takes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16relxw", "is_robot_indexable": true, "report_reasons": null, "author": "spoonforkknives92", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16relxw/am_i_underprepared_or_overdoing_my_education_plan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16relxw/am_i_underprepared_or_overdoing_my_education_plan/", "subreddit_subscribers": 1059745, "created_utc": 1695604730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "23M, cs graduate 2022,\nHello, i m stucked in my life. Dont have any idea what am i doing in life. After graduation not got selected in campus becaus of low grads (59%).\nWasted 1 year by dropping for GATE. \nDont have any internship or industry experience. \nWhat should i do to come back on track.\nWell because of gate some of my subjects are strong like, databses, sql, programming(c ,python).maths, and aptitude.", "author_fullname": "t2_q7t4mmuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Possible to get data related job as a frehser with 1 year gap after graduation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rcxdk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695599997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;23M, cs graduate 2022,\nHello, i m stucked in my life. Dont have any idea what am i doing in life. After graduation not got selected in campus becaus of low grads (59%).\nWasted 1 year by dropping for GATE. \nDont have any internship or industry experience. \nWhat should i do to come back on track.\nWell because of gate some of my subjects are strong like, databses, sql, programming(c ,python).maths, and aptitude.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16rcxdk", "is_robot_indexable": true, "report_reasons": null, "author": "hardik_umretiya", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16rcxdk/possible_to_get_data_related_job_as_a_frehser/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16rcxdk/possible_to_get_data_related_job_as_a_frehser/", "subreddit_subscribers": 1059745, "created_utc": 1695599997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, I'm using AWS forcasting to forcast demand over different departments (fashion, home, elec, etc). Would it be suitable to do different forecasts for different departs that's have different related time series?\nSome related time series that I have tried reduce the WAPE on some departments but increase it on others", "author_fullname": "t2_7427v7db", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time series forcasting, when to split the target?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ra2yz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695592632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m using AWS forcasting to forcast demand over different departments (fashion, home, elec, etc). Would it be suitable to do different forecasts for different departs that&amp;#39;s have different related time series?\nSome related time series that I have tried reduce the WAPE on some departments but increase it on others&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ra2yz", "is_robot_indexable": true, "report_reasons": null, "author": "Grovesy158", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ra2yz/time_series_forcasting_when_to_split_the_target/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ra2yz/time_series_forcasting_when_to_split_the_target/", "subreddit_subscribers": 1059745, "created_utc": 1695592632.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've found the troves of data from the department of education on undergraduate admissions. School acceptance rates, ACT / SATs, etc. \n\nIs there any such data for graduate schools or programs? For example, GRE / GMAT data, or simply acceptance rates. Any help would be greatly appreciated!", "author_fullname": "t2_lrgivisy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for graduate admissions data, DoED", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r1z2f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695572951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve found the troves of data from the department of education on undergraduate admissions. School acceptance rates, ACT / SATs, etc. &lt;/p&gt;\n\n&lt;p&gt;Is there any such data for graduate schools or programs? For example, GRE / GMAT data, or simply acceptance rates. Any help would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r1z2f", "is_robot_indexable": true, "report_reasons": null, "author": "crimefog", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r1z2f/looking_for_graduate_admissions_data_doed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r1z2f/looking_for_graduate_admissions_data_doed/", "subreddit_subscribers": 1059745, "created_utc": 1695572951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi\nI've wrote a CRM for shipyards, and other professionals that do boat maintenance.\n\nEach customer of this software will enter data about work orders, products costs and labour...\nThose data will be tied to boat makes, end customers and so on ...\n\nI'd like to be able to provide some useful data to the shipyards from this data. I'm pretty new to data analysis and don't know of there are tools that can help me to do so ?\nI.e. I can imagine when creating a new work order for some task (let's say an engine periodical maintenance), I could provide historical data about how much time it does take for this kind of task... or even when a special engine is concerned, this one is specifically harder to work with, so the  planned hour count should be higher and so on...\n\nIs there models that could be trained against the customer data to provide those features?\n\nSorry if it's in the wrong place or If my question seems dumb !\n\nThanks", "author_fullname": "t2_u3p6b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Writing a CRM : how to extract valued data to customers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qvvl0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695556386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi\nI&amp;#39;ve wrote a CRM for shipyards, and other professionals that do boat maintenance.&lt;/p&gt;\n\n&lt;p&gt;Each customer of this software will enter data about work orders, products costs and labour...\nThose data will be tied to boat makes, end customers and so on ...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to be able to provide some useful data to the shipyards from this data. I&amp;#39;m pretty new to data analysis and don&amp;#39;t know of there are tools that can help me to do so ?\nI.e. I can imagine when creating a new work order for some task (let&amp;#39;s say an engine periodical maintenance), I could provide historical data about how much time it does take for this kind of task... or even when a special engine is concerned, this one is specifically harder to work with, so the  planned hour count should be higher and so on...&lt;/p&gt;\n\n&lt;p&gt;Is there models that could be trained against the customer data to provide those features?&lt;/p&gt;\n\n&lt;p&gt;Sorry if it&amp;#39;s in the wrong place or If my question seems dumb !&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qvvl0", "is_robot_indexable": true, "report_reasons": null, "author": "Napo7", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qvvl0/writing_a_crm_how_to_extract_valued_data_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qvvl0/writing_a_crm_how_to_extract_valued_data_to/", "subreddit_subscribers": 1059745, "created_utc": 1695556386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am an aspiring data scientist currently pursuing a BCs in CS (2nd year). Lately, I've been actively searching for remote data science internships to gain practical experience and make my resume strong. I've come across several online virtual internship programs that are open to anyone. I am wondering if these opportunities are worth the time and if it's appropriate to include them in my work experience. Here are a few examples of such programs:\n\n1. [iNeuron](https://internship.ineuron.ai/)\n2. [OpenWeaver](https://community.openweaver.com/t/virtual-internship-in-data-science-apply-now/114521)\n3. [The Spark Foundation](https://internship.thesparksfoundation.info/)\n4. [Let's Grown More](https://letsgrowmore.in/vip/)", "author_fullname": "t2_6oagcr1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are these DS Virtual Internships good to get started?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qt2n3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695546597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an aspiring data scientist currently pursuing a BCs in CS (2nd year). Lately, I&amp;#39;ve been actively searching for remote data science internships to gain practical experience and make my resume strong. I&amp;#39;ve come across several online virtual internship programs that are open to anyone. I am wondering if these opportunities are worth the time and if it&amp;#39;s appropriate to include them in my work experience. Here are a few examples of such programs:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://internship.ineuron.ai/\"&gt;iNeuron&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://community.openweaver.com/t/virtual-internship-in-data-science-apply-now/114521\"&gt;OpenWeaver&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://internship.thesparksfoundation.info/\"&gt;The Spark Foundation&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://letsgrowmore.in/vip/\"&gt;Let&amp;#39;s Grown More&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qt2n3", "is_robot_indexable": true, "report_reasons": null, "author": "hashirbhatti", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qt2n3/are_these_ds_virtual_internships_good_to_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qt2n3/are_these_ds_virtual_internships_good_to_get/", "subreddit_subscribers": 1059745, "created_utc": 1695546597.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\nNavigating some unexpected twists in my data science journey and could really benefit from the collective wisdom of this community.\n\nFor about 2.5 years, I delved deep into predictive modeling within the heavy industry sector, leveraging my physics degree for a robust quantitative approach. Things took an unforeseen turn when my company closed its data science department. Though it was a jolt, I was offered a lifeline in the form of an analyst role within the fintech BaaS segment of our company. Here, I've been testing mobile apps, setting up APIs for clients, and gaining a different perspective.\n\nYet, as intriguing as fintech is, I've felt a drift from the heart of data science that once ignited my passion, and it's been a hurdle trying to relocate to a role that resonates more with my prior expertise.\n\nI'm at a juncture and could use some insights on these potential pathways:\n\n1. **Dive Deeper into Fintech**: Given my recent experiences, should I immerse myself further in fintech? I'm contemplating gaining a richer understanding of finance and then scouting for a data science role within this arena.\n2. **Physics and Data Science Fusion**: My love for physics remains undiminished. Could a focus on intertwining data science with physics be a promising avenue? Has anyone ventured this path and can share their experience?\n3. **Returning to Familiar Grounds or Exploring Fresh Horizons**: Is it more pragmatic to gravitate back to industries with which I'm acquainted or explore entirely new terrains where my data science prowess might be a good fit?\n\nYour experiences, advice, and reflections would be a beacon for me during this transitional phase.\n\nHeartfelt thanks for reading and offering your perspective!", "author_fullname": "t2_cyno6wxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Career Guidance: From Heavy Industry to Fintech, with a Physics Twist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qsg1r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695544336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;Navigating some unexpected twists in my data science journey and could really benefit from the collective wisdom of this community.&lt;/p&gt;\n\n&lt;p&gt;For about 2.5 years, I delved deep into predictive modeling within the heavy industry sector, leveraging my physics degree for a robust quantitative approach. Things took an unforeseen turn when my company closed its data science department. Though it was a jolt, I was offered a lifeline in the form of an analyst role within the fintech BaaS segment of our company. Here, I&amp;#39;ve been testing mobile apps, setting up APIs for clients, and gaining a different perspective.&lt;/p&gt;\n\n&lt;p&gt;Yet, as intriguing as fintech is, I&amp;#39;ve felt a drift from the heart of data science that once ignited my passion, and it&amp;#39;s been a hurdle trying to relocate to a role that resonates more with my prior expertise.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m at a juncture and could use some insights on these potential pathways:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Dive Deeper into Fintech&lt;/strong&gt;: Given my recent experiences, should I immerse myself further in fintech? I&amp;#39;m contemplating gaining a richer understanding of finance and then scouting for a data science role within this arena.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Physics and Data Science Fusion&lt;/strong&gt;: My love for physics remains undiminished. Could a focus on intertwining data science with physics be a promising avenue? Has anyone ventured this path and can share their experience?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Returning to Familiar Grounds or Exploring Fresh Horizons&lt;/strong&gt;: Is it more pragmatic to gravitate back to industries with which I&amp;#39;m acquainted or explore entirely new terrains where my data science prowess might be a good fit?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Your experiences, advice, and reflections would be a beacon for me during this transitional phase.&lt;/p&gt;\n\n&lt;p&gt;Heartfelt thanks for reading and offering your perspective!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qsg1r", "is_robot_indexable": true, "report_reasons": null, "author": "MachineSilly576", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qsg1r/seeking_career_guidance_from_heavy_industry_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qsg1r/seeking_career_guidance_from_heavy_industry_to/", "subreddit_subscribers": 1059745, "created_utc": 1695544336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am working on reproducing results claimed by an author in a research paper. \n\nI have followed the same process and have a model training and running.\n\nSo the MAE they claim is 5. And I am able to reproduce 23. \n\nI am a bit new to this, so when talking about reproducing results; how different can they be to say \"reproduce successful\"?", "author_fullname": "t2_123jiosa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comparing \"claimed\" results vs \"reproduced\" results.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qpup6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695535075.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on reproducing results claimed by an author in a research paper. &lt;/p&gt;\n\n&lt;p&gt;I have followed the same process and have a model training and running.&lt;/p&gt;\n\n&lt;p&gt;So the MAE they claim is 5. And I am able to reproduce 23. &lt;/p&gt;\n\n&lt;p&gt;I am a bit new to this, so when talking about reproducing results; how different can they be to say &amp;quot;reproduce successful&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qpup6", "is_robot_indexable": true, "report_reasons": null, "author": "Dump7", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qpup6/comparing_claimed_results_vs_reproduced_results/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qpup6/comparing_claimed_results_vs_reproduced_results/", "subreddit_subscribers": 1059745, "created_utc": 1695535075.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am studying data science and from media and asking people who are already working in the field strated to be concerned about finding a DS job after graduation.\nSo I am picking a plan B besides my DS BSc but still want to work in DS in the long term so what would you recommend me \nSoftware Engineering or Data engineeing\nI know some people would say data analyst but I just don't know anything about the jobs and from reading job discribtions I feel like using excel and dashboards is a waste for all the Math and programming and ML I have been studying in the last 3 years", "author_fullname": "t2_h5su2eud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software engineering or data engineeing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qptlb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695534965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am studying data science and from media and asking people who are already working in the field strated to be concerned about finding a DS job after graduation.\nSo I am picking a plan B besides my DS BSc but still want to work in DS in the long term so what would you recommend me \nSoftware Engineering or Data engineeing\nI know some people would say data analyst but I just don&amp;#39;t know anything about the jobs and from reading job discribtions I feel like using excel and dashboards is a waste for all the Math and programming and ML I have been studying in the last 3 years&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qptlb", "is_robot_indexable": true, "report_reasons": null, "author": "Emotional-Rhubarb725", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qptlb/software_engineering_or_data_engineeing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qptlb/software_engineering_or_data_engineeing/", "subreddit_subscribers": 1059745, "created_utc": 1695534965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nHi All,\n\nIs there a way to collect real time data regarding Packaged food and its ingredients. I'm currently focussing on Indian packaged foods. Any sugestion would be extemely helpful.\n\nThanks a lot!", "author_fullname": "t2_3dgxudc7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Packaged food related ingredients and its proportions in a dataset", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qo5es", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695529018.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;Is there a way to collect real time data regarding Packaged food and its ingredients. I&amp;#39;m currently focussing on Indian packaged foods. Any sugestion would be extemely helpful.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qo5es", "is_robot_indexable": true, "report_reasons": null, "author": "ashnel11", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qo5es/packaged_food_related_ingredients_and_its/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qo5es/packaged_food_related_ingredients_and_its/", "subreddit_subscribers": 1059745, "created_utc": 1695529018.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I see a large amount of relevant open source tools and libraries to assist in peripheral (not the actual data processing or modeling) areas of data science. I mean tools that make certain important tasks easier. For instance: kedro, hydra-conf, nannyml, streamlit, docker, devpod, black, ruff, pandera, mage, fugue, datapane, adn probably a lot more.\n\nWhat do you guys use for your data science project?", "author_fullname": "t2_hji49", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tools do you use on your data science projects from proof of concept to production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r7elg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695586274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see a large amount of relevant open source tools and libraries to assist in peripheral (not the actual data processing or modeling) areas of data science. I mean tools that make certain important tasks easier. For instance: kedro, hydra-conf, nannyml, streamlit, docker, devpod, black, ruff, pandera, mage, fugue, datapane, adn probably a lot more.&lt;/p&gt;\n\n&lt;p&gt;What do you guys use for your data science project?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r7elg", "is_robot_indexable": true, "report_reasons": null, "author": "vmgustavo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r7elg/what_tools_do_you_use_on_your_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r7elg/what_tools_do_you_use_on_your_data_science/", "subreddit_subscribers": 1059745, "created_utc": 1695586274.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}