{"kind": "Listing", "data": {"after": "t3_16l6sj0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I work in a large government agency, and I, along with most of my colleagues in the Data Science/Machine Learning field, are self-taught. Over the past few years, I've put in a tremendous amount of effort to keep my ML skills up to date.  I haven't seen the same level of enthusiasm and dedication from my managers and coworkers. They tend to view ML as an extension of dashboarding, where ML is expected to 'fit' into a quick 1-2 week development cycle. There seems to be a level of denial when it comes to acknowledging the time and steps required for a successful ML implementation. My manager's management style often involves suggesting ML for nearly every project that comes our way, and they become frustrated when feasibility isn't immediately apparent. ", "author_fullname": "t2_5fbmh3va", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do managers and leadership think DS/ML is easy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lhttw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 87, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 87, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694999646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a large government agency, and I, along with most of my colleagues in the Data Science/Machine Learning field, are self-taught. Over the past few years, I&amp;#39;ve put in a tremendous amount of effort to keep my ML skills up to date.  I haven&amp;#39;t seen the same level of enthusiasm and dedication from my managers and coworkers. They tend to view ML as an extension of dashboarding, where ML is expected to &amp;#39;fit&amp;#39; into a quick 1-2 week development cycle. There seems to be a level of denial when it comes to acknowledging the time and steps required for a successful ML implementation. My manager&amp;#39;s management style often involves suggesting ML for nearly every project that comes our way, and they become frustrated when feasibility isn&amp;#39;t immediately apparent. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lhttw", "is_robot_indexable": true, "report_reasons": null, "author": "Excellent_Cost170", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lhttw/why_do_managers_and_leadership_think_dsml_is_easy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lhttw/why_do_managers_and_leadership_think_dsml_is_easy/", "subreddit_subscribers": 1050659, "created_utc": 1694999646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My employer is conning Fortune 500 companies into thinking they\u2019re getting a team of data engineers. What they\u2019re actually getting is a bunch of barely functioning script kiddies and a \u201cdata scientist\u201d who has created a bunch of tools that barely accomplish basic analytics tasks and data quality checks but that create flashy visuals.\n\nWhat do I do? Do I ride out a crappy economy on the tailcoats of a bunch of con artists, or do I gtfo at the first opportunity so that I don\u2019t end up taking any liability or bad rep when they eventually get sued?", "author_fullname": "t2_b83zsu83", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Con artist company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lnmyz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 76, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 76, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695017864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My employer is conning Fortune 500 companies into thinking they\u2019re getting a team of data engineers. What they\u2019re actually getting is a bunch of barely functioning script kiddies and a \u201cdata scientist\u201d who has created a bunch of tools that barely accomplish basic analytics tasks and data quality checks but that create flashy visuals.&lt;/p&gt;\n\n&lt;p&gt;What do I do? Do I ride out a crappy economy on the tailcoats of a bunch of con artists, or do I gtfo at the first opportunity so that I don\u2019t end up taking any liability or bad rep when they eventually get sued?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lnmyz", "is_robot_indexable": true, "report_reasons": null, "author": "Burner_McBurnstein", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lnmyz/con_artist_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lnmyz/con_artist_company/", "subreddit_subscribers": 1050659, "created_utc": 1695017864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am planning to quit my job as a Data Science consultant (more Data Analytics tbh) soon. Since I was in secondary school, I have been wanting to take a gap year. During uni breaks/summer holidays, I''ve always done some kind of internship or work shadowing  before eventually working full time right after uni (I was lucky to find a job within one month of graduating). I've never really took a long break.\n\nAt the moment, I'm likely to go with 6 months of travelling rather than a year as I'm afraid of being out of a job for too long and it ruining my career prospects. I'm fortunate enough to have money to sustain myself for a year without pay though, including any emergencies.\n\nMy biggest worry is being able to land a job when I'm back.. I have been wanting to leave consulting for a while now, so my goal is to secure a data role that is 'good enough' in an industry. For the past few months, I've been passively applying for jobs and going for interviews, but I feel that the market is definitely still bad. Not sure if this helps, but I'm based in London. I was a Data Analyst (corporate firm) for a year, then currently a Data Analytics consultant for almost 3 years. If things were different and my firm weren't too affected by the market downturn, I would have tried to go for Senior Consultant this year (but still secretly want to take a break to travel).", "author_fullname": "t2_tlgeufew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone here quit their job in DS to travel for an extended time (3-12 months)... then managed to land a job when you're back?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16l4pha", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694967130.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning to quit my job as a Data Science consultant (more Data Analytics tbh) soon. Since I was in secondary school, I have been wanting to take a gap year. During uni breaks/summer holidays, I&amp;#39;&amp;#39;ve always done some kind of internship or work shadowing  before eventually working full time right after uni (I was lucky to find a job within one month of graduating). I&amp;#39;ve never really took a long break.&lt;/p&gt;\n\n&lt;p&gt;At the moment, I&amp;#39;m likely to go with 6 months of travelling rather than a year as I&amp;#39;m afraid of being out of a job for too long and it ruining my career prospects. I&amp;#39;m fortunate enough to have money to sustain myself for a year without pay though, including any emergencies.&lt;/p&gt;\n\n&lt;p&gt;My biggest worry is being able to land a job when I&amp;#39;m back.. I have been wanting to leave consulting for a while now, so my goal is to secure a data role that is &amp;#39;good enough&amp;#39; in an industry. For the past few months, I&amp;#39;ve been passively applying for jobs and going for interviews, but I feel that the market is definitely still bad. Not sure if this helps, but I&amp;#39;m based in London. I was a Data Analyst (corporate firm) for a year, then currently a Data Analytics consultant for almost 3 years. If things were different and my firm weren&amp;#39;t too affected by the market downturn, I would have tried to go for Senior Consultant this year (but still secretly want to take a break to travel).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16l4pha", "is_robot_indexable": true, "report_reasons": null, "author": "zumba4lyf", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16l4pha/anyone_here_quit_their_job_in_ds_to_travel_for_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16l4pha/anyone_here_quit_their_job_in_ds_to_travel_for_an/", "subreddit_subscribers": 1050659, "created_utc": 1694967130.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I graduated with a computer engineering degree 12 years ago and only worked as a programmer for 1 year. The rest are bunch of different things. The last 6 years I was an ERP consultant and worked with javascript and SQL for a while. \n\nI just landed a job at a federal government and started a week ago. To my surprise, the job description is different from my supervisor told me. He said I will be a data engineer/architect and started asking me about experience with python, R, Tableau and databases.\n\nRealistically, how long will it take me to adequately understand the fundamentals of the job? What resources should I use for my learning path? I see a lot of bootcamps and trainings but I don\u2019t know where to start.", "author_fullname": "t2_8qf28hn1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I\u2019m panicking! Is there hope for me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lh6oq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694997788.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I graduated with a computer engineering degree 12 years ago and only worked as a programmer for 1 year. The rest are bunch of different things. The last 6 years I was an ERP consultant and worked with javascript and SQL for a while. &lt;/p&gt;\n\n&lt;p&gt;I just landed a job at a federal government and started a week ago. To my surprise, the job description is different from my supervisor told me. He said I will be a data engineer/architect and started asking me about experience with python, R, Tableau and databases.&lt;/p&gt;\n\n&lt;p&gt;Realistically, how long will it take me to adequately understand the fundamentals of the job? What resources should I use for my learning path? I see a lot of bootcamps and trainings but I don\u2019t know where to start.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lh6oq", "is_robot_indexable": true, "report_reasons": null, "author": "Material_Writing3799", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lh6oq/im_panicking_is_there_hope_for_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lh6oq/im_panicking_is_there_hope_for_me/", "subreddit_subscribers": 1050659, "created_utc": 1694997788.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Looking at upskilling myself in my current role as an Analyst and beginning a project in python my manager has loosely provided parameters for.\n\nGoal of project: Try to predict customers who might churn / analyse retention. Dataset i am using does not have any customers who already churned.\n\ndataset: 3 years of daily user activity using company's service. not much segmentation except for their general industry and region.\n\nHypothesis: Decreasing activity relative to their past consumption may indicate about to churn. \n\nI am having issues beginning this analysis. My main issue with starting out, it seems like most churn/retention models already use customers who have already churned so I'm not sure if the dataset I am working with will help me analyse churn/retention.\n\nAny tips for beginning this analysis and which direction to take would be very appreciated.", "author_fullname": "t2_c6kxfwrh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Customer Churn", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16l99c4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694977849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking at upskilling myself in my current role as an Analyst and beginning a project in python my manager has loosely provided parameters for.&lt;/p&gt;\n\n&lt;p&gt;Goal of project: Try to predict customers who might churn / analyse retention. Dataset i am using does not have any customers who already churned.&lt;/p&gt;\n\n&lt;p&gt;dataset: 3 years of daily user activity using company&amp;#39;s service. not much segmentation except for their general industry and region.&lt;/p&gt;\n\n&lt;p&gt;Hypothesis: Decreasing activity relative to their past consumption may indicate about to churn. &lt;/p&gt;\n\n&lt;p&gt;I am having issues beginning this analysis. My main issue with starting out, it seems like most churn/retention models already use customers who have already churned so I&amp;#39;m not sure if the dataset I am working with will help me analyse churn/retention.&lt;/p&gt;\n\n&lt;p&gt;Any tips for beginning this analysis and which direction to take would be very appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16l99c4", "is_robot_indexable": true, "report_reasons": null, "author": "breakfastisnice123", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16l99c4/customer_churn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16l99c4/customer_churn/", "subreddit_subscribers": 1050659, "created_utc": 1694977849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I would like to transition to a career that involves some field work, because I am tired of working at my desk all the time. I am a data scientist in natural language processing, with a bachelor's in computer science and a master's in NLP, although of course neither of those involve any kind of field work. But I'm open to advice on what I should study to make the career change.", "author_fullname": "t2_6amo8yyb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any data science jobs that involve field work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lkmug", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695007941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to transition to a career that involves some field work, because I am tired of working at my desk all the time. I am a data scientist in natural language processing, with a bachelor&amp;#39;s in computer science and a master&amp;#39;s in NLP, although of course neither of those involve any kind of field work. But I&amp;#39;m open to advice on what I should study to make the career change.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lkmug", "is_robot_indexable": true, "report_reasons": null, "author": "mindmech", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lkmug/are_there_any_data_science_jobs_that_involve/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lkmug/are_there_any_data_science_jobs_that_involve/", "subreddit_subscribers": 1050659, "created_utc": 1695007941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Context: \n\nI\u2019m a junior-ish employee, with 3 YoE and an MS that I just finished while working. I hope to be firmly in the data scientist camp soon, although at the moment I think I\u2019m more of a data analyst. My role usually involves a lot of data wrangling and organizing/cleaning and then applying descriptive statistics and making visualizations/presentations/reports of these metrics. The metrics we care about are never the actual values in our data but require a good amount of transformation, the data is often geospatial, text based, or raw measurements that we derive metrics from, so the data wrangling/cleaning is always pretty time consuming but I figure that\u2019s probably typical across industries.\n\nQuestion: \nAt my current company I\u2019ve almost always been spread across 3-5 tasks at a time. They\u2019re all in the same general domain but span different projects and sometimes different teams. My company is kind of a govt contractor/consultant type shop so I think that plays a role. Is this fairly typical, even in private sector and/or non-consulting type roles? \n\nEven though I get decent analyses done I feel like I don\u2019t always have enough time to double check results, properly document things, and think about better ways to answer the questions we have. I also feel like there\u2019s always a scramble to do additional last minute substantive analysis ahead of deliverable deadlines where I kind of feel like things go wrong and there\u2019s less opportunity to use good practices.", "author_fullname": "t2_18lpef9w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How common is it to support multiple projects simultaneously?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16la63e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694979994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context: &lt;/p&gt;\n\n&lt;p&gt;I\u2019m a junior-ish employee, with 3 YoE and an MS that I just finished while working. I hope to be firmly in the data scientist camp soon, although at the moment I think I\u2019m more of a data analyst. My role usually involves a lot of data wrangling and organizing/cleaning and then applying descriptive statistics and making visualizations/presentations/reports of these metrics. The metrics we care about are never the actual values in our data but require a good amount of transformation, the data is often geospatial, text based, or raw measurements that we derive metrics from, so the data wrangling/cleaning is always pretty time consuming but I figure that\u2019s probably typical across industries.&lt;/p&gt;\n\n&lt;p&gt;Question: \nAt my current company I\u2019ve almost always been spread across 3-5 tasks at a time. They\u2019re all in the same general domain but span different projects and sometimes different teams. My company is kind of a govt contractor/consultant type shop so I think that plays a role. Is this fairly typical, even in private sector and/or non-consulting type roles? &lt;/p&gt;\n\n&lt;p&gt;Even though I get decent analyses done I feel like I don\u2019t always have enough time to double check results, properly document things, and think about better ways to answer the questions we have. I also feel like there\u2019s always a scramble to do additional last minute substantive analysis ahead of deliverable deadlines where I kind of feel like things go wrong and there\u2019s less opportunity to use good practices.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16la63e", "is_robot_indexable": true, "report_reasons": null, "author": "reddituser378", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16la63e/how_common_is_it_to_support_multiple_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16la63e/how_common_is_it_to_support_multiple_projects/", "subreddit_subscribers": 1050659, "created_utc": 1694979994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Use best practices and real-world examples to demonstrate the powerful text parser library \n\n[ The parse library is very simple to use. Photo by Amanda Jones on Unsplash ](https://preview.redd.it/504sy5vdyzob1.jpg?width=720&amp;format=pjpg&amp;auto=webp&amp;s=d747d05be9cd97e3269887c675e53337ce5a3102)\n\nThis article introduces a Python library called [parse](https://pypi.org/project/parse/?ref=dataleadsfuture.com) for quickly and conveniently parsing and extracting data from text, serving as a great alternative to [Python regular expressions](https://docs.python.org/3/library/re.html?ref=dataleadsfuture.com).\n\nAnd which covers the best practices with the [parse](https://pypi.org/project/parse/?ref=dataleadsfuture.com) library and a real-world example of parsing [nginx log text](http://nginx.org/en/docs/http/ngx_http_log_module.html?ref=dataleadsfuture.com#log_format).\n\n## Introduction\n\nI have a colleague named Wang. One day, he came to me with a worried expression, saying he encountered a complex problem: his boss wanted him to analyze the server logs from the past month and provide statistics on visitor traffic.\n\nI told him it was simple. Just use regular expressions. For example, to analyze nginx logs, use the following regular expression, and it\u2019s elementary.\n\n    content: \n    192.168.0.2 - - [04/Jan/2019:16:06:38 +0800] \"GET http://example.aliyundoc.com/_astats?application=&amp;inf.name=eth0 HTTP/1.1\" 200 273932 \n    \n    regular expression: \n    (?&lt;ip&gt;\\d+\\.\\d+\\.\\d+\\.\\d+)( - - \\[)(?&lt;datetime&gt;[\\s\\S]+)(?&lt;t1&gt;\\][\\s\"]+)(?&lt;request&gt;[A-Z]+) (?&lt;url&gt;[\\S]*) (?&lt;protocol&gt;[\\S]+)[\"] (?&lt;code&gt;\\d+) (?&lt;sendbytes&gt;\\d+)\n\nBut Wang was still worried, saying that learning regular expressions is too tricky. Although there are many ready-made examples online to learn from, he needs help with parsing uncommon text formats.\n\nMoreover, even if he could solve the problem this time, what if his boss asked for changes in the parsing rules when he submitted the analysis? Wouldn\u2019t he need to fumble around for a long time again?\n\nIs there a simpler and more convenient method?\n\nI thought about it and said, of course, there is. Let\u2019s introduce our protagonist today: the Python parse library.\n\n## Installation &amp; Setup\n\nAs described on [the parse GitHub page](https://github.com/r1chardj0n3s/parse?ref=dataleadsfuture.com), it uses [Python\u2019s format() syntax](https://docs.python.org/3/library/string.html?ref=dataleadsfuture.com#format-string-syntax) to parse text, essentially serving as a reverse operation of [Python f-strings](https://docs.python.org/3/reference/lexical_analysis.html?ref=dataleadsfuture.com#f-strings).\n\nBefore starting to use parse, let\u2019s see how to install the library.\n\nDirect installation with pip:\n\n    python -m pip install parse\n\nInstallation with conda can be more troublesome, as parse is not in the default conda channel and needs to be installed through conda-forge:\n\n    conda install -c conda-forge parse\n\nAfter installation, you can use from parse import \\* in your code to use the library\u2019s methods directly.\n\n## Features &amp; Usage\n\nThe parse API is similar to [Python Regular Expressions](https://docs.python.org/3/library/re.html?ref=dataleadsfuture.com#functions), mainly consisting of the parse, search  \n, and findall methods. Basic usage can be learned from [the parse documentation](https://pypi.org/project/parse/?ref=dataleadsfuture.com).\n\n### Pattern format\n\nThe parse format is very similar to the Python format syntax. You can capture matched text using {}  \n or {field\\_name}.\n\nFor example, in the following text, if I want to get the profile URL and username, I can write it like this:\n\n    content: \n    Hello everyone, my Medium profile url is https://qtalen.medium.com, and my username is @qtalen.  \n    \n    parse pattern: \n    Hello everyone, my Medium profile url is {profile}, and my username is {username}.\n\nOr you want to extract multiple phone numbers. Still, the phone numbers have different formats of country codes in front, and the phone numbers are of a fixed length of 11 digits. You can write it like this:\n\n    compiler = Parser(\"{country_code}{phone:11.11},\") \n    content = \"0085212345678901, +85212345678902, (852)12345678903,\"  \n    \n    results = compiler.findall(content) \n    \n    for result in results:     \n        print(result)\n\nOr if you need to process a piece of text in an HTML tag, but the text is preceded and followed by an indefinite length of whitespace, you can write it like this:\n\n    content: \n    &lt;div&gt;           Hello World               &lt;/div&gt;  \n    \n    pattern: \n    &lt;div&gt;{:^}&lt;/div&gt;\n\nIn the code above, {:11} refers to the width, which means to capture at least 11 characters, equivalent to the regular expression (.{11,})?. {:.11}  \n refers to the precision, which means to capture at most 11 characters, equivalent to the regular expression (.{,11})?. So when combined, it means (.{11, 11})?. The result is:\n\n&amp;#x200B;\n\n[ Capture fixed-width characters. Image by Author ](https://preview.redd.it/8gz6ibp2zzob1.png?width=542&amp;format=png&amp;auto=webp&amp;s=c85c447fa2315ed915b5d6401c1445ba8d096ed2)\n\n The most powerful feature of parse is its handling of time text, which can be directly parsed into Python datetime objects. For example, if we want to parse the time in an HTTP log: \n\n    content:\n    [04/Jan/2019:16:06:38 +0800]\n    \n    pattern:\n    [{:th}]\n\n### Retrieving results\n\nThere are two ways to retrieve the results:\n\n1. For capturing methods that use {} without a field name, you can directly use result.fixed  \n to get the result as a tuple.\n2. For capturing methods that use {field\\_name}, you can use result.named to get the result as a dictionary.\n\n### Custom Type Conversions\n\nAlthough using {field\\_name} is already quite simple, the source code reveals that {field\\_name} is internally converted to (?P&lt;field\\_name&gt;.+?). So, parse still uses regular expressions for matching. .+? represents one or more random characters in non-greedy mode.\n\n&amp;#x200B;\n\n[ The transformation process of parse format to regular expressions. Image by Author ](https://preview.redd.it/50g4mp4fzzob1.png?width=720&amp;format=png&amp;auto=webp&amp;s=bdd7b41bcbd516db509874000807ebbae09145d5)\n\n However, often we hope to match more precisely. For example, the text \u201cmy email is [xxx@xxx.com](mailto:xxx@xxx.com)\u201d, \u201cmy email is {email}\u201dcan capture the email. Sometimes we may get dirty data, for example, \u201cmy email is xxxx@xxxx\u201d, and we don\u2019t want to grab it. \n\nIs there a way to use regular expressions for more accurate matching?\n\nThat\u2019s when the with\\_pattern decorator comes in handy.\n\nFor example, for capturing email addresses, we can write it like this:\n\n    @with_pattern(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n    def email(text: str) -&gt; str:\n        return text\n    \n    \n    compiler = Parser(\"my email address is {email:Email}\", dict(Email=email))\n    \n    legal_result = compiler.parse(\"my email address is xx@xxx.com\")  # legal email\n    illegal_result = compiler.parse(\"my email address is xx@xx\") \n\n Using the with\\_pattern decorator, we can define a custom field type, in this case, Email which will match the email address in the text. We can also use this approach to match other complicated patterns. \n\n## A Real-world Example: Parsing Nginx Log\n\nAfter understanding the basic usage of parse, let\u2019s return to the troubles of Wang mentioned at the beginning of the article. Let\u2019s see how to parse logs if we have server log files for the past month.\n\n**Note:** We chose [NASA\u2019s HTTP log dataset](https://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html?ref=dataleadsfuture.com) for this experiment, which is free to use.\n\nThe text fragment to be parsed looks like this\uff1a\n\n[ What is the text fragment look like. Screenshot by Author ](https://preview.redd.it/e660m7vrzzob1.png?width=720&amp;format=png&amp;auto=webp&amp;s=36d88e544acddb31e5b4fdd84dc622071236f765)\n\n First, we need to preprocess the parse expression. This way, when parsing large files, we don\u2019t have to compile the regular expression for each line of text, thus improving performance. \n\n    from parse import Parser, with_pattern\n    import pandas as pd\n    \n    # https://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html\n    FILE_NAME = \"../../data/access_log_Jul95_min\"\n    compiler = Parser('{source} - - [{timestamp:th}] \"{method} {path} {version}\" {status_code} {length}\\n')\n\n Next, the parse\\_line method is the core of this example. It uses the preprocessed expression to parse the text, returning the corresponding match if there is one and an empty dictionary if not. \n\n    def process_line(text: str) -&gt; dict:\n        parse_result = compiler.parse(text)\n        return parse_result.named if parse_result else {}\n\n Then, we use the read\\_file method to process the text line by line using a generator, which can minimize memory usage. However, due to the disk\u2019s 4k capability limitations, this method may not guarantee performance. \n\n    def read_file(name: str) -&gt; list[dict]:\n        result = []\n        with open(name, 'r') as f:\n            for line in f:\n                obj: dict = process_line(line)\n                result.append(obj)\n    \n        return result\n\n Since we need to perform statistics on the log files, we must use the from\\_records method to construct a DataFrame from the matched results. \n\n    def build_dataframe(records: list[dict]) -&gt; pd.DataFrame:\n        result: pd.DataFrame = pd.DataFrame.from_records(records, index='timestamp')\n        return result\n\n Finally, in the main method, we put all the methods together and try to count the different status\\_code occurrences: \n\n    def main():\n        records: list[dict] = read_file(FILE_NAME)\n        dataframe = build_dataframe(records)\n        print(dataframe.groupby('status_code').count())\n\n[ Wang\u2019s troubles have been easily solved. Image by Author ](https://preview.redd.it/zinr2q0700pb1.png?width=528&amp;format=png&amp;auto=webp&amp;s=210be348eecd4fdf890e046c4febfb0020a6d05d)\n\n That\u2019s it. Wang\u2019s troubles have been easily solved. \n\n## Best Practices with parse Library\n\nAlthough the parse library is so simple that I only have a little to write about in the article. There are still some best practices to follow, just like regular expressions.\n\n### Readability and maintainability\n\nTo efficiently capture text and maintain expressions, it is recommended to always use {field\\_name}  \n instead of {}. This way, you can directly use result.named to obtain key-value results.\n\nUsing Parser(pattern) to preprocess the expression is recommended, rather than parse(pattern, text).\n\nOn the one hand, this can improve performance. On the other hand, when using Custom Type Conversions, you can keep the pattern and extra\\_type together, making it easier to maintain.\n\n### Optimizing performance for large datasets\n\nIf you look at the source code, you can see that {} and {field\\_name} use the regular expressions (.+?) and (?P&lt;field\\_name&gt;.+?) for capture, respectively. Both expressions use the [non-greedy mode](https://docs.python.org/3/library/re.html?ref=dataleadsfuture.com#regular-expression-syntax). So when you use with\\_pattern to write your own expressions, also try to use non-greedy mode.\n\nAt the same time, when writing with\\_pattern, if you use () for capture grouping, please use regex\\_group\\_count to specify the specific groups like this: [@with\\_pattern](http://twitter.com/with_pattern?ref=dataleadsfuture.com)(r\u2019((\\\\d+))\u2019, regex\\_group\\_count=2) .\n\nFinally, if a group is not needed in with\\_pattern, use (?:x) instead. u/with_pattern(r\u2019(?:&lt;input.*?&gt;)(.*?)(?:&lt;/input&gt;)\u2019, regex\\_group\\_count=1) means you want to capture the content between input tags. The input tags will not be captured.\n\n## Conclusion\n\nIn this article, I changed my usual way of writing lengthy papers. By solving a colleague\u2019s problem, I briefly introduced the use of the parse library. I hope you like this style.\n\nThis article does not cover the detailed usage methods on the official website. Still, it introduces some best practices and performance optimization solutions based on my experience.\n\nAt the same time, I explained in detail the use of the parse library to parse nginx logs with a practical example.\n\nAs the new series title suggests, besides improving code execution speed and performance, using various tools to improve work efficiency is also a performance enhancement.\n\nThis article helps data scientists simplify text parsing and spend time on more critical tasks. If you have any thoughts on this article, feel free to leave a comment and discuss.\n\n&amp;#x200B;\n\n This article was originally published on my personal blog [Data Leads Future.](https://www.dataleadsfuture.com/introducing-pythons-parse-the-ultimate-alternative-to-regular-expressions/) \n\n&amp;#x200B;", "author_fullname": "t2_9r8ft2a0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Python\u2019s Parse: The Ultimate Alternative to Regular Expressions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"50g4mp4fzzob1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 33, "x": 108, "u": "https://preview.redd.it/50g4mp4fzzob1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c300c11223bc6153314e960d166168c2aca84656"}, {"y": 66, "x": 216, "u": "https://preview.redd.it/50g4mp4fzzob1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8010db97662d5212e625ef9a5eb22538f70e16e0"}, {"y": 99, "x": 320, "u": "https://preview.redd.it/50g4mp4fzzob1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d8201f55d972b5983e430797c491f3aef232bc5"}, {"y": 198, "x": 640, "u": "https://preview.redd.it/50g4mp4fzzob1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a1079611cc8bbd0f31a310269c77ce83fb7f4de1"}], "s": {"y": 223, "x": 720, "u": "https://preview.redd.it/50g4mp4fzzob1.png?width=720&amp;format=png&amp;auto=webp&amp;s=bdd7b41bcbd516db509874000807ebbae09145d5"}, "id": "50g4mp4fzzob1"}, "zinr2q0700pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 44, "x": 108, "u": "https://preview.redd.it/zinr2q0700pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33e727f6c19ffeeb7d73491a8afcb6e03f898d8e"}, {"y": 89, "x": 216, "u": "https://preview.redd.it/zinr2q0700pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe87ff22d31b89e90c799520b60235afe6ca9fc5"}, {"y": 132, "x": 320, "u": "https://preview.redd.it/zinr2q0700pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c61b2bde65ac88bba5a535981f702da0e1da9a97"}], "s": {"y": 219, "x": 528, "u": "https://preview.redd.it/zinr2q0700pb1.png?width=528&amp;format=png&amp;auto=webp&amp;s=210be348eecd4fdf890e046c4febfb0020a6d05d"}, "id": "zinr2q0700pb1"}, "e660m7vrzzob1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 23, "x": 108, "u": "https://preview.redd.it/e660m7vrzzob1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a20c6a939ebfdcb048f2e484fbafae1f615a5820"}, {"y": 47, "x": 216, "u": "https://preview.redd.it/e660m7vrzzob1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed607319d3171dc249c43a7751b1a98a9d571655"}, {"y": 70, "x": 320, "u": "https://preview.redd.it/e660m7vrzzob1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5ccdfadccefd67ebf03b24423e60a6a7262ea1e4"}, {"y": 140, "x": 640, "u": "https://preview.redd.it/e660m7vrzzob1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=74b846f168fca161de277b6703c3ce6804efcfd1"}], "s": {"y": 158, "x": 720, "u": "https://preview.redd.it/e660m7vrzzob1.png?width=720&amp;format=png&amp;auto=webp&amp;s=36d88e544acddb31e5b4fdd84dc622071236f765"}, "id": "e660m7vrzzob1"}, "8gz6ibp2zzob1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 25, "x": 108, "u": "https://preview.redd.it/8gz6ibp2zzob1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c632ed06306035f19080cd7024c20657d946c1c6"}, {"y": 51, "x": 216, "u": "https://preview.redd.it/8gz6ibp2zzob1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd3fb859bdf67fbdc927d7d186fe5cf05be77cdf"}, {"y": 76, "x": 320, "u": "https://preview.redd.it/8gz6ibp2zzob1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f1bf8ea51f312c81a397b24673d2d151505c37d6"}], "s": {"y": 130, "x": 542, "u": "https://preview.redd.it/8gz6ibp2zzob1.png?width=542&amp;format=png&amp;auto=webp&amp;s=c85c447fa2315ed915b5d6401c1445ba8d096ed2"}, "id": "8gz6ibp2zzob1"}, "504sy5vdyzob1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 72, "x": 108, "u": "https://preview.redd.it/504sy5vdyzob1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3525ea61d90ef4be54081fd1c12b665d07026605"}, {"y": 144, "x": 216, "u": "https://preview.redd.it/504sy5vdyzob1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ecf26549b1cd0a6c61d84df3c4f314033b5a19be"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/504sy5vdyzob1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fbaec945c68b581e8822c56a063d92e5dc7fde79"}, {"y": 426, "x": 640, "u": "https://preview.redd.it/504sy5vdyzob1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c7a79c1efb10ab17db9ac86539c51b09de5b7dd9"}], "s": {"y": 480, "x": 720, "u": "https://preview.redd.it/504sy5vdyzob1.jpg?width=720&amp;format=pjpg&amp;auto=webp&amp;s=d747d05be9cd97e3269887c675e53337ce5a3102"}, "id": "504sy5vdyzob1"}}, "name": "t3_16lsuln", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fA7JwEMRE6ga1UaZ_caTg6XU5QwRpY-PQTil_-GIuic.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695036240.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Use best practices and real-world examples to demonstrate the powerful text parser library &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/504sy5vdyzob1.jpg?width=720&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d747d05be9cd97e3269887c675e53337ce5a3102\"&gt; The parse library is very simple to use. Photo by Amanda Jones on Unsplash &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This article introduces a Python library called &lt;a href=\"https://pypi.org/project/parse/?ref=dataleadsfuture.com\"&gt;parse&lt;/a&gt; for quickly and conveniently parsing and extracting data from text, serving as a great alternative to &lt;a href=\"https://docs.python.org/3/library/re.html?ref=dataleadsfuture.com\"&gt;Python regular expressions&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;And which covers the best practices with the &lt;a href=\"https://pypi.org/project/parse/?ref=dataleadsfuture.com\"&gt;parse&lt;/a&gt; library and a real-world example of parsing &lt;a href=\"http://nginx.org/en/docs/http/ngx_http_log_module.html?ref=dataleadsfuture.com#log_format\"&gt;nginx log text&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h2&gt;Introduction&lt;/h2&gt;\n\n&lt;p&gt;I have a colleague named Wang. One day, he came to me with a worried expression, saying he encountered a complex problem: his boss wanted him to analyze the server logs from the past month and provide statistics on visitor traffic.&lt;/p&gt;\n\n&lt;p&gt;I told him it was simple. Just use regular expressions. For example, to analyze nginx logs, use the following regular expression, and it\u2019s elementary.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;content: \n192.168.0.2 - - [04/Jan/2019:16:06:38 +0800] &amp;quot;GET http://example.aliyundoc.com/_astats?application=&amp;amp;inf.name=eth0 HTTP/1.1&amp;quot; 200 273932 \n\nregular expression: \n(?&amp;lt;ip&amp;gt;\\d+\\.\\d+\\.\\d+\\.\\d+)( - - \\[)(?&amp;lt;datetime&amp;gt;[\\s\\S]+)(?&amp;lt;t1&amp;gt;\\][\\s&amp;quot;]+)(?&amp;lt;request&amp;gt;[A-Z]+) (?&amp;lt;url&amp;gt;[\\S]*) (?&amp;lt;protocol&amp;gt;[\\S]+)[&amp;quot;] (?&amp;lt;code&amp;gt;\\d+) (?&amp;lt;sendbytes&amp;gt;\\d+)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But Wang was still worried, saying that learning regular expressions is too tricky. Although there are many ready-made examples online to learn from, he needs help with parsing uncommon text formats.&lt;/p&gt;\n\n&lt;p&gt;Moreover, even if he could solve the problem this time, what if his boss asked for changes in the parsing rules when he submitted the analysis? Wouldn\u2019t he need to fumble around for a long time again?&lt;/p&gt;\n\n&lt;p&gt;Is there a simpler and more convenient method?&lt;/p&gt;\n\n&lt;p&gt;I thought about it and said, of course, there is. Let\u2019s introduce our protagonist today: the Python parse library.&lt;/p&gt;\n\n&lt;h2&gt;Installation &amp;amp; Setup&lt;/h2&gt;\n\n&lt;p&gt;As described on &lt;a href=\"https://github.com/r1chardj0n3s/parse?ref=dataleadsfuture.com\"&gt;the parse GitHub page&lt;/a&gt;, it uses &lt;a href=\"https://docs.python.org/3/library/string.html?ref=dataleadsfuture.com#format-string-syntax\"&gt;Python\u2019s format() syntax&lt;/a&gt; to parse text, essentially serving as a reverse operation of &lt;a href=\"https://docs.python.org/3/reference/lexical_analysis.html?ref=dataleadsfuture.com#f-strings\"&gt;Python f-strings&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Before starting to use parse, let\u2019s see how to install the library.&lt;/p&gt;\n\n&lt;p&gt;Direct installation with pip:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;python -m pip install parse\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Installation with conda can be more troublesome, as parse is not in the default conda channel and needs to be installed through conda-forge:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;conda install -c conda-forge parse\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;After installation, you can use from parse import * in your code to use the library\u2019s methods directly.&lt;/p&gt;\n\n&lt;h2&gt;Features &amp;amp; Usage&lt;/h2&gt;\n\n&lt;p&gt;The parse API is similar to &lt;a href=\"https://docs.python.org/3/library/re.html?ref=dataleadsfuture.com#functions\"&gt;Python Regular Expressions&lt;/a&gt;, mainly consisting of the parse, search&lt;br/&gt;\n, and findall methods. Basic usage can be learned from &lt;a href=\"https://pypi.org/project/parse/?ref=dataleadsfuture.com\"&gt;the parse documentation&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h3&gt;Pattern format&lt;/h3&gt;\n\n&lt;p&gt;The parse format is very similar to the Python format syntax. You can capture matched text using {}&lt;br/&gt;\n or {field_name}.&lt;/p&gt;\n\n&lt;p&gt;For example, in the following text, if I want to get the profile URL and username, I can write it like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;content: \nHello everyone, my Medium profile url is https://qtalen.medium.com, and my username is @qtalen.  \n\nparse pattern: \nHello everyone, my Medium profile url is {profile}, and my username is {username}.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Or you want to extract multiple phone numbers. Still, the phone numbers have different formats of country codes in front, and the phone numbers are of a fixed length of 11 digits. You can write it like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;compiler = Parser(&amp;quot;{country_code}{phone:11.11},&amp;quot;) \ncontent = &amp;quot;0085212345678901, +85212345678902, (852)12345678903,&amp;quot;  \n\nresults = compiler.findall(content) \n\nfor result in results:     \n    print(result)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Or if you need to process a piece of text in an HTML tag, but the text is preceded and followed by an indefinite length of whitespace, you can write it like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;content: \n&amp;lt;div&amp;gt;           Hello World               &amp;lt;/div&amp;gt;  \n\npattern: \n&amp;lt;div&amp;gt;{:^}&amp;lt;/div&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;In the code above, {:11} refers to the width, which means to capture at least 11 characters, equivalent to the regular expression (.{11,})?. {:.11}&lt;br/&gt;\n refers to the precision, which means to capture at most 11 characters, equivalent to the regular expression (.{,11})?. So when combined, it means (.{11, 11})?. The result is:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8gz6ibp2zzob1.png?width=542&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c85c447fa2315ed915b5d6401c1445ba8d096ed2\"&gt; Capture fixed-width characters. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The most powerful feature of parse is its handling of time text, which can be directly parsed into Python datetime objects. For example, if we want to parse the time in an HTTP log: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;content:\n[04/Jan/2019:16:06:38 +0800]\n\npattern:\n[{:th}]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;Retrieving results&lt;/h3&gt;\n\n&lt;p&gt;There are two ways to retrieve the results:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;For capturing methods that use {} without a field name, you can directly use result.fixed&lt;br/&gt;\nto get the result as a tuple.&lt;/li&gt;\n&lt;li&gt;For capturing methods that use {field_name}, you can use result.named to get the result as a dictionary.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h3&gt;Custom Type Conversions&lt;/h3&gt;\n\n&lt;p&gt;Although using {field_name} is already quite simple, the source code reveals that {field_name} is internally converted to (?P&amp;lt;field\\_name&amp;gt;.+?). So, parse still uses regular expressions for matching. .+? represents one or more random characters in non-greedy mode.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/50g4mp4fzzob1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdd7b41bcbd516db509874000807ebbae09145d5\"&gt; The transformation process of parse format to regular expressions. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;However, often we hope to match more precisely. For example, the text \u201cmy email is [&lt;a href=\"mailto:xxx@xxx.com\"&gt;xxx@xxx.com&lt;/a&gt;](mailto:&lt;a href=\"mailto:xxx@xxx.com\"&gt;xxx@xxx.com&lt;/a&gt;)\u201d, \u201cmy email is {email}\u201dcan capture the email. Sometimes we may get dirty data, for example, \u201cmy email is xxxx@xxxx\u201d, and we don\u2019t want to grab it. &lt;/p&gt;\n\n&lt;p&gt;Is there a way to use regular expressions for more accurate matching?&lt;/p&gt;\n\n&lt;p&gt;That\u2019s when the with_pattern decorator comes in handy.&lt;/p&gt;\n\n&lt;p&gt;For example, for capturing email addresses, we can write it like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@with_pattern(r&amp;#39;\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b&amp;#39;)\ndef email(text: str) -&amp;gt; str:\n    return text\n\n\ncompiler = Parser(&amp;quot;my email address is {email:Email}&amp;quot;, dict(Email=email))\n\nlegal_result = compiler.parse(&amp;quot;my email address is xx@xxx.com&amp;quot;)  # legal email\nillegal_result = compiler.parse(&amp;quot;my email address is xx@xx&amp;quot;) \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Using the with_pattern decorator, we can define a custom field type, in this case, Email which will match the email address in the text. We can also use this approach to match other complicated patterns. &lt;/p&gt;\n\n&lt;h2&gt;A Real-world Example: Parsing Nginx Log&lt;/h2&gt;\n\n&lt;p&gt;After understanding the basic usage of parse, let\u2019s return to the troubles of Wang mentioned at the beginning of the article. Let\u2019s see how to parse logs if we have server log files for the past month.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We chose &lt;a href=\"https://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html?ref=dataleadsfuture.com\"&gt;NASA\u2019s HTTP log dataset&lt;/a&gt; for this experiment, which is free to use.&lt;/p&gt;\n\n&lt;p&gt;The text fragment to be parsed looks like this\uff1a&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/e660m7vrzzob1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=36d88e544acddb31e5b4fdd84dc622071236f765\"&gt; What is the text fragment look like. Screenshot by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;First, we need to preprocess the parse expression. This way, when parsing large files, we don\u2019t have to compile the regular expression for each line of text, thus improving performance. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from parse import Parser, with_pattern\nimport pandas as pd\n\n# https://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html\nFILE_NAME = &amp;quot;../../data/access_log_Jul95_min&amp;quot;\ncompiler = Parser(&amp;#39;{source} - - [{timestamp:th}] &amp;quot;{method} {path} {version}&amp;quot; {status_code} {length}\\n&amp;#39;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Next, the parse_line method is the core of this example. It uses the preprocessed expression to parse the text, returning the corresponding match if there is one and an empty dictionary if not. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def process_line(text: str) -&amp;gt; dict:\n    parse_result = compiler.parse(text)\n    return parse_result.named if parse_result else {}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then, we use the read_file method to process the text line by line using a generator, which can minimize memory usage. However, due to the disk\u2019s 4k capability limitations, this method may not guarantee performance. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def read_file(name: str) -&amp;gt; list[dict]:\n    result = []\n    with open(name, &amp;#39;r&amp;#39;) as f:\n        for line in f:\n            obj: dict = process_line(line)\n            result.append(obj)\n\n    return result\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Since we need to perform statistics on the log files, we must use the from_records method to construct a DataFrame from the matched results. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def build_dataframe(records: list[dict]) -&amp;gt; pd.DataFrame:\n    result: pd.DataFrame = pd.DataFrame.from_records(records, index=&amp;#39;timestamp&amp;#39;)\n    return result\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Finally, in the main method, we put all the methods together and try to count the different status_code occurrences: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def main():\n    records: list[dict] = read_file(FILE_NAME)\n    dataframe = build_dataframe(records)\n    print(dataframe.groupby(&amp;#39;status_code&amp;#39;).count())\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zinr2q0700pb1.png?width=528&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=210be348eecd4fdf890e046c4febfb0020a6d05d\"&gt; Wang\u2019s troubles have been easily solved. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;That\u2019s it. Wang\u2019s troubles have been easily solved. &lt;/p&gt;\n\n&lt;h2&gt;Best Practices with parse Library&lt;/h2&gt;\n\n&lt;p&gt;Although the parse library is so simple that I only have a little to write about in the article. There are still some best practices to follow, just like regular expressions.&lt;/p&gt;\n\n&lt;h3&gt;Readability and maintainability&lt;/h3&gt;\n\n&lt;p&gt;To efficiently capture text and maintain expressions, it is recommended to always use {field_name}&lt;br/&gt;\n instead of {}. This way, you can directly use result.named to obtain key-value results.&lt;/p&gt;\n\n&lt;p&gt;Using Parser(pattern) to preprocess the expression is recommended, rather than parse(pattern, text).&lt;/p&gt;\n\n&lt;p&gt;On the one hand, this can improve performance. On the other hand, when using Custom Type Conversions, you can keep the pattern and extra_type together, making it easier to maintain.&lt;/p&gt;\n\n&lt;h3&gt;Optimizing performance for large datasets&lt;/h3&gt;\n\n&lt;p&gt;If you look at the source code, you can see that {} and {field_name} use the regular expressions (.+?) and (?P&amp;lt;field\\_name&amp;gt;.+?) for capture, respectively. Both expressions use the &lt;a href=\"https://docs.python.org/3/library/re.html?ref=dataleadsfuture.com#regular-expression-syntax\"&gt;non-greedy mode&lt;/a&gt;. So when you use with_pattern to write your own expressions, also try to use non-greedy mode.&lt;/p&gt;\n\n&lt;p&gt;At the same time, when writing with_pattern, if you use () for capture grouping, please use regex_group_count to specify the specific groups like this: &lt;a href=\"http://twitter.com/with_pattern?ref=dataleadsfuture.com\"&gt;@with_pattern&lt;/a&gt;(r\u2019((\\d+))\u2019, regex_group_count=2) .&lt;/p&gt;\n\n&lt;p&gt;Finally, if a group is not needed in with_pattern, use (?:x) instead. &lt;a href=\"/u/with_pattern\"&gt;u/with_pattern&lt;/a&gt;(r\u2019(?:&amp;lt;input.*?&amp;gt;)(.*?)(?:&amp;lt;/input&amp;gt;)\u2019, regex_group_count=1) means you want to capture the content between input tags. The input tags will not be captured.&lt;/p&gt;\n\n&lt;h2&gt;Conclusion&lt;/h2&gt;\n\n&lt;p&gt;In this article, I changed my usual way of writing lengthy papers. By solving a colleague\u2019s problem, I briefly introduced the use of the parse library. I hope you like this style.&lt;/p&gt;\n\n&lt;p&gt;This article does not cover the detailed usage methods on the official website. Still, it introduces some best practices and performance optimization solutions based on my experience.&lt;/p&gt;\n\n&lt;p&gt;At the same time, I explained in detail the use of the parse library to parse nginx logs with a practical example.&lt;/p&gt;\n\n&lt;p&gt;As the new series title suggests, besides improving code execution speed and performance, using various tools to improve work efficiency is also a performance enhancement.&lt;/p&gt;\n\n&lt;p&gt;This article helps data scientists simplify text parsing and spend time on more critical tasks. If you have any thoughts on this article, feel free to leave a comment and discuss.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/introducing-pythons-parse-the-ultimate-alternative-to-regular-expressions/\"&gt;Data Leads Future.&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lsuln", "is_robot_indexable": true, "report_reasons": null, "author": "qtalen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lsuln/introducing_pythons_parse_the_ultimate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lsuln/introducing_pythons_parse_the_ultimate/", "subreddit_subscribers": 1050659, "created_utc": 1695036240.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Recently saw a tweet which got quite some traction talking about how many people haven't used sci-kit learn in months as data scientists.\n\nThis has been replaced with \"PyTorch, SuperGradients, HuggingFace stack, langchain\".\n\nThis didn't really make sense to me as the tooling mentioned isn't really comparable to sci-kit learn but I'm curious and slightly worried I might be falling behind and not up to date with things so just asking if I'm just behind the curve or what you guys think/ do.", "author_fullname": "t2_14bxtq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do people not use sci-kit learn / other traditional libraries anymore?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lu9ni", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695040259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently saw a tweet which got quite some traction talking about how many people haven&amp;#39;t used sci-kit learn in months as data scientists.&lt;/p&gt;\n\n&lt;p&gt;This has been replaced with &amp;quot;PyTorch, SuperGradients, HuggingFace stack, langchain&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;This didn&amp;#39;t really make sense to me as the tooling mentioned isn&amp;#39;t really comparable to sci-kit learn but I&amp;#39;m curious and slightly worried I might be falling behind and not up to date with things so just asking if I&amp;#39;m just behind the curve or what you guys think/ do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lu9ni", "is_robot_indexable": true, "report_reasons": null, "author": "15150776", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lu9ni/do_people_not_use_scikit_learn_other_traditional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lu9ni/do_people_not_use_scikit_learn_other_traditional/", "subreddit_subscribers": 1050659, "created_utc": 1695040259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As the title says. For someone who's looking to I hate to say this, do DS as a hobby? I'm not trying to insult anyone here just looking for advice before committing to school etc.", "author_fullname": "t2_c4bgqzqrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What free education would you recommend", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lt9mp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695037500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says. For someone who&amp;#39;s looking to I hate to say this, do DS as a hobby? I&amp;#39;m not trying to insult anyone here just looking for advice before committing to school etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lt9mp", "is_robot_indexable": true, "report_reasons": null, "author": "OwlDry6530", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lt9mp/what_free_education_would_you_recommend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lt9mp/what_free_education_would_you_recommend/", "subreddit_subscribers": 1050659, "created_utc": 1695037500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys! I(25F) am working as a QA Analyst in a Structural Engineering and Precast Solutions company. I have a Bachelors degree in Architecture. Based on my current role, I am pursuing a PG certification in Data Science and Machine Learning from MIT(US) and I\u2019m planning to get a Masters in Data Science next year from a University in Berlin. What are the odds of me getting successful jobs after my masters? Will my profile be considered for jobs in Germany? I would appreciate your replies! Thanks you! \u263a\ufe0f", "author_fullname": "t2_vi38c485", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Non technical background", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lsa0m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695034462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys! I(25F) am working as a QA Analyst in a Structural Engineering and Precast Solutions company. I have a Bachelors degree in Architecture. Based on my current role, I am pursuing a PG certification in Data Science and Machine Learning from MIT(US) and I\u2019m planning to get a Masters in Data Science next year from a University in Berlin. What are the odds of me getting successful jobs after my masters? Will my profile be considered for jobs in Germany? I would appreciate your replies! Thanks you! \u263a\ufe0f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lsa0m", "is_robot_indexable": true, "report_reasons": null, "author": "Ashamed_Chemical_207", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lsa0m/non_technical_background/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lsa0m/non_technical_background/", "subreddit_subscribers": 1050659, "created_utc": 1695034462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am working on a time series forecasting problem and I am trying to do training, validation and test but I am getting confused.  \n\n\nI was looking up online and I see that people often use an expanding window for training and backtesting. For instance, if you have one model that has an input step of 10 timesteps and forecast 1 timestep, your first iteration will take the 11 first timesteps and train the model, then it will evaluate on the next time step, then you shift your window and redo the same step. Your validation error will be the average of all the validation errors.  \n\n\nHowever, I feel it is wrong to use that metric as a Test metric if you want to choose between 2 or more models. if you wanna choose between linear regression and ARIMA for instance, you should evaluate both models on a new set of data that is for instance the last year that you discarded in the backtesting. ***Is that a correct statement?***\n\n I haven't seen that on any of the websites I searched through.  \n\n\nAlso, is there an ideal way of choosing the backtest period? So for example, in the example illustrated above, the first 11 points are used to train your model, 10 input steps, and 1 forecast. Then you validate on the next x steps, so if x =1, you evaluate on the 12th timestep. ***Is that correct?*** and should x be greater than 1 or it doesn't really matter?  \n", "author_fullname": "t2_4fa0ibvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Difference between backtesting and testing in time series forecasting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lg3s1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694994748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a time series forecasting problem and I am trying to do training, validation and test but I am getting confused.  &lt;/p&gt;\n\n&lt;p&gt;I was looking up online and I see that people often use an expanding window for training and backtesting. For instance, if you have one model that has an input step of 10 timesteps and forecast 1 timestep, your first iteration will take the 11 first timesteps and train the model, then it will evaluate on the next time step, then you shift your window and redo the same step. Your validation error will be the average of all the validation errors.  &lt;/p&gt;\n\n&lt;p&gt;However, I feel it is wrong to use that metric as a Test metric if you want to choose between 2 or more models. if you wanna choose between linear regression and ARIMA for instance, you should evaluate both models on a new set of data that is for instance the last year that you discarded in the backtesting. &lt;strong&gt;&lt;em&gt;Is that a correct statement?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t seen that on any of the websites I searched through.  &lt;/p&gt;\n\n&lt;p&gt;Also, is there an ideal way of choosing the backtest period? So for example, in the example illustrated above, the first 11 points are used to train your model, 10 input steps, and 1 forecast. Then you validate on the next x steps, so if x =1, you evaluate on the 12th timestep. &lt;strong&gt;&lt;em&gt;Is that correct?&lt;/em&gt;&lt;/strong&gt; and should x be greater than 1 or it doesn&amp;#39;t really matter?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lg3s1", "is_robot_indexable": true, "report_reasons": null, "author": "dimem16", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lg3s1/difference_between_backtesting_and_testing_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lg3s1/difference_between_backtesting_and_testing_in/", "subreddit_subscribers": 1050659, "created_utc": 1694994748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been working as a data scientist in one of the big five banks of Canada in Toronto for two years.\n\nHere is the question: What should be my plan to land a DS position in a top-tier tech company?\n\n**The following are the** **concerning limitations**:\n\n\\- My current role is in the area of capital markets and it is not giving me real experience in the realm of customer-facing product DS challenges.\n\n\\- I came to Canada 3.5 years ago to do a master's in math and stats at York University. Almost none of my cohort ended up working in tech so I have a very limited network in the industry. (The few people I know are classmates from back home who are now working in big tech in the US or Canada)\n\n\\- My current status in Canada is PR so my options have to be limited to the job postings that big tech companies post in Canada.\n\n**P.S.:**\n\n\\- I know I can do (and I am doing) several things to address the concerns above (for example doing a personal project or reading blog posts to cover the first item), but the question is how would you prioritize among lots of possible actions? What approach do you think has the key leverage in this situation?\n\n\\- I would like to especially stress this aspect of my question: What can I do to expand my network in tech now that I am working in finance? Considering that I am not very much networking-event-kind-of-person and I only build quality connections when I have a chance to interact with people on an ongoing basis. \n\nThank you in advance for your advice and help.", "author_fullname": "t2_7vls2dk3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching from a Canadian bank to a top-tier tech company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lee0p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694990168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working as a data scientist in one of the big five banks of Canada in Toronto for two years.&lt;/p&gt;\n\n&lt;p&gt;Here is the question: What should be my plan to land a DS position in a top-tier tech company?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The following are the&lt;/strong&gt; &lt;strong&gt;concerning limitations&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;- My current role is in the area of capital markets and it is not giving me real experience in the realm of customer-facing product DS challenges.&lt;/p&gt;\n\n&lt;p&gt;- I came to Canada 3.5 years ago to do a master&amp;#39;s in math and stats at York University. Almost none of my cohort ended up working in tech so I have a very limited network in the industry. (The few people I know are classmates from back home who are now working in big tech in the US or Canada)&lt;/p&gt;\n\n&lt;p&gt;- My current status in Canada is PR so my options have to be limited to the job postings that big tech companies post in Canada.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;P.S.:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- I know I can do (and I am doing) several things to address the concerns above (for example doing a personal project or reading blog posts to cover the first item), but the question is how would you prioritize among lots of possible actions? What approach do you think has the key leverage in this situation?&lt;/p&gt;\n\n&lt;p&gt;- I would like to especially stress this aspect of my question: What can I do to expand my network in tech now that I am working in finance? Considering that I am not very much networking-event-kind-of-person and I only build quality connections when I have a chance to interact with people on an ongoing basis. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your advice and help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lee0p", "is_robot_indexable": true, "report_reasons": null, "author": "Kooky-Nail-3743", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lee0p/switching_from_a_canadian_bank_to_a_toptier_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lee0p/switching_from_a_canadian_bank_to_a_toptier_tech/", "subreddit_subscribers": 1050659, "created_utc": 1694990168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work as a Data Scientist in an academic leaning/ public health center. Recently, with AI like LLMs being in the news, there has been a push for integrating AI into the work. However, I am having a hard time understanding the goals specifically in the terms of how the non-technical leadership thinks about AI. Specifically,  when I think of AI applied in our work I think of more classical algorithms like  forests, glms or like Bayesian methods, since the work is often focused on statistical inference on tabular data. Leadership seems to want more AI-y appearing applications, like NN or LLMs, but generally I don\u2019t see the utility of focusing in this directions (other than something like extracting data from clinical notes). \n\nAm I missing something regarding the utility of DL, NN, LLM  for the more statistical inference type of work? Even in a purely predictive use case, it is my understanding that ensemble trees are typically top performers for tabular data. \n\nWould love to hear y\u2019all\u2019s thoughts on directions to look in!", "author_fullname": "t2_7hba1g3m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are LLMs going to replace other types of AI/ML based analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16l3p6h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694964797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a Data Scientist in an academic leaning/ public health center. Recently, with AI like LLMs being in the news, there has been a push for integrating AI into the work. However, I am having a hard time understanding the goals specifically in the terms of how the non-technical leadership thinks about AI. Specifically,  when I think of AI applied in our work I think of more classical algorithms like  forests, glms or like Bayesian methods, since the work is often focused on statistical inference on tabular data. Leadership seems to want more AI-y appearing applications, like NN or LLMs, but generally I don\u2019t see the utility of focusing in this directions (other than something like extracting data from clinical notes). &lt;/p&gt;\n\n&lt;p&gt;Am I missing something regarding the utility of DL, NN, LLM  for the more statistical inference type of work? Even in a purely predictive use case, it is my understanding that ensemble trees are typically top performers for tabular data. &lt;/p&gt;\n\n&lt;p&gt;Would love to hear y\u2019all\u2019s thoughts on directions to look in!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16l3p6h", "is_robot_indexable": true, "report_reasons": null, "author": "JTcyto", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16l3p6h/are_llms_going_to_replace_other_types_of_aiml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16l3p6h/are_llms_going_to_replace_other_types_of_aiml/", "subreddit_subscribers": 1050659, "created_utc": 1694964797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm considering applying to a university to get a certificate in DataScience / DataAnalysis (with a possibility of continuing to earn an MS). But most of the programs I've seen (at state universities in IL) emphasize a couple of years of Java, object / data structure courses, etc.\n\nOnline schools (like DataCamp) emphasize Python, pandas, etc., with some stats / linear algebra, etc.\n\nThe former seem to be capitalizing on the skills of their current employees and courses they've been teaching for years (i.e., traditional CS curriculum with some modifications); the latter argue that what they offer is what employers are seeking, but have a vested interest in making this argument -- it's what they are selling.\n\nA third model doing something like Google's professional data engineer course (or AWS or another corporate sponsored training program).\n\nFor folks actually working in the field (and hiring people), which of these models is more attractive? How important is it to have an academic certificate degree in DS to get into the field?\n\nThanks for your feedback", "author_fullname": "t2_22fsxqwq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Academic MS/certificate in DS or certificate from Google, Amazon, etc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16luxng", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695042090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m considering applying to a university to get a certificate in DataScience / DataAnalysis (with a possibility of continuing to earn an MS). But most of the programs I&amp;#39;ve seen (at state universities in IL) emphasize a couple of years of Java, object / data structure courses, etc.&lt;/p&gt;\n\n&lt;p&gt;Online schools (like DataCamp) emphasize Python, pandas, etc., with some stats / linear algebra, etc.&lt;/p&gt;\n\n&lt;p&gt;The former seem to be capitalizing on the skills of their current employees and courses they&amp;#39;ve been teaching for years (i.e., traditional CS curriculum with some modifications); the latter argue that what they offer is what employers are seeking, but have a vested interest in making this argument -- it&amp;#39;s what they are selling.&lt;/p&gt;\n\n&lt;p&gt;A third model doing something like Google&amp;#39;s professional data engineer course (or AWS or another corporate sponsored training program).&lt;/p&gt;\n\n&lt;p&gt;For folks actually working in the field (and hiring people), which of these models is more attractive? How important is it to have an academic certificate degree in DS to get into the field?&lt;/p&gt;\n\n&lt;p&gt;Thanks for your feedback&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16luxng", "is_robot_indexable": true, "report_reasons": null, "author": "BeeApiary", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16luxng/academic_mscertificate_in_ds_or_certificate_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16luxng/academic_mscertificate_in_ds_or_certificate_from/", "subreddit_subscribers": 1050659, "created_utc": 1695042090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm at a crossroads in my academic journey and could use some guidance. I'm contemplating pursuing a BS Economics with Data Sciences (BSEDS) degree, but I have a few questions and uncertainties.\n\nInterest in Computing: I've always been intrigued by the computing field, and the idea of combining it with economics and data science seems appealing.\n\nCareer Prospects: Can anyone shed light on the career opportunities this degree might open up? Is it a promising path with good job prospects in today's market?\n\nPersonal Experiences: If you've pursued this degree or have experience in related fields, I'd love to hear about your journey. What challenges and rewards did you encounter along the way?\n\nEconomics and Data Science Combo: Is the fusion of economics and data science a strong combination? Are there any unique advantages or challenges associated with this blend of disciplines?\n\nI'd really appreciate any insights, advice, or personal anecdotes you can share. Making this decision is a bit daunting, and your input could be a game-changer for me. Thanks in advance!", "author_fullname": "t2_89s1rm2o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Considering a BS Economics with Data Sciences (BSEDS) Degree - Seeking Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16lulvp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695041195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m at a crossroads in my academic journey and could use some guidance. I&amp;#39;m contemplating pursuing a BS Economics with Data Sciences (BSEDS) degree, but I have a few questions and uncertainties.&lt;/p&gt;\n\n&lt;p&gt;Interest in Computing: I&amp;#39;ve always been intrigued by the computing field, and the idea of combining it with economics and data science seems appealing.&lt;/p&gt;\n\n&lt;p&gt;Career Prospects: Can anyone shed light on the career opportunities this degree might open up? Is it a promising path with good job prospects in today&amp;#39;s market?&lt;/p&gt;\n\n&lt;p&gt;Personal Experiences: If you&amp;#39;ve pursued this degree or have experience in related fields, I&amp;#39;d love to hear about your journey. What challenges and rewards did you encounter along the way?&lt;/p&gt;\n\n&lt;p&gt;Economics and Data Science Combo: Is the fusion of economics and data science a strong combination? Are there any unique advantages or challenges associated with this blend of disciplines?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d really appreciate any insights, advice, or personal anecdotes you can share. Making this decision is a bit daunting, and your input could be a game-changer for me. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lulvp", "is_robot_indexable": true, "report_reasons": null, "author": "GuciBanana", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lulvp/considering_a_bs_economics_with_data_sciences/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lulvp/considering_a_bs_economics_with_data_sciences/", "subreddit_subscribers": 1050659, "created_utc": 1695041195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "If you're looking for a LLM+OLAP solution, you might be inspired from the practice at Tencent.\n\n**By** [**Jun Zhang**](https://medium.com/geekculture/llm-powered-olap-the-tencent-experience-with-apache-doris-9ecc779450e)**, data platform engineer at Tencent**\n\nSix months ago, I wrote about [why we replaced ClickHouse with Apache Doris as an OLAP engine](https://medium.com/geekculture/tencent-data-engineer-why-we-go-from-clickhouse-to-apache-doris-db120f324290) for our data management system. Back then, we were struggling with the auto-generation of SQL statements. As days pass, we have made progresses big enough to be references for you (I think), so here I am again. \n\nWe have adopted Large Language Models (LLM) to empower our Doris-based OLAP services.\n\n## LLM + OLAP\n\nOur incentive was to save our internal staff from the steep learning curve of SQL writing. Thus, we used LLM as an intermediate. It transforms natural language questions into SQL statements and sends the SQLs to the OLAP engine for execution.\n\nhttps://preview.redd.it/x640kok470pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=5b16f5cdd9282ce2b35a0b3f1329f8906e9a4580\n\nLike every AI-related experience, we came across some friction:\n\n1. LLM does not understand data jargons, like \"fields\", \"rows\", \"columns\" and \"tables\". Instead, they can perfectly translate business terms like \"corporate income\" and \"DAU\", which are basically what the fields/rows/columns are about. That means it can work well only if the analysts use the exact right word to refer to the metric they need when typing their questions.\n2. The LLM we are using is slow in inference. It takes over 10 seconds to respond. As it charges fees by token, cost-effectiveness becomes a problem.\n3. Although the LLM is trained on a large collection of public datasets, it is under-informed of niche knowledge. In our case, the LLM is super unfamiliar with indie songs, so even if the songs are included in our database, the LLM will not able to identify them properly. \n4. Sometimes our input questions require adequate and latest legal, political, financial, and regulatory information, which is hard to be included in a training dataset or knowledge base. We need to connect the LLM to wider info bases in order to perform more diversified tasks.\n\nWe knock these problems down one by one.\n\n### 1. A semantic layer\n\nFor problem No.1, we introduce a semantic layer between the LLM and the OLAP engine. This layer translates business terms into the corresponding data fields. It can identify data filtering conditions from the various natural language wordings, relate them to the metrics involved, and then generate SQL statements. \n\nBesides that, the semantic layer can optimize the computation logic. When analysts input a question that involves a complicated query, let's say, a multi-table join, the semantic layer can split that into multiple single-table queries to reduce semantic distortion.\n\nhttps://preview.redd.it/vbpz86j770pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=19ecffffeebfff0f6add8e655cd91d0fd0ff3c96\n\n### 2. LLM parsing rules\n\nTo increase cost-effectiveness in using LLM, we evaluate the computation complexity of all scenarios, such as metric computation, detailed record retrieval, and user segmentation. Then, we create rules and dedicate the LLM-parsing step to only complicated tasks. That means for the simple computation tasks, it will skip the parsing. \n\nFor example, when an analyst inputs \"tell me the earnings of the major musical platforms\", the LLM identifies that this question only entails several metrics or dimensions, so it will not further parse it but send it straight for SQL generation and execution. This can largely shorten query response time and reduce API expenses. \n\nhttps://preview.redd.it/7lq8zqv870pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=5b8aae6c9f9d5313a771cc25ce33654f99178b26\n\n### 3. Schema Mapper and external knowledge base\n\nTo empower the LLM with niche knowledge, we added a Schema Mapper upstream from the LLM. The Schema Mapper maps the input question to an external knowledge base, and then the LLM will do parsing.\n\nWe are constantly testing and optimizing the Schema Mapper. We categorize and rate content in the external knowledge base, and do various levels of mapping (full-text mapping and fuzzy mapping) to enable better semantic parsing.\n\nhttps://preview.redd.it/9y954hia70pb1.png?width=2001&amp;format=png&amp;auto=webp&amp;s=e78d5f04dd16584d2dcc773caf76a12fdb7c32ca\n\n### 4. Plugins\n\nWe used plugins to connect the LLM to more fields of information, and we have different integration methods for different types of plugins:\n\n* **Embedding local files**: This is especially useful when we need to \"teach\" the LLM the latest regulatory policies, which are often text files. Firstly, the system vectorizes the local text file, executes semantic searches to find matching or similar terms in the local file, extracts the relevant contents and puts them into the LLM parsing window to generate output. \n* **Third-party plugins**: The marketplace is full of third-party plugins that are designed for all kinds of sectors. With them, the LLM is able to deal with wide-ranging topics. Each plugin has its own prompts and calling function. Once the input question hits a prompt, the relevant plugin will be called.\n\nhttps://preview.redd.it/v1mp12pc70pb1.png?width=2001&amp;format=png&amp;auto=webp&amp;s=a32e44e72befa0dc6f06c57f8bfe38de37848cbe\n\nAfter we are done with above four optimizations, the SuperSonic framework comes into being.\n\n## The SuperSonic framework\n\nNow let me walk you through this [framework](https://github.com/tencentmusic/supersonic):\n\nhttps://preview.redd.it/enc3w1ne70pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=fd59180d437a7676b5d0bb9d4d9967956abc2638\n\n* An analyst inputs a question.\n* The Schema Mapper maps the question to an external knowledge base.\n* If there are matching fields in the external knowledge base, the question will not be parsed by the LLM. Instead, a metric computation formula will trigger the OLAP engine to start querying. If there is no matching field, the question will enter the LLM.\n* Based on the pre-defined rules, the LLM rates the complexity level of the question. If it is a simple query, it will go directly to the OLAP engine; if it is a complicated query, it will be semantically parsed and converted to a DSL statement.\n* At the Semantic Layer, the DSL statement will be split based on its query scenario. For example, if it is a multi-table join query, this layer will generate multiple single-table query SQL statements.\n* If the question involves external knowledge, the LLM will call a third-party plugin.\n\n**Example**\n\nhttps://preview.redd.it/rkm8exyg70pb1.png?width=2001&amp;format=png&amp;auto=webp&amp;s=5f197bef98beca33e62d871494b610eca3823749\n\nTo answer whether a certain song can be performed on variety shows, the system retrieves the OLAP data warehouse for details about the song, and presents it with results from the Commercial Use Query third-party plugin.\n\n## OLAP Architecture\n\nAs for the OLAP part of this framework, after several rounds of architectural evolution, this is what our current OLAP pipeline looks like. \n\nRaw data is sorted into tags and metrics, which are custom-defined by the analysts. The tags and metrics are under unified management in order to avoid inconsistent definitions. Then, they are combined into various tagsets and metricsets for various queries. \n\nhttps://preview.redd.it/ktxjcr5j70pb1.png?width=1709&amp;format=png&amp;auto=webp&amp;s=74a20a6efcbc0bbe1a4fbfc3c0926beaa2ea60d9\n\nWe have drawn two main takeaways for you from our architectural optimization experience.\n\n**1. Streamline the links**\n\nBefore we adopted Apache Doris, we used to have ClickHouse to accelerate the computation of tags and metrics, and Elasticsearch to process dimensional data. That's two analytic engines and requires us to adapt the query statements to both of them. It was high-maintenance.\n\nThus, we replaced ClickHouse with Apache Doris, and utilized the [Elasticsearch Catalog](https://doris.apache.org/docs/dev/lakehouse/multi-catalog/es) functionality to connect Elasticsearch data to Doris. In this way, we make Doris our unified query gateway. \n\n**2. Split the flat tables**\n\nIn early versions of our OLAP architecture, we used to put data into flat tables, which made things tricky. For one thing, flat tables absorbed all the writing latency from upstreams, and that added up to considerable loss in data realtimeliness. For another, 50% of data in a flat table was dimensional data, which was rarely updated. With every new flat table came some bulky dimensional data that consumed lots of storage space. \n\nTherefore, we split the flat tables into metric tables and dimension tables. As they are updated in different paces, we put them into different data models.\n\n* **Metric tables**: We arrange metric data in the Aggregate Key model of Apache Doris, which means new data will be merged with the old data by way of SUM, MAX, MIN, etc.\n* **Dimension tables**: These tables are in the Unique Key model of Apache Doris, which means new data record will replace the old. This can greatly increase performance in our query scenarios.\n\nYou might ask, does this cause trouble in queries, since most queries require data from both types of tables? Don't worry, we address that with the Rollup feature of Doris. On the basis of the base tables, we can select the dimensions we need to create Rollup views, which will automatically execute GROUP BY. This relieves us of the need to define tags for each Rollup view and largely speed up queries.\n\n## Other Tricks\n\nIn our experience with Apache Doris, we also find some other functionalities handy, so I list them here for you, too:\n\n**1. Materialized View**\n\nA Materialized View is a pre-computed dataset. It is a way to accelerate queries when you frequently need to access data of certain dimensions. In these scenarios, we define derived tags and metrics based on the original ones. For example, we create a derived metric by combining Metric 1, Metric 2, and Metric 3: sum(m1+m2+m3). Then, we can create a Materialized View for it. According to the Doris release schedule, version 2.1 will support multi-table Materialized Views, and we look forward to that.\n\n**2. Flink-Doris-Connector**\n\nThis is for Exactly-Once guarantee in data ingestion. The Flink-Doris-Connector implements a checkpoint mechanism and two-stage commit, and allows for auto data synchronization from relational databases to Doris.\n\n**3. Compaction**\n\nWhen the number of aggregation tasks or data volume becomes overwhelming for Flink, there might be huge latency in data compaction. We solve that with Vertical Compaction and Segment Compaction. Vertical Compaction supports loading of only part of the columns, so it can reduce storage consumption when compacting flat tables. Segment Compaction can avoid generating too much segments during data writing, and allows for compaction while writing simultaneously.   \n\n## What's Next\n\nWith an aim to reduce costs and increase service availability, we plan to test the newly released Storage-Compute Separation and Cross-Cluster Replication of Doris, and we embrace any ideas and inputs about the SuperSonic framework and the [Apache Doris](https://doris.apache.org) project.", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LLM-Powered OLAP: the Tencent Experience with Apache Doris", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"vbpz86j770pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 24, "x": 108, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5011f0dc0d1a2ec7b4155fd8a3cbcbad2bad2e2c"}, {"y": 48, "x": 216, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bcd6992aa1f23379a4436a1e731d4bfba013e539"}, {"y": 72, "x": 320, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ea6752b7d6bed156f127736a5d18a1f14181fcd"}, {"y": 144, "x": 640, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c3d607ec9090e8f8b848b3f358136c24060ef185"}, {"y": 216, "x": 960, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=610671041c925f55f63fbb09f6a330523004fea7"}, {"y": 243, "x": 1080, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=99f1ff878dfc02bb03899f68fca90c9bc7094771"}], "s": {"y": 289, "x": 1280, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=19ecffffeebfff0f6add8e655cd91d0fd0ff3c96"}, "id": "vbpz86j770pb1"}, "9y954hia70pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 34, "x": 108, "u": "https://preview.redd.it/9y954hia70pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9c201f66822514e79da322017c64b409f9ea24b"}, {"y": 69, "x": 216, "u": "https://preview.redd.it/9y954hia70pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed64d14e678ec53942372ee4fcabe173bc900910"}, {"y": 103, "x": 320, "u": "https://preview.redd.it/9y954hia70pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=81ec11213113b633f48521931db117dfcad94881"}, {"y": 206, "x": 640, "u": "https://preview.redd.it/9y954hia70pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=972f599321e1027dcde0a79311a47480234ef30b"}, {"y": 310, "x": 960, "u": "https://preview.redd.it/9y954hia70pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=56205a41e5767b90619b6c325509b8f1631ac64f"}, {"y": 349, "x": 1080, "u": "https://preview.redd.it/9y954hia70pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=962ea1fef09fa1407abd84a8bc3b062cebf6fb1e"}], "s": {"y": 647, "x": 2001, "u": "https://preview.redd.it/9y954hia70pb1.png?width=2001&amp;format=png&amp;auto=webp&amp;s=e78d5f04dd16584d2dcc773caf76a12fdb7c32ca"}, "id": "9y954hia70pb1"}, "enc3w1ne70pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 94, "x": 108, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=42bcfc462e561ff5af366b528611852416c9b8a4"}, {"y": 188, "x": 216, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c227e77b331648ead6b109041ce8473155ab7457"}, {"y": 279, "x": 320, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=03e897f97ad07a459ede93cd8a1c195be4d9ed69"}, {"y": 558, "x": 640, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5c51c058338bf94061e6f84b11f83f192b108d5"}, {"y": 837, "x": 960, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4abf4b498a679b70beaf5475223390586986c25e"}, {"y": 942, "x": 1080, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6aaa582b0e3446e4d49c1d0e73fb1a14cef922a"}], "s": {"y": 1117, "x": 1280, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=fd59180d437a7676b5d0bb9d4d9967956abc2638"}, "id": "enc3w1ne70pb1"}, "ktxjcr5j70pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 70, "x": 108, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=85b630d51c4c94819641df2acdbb4d7d84673be0"}, {"y": 141, "x": 216, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=22f57888c844e1c4ea927026c481951360ff129c"}, {"y": 209, "x": 320, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=af88476b1ca90bcf665560b2f263d2275f4f9329"}, {"y": 419, "x": 640, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=10d591e9e4f2aaa8a8153d404a88c834eaf4f7e3"}, {"y": 628, "x": 960, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b63f1f9bfce357da1f57a02d77be78e19a3dad4f"}, {"y": 707, "x": 1080, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4d462c0bf09d697485237a6fc9d0afd216903e0"}], "s": {"y": 1119, "x": 1709, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=1709&amp;format=png&amp;auto=webp&amp;s=74a20a6efcbc0bbe1a4fbfc3c0926beaa2ea60d9"}, "id": "ktxjcr5j70pb1"}, "x640kok470pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 21, "x": 108, "u": "https://preview.redd.it/x640kok470pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d300f206e846fd2f3b9d3ce88485bf2bbd0de1a9"}, {"y": 42, "x": 216, "u": "https://preview.redd.it/x640kok470pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ff74a758702857bef15352b540b1fd2e3bceb3c"}, {"y": 63, "x": 320, "u": "https://preview.redd.it/x640kok470pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17d9ea618da053949698f43e51963cb69581fd81"}, {"y": 126, "x": 640, "u": "https://preview.redd.it/x640kok470pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8504244286c17884a71ba059ee7761eb4d3ecc2d"}, {"y": 189, "x": 960, "u": "https://preview.redd.it/x640kok470pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=730a83d8bf6d3d51e13b8b4508bfc593e94c04a0"}, {"y": 213, "x": 1080, "u": "https://preview.redd.it/x640kok470pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=048586bf7b52b012aaf22c305513cc43e25e60ad"}], "s": {"y": 253, "x": 1280, "u": "https://preview.redd.it/x640kok470pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=5b16f5cdd9282ce2b35a0b3f1329f8906e9a4580"}, "id": "x640kok470pb1"}, "v1mp12pc70pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 34, "x": 108, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=357427b3ea7f5e16f02709bbf50cd3af373da215"}, {"y": 69, "x": 216, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3d21b2f7cf3e6db96b9607946ed284eb14b6453"}, {"y": 103, "x": 320, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cdec5832a1cc8593b6d846bbb5cf336db34abc40"}, {"y": 206, "x": 640, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d3150f48769e9e2172320a8c8763304846a4f50"}, {"y": 309, "x": 960, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=03d0e9d743625a8be7c824ae8773cc5a95b8754a"}, {"y": 348, "x": 1080, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6f8adc68080bc941cee9b0cea6ae9f1e2ba37980"}], "s": {"y": 645, "x": 2001, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=2001&amp;format=png&amp;auto=webp&amp;s=a32e44e72befa0dc6f06c57f8bfe38de37848cbe"}, "id": "v1mp12pc70pb1"}, "7lq8zqv870pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 34, "x": 108, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=368bef3a7d854eed935982aa299905e70734ca9a"}, {"y": 68, "x": 216, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9cf14f63c0087b417525337767bde89cb02668c5"}, {"y": 101, "x": 320, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=535170a191f09050775ef75973c97021b22c42b9"}, {"y": 203, "x": 640, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3e741fdade184cc6c0ff2facd11d31f0440a74a"}, {"y": 304, "x": 960, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=883cce243ec1c26b0fd3cae8c3d4c1580048fea0"}, {"y": 342, "x": 1080, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=da189f6022e9c437128145b014fb23b8c3998a3a"}], "s": {"y": 406, "x": 1280, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=5b8aae6c9f9d5313a771cc25ce33654f99178b26"}, "id": "7lq8zqv870pb1"}, "rkm8exyg70pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f6b398cb2ccfcd274a9fd145d164c8d6e4cc172e"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b8eaf8fae460bef52c3aa141890ed91d38eb8b94"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8c052b100dcf613b2e1ec60c94408ac38a69d15"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=78e674f9c90dc105663162195b9de99b7f1bc639"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd7695543312c59e43244bbf586b06da46887d23"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cc93c03b48a6f7bdf12a507c428348fe381b876a"}], "s": {"y": 1126, "x": 2001, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=2001&amp;format=png&amp;auto=webp&amp;s=5f197bef98beca33e62d871494b610eca3823749"}, "id": "rkm8exyg70pb1"}}, "name": "t3_16ltnjk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4igAIN6ENlVpJBCJMi0pcMl9zyK07LJvjL_Cz2DDY8M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1695038544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re looking for a LLM+OLAP solution, you might be inspired from the practice at Tencent.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;By&lt;/strong&gt; &lt;a href=\"https://medium.com/geekculture/llm-powered-olap-the-tencent-experience-with-apache-doris-9ecc779450e\"&gt;&lt;strong&gt;Jun Zhang&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;, data platform engineer at Tencent&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Six months ago, I wrote about &lt;a href=\"https://medium.com/geekculture/tencent-data-engineer-why-we-go-from-clickhouse-to-apache-doris-db120f324290\"&gt;why we replaced ClickHouse with Apache Doris as an OLAP engine&lt;/a&gt; for our data management system. Back then, we were struggling with the auto-generation of SQL statements. As days pass, we have made progresses big enough to be references for you (I think), so here I am again. &lt;/p&gt;\n\n&lt;p&gt;We have adopted Large Language Models (LLM) to empower our Doris-based OLAP services.&lt;/p&gt;\n\n&lt;h2&gt;LLM + OLAP&lt;/h2&gt;\n\n&lt;p&gt;Our incentive was to save our internal staff from the steep learning curve of SQL writing. Thus, we used LLM as an intermediate. It transforms natural language questions into SQL statements and sends the SQLs to the OLAP engine for execution.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/x640kok470pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b16f5cdd9282ce2b35a0b3f1329f8906e9a4580\"&gt;https://preview.redd.it/x640kok470pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b16f5cdd9282ce2b35a0b3f1329f8906e9a4580&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Like every AI-related experience, we came across some friction:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;LLM does not understand data jargons, like &amp;quot;fields&amp;quot;, &amp;quot;rows&amp;quot;, &amp;quot;columns&amp;quot; and &amp;quot;tables&amp;quot;. Instead, they can perfectly translate business terms like &amp;quot;corporate income&amp;quot; and &amp;quot;DAU&amp;quot;, which are basically what the fields/rows/columns are about. That means it can work well only if the analysts use the exact right word to refer to the metric they need when typing their questions.&lt;/li&gt;\n&lt;li&gt;The LLM we are using is slow in inference. It takes over 10 seconds to respond. As it charges fees by token, cost-effectiveness becomes a problem.&lt;/li&gt;\n&lt;li&gt;Although the LLM is trained on a large collection of public datasets, it is under-informed of niche knowledge. In our case, the LLM is super unfamiliar with indie songs, so even if the songs are included in our database, the LLM will not able to identify them properly. &lt;/li&gt;\n&lt;li&gt;Sometimes our input questions require adequate and latest legal, political, financial, and regulatory information, which is hard to be included in a training dataset or knowledge base. We need to connect the LLM to wider info bases in order to perform more diversified tasks.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;We knock these problems down one by one.&lt;/p&gt;\n\n&lt;h3&gt;1. A semantic layer&lt;/h3&gt;\n\n&lt;p&gt;For problem No.1, we introduce a semantic layer between the LLM and the OLAP engine. This layer translates business terms into the corresponding data fields. It can identify data filtering conditions from the various natural language wordings, relate them to the metrics involved, and then generate SQL statements. &lt;/p&gt;\n\n&lt;p&gt;Besides that, the semantic layer can optimize the computation logic. When analysts input a question that involves a complicated query, let&amp;#39;s say, a multi-table join, the semantic layer can split that into multiple single-table queries to reduce semantic distortion.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vbpz86j770pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19ecffffeebfff0f6add8e655cd91d0fd0ff3c96\"&gt;https://preview.redd.it/vbpz86j770pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19ecffffeebfff0f6add8e655cd91d0fd0ff3c96&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;2. LLM parsing rules&lt;/h3&gt;\n\n&lt;p&gt;To increase cost-effectiveness in using LLM, we evaluate the computation complexity of all scenarios, such as metric computation, detailed record retrieval, and user segmentation. Then, we create rules and dedicate the LLM-parsing step to only complicated tasks. That means for the simple computation tasks, it will skip the parsing. &lt;/p&gt;\n\n&lt;p&gt;For example, when an analyst inputs &amp;quot;tell me the earnings of the major musical platforms&amp;quot;, the LLM identifies that this question only entails several metrics or dimensions, so it will not further parse it but send it straight for SQL generation and execution. This can largely shorten query response time and reduce API expenses. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7lq8zqv870pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b8aae6c9f9d5313a771cc25ce33654f99178b26\"&gt;https://preview.redd.it/7lq8zqv870pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b8aae6c9f9d5313a771cc25ce33654f99178b26&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;3. Schema Mapper and external knowledge base&lt;/h3&gt;\n\n&lt;p&gt;To empower the LLM with niche knowledge, we added a Schema Mapper upstream from the LLM. The Schema Mapper maps the input question to an external knowledge base, and then the LLM will do parsing.&lt;/p&gt;\n\n&lt;p&gt;We are constantly testing and optimizing the Schema Mapper. We categorize and rate content in the external knowledge base, and do various levels of mapping (full-text mapping and fuzzy mapping) to enable better semantic parsing.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9y954hia70pb1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e78d5f04dd16584d2dcc773caf76a12fdb7c32ca\"&gt;https://preview.redd.it/9y954hia70pb1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e78d5f04dd16584d2dcc773caf76a12fdb7c32ca&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;4. Plugins&lt;/h3&gt;\n\n&lt;p&gt;We used plugins to connect the LLM to more fields of information, and we have different integration methods for different types of plugins:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Embedding local files&lt;/strong&gt;: This is especially useful when we need to &amp;quot;teach&amp;quot; the LLM the latest regulatory policies, which are often text files. Firstly, the system vectorizes the local text file, executes semantic searches to find matching or similar terms in the local file, extracts the relevant contents and puts them into the LLM parsing window to generate output. &lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Third-party plugins&lt;/strong&gt;: The marketplace is full of third-party plugins that are designed for all kinds of sectors. With them, the LLM is able to deal with wide-ranging topics. Each plugin has its own prompts and calling function. Once the input question hits a prompt, the relevant plugin will be called.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v1mp12pc70pb1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a32e44e72befa0dc6f06c57f8bfe38de37848cbe\"&gt;https://preview.redd.it/v1mp12pc70pb1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a32e44e72befa0dc6f06c57f8bfe38de37848cbe&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;After we are done with above four optimizations, the SuperSonic framework comes into being.&lt;/p&gt;\n\n&lt;h2&gt;The SuperSonic framework&lt;/h2&gt;\n\n&lt;p&gt;Now let me walk you through this &lt;a href=\"https://github.com/tencentmusic/supersonic\"&gt;framework&lt;/a&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/enc3w1ne70pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd59180d437a7676b5d0bb9d4d9967956abc2638\"&gt;https://preview.redd.it/enc3w1ne70pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd59180d437a7676b5d0bb9d4d9967956abc2638&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;An analyst inputs a question.&lt;/li&gt;\n&lt;li&gt;The Schema Mapper maps the question to an external knowledge base.&lt;/li&gt;\n&lt;li&gt;If there are matching fields in the external knowledge base, the question will not be parsed by the LLM. Instead, a metric computation formula will trigger the OLAP engine to start querying. If there is no matching field, the question will enter the LLM.&lt;/li&gt;\n&lt;li&gt;Based on the pre-defined rules, the LLM rates the complexity level of the question. If it is a simple query, it will go directly to the OLAP engine; if it is a complicated query, it will be semantically parsed and converted to a DSL statement.&lt;/li&gt;\n&lt;li&gt;At the Semantic Layer, the DSL statement will be split based on its query scenario. For example, if it is a multi-table join query, this layer will generate multiple single-table query SQL statements.&lt;/li&gt;\n&lt;li&gt;If the question involves external knowledge, the LLM will call a third-party plugin.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rkm8exyg70pb1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f197bef98beca33e62d871494b610eca3823749\"&gt;https://preview.redd.it/rkm8exyg70pb1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f197bef98beca33e62d871494b610eca3823749&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To answer whether a certain song can be performed on variety shows, the system retrieves the OLAP data warehouse for details about the song, and presents it with results from the Commercial Use Query third-party plugin.&lt;/p&gt;\n\n&lt;h2&gt;OLAP Architecture&lt;/h2&gt;\n\n&lt;p&gt;As for the OLAP part of this framework, after several rounds of architectural evolution, this is what our current OLAP pipeline looks like. &lt;/p&gt;\n\n&lt;p&gt;Raw data is sorted into tags and metrics, which are custom-defined by the analysts. The tags and metrics are under unified management in order to avoid inconsistent definitions. Then, they are combined into various tagsets and metricsets for various queries. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ktxjcr5j70pb1.png?width=1709&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=74a20a6efcbc0bbe1a4fbfc3c0926beaa2ea60d9\"&gt;https://preview.redd.it/ktxjcr5j70pb1.png?width=1709&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=74a20a6efcbc0bbe1a4fbfc3c0926beaa2ea60d9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We have drawn two main takeaways for you from our architectural optimization experience.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Streamline the links&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Before we adopted Apache Doris, we used to have ClickHouse to accelerate the computation of tags and metrics, and Elasticsearch to process dimensional data. That&amp;#39;s two analytic engines and requires us to adapt the query statements to both of them. It was high-maintenance.&lt;/p&gt;\n\n&lt;p&gt;Thus, we replaced ClickHouse with Apache Doris, and utilized the &lt;a href=\"https://doris.apache.org/docs/dev/lakehouse/multi-catalog/es\"&gt;Elasticsearch Catalog&lt;/a&gt; functionality to connect Elasticsearch data to Doris. In this way, we make Doris our unified query gateway. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Split the flat tables&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In early versions of our OLAP architecture, we used to put data into flat tables, which made things tricky. For one thing, flat tables absorbed all the writing latency from upstreams, and that added up to considerable loss in data realtimeliness. For another, 50% of data in a flat table was dimensional data, which was rarely updated. With every new flat table came some bulky dimensional data that consumed lots of storage space. &lt;/p&gt;\n\n&lt;p&gt;Therefore, we split the flat tables into metric tables and dimension tables. As they are updated in different paces, we put them into different data models.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Metric tables&lt;/strong&gt;: We arrange metric data in the Aggregate Key model of Apache Doris, which means new data will be merged with the old data by way of SUM, MAX, MIN, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dimension tables&lt;/strong&gt;: These tables are in the Unique Key model of Apache Doris, which means new data record will replace the old. This can greatly increase performance in our query scenarios.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You might ask, does this cause trouble in queries, since most queries require data from both types of tables? Don&amp;#39;t worry, we address that with the Rollup feature of Doris. On the basis of the base tables, we can select the dimensions we need to create Rollup views, which will automatically execute GROUP BY. This relieves us of the need to define tags for each Rollup view and largely speed up queries.&lt;/p&gt;\n\n&lt;h2&gt;Other Tricks&lt;/h2&gt;\n\n&lt;p&gt;In our experience with Apache Doris, we also find some other functionalities handy, so I list them here for you, too:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Materialized View&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A Materialized View is a pre-computed dataset. It is a way to accelerate queries when you frequently need to access data of certain dimensions. In these scenarios, we define derived tags and metrics based on the original ones. For example, we create a derived metric by combining Metric 1, Metric 2, and Metric 3: sum(m1+m2+m3). Then, we can create a Materialized View for it. According to the Doris release schedule, version 2.1 will support multi-table Materialized Views, and we look forward to that.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Flink-Doris-Connector&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This is for Exactly-Once guarantee in data ingestion. The Flink-Doris-Connector implements a checkpoint mechanism and two-stage commit, and allows for auto data synchronization from relational databases to Doris.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Compaction&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;When the number of aggregation tasks or data volume becomes overwhelming for Flink, there might be huge latency in data compaction. We solve that with Vertical Compaction and Segment Compaction. Vertical Compaction supports loading of only part of the columns, so it can reduce storage consumption when compacting flat tables. Segment Compaction can avoid generating too much segments during data writing, and allows for compaction while writing simultaneously.   &lt;/p&gt;\n\n&lt;h2&gt;What&amp;#39;s Next&lt;/h2&gt;\n\n&lt;p&gt;With an aim to reduce costs and increase service availability, we plan to test the newly released Storage-Compute Separation and Cross-Cluster Replication of Doris, and we embrace any ideas and inputs about the SuperSonic framework and the &lt;a href=\"https://doris.apache.org\"&gt;Apache Doris&lt;/a&gt; project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?auto=webp&amp;s=5f61580d72d15c4af88138da6f130fbdf4aeefd4", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3bd0a5e9400495e817e7981e0a0ff040f121c18", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=01512dd55fb2d618dbcf83206232ffe88e8a75e8", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=86eeafc727666bcfb0a0f172af6631e8557fee71", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df7a0cd5b0d2be4f7aa5734db21b3181fccb140c", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c48a827ed90e2de27938c21d1bc01e845e9855c", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a0931acbd50daea14a90b69d78a47e6d08013d26", "width": 1080, "height": 720}], "variants": {}, "id": "Y6TDB5YBDkbhyUsK-oEnrD9W1hY5aPbd3qPYbOJTRyM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ltnjk", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ltnjk/llmpowered_olap_the_tencent_experience_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ltnjk/llmpowered_olap_the_tencent_experience_with/", "subreddit_subscribers": 1050659, "created_utc": 1695038544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi folks,\n\nI've been looking for a data science or adjacent job since May 2023. Because I have a dual citizenship for both the US and Austria, I could apply in both countries. I've noticed, that the job market is way harsher to get in in the US and that ghosting after follow up emails are common. Now I'm just really happy that I finally have two jobs to choose from. Do you also have similar experiences comparing application processes on different continents?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/niulhk65q0pb1.png?width=3200&amp;format=png&amp;auto=webp&amp;s=94f5ec9075831646993442e2852467788e99bf0b", "author_fullname": "t2_66gywg12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finally got job offer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 52, "top_awarded_type": null, "hide_score": true, "media_metadata": {"niulhk65q0pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 40, "x": 108, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=63d947a8fd8eff2bd0868f51d54f03b17492068b"}, {"y": 81, "x": 216, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac2dab363e8ddf7d3022a95ae72b538407af2bc7"}, {"y": 120, "x": 320, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf8287d9891c577eb50acf22529e2e10d1d2ca52"}, {"y": 240, "x": 640, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=eacc69ffb028f649057013e49a35341e060670c0"}, {"y": 360, "x": 960, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=02404eb520b2ff321d4189918f53f6ed0ff4b275"}, {"y": 405, "x": 1080, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28ef1dc8ea7aac5ee98b202e59e017bbac006c29"}], "s": {"y": 1200, "x": 3200, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=3200&amp;format=png&amp;auto=webp&amp;s=94f5ec9075831646993442e2852467788e99bf0b"}, "id": "niulhk65q0pb1"}}, "name": "t3_16lvzzt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/LrTQSECqY8c_h4QnA9-Cy-9UOGmbJJxuwhAtVG9zKPQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695044712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking for a data science or adjacent job since May 2023. Because I have a dual citizenship for both the US and Austria, I could apply in both countries. I&amp;#39;ve noticed, that the job market is way harsher to get in in the US and that ghosting after follow up emails are common. Now I&amp;#39;m just really happy that I finally have two jobs to choose from. Do you also have similar experiences comparing application processes on different continents?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/niulhk65q0pb1.png?width=3200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94f5ec9075831646993442e2852467788e99bf0b\"&gt;https://preview.redd.it/niulhk65q0pb1.png?width=3200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94f5ec9075831646993442e2852467788e99bf0b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lvzzt", "is_robot_indexable": true, "report_reasons": null, "author": "layzrblayzr", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lvzzt/finally_got_job_offer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lvzzt/finally_got_job_offer/", "subreddit_subscribers": 1050659, "created_utc": 1695044712.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So Im a fresher whose been selected for the role of a junior data scientist in FMCG domain. What all should I know about work as a junior DS in a company ? Also what libraries, models etc are used in the industry ?   \nBecause in my undergrad all Ive been doing is a lot of data cleaning, dashboarding and then fitting ML models. Ive not worked on very complex projects and have less intuition about the mathematics behind NLP, DL ( which is why Im kind of nervous. Im good with statistscs, and math behind ML models but not the DL, NLP part  ).   \nSo what all should I do to start off in the right direction and not be struggling in the beginning?   \nThanks   \nSorry for the bad english.", "author_fullname": "t2_4yh7qc8e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice for fresher going to work as a junior data scientist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16lvxs8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695044554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So Im a fresher whose been selected for the role of a junior data scientist in FMCG domain. What all should I know about work as a junior DS in a company ? Also what libraries, models etc are used in the industry ?&lt;br/&gt;\nBecause in my undergrad all Ive been doing is a lot of data cleaning, dashboarding and then fitting ML models. Ive not worked on very complex projects and have less intuition about the mathematics behind NLP, DL ( which is why Im kind of nervous. Im good with statistscs, and math behind ML models but not the DL, NLP part  ).&lt;br/&gt;\nSo what all should I do to start off in the right direction and not be struggling in the beginning?&lt;br/&gt;\nThanks&lt;br/&gt;\nSorry for the bad english.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lvxs8", "is_robot_indexable": true, "report_reasons": null, "author": "JackOfFarts69", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lvxs8/need_advice_for_fresher_going_to_work_as_a_junior/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lvxs8/need_advice_for_fresher_going_to_work_as_a_junior/", "subreddit_subscribers": 1050659, "created_utc": 1695044554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am reading these 3 articles below and it is still not clear to me what\u2019s the best practice to follow to guide me in choosing which quantized Llama 2 model to use.\n\nhttps://huggingface.co/blog/gptq-integration\n\nhttps://huggingface.co/blog/overview-quantization-transformers\n\nhttps://towardsai.net/p/machine-learning/gptq-quantization-on-a-llama-2-7b-fine-tuned-model-with-huggingface?amp=1\n\nQuestions:\n1) I understand there are currently 4 quantized Llama 2 models (8, 4, 3, and 2-bit precision) to choose from. Is this right?\n2) with the default Llama 2 model, how many bit precision is it?\n3) are there any best practice guide to choose which quantized Llama 2 model to use?\n\nWould really appreciate any input on the above, even if you only know the answer to 1 or 2 of the questions above. Many thanks!", "author_fullname": "t2_hjlrj5fp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best practice in choosing which quantized Llama 2 model to use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16lvg09", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695043348.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am reading these 3 articles below and it is still not clear to me what\u2019s the best practice to follow to guide me in choosing which quantized Llama 2 model to use.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/blog/gptq-integration\"&gt;https://huggingface.co/blog/gptq-integration&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/blog/overview-quantization-transformers\"&gt;https://huggingface.co/blog/overview-quantization-transformers&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://towardsai.net/p/machine-learning/gptq-quantization-on-a-llama-2-7b-fine-tuned-model-with-huggingface?amp=1\"&gt;https://towardsai.net/p/machine-learning/gptq-quantization-on-a-llama-2-7b-fine-tuned-model-with-huggingface?amp=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Questions:\n1) I understand there are currently 4 quantized Llama 2 models (8, 4, 3, and 2-bit precision) to choose from. Is this right?\n2) with the default Llama 2 model, how many bit precision is it?\n3) are there any best practice guide to choose which quantized Llama 2 model to use?&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate any input on the above, even if you only know the answer to 1 or 2 of the questions above. Many thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?auto=webp&amp;s=47ea02129ac533db4c597a2f80a442fb31eb4e73", "width": 1300, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=13f69936519f3ed1083e454bff24dc4067317498", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=901c6c6fe4cc140b211c843a661846e5cdf23dc8", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=35112aea46fb9972dd37e3c093cf543164e00c4f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9bc71b145c028b1c883545f904b034d09ec34de6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=13055a88a25666e7555bcd9ddaadaa1b137f2f92", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3c0a09f4143e7814866d28f7c4b5ef8d51d8b81", "width": 1080, "height": 540}], "variants": {}, "id": "awo5B8mlygGaLesfGNL3bFrSvsu-bP1smNljz-2E3vo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lvg09", "is_robot_indexable": true, "report_reasons": null, "author": "--leockl--", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lvg09/whats_the_best_practice_in_choosing_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lvg09/whats_the_best_practice_in_choosing_which/", "subreddit_subscribers": 1050659, "created_utc": 1695043348.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey!\n\nDo you have any reviews on the above mentioned courses from IIT Patna? I seem to have very few testimonials, if I out them under that umbrellas and were not convincing enough. Planning to go for this one but dicy on the value this is going to add since is a 6 month course only.", "author_fullname": "t2_o9ra26w3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PG Certification in Business Data Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lsz9r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695036643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;Do you have any reviews on the above mentioned courses from IIT Patna? I seem to have very few testimonials, if I out them under that umbrellas and were not convincing enough. Planning to go for this one but dicy on the value this is going to add since is a 6 month course only.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lsz9r", "is_robot_indexable": true, "report_reasons": null, "author": "sortchatmetgreet", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lsz9r/pg_certification_in_business_data_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lsz9r/pg_certification_in_business_data_analytics/", "subreddit_subscribers": 1050659, "created_utc": 1695036643.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I ( 20 M ) have an interview this week Friday 22nd 2023 at a Company. I am nervous as this is my first interview as a data science.\nI need guidance on how should I prepare for the interview, what should be my main points for preparation?\n\nAny sources where I can prepare from, will be of great help. Should I make my DSA strong as well, because I have weak knowledge of dsa. I am good in Python, MySql, Power Bi and MS Excel.\n\nI have 5 days left, if through any means I can improve my chances of selection, Please help and guide.", "author_fullname": "t2_g4971z5m2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science Trainee Interview next week, Guidance require !", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_16lsb28", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://a.thumbs.redditmedia.com/lbSiXbMIBHs_IQWffuHzem41r_yL3Mn7Lpo6iPZklp8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695034555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I ( 20 M ) have an interview this week Friday 22nd 2023 at a Company. I am nervous as this is my first interview as a data science.\nI need guidance on how should I prepare for the interview, what should be my main points for preparation?&lt;/p&gt;\n\n&lt;p&gt;Any sources where I can prepare from, will be of great help. Should I make my DSA strong as well, because I have weak knowledge of dsa. I am good in Python, MySql, Power Bi and MS Excel.&lt;/p&gt;\n\n&lt;p&gt;I have 5 days left, if through any means I can improve my chances of selection, Please help and guide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/y9izcngzvzob1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/y9izcngzvzob1.png?auto=webp&amp;s=6058ac12c635888695a3dda53a4e4b895d2a4673", "width": 1080, "height": 1748}, "resolutions": [{"url": "https://preview.redd.it/y9izcngzvzob1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=06a14b50949addf5f163777fbd82ec4e18c9ac85", "width": 108, "height": 174}, {"url": "https://preview.redd.it/y9izcngzvzob1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b393f30a1af05ff88a7047e5303d2d36c5f1cd42", "width": 216, "height": 349}, {"url": "https://preview.redd.it/y9izcngzvzob1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5ff78056e00b1a627188268021cb3fd64c4f198", "width": 320, "height": 517}, {"url": "https://preview.redd.it/y9izcngzvzob1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=665dc6367cdcf6bc8ded62fcd008089a99a7d4fd", "width": 640, "height": 1035}, {"url": "https://preview.redd.it/y9izcngzvzob1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d602a280ea83a55ab83298c0f70f283f9103db39", "width": 960, "height": 1553}, {"url": "https://preview.redd.it/y9izcngzvzob1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9cdb5b65d694f103f1bea902cbceb2427eeeac0c", "width": 1080, "height": 1748}], "variants": {}, "id": "3V_fB24gbhBwu6cmFQC-q4Dcwj7dFc3a6PxVnvktCLw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lsb28", "is_robot_indexable": true, "report_reasons": null, "author": "SlickDrip69", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lsb28/data_science_trainee_interview_next_week_guidance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/y9izcngzvzob1.png", "subreddit_subscribers": 1050659, "created_utc": 1695034555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Weekly Entering &amp; Transitioning - Thread 18 Sep, 2023 - 25 Sep, 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ll6ro", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695009704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;\n&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;\n&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;\n&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;\n&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=\"https://www.reddit.com/r/datascience/wiki/frequently-asked-questions\"&gt;FAQ&lt;/a&gt; and Resources pages on our wiki. You can also search for answers in &lt;a href=\"https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new\"&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "new", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ll6ro", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ll6ro/weekly_entering_transitioning_thread_18_sep_2023/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/datascience/comments/16ll6ro/weekly_entering_transitioning_thread_18_sep_2023/", "subreddit_subscribers": 1050659, "created_utc": 1695009704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Apache Iceberg is a popular open-source table format for large data sets, but I'm curious to know more about how it's being used in the real world. What are some specific use cases where Iceberg is solving problems that were difficult or impossible to solve before? What were the challenges before Iceberg, and how has it made things easier?\n\nI'm particularly interested in hearing from data engineers and scientists who have used Iceberg in production. What are your favorite features? What are some of the challenges you've faced, and how have you overcome them?", "author_fullname": "t2_6cx43fk3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the real use cases being solved using Apache Iceberg and how was it done before or what were the challenges?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lkrwg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695008373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apache Iceberg is a popular open-source table format for large data sets, but I&amp;#39;m curious to know more about how it&amp;#39;s being used in the real world. What are some specific use cases where Iceberg is solving problems that were difficult or impossible to solve before? What were the challenges before Iceberg, and how has it made things easier?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m particularly interested in hearing from data engineers and scientists who have used Iceberg in production. What are your favorite features? What are some of the challenges you&amp;#39;ve faced, and how have you overcome them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lkrwg", "is_robot_indexable": true, "report_reasons": null, "author": "SnooHesitations2050", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lkrwg/what_are_the_real_use_cases_being_solved_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lkrwg/what_are_the_real_use_cases_being_solved_using/", "subreddit_subscribers": 1050659, "created_utc": 1695008373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\n\nAssuming I have data on when a dog runs up the street and when it runs down the street in a time series (let us assume he never gets tired).\n\n\nSo I have data with columns: time, is_going_up, distance_elapsed (percentage change)\n\n\nwhere time is in seconds, and is_going_up is either true or false (if false, the dog is going down).\n\n\nso data might look as such:\n\n1 True 2\n\n2 True 3\n\n3 True 6\n\n4 False 1.3\n\n5 False 4\n\n6 False 4.3\n\n...\n\n100 True 1.5\n\n\n\nI have converted the 2nd column (is_going_up)  to 0 and 1, and have put it through xgboost and got around 70% accuracy.\n\n\nHowever, I am only interested in predicting when the 2nd column will end its current direction. \n\n\nShould I train it with only rows where a new direction started? So that it will predict when new direction will start. Or how should I set up my data for training?\n\n\nMy goal is to have xgboost return True or False. If current direction is True, and xgboost says False, that means it is predicting a change in direction.", "author_fullname": "t2_dk0e7uek2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you setup data for this to train in xgboost (binary classification)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16l6sj0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694971995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Assuming I have data on when a dog runs up the street and when it runs down the street in a time series (let us assume he never gets tired).&lt;/p&gt;\n\n&lt;p&gt;So I have data with columns: time, is_going_up, distance_elapsed (percentage change)&lt;/p&gt;\n\n&lt;p&gt;where time is in seconds, and is_going_up is either true or false (if false, the dog is going down).&lt;/p&gt;\n\n&lt;p&gt;so data might look as such:&lt;/p&gt;\n\n&lt;p&gt;1 True 2&lt;/p&gt;\n\n&lt;p&gt;2 True 3&lt;/p&gt;\n\n&lt;p&gt;3 True 6&lt;/p&gt;\n\n&lt;p&gt;4 False 1.3&lt;/p&gt;\n\n&lt;p&gt;5 False 4&lt;/p&gt;\n\n&lt;p&gt;6 False 4.3&lt;/p&gt;\n\n&lt;p&gt;...&lt;/p&gt;\n\n&lt;p&gt;100 True 1.5&lt;/p&gt;\n\n&lt;p&gt;I have converted the 2nd column (is_going_up)  to 0 and 1, and have put it through xgboost and got around 70% accuracy.&lt;/p&gt;\n\n&lt;p&gt;However, I am only interested in predicting when the 2nd column will end its current direction. &lt;/p&gt;\n\n&lt;p&gt;Should I train it with only rows where a new direction started? So that it will predict when new direction will start. Or how should I set up my data for training?&lt;/p&gt;\n\n&lt;p&gt;My goal is to have xgboost return True or False. If current direction is True, and xgboost says False, that means it is predicting a change in direction.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16l6sj0", "is_robot_indexable": true, "report_reasons": null, "author": "oniongarlic88", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16l6sj0/how_would_you_setup_data_for_this_to_train_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16l6sj0/how_would_you_setup_data_for_this_to_train_in/", "subreddit_subscribers": 1050659, "created_utc": 1694971995.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}