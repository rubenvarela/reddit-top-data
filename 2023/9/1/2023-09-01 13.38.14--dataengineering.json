{"kind": "Listing", "data": {"after": null, "dist": 20, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry about the rant but man it\u2019s frustrating to see no data engineering job opportunities in Canada even at senior level.\n\nThe few that are there have 400-500 applicants. Just saw a job post which I thought was interesting only to find it has close to 900+ applicants. How will you even get a call. \n\nEnd of rant.", "author_fullname": "t2_4ah8jqqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "There are no jobs and it is mildly frustrating!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166nu11", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693517707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry about the rant but man it\u2019s frustrating to see no data engineering job opportunities in Canada even at senior level.&lt;/p&gt;\n\n&lt;p&gt;The few that are there have 400-500 applicants. Just saw a job post which I thought was interesting only to find it has close to 900+ applicants. How will you even get a call. &lt;/p&gt;\n\n&lt;p&gt;End of rant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166nu11", "is_robot_indexable": true, "report_reasons": null, "author": "idreamoffood101", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166nu11/there_are_no_jobs_and_it_is_mildly_frustrating/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166nu11/there_are_no_jobs_and_it_is_mildly_frustrating/", "subreddit_subscribers": 126155, "created_utc": 1693517707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_viq1t90m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipelines with Python and SQL - Part 1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_166rste", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6FoFuooprnsoo_O92yU7D5QwKRgX2jMp3Z9qkn0aDPc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693527254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "stephendavidwilliams.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://stephendavidwilliams.com/data-pipelines-with-python-and-sql-part-1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?auto=webp&amp;s=e69f6e29dcdebd0a3dee751d2b1929e07664b4e9", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6bfac54f8dfdd46f74556e776eaa81a50d7f8842", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=36f68e2eba4566517be4d4229dc9ba76dc515d56", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c70788f4e382f796dcb1f82d6ef8cbcd26bd755", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c9de6060ba8c380b2abe2b5fea8e8cc8c84546ea", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e8ed329f03658b8866641d22bdee2bd47429dcb1", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02d1773f6e573fd02e765a244977bac8af1408ef", "width": 1080, "height": 567}], "variants": {}, "id": "lo0sGmJanYdR0vXmZrljanaI_RhUtYEp-GDYPws1knE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "166rste", "is_robot_indexable": true, "report_reasons": null, "author": "sdw_solutions", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166rste/data_pipelines_with_python_and_sql_part_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://stephendavidwilliams.com/data-pipelines-with-python-and-sql-part-1", "subreddit_subscribers": 126155, "created_utc": 1693527254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi friends,\n\nSo I have some experience as Data Analyst. Have some python and SQL knowledge. But want to be an AWS data engineer. Some comments here and there said **Cantrill** is good. I checked out the courses but didn't understand the naming of the course tracks.\n\nI'm not familiar with AWS certification titles. What would you suggest I start with on that website, or do you have anything I can take a look at to understand all the technical roles, designations, jargons, so that I can have a better understanding before I can start my DE studies.", "author_fullname": "t2_jl4hi61l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DA to DE - how should I prepare?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1673fap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693564246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi friends,&lt;/p&gt;\n\n&lt;p&gt;So I have some experience as Data Analyst. Have some python and SQL knowledge. But want to be an AWS data engineer. Some comments here and there said &lt;strong&gt;Cantrill&lt;/strong&gt; is good. I checked out the courses but didn&amp;#39;t understand the naming of the course tracks.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not familiar with AWS certification titles. What would you suggest I start with on that website, or do you have anything I can take a look at to understand all the technical roles, designations, jargons, so that I can have a better understanding before I can start my DE studies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1673fap", "is_robot_indexable": true, "report_reasons": null, "author": "pale-blue-dotter", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1673fap/da_to_de_how_should_i_prepare/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1673fap/da_to_de_how_should_i_prepare/", "subreddit_subscribers": 126155, "created_utc": 1693564246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am recently grinding SQL on both platform. \n\nI just found StrataScratch SQL questions are more straightforward, and I can do 10+ medium questions per day. Its difficulty is more focus on how well you understand the basic fundamental commands of SQL, aka how you can use join, subquery and cte altogether. The learning curve is very smooth. Hence the user interface is more friendly, every question has its official solution. \n\nOn the other hand, Leetcode SQL's difficulty is more \"tricky\", it focuses on how well you understand the problem and how you can find a \"trick\" to hack the problem. Honestly, my progress is very slow on Leetcode style SQL, and I have learned very few from it...\n\nAny thoughts? \n\n&amp;#x200B;", "author_fullname": "t2_csk6gf7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some thoughts about Leetcode SQL and StrataScratch SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166wmef", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693540773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am recently grinding SQL on both platform. &lt;/p&gt;\n\n&lt;p&gt;I just found StrataScratch SQL questions are more straightforward, and I can do 10+ medium questions per day. Its difficulty is more focus on how well you understand the basic fundamental commands of SQL, aka how you can use join, subquery and cte altogether. The learning curve is very smooth. Hence the user interface is more friendly, every question has its official solution. &lt;/p&gt;\n\n&lt;p&gt;On the other hand, Leetcode SQL&amp;#39;s difficulty is more &amp;quot;tricky&amp;quot;, it focuses on how well you understand the problem and how you can find a &amp;quot;trick&amp;quot; to hack the problem. Honestly, my progress is very slow on Leetcode style SQL, and I have learned very few from it...&lt;/p&gt;\n\n&lt;p&gt;Any thoughts? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166wmef", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Astronomer-471", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166wmef/some_thoughts_about_leetcode_sql_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166wmef/some_thoughts_about_leetcode_sql_and/", "subreddit_subscribers": 126155, "created_utc": 1693540773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was wondering how other teams use dbt, so I made this very short survey to understand better. I'll share the results later in the comments.  \n\n\n[https://forms.gle/zwwfDAVYGu37nz2W6](https://forms.gle/zwwfDAVYGu37nz2W6)", "author_fullname": "t2_tbjno", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt. What is it good for? A 42-second survey.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166nao6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693516475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering how other teams use dbt, so I made this very short survey to understand better. I&amp;#39;ll share the results later in the comments.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://forms.gle/zwwfDAVYGu37nz2W6\"&gt;https://forms.gle/zwwfDAVYGu37nz2W6&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?auto=webp&amp;s=3e5851ed8f8cbec67e9d3d95ce1a6ab2fd5766a4", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95d563b3730f933218dfe75138324b3840fffa23", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=39f7c8c738ffa952f983e1213eb4a3465e80a6f4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c2cf8cfd3a17b21357728bbed54d062c79cd3d4", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0450a7adb08aa6c7a15f18ba4e1551c53a5a1cc", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c36a1277013d617e183e04aeec20fd0d1b96478b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ed8af9722ace6f0c6b022974f642c2b2e4ac3521", "width": 1080, "height": 567}], "variants": {}, "id": "L38pNlP_aAcmNKnS5YBL_J4J7QdBMDhPuBkZmM20ZLY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166nao6", "is_robot_indexable": true, "report_reasons": null, "author": "PrivateH", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166nao6/dbt_what_is_it_good_for_a_42second_survey/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166nao6/dbt_what_is_it_good_for_a_42second_survey/", "subreddit_subscribers": 126155, "created_utc": 1693516475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at a small startup company, in a data intensive field. I started as a SWE developer, but as data needs grew I transitioned to a more data oriented work such as: settings up our data pipeline (streaming using gcp tools + batch using airflow) lot of data modeling with dbt + dwh, reporting with bi tools, etc. But I also work on backend services that make real time use of our data and provide responses that other components in our backend need to operate, which is purely SWE engineering work (the challenges there are scale, performance and working with oltp databases). So I keep wondering, when I'm asked, what is my title?", "author_fullname": "t2_hf93fu6t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is my title?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1673me5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693564904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a small startup company, in a data intensive field. I started as a SWE developer, but as data needs grew I transitioned to a more data oriented work such as: settings up our data pipeline (streaming using gcp tools + batch using airflow) lot of data modeling with dbt + dwh, reporting with bi tools, etc. But I also work on backend services that make real time use of our data and provide responses that other components in our backend need to operate, which is purely SWE engineering work (the challenges there are scale, performance and working with oltp databases). So I keep wondering, when I&amp;#39;m asked, what is my title?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1673me5", "is_robot_indexable": true, "report_reasons": null, "author": "RiverPuzzleheaded710", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1673me5/what_is_my_title/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1673me5/what_is_my_title/", "subreddit_subscribers": 126155, "created_utc": 1693564904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a new(ish) DataBricks lakehouse with a traditional medallion architecture (bronze -&gt; silver -&gt; gold). \n\nIn a datawarehouse, I would expect to use surrogate keys (rather than natural keys) in the silver layer, to account for things like data coming from two different sources. \n\nWhat is the best practise in lakehouses? Should we still be using surrogate keys or has the practice changed?", "author_fullname": "t2_5cbo0joy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lakehouse Silver Layer - Surrogate or Natural Keys?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166zppb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693551045.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a new(ish) DataBricks lakehouse with a traditional medallion architecture (bronze -&amp;gt; silver -&amp;gt; gold). &lt;/p&gt;\n\n&lt;p&gt;In a datawarehouse, I would expect to use surrogate keys (rather than natural keys) in the silver layer, to account for things like data coming from two different sources. &lt;/p&gt;\n\n&lt;p&gt;What is the best practise in lakehouses? Should we still be using surrogate keys or has the practice changed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166zppb", "is_robot_indexable": true, "report_reasons": null, "author": "Edd037", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166zppb/lakehouse_silver_layer_surrogate_or_natural_keys/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166zppb/lakehouse_silver_layer_surrogate_or_natural_keys/", "subreddit_subscribers": 126155, "created_utc": 1693551045.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a pretty vast network of data professionals, most of which are data engineers, that I'm connected with, And while many of them have at least one certification, I would say that the vast majority of them our experts that have paved The way for their own career without certification farming. Recently, I've noticed a community here on Reddit based around Microsoft Azure That is completely focused on systematically farming entertaining certifications for the Microsoft Azure platform. This isn't entirely unusual, as there are many communities based around obtaining certifications, GCP and AWS for example. The thing that I find really strange, however, is that the vast majority of people that post in this community and are systematically farming these certifications are not data professionals, they are fakers and cheaters. It's exclusively stated that brain dumping is not allowed and is considered cheating on these exams, yet despite best attempts by mods and members of the community, it's impossible to completely negate systematic dumping and studying of information for these exams. This results in people who have half a dozen or more Microsoft Azure certifications, with practically no professional experience in anything related to data engineering, who then go on to try and seek a job.....\n\n\nLet me be clear as to why this is such a huge problem. Recruiters are often incredibly clueless as to what makes someone successful in a job. They see that a lot of people now have certifications in Azure. So they list that they want people who have at least several Microsoft Azure certifications. Then, they see a large pool of applicants applying for these positions that have no experience, no passion, have not proven themselves, they just cheated and brain dumped their way into several certifications. They also look much more advantageous and are chosen over actual skilled data engineers because they paid to win in the market\n\n\n\n**So who is benefiting from this, really?** It's Microsoft. Microsoft is slowly but surely building up a steady streamlined income of people who have zero experience in data engineering and are attempting to fake their way into the data engineering industry simply by studying and brain dumping the way you would for any amateur college course. They have a growing market of tens upon thousands of people taking exams for hundreds of dollars, several times each, and they're taking several exams. Potentially a multi-million dollar industry of certification seekers, which doesn't actually improve the market in any meaningful way.", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is no one talking about how Microsoft is exploiting data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1676h7r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693573181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a pretty vast network of data professionals, most of which are data engineers, that I&amp;#39;m connected with, And while many of them have at least one certification, I would say that the vast majority of them our experts that have paved The way for their own career without certification farming. Recently, I&amp;#39;ve noticed a community here on Reddit based around Microsoft Azure That is completely focused on systematically farming entertaining certifications for the Microsoft Azure platform. This isn&amp;#39;t entirely unusual, as there are many communities based around obtaining certifications, GCP and AWS for example. The thing that I find really strange, however, is that the vast majority of people that post in this community and are systematically farming these certifications are not data professionals, they are fakers and cheaters. It&amp;#39;s exclusively stated that brain dumping is not allowed and is considered cheating on these exams, yet despite best attempts by mods and members of the community, it&amp;#39;s impossible to completely negate systematic dumping and studying of information for these exams. This results in people who have half a dozen or more Microsoft Azure certifications, with practically no professional experience in anything related to data engineering, who then go on to try and seek a job.....&lt;/p&gt;\n\n&lt;p&gt;Let me be clear as to why this is such a huge problem. Recruiters are often incredibly clueless as to what makes someone successful in a job. They see that a lot of people now have certifications in Azure. So they list that they want people who have at least several Microsoft Azure certifications. Then, they see a large pool of applicants applying for these positions that have no experience, no passion, have not proven themselves, they just cheated and brain dumped their way into several certifications. They also look much more advantageous and are chosen over actual skilled data engineers because they paid to win in the market&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So who is benefiting from this, really?&lt;/strong&gt; It&amp;#39;s Microsoft. Microsoft is slowly but surely building up a steady streamlined income of people who have zero experience in data engineering and are attempting to fake their way into the data engineering industry simply by studying and brain dumping the way you would for any amateur college course. They have a growing market of tens upon thousands of people taking exams for hundreds of dollars, several times each, and they&amp;#39;re taking several exams. Potentially a multi-million dollar industry of certification seekers, which doesn&amp;#39;t actually improve the market in any meaningful way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1676h7r", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1676h7r/why_is_no_one_talking_about_how_microsoft_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1676h7r/why_is_no_one_talking_about_how_microsoft_is/", "subreddit_subscribers": 126155, "created_utc": 1693573181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Complete noob here. How would one go about creating an automated data pipeline with the export of test scores through a test lay form API, to a cloud server capable of running automated data transformation and basic analysis all while maintains privacy?\n\nIs there a place where someone can get higher to accomplish this task and what does that usually cost?", "author_fullname": "t2_gyk9qe35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Pipeline Creation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166ya6w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693546022.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Complete noob here. How would one go about creating an automated data pipeline with the export of test scores through a test lay form API, to a cloud server capable of running automated data transformation and basic analysis all while maintains privacy?&lt;/p&gt;\n\n&lt;p&gt;Is there a place where someone can get higher to accomplish this task and what does that usually cost?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166ya6w", "is_robot_indexable": true, "report_reasons": null, "author": "One-Construction-732", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166ya6w/data_pipeline_creation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166ya6w/data_pipeline_creation/", "subreddit_subscribers": 126155, "created_utc": 1693546022.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it really better pragmatically? What justifies for someone to migrate from siloed data lake - warehouse setup to a lakehouse one?", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Data Lakehouse has a strong business proposition?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166fjlo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693498692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it really better pragmatically? What justifies for someone to migrate from siloed data lake - warehouse setup to a lakehouse one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166fjlo", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166fjlo/does_data_lakehouse_has_a_strong_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166fjlo/does_data_lakehouse_has_a_strong_business/", "subreddit_subscribers": 126155, "created_utc": 1693498692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Folks,  \nAre there people in this community who have properly evaluated Kafka alternatives like RedPanda or others, and/or MQTT alternatives who are willing to share their experience?\n\nContext:  \nI am working on a project involving a handful of clients with edge devices and sensors in different verticals.\n\nThe broad use case is building data pipelines and analytics software for asset monitoring and management and predictive maintenance.\n\nBaseline standards are using MQTT, Kafka, Python and TypeScript code to build the solution.\n\nBut we are facing some hard challenges with the available capacity of edge devices, some issues with the stability of the network, and the edge cloud integration. As we explore alternatives to our stack, we  are looking for insights from practitioners who have experienced challenges with MQTT and Kafka on IoT projects and learn from what they have already explored.\n\nPlease let me know if anyone has this type of experience and is willing to help.\n\nThank You.", "author_fullname": "t2_6pheknqy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring MQTT, Kafka, alternatives for edge sensors and IoT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166bedb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693489009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks,&lt;br/&gt;\nAre there people in this community who have properly evaluated Kafka alternatives like RedPanda or others, and/or MQTT alternatives who are willing to share their experience?&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;br/&gt;\nI am working on a project involving a handful of clients with edge devices and sensors in different verticals.&lt;/p&gt;\n\n&lt;p&gt;The broad use case is building data pipelines and analytics software for asset monitoring and management and predictive maintenance.&lt;/p&gt;\n\n&lt;p&gt;Baseline standards are using MQTT, Kafka, Python and TypeScript code to build the solution.&lt;/p&gt;\n\n&lt;p&gt;But we are facing some hard challenges with the available capacity of edge devices, some issues with the stability of the network, and the edge cloud integration. As we explore alternatives to our stack, we  are looking for insights from practitioners who have experienced challenges with MQTT and Kafka on IoT projects and learn from what they have already explored.&lt;/p&gt;\n\n&lt;p&gt;Please let me know if anyone has this type of experience and is willing to help.&lt;/p&gt;\n\n&lt;p&gt;Thank You.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Product Manager - Data Platform ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166bedb", "is_robot_indexable": true, "report_reasons": null, "author": "drc1728", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/166bedb/exploring_mqtt_kafka_alternatives_for_edge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166bedb/exploring_mqtt_kafka_alternatives_for_edge/", "subreddit_subscribers": 126155, "created_utc": 1693489009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3g7ch6cj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Complex search, filtering and aggregations on DynamoDB - tech talk by AWS SA &amp; Rockset CTO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_166wh7n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tT3j0WbiyEk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tT3j0WbiyEk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset\"&gt;&lt;/iframe&gt;", "author_name": "Rockset", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/tT3j0WbiyEk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Rockset"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tT3j0WbiyEk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/166wh7n", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gMdeRakrRRLbAmsMYPixywB7jhXsfCwNS7pabrRBiOE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693540310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=tT3j0WbiyEk&amp;feature=youtu.be", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Xqy5BnDITq0PXIQR7G2g9alPeNpi_sGawWqscwjnaXA.jpg?auto=webp&amp;s=171147a81dccb6ef66ba97243782b74e44403160", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Xqy5BnDITq0PXIQR7G2g9alPeNpi_sGawWqscwjnaXA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb6d107ec2cc0f80c942e610775b27f0e75af139", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Xqy5BnDITq0PXIQR7G2g9alPeNpi_sGawWqscwjnaXA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80c57b082e11e86816e1343ec40d30dd90af095e", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Xqy5BnDITq0PXIQR7G2g9alPeNpi_sGawWqscwjnaXA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1681a572fe5a86b42eec8ed0e17d46e5862180c9", "width": 320, "height": 240}], "variants": {}, "id": "ua_5DddNv9ecnN9b6PuF45ubk6q3Y7b-8WWNQI4nPAY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "166wh7n", "is_robot_indexable": true, "report_reasons": null, "author": "ssb61", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166wh7n/complex_search_filtering_and_aggregations_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=tT3j0WbiyEk&amp;feature=youtu.be", "subreddit_subscribers": 126155, "created_utc": 1693540310.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tT3j0WbiyEk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset\"&gt;&lt;/iframe&gt;", "author_name": "Rockset", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/tT3j0WbiyEk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Rockset"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I have a substantial list of IDs that I need to work with. My objective is to retrieve data from various API endpoints for each individual ID, process the data, and then load it into a database. \n\nCurrently, I'm using Prefect with Kubernetes to structure the workflow. This involves organizing the tasks into a flow, with each task responsible for making requests to a distinct API endpoint. The flow follows this pattern:\n\n**Flow**\n\n* Task 1: Fetch data from API endpoint 1, then load the acquired data into table 1 in the Postgres database.\n* Task 2: Retrieve data from API endpoint 2, followed by loading it into table 2 in the Postgres database.\n* ...\n\nThese tasks can be executed concurrently. Given the extensive list of IDs, I divide them into batches of 100. Subsequently, I pass each batch as a parameter to initiate a new instance of the flow. At any given moment, a maximum of 5 batches can be actively running. Each task within the flow can access the list of IDs in a batch and process them in parallel.\n\nHowever, I am encountering several challenges:\n\n* In many cases, I only need to execute the entire workflow for individual IDs rather than batches. Unfortunately, each time the flow is triggered, Prefect creates a dedicated container for the job. This results in unnecessary resource usage. If I initiate 100 flows for 100 individual IDs, it could potentially overwhelm the Kubernetes cluster's resources.\n* The API enforces rate limits, including a concurrent limit and a limit per minute, for each ID. When using Prefect, what I can do is to employ retries, which is not an optimal solution. I am exploring options to implement a request queue mechanism that deal with these rate limits more effectively.\n\nBecause of these challenges, I'm considering alternative approaches. One of them is using Flask on Cloud Run and Cloud Functions to design a custom workflow and tasks, without relying on Prefect. I'd really appreciate any suggestions you might have for tackling this issue. ", "author_fullname": "t2_b23p74kt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing Data Retrieval and Processing Workflow for Multiple API Endpoints", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166pml8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693521945.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I have a substantial list of IDs that I need to work with. My objective is to retrieve data from various API endpoints for each individual ID, process the data, and then load it into a database. &lt;/p&gt;\n\n&lt;p&gt;Currently, I&amp;#39;m using Prefect with Kubernetes to structure the workflow. This involves organizing the tasks into a flow, with each task responsible for making requests to a distinct API endpoint. The flow follows this pattern:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Flow&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Task 1: Fetch data from API endpoint 1, then load the acquired data into table 1 in the Postgres database.&lt;/li&gt;\n&lt;li&gt;Task 2: Retrieve data from API endpoint 2, followed by loading it into table 2 in the Postgres database.&lt;/li&gt;\n&lt;li&gt;...&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;These tasks can be executed concurrently. Given the extensive list of IDs, I divide them into batches of 100. Subsequently, I pass each batch as a parameter to initiate a new instance of the flow. At any given moment, a maximum of 5 batches can be actively running. Each task within the flow can access the list of IDs in a batch and process them in parallel.&lt;/p&gt;\n\n&lt;p&gt;However, I am encountering several challenges:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In many cases, I only need to execute the entire workflow for individual IDs rather than batches. Unfortunately, each time the flow is triggered, Prefect creates a dedicated container for the job. This results in unnecessary resource usage. If I initiate 100 flows for 100 individual IDs, it could potentially overwhelm the Kubernetes cluster&amp;#39;s resources.&lt;/li&gt;\n&lt;li&gt;The API enforces rate limits, including a concurrent limit and a limit per minute, for each ID. When using Prefect, what I can do is to employ retries, which is not an optimal solution. I am exploring options to implement a request queue mechanism that deal with these rate limits more effectively.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Because of these challenges, I&amp;#39;m considering alternative approaches. One of them is using Flask on Cloud Run and Cloud Functions to design a custom workflow and tasks, without relying on Prefect. I&amp;#39;d really appreciate any suggestions you might have for tackling this issue. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166pml8", "is_robot_indexable": true, "report_reasons": null, "author": "kia_ora_st", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166pml8/optimizing_data_retrieval_and_processing_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166pml8/optimizing_data_retrieval_and_processing_workflow/", "subreddit_subscribers": 126155, "created_utc": 1693521945.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This type of question might not be allowed so I apologize if this is the case. I have a process that generates a list of known network interfaces (identified by MacAddress) in our networks every hour. It is a json array of objects with a hand-full of fields. So every hour we write a list of around 20k network interface objects to S3. For example\n\n    {  \n        \"MacAddress\": \"BF-6E-AB-B9-BA-96\",  \n        \"FirstSeen\": '2022-08-10T102200',  \n        \"LastSeen\": '2022-08-14T110100',  \n       ....  \n    }  \n\nThis data needs to make its way into a postgresql database where a django app uses it. Initially I simply deleted all entries in the table, and wrote the contents of the state file into the table. It should be noted that some MacAddresses from the past are no longer in the current dataset so they would be deleted. \n\nWe later decided we wanted to hold on to things from the past that are not in the current state. The obvious way to do that would be to simply stop deleting old entries an do an UPSERT. But I didn't understand the data well enough at the time to have the confidence that I didn't make a mistake in the ingestion process. Assuming I discovered a bug in the ingestion code I would be left with these options after fixing the bug. \n\n1. Re-running the ingestion from the start (would take around 20 days currently). \n1. Doing nothing about past data and simply accept that it is inaccurate\n1. Deleting the old data and starting fresh\n\nI didn't really want to be stuck with any of these options so I decided to try something else. I created a function that reads all of the files in S3 and merges them. It does this based on the more recent data. So you end up with a state file in json that contains all MacAddresses ever seen with the most recent data. This is cached as well, so if you have the state from an hour ago in S3 you only need to read that and the newest update to produce the latest one. \n\nWhat I like about this approach is that it incorporated the \"restore\" process into the daily data loading operation. So in this case if we discover a data error the fix is to simply re-run the entire state computation (about 2 hours) with the new code. Another important thing to consider is that we will need the state calculated for every day as well. That way we can ingest historical data as well. \n\nBut the problem is that this doesn't scale well. And we have had to change the ingestion process so we have had to rerun it many times. I would ideally like to come up with a solution that can handle 10x growth and 5 years. I think by that point with our current approach it would take days to recalculate the current state. \n\nCurrently I am investigating the database timescaledb to see if that can be beneficial to us. The compression works really well and it seems like we could load all of our S3 data there and do the work of the ingestion process in SQL. That way if something went wrong we could rebuild it in the database (quickly).\n\nBut now I am beginning to wonder about big-data platforms and how they could be used. If we didn't need to produce the daily state file this would be extremely parallelizable I think. Divide the S3 files into N number of slices and have N workers merge the files in their slice and then merge those N results. But I think it wouldn't be too bad to parallelize on MacAddress last digit. That is probably pretty evenly distributed. But you'd end up with 16x the number of downloads from S3 that would normally be required. Maybe there is a way to cache them?\n\nI am thinking of AWS Glue or AWS EMR. But the point is to come up with a solution that can be done hourly and hopefully without the need to recompute everything from scratch. I'd really just love to have a reliable solution that allows for changes in the merge logic to be possible with a much larger amount of data. Also any other suggestions for how to handle this would be welcome!", "author_fullname": "t2_m55mbvoc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to architect sequential state data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166hul7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693504140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This type of question might not be allowed so I apologize if this is the case. I have a process that generates a list of known network interfaces (identified by MacAddress) in our networks every hour. It is a json array of objects with a hand-full of fields. So every hour we write a list of around 20k network interface objects to S3. For example&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{  \n    &amp;quot;MacAddress&amp;quot;: &amp;quot;BF-6E-AB-B9-BA-96&amp;quot;,  \n    &amp;quot;FirstSeen&amp;quot;: &amp;#39;2022-08-10T102200&amp;#39;,  \n    &amp;quot;LastSeen&amp;quot;: &amp;#39;2022-08-14T110100&amp;#39;,  \n   ....  \n}  \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This data needs to make its way into a postgresql database where a django app uses it. Initially I simply deleted all entries in the table, and wrote the contents of the state file into the table. It should be noted that some MacAddresses from the past are no longer in the current dataset so they would be deleted. &lt;/p&gt;\n\n&lt;p&gt;We later decided we wanted to hold on to things from the past that are not in the current state. The obvious way to do that would be to simply stop deleting old entries an do an UPSERT. But I didn&amp;#39;t understand the data well enough at the time to have the confidence that I didn&amp;#39;t make a mistake in the ingestion process. Assuming I discovered a bug in the ingestion code I would be left with these options after fixing the bug. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Re-running the ingestion from the start (would take around 20 days currently). &lt;/li&gt;\n&lt;li&gt;Doing nothing about past data and simply accept that it is inaccurate&lt;/li&gt;\n&lt;li&gt;Deleting the old data and starting fresh&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I didn&amp;#39;t really want to be stuck with any of these options so I decided to try something else. I created a function that reads all of the files in S3 and merges them. It does this based on the more recent data. So you end up with a state file in json that contains all MacAddresses ever seen with the most recent data. This is cached as well, so if you have the state from an hour ago in S3 you only need to read that and the newest update to produce the latest one. &lt;/p&gt;\n\n&lt;p&gt;What I like about this approach is that it incorporated the &amp;quot;restore&amp;quot; process into the daily data loading operation. So in this case if we discover a data error the fix is to simply re-run the entire state computation (about 2 hours) with the new code. Another important thing to consider is that we will need the state calculated for every day as well. That way we can ingest historical data as well. &lt;/p&gt;\n\n&lt;p&gt;But the problem is that this doesn&amp;#39;t scale well. And we have had to change the ingestion process so we have had to rerun it many times. I would ideally like to come up with a solution that can handle 10x growth and 5 years. I think by that point with our current approach it would take days to recalculate the current state. &lt;/p&gt;\n\n&lt;p&gt;Currently I am investigating the database timescaledb to see if that can be beneficial to us. The compression works really well and it seems like we could load all of our S3 data there and do the work of the ingestion process in SQL. That way if something went wrong we could rebuild it in the database (quickly).&lt;/p&gt;\n\n&lt;p&gt;But now I am beginning to wonder about big-data platforms and how they could be used. If we didn&amp;#39;t need to produce the daily state file this would be extremely parallelizable I think. Divide the S3 files into N number of slices and have N workers merge the files in their slice and then merge those N results. But I think it wouldn&amp;#39;t be too bad to parallelize on MacAddress last digit. That is probably pretty evenly distributed. But you&amp;#39;d end up with 16x the number of downloads from S3 that would normally be required. Maybe there is a way to cache them?&lt;/p&gt;\n\n&lt;p&gt;I am thinking of AWS Glue or AWS EMR. But the point is to come up with a solution that can be done hourly and hopefully without the need to recompute everything from scratch. I&amp;#39;d really just love to have a reliable solution that allows for changes in the merge logic to be possible with a much larger amount of data. Also any other suggestions for how to handle this would be welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166hul7", "is_robot_indexable": true, "report_reasons": null, "author": "Fantastic_Search_504", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166hul7/best_way_to_architect_sequential_state_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166hul7/best_way_to_architect_sequential_state_data/", "subreddit_subscribers": 126155, "created_utc": 1693504140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI am trying to move data from one snowflake table to another using snowflake.\n\nMy first table has 10 columns out of which 2 have Json data.\n\nA, B, C, D, E, F, G, H, I, J where A and B are json columns.\n\nI want to map it to a target table which has 12 columns. \n\nA_key, A_value, B_key, B_value, C, D, E, F, G, H, I, J\n\nI tried using an intelligent structure model where I could parse A as A_key, A_value but I\u2019m unable to map other columns to my target.\n\nI also tried using parser but that didn\u2019t work out great.\n\nHow do I do this using informatica cloud?", "author_fullname": "t2_59ygsznq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[informatica] Parsing a column which has json data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166icik", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693505255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am trying to move data from one snowflake table to another using snowflake.&lt;/p&gt;\n\n&lt;p&gt;My first table has 10 columns out of which 2 have Json data.&lt;/p&gt;\n\n&lt;p&gt;A, B, C, D, E, F, G, H, I, J where A and B are json columns.&lt;/p&gt;\n\n&lt;p&gt;I want to map it to a target table which has 12 columns. &lt;/p&gt;\n\n&lt;p&gt;A_key, A_value, B_key, B_value, C, D, E, F, G, H, I, J&lt;/p&gt;\n\n&lt;p&gt;I tried using an intelligent structure model where I could parse A as A_key, A_value but I\u2019m unable to map other columns to my target.&lt;/p&gt;\n\n&lt;p&gt;I also tried using parser but that didn\u2019t work out great.&lt;/p&gt;\n\n&lt;p&gt;How do I do this using informatica cloud?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166icik", "is_robot_indexable": true, "report_reasons": null, "author": "lnx2n", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166icik/informatica_parsing_a_column_which_has_json_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166icik/informatica_parsing_a_column_which_has_json_data/", "subreddit_subscribers": 126155, "created_utc": 1693505255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy!  \n\n\nSo I know how to implement testing for my models and I can also set the individual tests configuration to set the severity to either warn or error. All good for now. My question is: if the dbt test command does in fact encounter errors of increased severity, the pipeline still passes. Ideally, I would like to not include this broken data further down stream. Additionally I would like to push the broken data to a separate view or something so that I can inspect it. How do I do that. Or better still, how do you actually use this command in production? What do you actually do with the errors? I am a bit lost.", "author_fullname": "t2_genrcwic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I handle errors in DBT tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166fa39", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693498074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy!  &lt;/p&gt;\n\n&lt;p&gt;So I know how to implement testing for my models and I can also set the individual tests configuration to set the severity to either warn or error. All good for now. My question is: if the dbt test command does in fact encounter errors of increased severity, the pipeline still passes. Ideally, I would like to not include this broken data further down stream. Additionally I would like to push the broken data to a separate view or something so that I can inspect it. How do I do that. Or better still, how do you actually use this command in production? What do you actually do with the errors? I am a bit lost.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166fa39", "is_robot_indexable": true, "report_reasons": null, "author": "nacho_biznis", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166fa39/how_do_i_handle_errors_in_dbt_tests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166fa39/how_do_i_handle_errors_in_dbt_tests/", "subreddit_subscribers": 126155, "created_utc": 1693498074.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Very new to this and looking for possible solutions.  Trying to connect to a dba via an ODBC connection to migrate the data out to a mysql server. ODBC is the only way for me to connect. It's a Windows 2012 server/32 bit.  What's the best IDE for this?", "author_fullname": "t2_7hcvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ODBC Connection on 32 bit - best IDE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166bf7j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693489067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Very new to this and looking for possible solutions.  Trying to connect to a dba via an ODBC connection to migrate the data out to a mysql server. ODBC is the only way for me to connect. It&amp;#39;s a Windows 2012 server/32 bit.  What&amp;#39;s the best IDE for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166bf7j", "is_robot_indexable": true, "report_reasons": null, "author": "scrupio", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166bf7j/odbc_connection_on_32_bit_best_ide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166bf7j/odbc_connection_on_32_bit_best_ide/", "subreddit_subscribers": 126155, "created_utc": 1693489067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For context, my background is in data analytics and science. But I\u2019m helping with ETL on an API at my company. And while I\u2019ve dealt with nested data before, I\u2019ve never spent much time with JSON. \n\nDo you all have any resources that would be helpful for doing data analysis and normalization of JSON in either R or Python? And please don\u2019t suggest anything that uses the state and counties example in the json_normalize() documentation, which seems to be used in examples all across the web for some reason. Videos of people working on multi-level nested JSON would be great, but books and articles would also be helpful.\n\nThanks!", "author_fullname": "t2_9ytsa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "JSON learning resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166b6v8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693488499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context, my background is in data analytics and science. But I\u2019m helping with ETL on an API at my company. And while I\u2019ve dealt with nested data before, I\u2019ve never spent much time with JSON. &lt;/p&gt;\n\n&lt;p&gt;Do you all have any resources that would be helpful for doing data analysis and normalization of JSON in either R or Python? And please don\u2019t suggest anything that uses the state and counties example in the json_normalize() documentation, which seems to be used in examples all across the web for some reason. Videos of people working on multi-level nested JSON would be great, but books and articles would also be helpful.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166b6v8", "is_robot_indexable": true, "report_reasons": null, "author": "ursamajorm82", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166b6v8/json_learning_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166b6v8/json_learning_resources/", "subreddit_subscribers": 126155, "created_utc": 1693488499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,   \n\n\ndlt is making a push towards adding more governance features, starting with **data contracts.**   \n\n\nWe will add the following modes as alternative to data contracts:  \n\u00a0**\\* evolve**: The current standard behavior, adapt the schema of the destination to the incoming data.  \n\u00a0**\\* freeze-and-trim**: Freeze the schema, and trim additional fields and subtables of incoming rows. This will still store all the incoming rows but will discard fields and subtable unknown to the schema.  \n\u00a0**\\* freeze-and-raise**: If any of the incoming rows do not fit the current schema, fail the current load. This will not store any data in the destination for this load and also not change the schema.  \n\u00a0**\\* freeze-and-discard:** Freeze the schema and completely discard any row that does not fit the current schema. Rows that fit the schema will be stored in the destination.  \n\n\nif you want to give feedback to the above and influence direction, please comment here or join out slack (link on page on top)\n\nIn the meantime, I wrote a short article about what row and column lineages are, why we use them, and how to get them at loading with dlt.  \n\n\nI hope this is useful!\n\n[https://dlthub.com/docs/blog/dlt-lineage-support](https://dlthub.com/docs/blog/dlt-lineage-support)", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trust your data - how to do simple row and column level lineage with dlt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166c3n5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693490730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,   &lt;/p&gt;\n\n&lt;p&gt;dlt is making a push towards adding more governance features, starting with &lt;strong&gt;data contracts.&lt;/strong&gt;   &lt;/p&gt;\n\n&lt;p&gt;We will add the following modes as alternative to data contracts:&lt;br/&gt;\n\u00a0&lt;strong&gt;* evolve&lt;/strong&gt;: The current standard behavior, adapt the schema of the destination to the incoming data.&lt;br/&gt;\n\u00a0&lt;strong&gt;* freeze-and-trim&lt;/strong&gt;: Freeze the schema, and trim additional fields and subtables of incoming rows. This will still store all the incoming rows but will discard fields and subtable unknown to the schema.&lt;br/&gt;\n\u00a0&lt;strong&gt;* freeze-and-raise&lt;/strong&gt;: If any of the incoming rows do not fit the current schema, fail the current load. This will not store any data in the destination for this load and also not change the schema.&lt;br/&gt;\n\u00a0&lt;strong&gt;* freeze-and-discard:&lt;/strong&gt; Freeze the schema and completely discard any row that does not fit the current schema. Rows that fit the schema will be stored in the destination.  &lt;/p&gt;\n\n&lt;p&gt;if you want to give feedback to the above and influence direction, please comment here or join out slack (link on page on top)&lt;/p&gt;\n\n&lt;p&gt;In the meantime, I wrote a short article about what row and column lineages are, why we use them, and how to get them at loading with dlt.  &lt;/p&gt;\n\n&lt;p&gt;I hope this is useful!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dlthub.com/docs/blog/dlt-lineage-support\"&gt;https://dlthub.com/docs/blog/dlt-lineage-support&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?auto=webp&amp;s=7990e0b7bda28e0b896446b9feb29a76037878b8", "width": 1200, "height": 996}, "resolutions": [{"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bfefd634af4380c1030e9656e1354749bb4a05bb", "width": 108, "height": 89}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ab83290a6cf23b2ac6974f064275ad182be6cf3", "width": 216, "height": 179}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e87ff109cc91fe7f974d4fb6959e2336f5b10ff2", "width": 320, "height": 265}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7077fbf6f66bee841cd5bb27c0bc8ab6312c4952", "width": 640, "height": 531}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bb59b243274e62869fcca0f37970db9227063740", "width": 960, "height": 796}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fe2a351dc3c7fcfa194482d6e2da298e9c9edce5", "width": 1080, "height": 896}], "variants": {}, "id": "dHy9-9qK5psAYmJuOmTrdz4FUBDbwmI7B4kzzPMoWZk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "166c3n5", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166c3n5/trust_your_data_how_to_do_simple_row_and_column/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166c3n5/trust_your_data_how_to_do_simple_row_and_column/", "subreddit_subscribers": 126155, "created_utc": 1693490730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w07bcus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Kafka Is the New Data Lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 102, "top_awarded_type": null, "hide_score": false, "name": "t3_166b2ic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.21, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ha48ThKEN_w2J9wipekBKlh67100X6d9wXPSwqITa5o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693488193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "risingwave.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.risingwave.com/blog/why-kafka-is-the-new-data-lake/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/i8-zzJXS3o8C0HIq4qProRITPINi_0e1sCZv62XhI3g.jpg?auto=webp&amp;s=df5f70d626df82f0d3dab4c8c805f0afd06d703f", "width": 988, "height": 722}, "resolutions": [{"url": "https://external-preview.redd.it/i8-zzJXS3o8C0HIq4qProRITPINi_0e1sCZv62XhI3g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e9c98375bd457970e031718e0117a61e38280157", "width": 108, "height": 78}, {"url": "https://external-preview.redd.it/i8-zzJXS3o8C0HIq4qProRITPINi_0e1sCZv62XhI3g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e3efcbbee3475dc620b1991c433a6b73af7d74b0", "width": 216, "height": 157}, {"url": "https://external-preview.redd.it/i8-zzJXS3o8C0HIq4qProRITPINi_0e1sCZv62XhI3g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7db0152b96461ec75f3b2815ac8a114398910456", "width": 320, "height": 233}, {"url": "https://external-preview.redd.it/i8-zzJXS3o8C0HIq4qProRITPINi_0e1sCZv62XhI3g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bffc1aff94df3538722c18e1f76d968ce5a236f4", "width": 640, "height": 467}, {"url": "https://external-preview.redd.it/i8-zzJXS3o8C0HIq4qProRITPINi_0e1sCZv62XhI3g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f4a4430538d01c6a842658073df63932657407a9", "width": 960, "height": 701}], "variants": {}, "id": "5Bxhz29v1dCNko_FyUZjSoPYgXVXjo1EsXbPPHeH_LQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "166b2ic", "is_robot_indexable": true, "report_reasons": null, "author": "yingjunwu", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166b2ic/why_kafka_is_the_new_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.risingwave.com/blog/why-kafka-is-the-new-data-lake/", "subreddit_subscribers": 126155, "created_utc": 1693488193.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}