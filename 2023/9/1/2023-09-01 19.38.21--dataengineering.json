{"kind": "Listing", "data": {"after": "t3_167b8eg", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \n\nI am developing an ETL pipeline using snowpark Python APIs and I am having some problems with it, because I need to execute multiple parallel queries, and to do so I have tried both `multiprocessing` and `concurrent.futures`.\n\nIt looks like snowpark doesn't like to reuse the same session in multiple threads, as I get random ValueError or IndexError when I perform some `.collect()`, `.count()` or `table.merge()` operations. \n\nTo reuse the session I am using `snowpark.context.get_active_session()`. I have tried to run this code iteratively instead of using threads and it runs just fine. Creating a new session in each thread seems to mitigate this behaviour, but if I create too many the snowflake https endpoint goes into throttling mode and will stop responding.\n\nRight now, I am catching exceptions because for `table.merge()` the underlying query seems to run anyways, and when I call `.collect()` or `.count()` I use a while loop to keep retrying until I get a result, but this is far from ideal. \n\n&amp;#x200B;\n\nHas anyone encountered a similar issue before? Any ways I could fix/mitigate it? \n\n&amp;#x200B;", "author_fullname": "t2_do2bj7xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowpark (Python) and multithreading issues?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167a2yj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693581716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;I am developing an ETL pipeline using snowpark Python APIs and I am having some problems with it, because I need to execute multiple parallel queries, and to do so I have tried both &lt;code&gt;multiprocessing&lt;/code&gt; and &lt;code&gt;concurrent.futures&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;It looks like snowpark doesn&amp;#39;t like to reuse the same session in multiple threads, as I get random ValueError or IndexError when I perform some &lt;code&gt;.collect()&lt;/code&gt;, &lt;code&gt;.count()&lt;/code&gt; or &lt;code&gt;table.merge()&lt;/code&gt; operations. &lt;/p&gt;\n\n&lt;p&gt;To reuse the session I am using &lt;code&gt;snowpark.context.get_active_session()&lt;/code&gt;. I have tried to run this code iteratively instead of using threads and it runs just fine. Creating a new session in each thread seems to mitigate this behaviour, but if I create too many the snowflake https endpoint goes into throttling mode and will stop responding.&lt;/p&gt;\n\n&lt;p&gt;Right now, I am catching exceptions because for &lt;code&gt;table.merge()&lt;/code&gt; the underlying query seems to run anyways, and when I call &lt;code&gt;.collect()&lt;/code&gt; or &lt;code&gt;.count()&lt;/code&gt; I use a while loop to keep retrying until I get a result, but this is far from ideal. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anyone encountered a similar issue before? Any ways I could fix/mitigate it? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "167a2yj", "is_robot_indexable": true, "report_reasons": null, "author": "somerandomdataeng", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/167a2yj/snowpark_python_and_multithreading_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167a2yj/snowpark_python_and_multithreading_issues/", "subreddit_subscribers": 126198, "created_utc": 1693581716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://preview.redd.it/ia7kdykk8dlb1.png?width=500&amp;format=png&amp;auto=webp&amp;s=5cbb667f30e089119bae1fcb2922ffac0700aecd\n\nThis is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.\n\n# [Submit your salary here](https://tally.so/r/nraYkN)\n\n&amp;#x200B;\n\nIf you'd like to share publicly as well you can optionally comment below and include the following:\n\n1. Current title\n2. Years of experience (YOE)\n3. Location\n4. Base salary &amp; currency (dollars, euro, pesos, etc.)\n5. Bonuses/Equity (optional)\n6. Industry (optional)\n7. Tech stack (optional)", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quarterly Salary Discussion - Sep 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/ef3eb514-328d-4549-a705-94c26963d79b", "link_ids": ["t3_npxcqc", "t3_pfwuyg", "t3_r6jfnm", "t3_t4clep", "t3_v2ka3w", "t3_x3bb11", "t3_z9szj1", "t3_11f8yxo", "t3_13xldpd", "t3_167b3ep"], "description": "", "title": "Data Engineering Salaries", "created_at_utc": 1621559056.076, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "ef3eb514-328d-4549-a705-94c26963d79b", "author_id": "t2_2tv9i42n", "last_update_utc": 1693584061.122, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ia7kdykk8dlb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 108, "x": 108, "u": "https://preview.redd.it/ia7kdykk8dlb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=814f58d3eef18e16ebfd881a24dc42c6278c74a5"}, {"y": 216, "x": 216, "u": "https://preview.redd.it/ia7kdykk8dlb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=220aef8c88d2d3542556dbc0ceda11308fae54cd"}, {"y": 320, "x": 320, "u": "https://preview.redd.it/ia7kdykk8dlb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc0f5873d0a5e748e4664a4925eb409775331c20"}], "s": {"y": 500, "x": 500, "u": "https://preview.redd.it/ia7kdykk8dlb1.png?width=500&amp;format=png&amp;auto=webp&amp;s=5cbb667f30e089119bae1fcb2922ffac0700aecd"}, "id": "ia7kdykk8dlb1"}}, "name": "t3_167b3ep", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 24, "domain": "self.dataengineering", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/xNIBK13VhkA-Vai66r1V_765Zl5rEmwRwiDZDvS1oDs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1693584060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/ia7kdykk8dlb1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5cbb667f30e089119bae1fcb2922ffac0700aecd\"&gt;https://preview.redd.it/ia7kdykk8dlb1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5cbb667f30e089119bae1fcb2922ffac0700aecd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.&lt;/p&gt;\n\n&lt;h1&gt;&lt;a href=\"https://tally.so/r/nraYkN\"&gt;Submit your salary here&lt;/a&gt;&lt;/h1&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;d like to share publicly as well you can optionally comment below and include the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Current title&lt;/li&gt;\n&lt;li&gt;Years of experience (YOE)&lt;/li&gt;\n&lt;li&gt;Location&lt;/li&gt;\n&lt;li&gt;Base salary &amp;amp; currency (dollars, euro, pesos, etc.)&lt;/li&gt;\n&lt;li&gt;Bonuses/Equity (optional)&lt;/li&gt;\n&lt;li&gt;Industry (optional)&lt;/li&gt;\n&lt;li&gt;Tech stack (optional)&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?auto=webp&amp;s=9f6c46430d6cb3a6aa187e779989676562dd5f94", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b0c3164daff944f36c4a26d19d1d36e164d8969", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58dd7064505b3bc1c2bc7aa63422d46899e6906a", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9272e4bd81620e7124428e0489e96fd1984def7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=698a831c5a4f33799cf06d5b6ca67594c2422447", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1156d73438af281337e45b3974b40a0a7d2cb361", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b423bb38bf88570da5a0c468d5b346433c4ca69c", "width": 1080, "height": 567}], "variants": {}, "id": "vXOF8G9GBUU_-_vM38jf2S1-5UiTZqBcFWecpk4eHS4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "167b3ep", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 43, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167b3ep/quarterly_salary_discussion_sep_2023/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/167b3ep/quarterly_salary_discussion_sep_2023/", "subreddit_subscribers": 126198, "created_utc": 1693584060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi friends,\n\nSo I have some experience as Data Analyst. Have some python and SQL knowledge. But want to be an AWS data engineer. Some comments here and there said **Cantrill** is good. I checked out the courses but didn't understand the naming of the course tracks.\n\nI'm not familiar with AWS certification titles. What would you suggest I start with on that website, or do you have anything I can take a look at to understand all the technical roles, designations, jargons, so that I can have a better understanding before I can start my DE studies.", "author_fullname": "t2_jl4hi61l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DA to DE - how should I prepare?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1673fap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693564246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi friends,&lt;/p&gt;\n\n&lt;p&gt;So I have some experience as Data Analyst. Have some python and SQL knowledge. But want to be an AWS data engineer. Some comments here and there said &lt;strong&gt;Cantrill&lt;/strong&gt; is good. I checked out the courses but didn&amp;#39;t understand the naming of the course tracks.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not familiar with AWS certification titles. What would you suggest I start with on that website, or do you have anything I can take a look at to understand all the technical roles, designations, jargons, so that I can have a better understanding before I can start my DE studies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1673fap", "is_robot_indexable": true, "report_reasons": null, "author": "pale-blue-dotter", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1673fap/da_to_de_how_should_i_prepare/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1673fap/da_to_de_how_should_i_prepare/", "subreddit_subscribers": 126198, "created_utc": 1693564246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_viq1t90m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipelines with Python and SQL - Part 1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_166rste", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6FoFuooprnsoo_O92yU7D5QwKRgX2jMp3Z9qkn0aDPc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693527254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "stephendavidwilliams.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://stephendavidwilliams.com/data-pipelines-with-python-and-sql-part-1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?auto=webp&amp;s=e69f6e29dcdebd0a3dee751d2b1929e07664b4e9", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6bfac54f8dfdd46f74556e776eaa81a50d7f8842", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=36f68e2eba4566517be4d4229dc9ba76dc515d56", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c70788f4e382f796dcb1f82d6ef8cbcd26bd755", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c9de6060ba8c380b2abe2b5fea8e8cc8c84546ea", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e8ed329f03658b8866641d22bdee2bd47429dcb1", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02d1773f6e573fd02e765a244977bac8af1408ef", "width": 1080, "height": 567}], "variants": {}, "id": "lo0sGmJanYdR0vXmZrljanaI_RhUtYEp-GDYPws1knE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "166rste", "is_robot_indexable": true, "report_reasons": null, "author": "sdw_solutions", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166rste/data_pipelines_with_python_and_sql_part_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://stephendavidwilliams.com/data-pipelines-with-python-and-sql-part-1", "subreddit_subscribers": 126198, "created_utc": 1693527254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am recently grinding SQL on both platform. \n\nI just found StrataScratch SQL questions are more straightforward, and I can do 10+ medium questions per day. Its difficulty is more focus on how well you understand the basic fundamental commands of SQL, aka how you can use join, subquery and cte altogether. The learning curve is very smooth. Hence the user interface is more friendly, every question has its official solution. \n\nOn the other hand, Leetcode SQL's difficulty is more \"tricky\", it focuses on how well you understand the problem and how you can find a \"trick\" to hack the problem. Honestly, my progress is very slow on Leetcode style SQL, and I have learned very few from it...\n\nAny thoughts? \n\n&amp;#x200B;", "author_fullname": "t2_csk6gf7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some thoughts about Leetcode SQL and StrataScratch SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166wmef", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693540773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am recently grinding SQL on both platform. &lt;/p&gt;\n\n&lt;p&gt;I just found StrataScratch SQL questions are more straightforward, and I can do 10+ medium questions per day. Its difficulty is more focus on how well you understand the basic fundamental commands of SQL, aka how you can use join, subquery and cte altogether. The learning curve is very smooth. Hence the user interface is more friendly, every question has its official solution. &lt;/p&gt;\n\n&lt;p&gt;On the other hand, Leetcode SQL&amp;#39;s difficulty is more &amp;quot;tricky&amp;quot;, it focuses on how well you understand the problem and how you can find a &amp;quot;trick&amp;quot; to hack the problem. Honestly, my progress is very slow on Leetcode style SQL, and I have learned very few from it...&lt;/p&gt;\n\n&lt;p&gt;Any thoughts? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166wmef", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Astronomer-471", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166wmef/some_thoughts_about_leetcode_sql_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166wmef/some_thoughts_about_leetcode_sql_and/", "subreddit_subscribers": 126198, "created_utc": 1693540773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I need to execute a databricka workflow using adf pipeline which I have done successfully but on failure I need tk restart the pipeline in such a way that the databricks workflow restarts from the point of failure instead of restarting again from the start. Any idea on how to achieve this using adf. I know there is a repair run option inside the workflow but how do I achieve this using ADF ?", "author_fullname": "t2_5a9iwwnc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Workflows using ADF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1676ur9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693574100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I need to execute a databricka workflow using adf pipeline which I have done successfully but on failure I need tk restart the pipeline in such a way that the databricks workflow restarts from the point of failure instead of restarting again from the start. Any idea on how to achieve this using adf. I know there is a repair run option inside the workflow but how do I achieve this using ADF ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1676ur9", "is_robot_indexable": true, "report_reasons": null, "author": "willywonka786", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1676ur9/databricks_workflows_using_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1676ur9/databricks_workflows_using_adf/", "subreddit_subscribers": 126198, "created_utc": 1693574100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was wondering how other teams use dbt, so I made this very short survey to understand better. I'll share the results later in the comments.  \n\n\n[https://forms.gle/zwwfDAVYGu37nz2W6](https://forms.gle/zwwfDAVYGu37nz2W6)", "author_fullname": "t2_tbjno", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt. What is it good for? A 42-second survey.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166nao6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693516475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering how other teams use dbt, so I made this very short survey to understand better. I&amp;#39;ll share the results later in the comments.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://forms.gle/zwwfDAVYGu37nz2W6\"&gt;https://forms.gle/zwwfDAVYGu37nz2W6&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?auto=webp&amp;s=3e5851ed8f8cbec67e9d3d95ce1a6ab2fd5766a4", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95d563b3730f933218dfe75138324b3840fffa23", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=39f7c8c738ffa952f983e1213eb4a3465e80a6f4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c2cf8cfd3a17b21357728bbed54d062c79cd3d4", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0450a7adb08aa6c7a15f18ba4e1551c53a5a1cc", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c36a1277013d617e183e04aeec20fd0d1b96478b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ed8af9722ace6f0c6b022974f642c2b2e4ac3521", "width": 1080, "height": 567}], "variants": {}, "id": "L38pNlP_aAcmNKnS5YBL_J4J7QdBMDhPuBkZmM20ZLY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166nao6", "is_robot_indexable": true, "report_reasons": null, "author": "PrivateH", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166nao6/dbt_what_is_it_good_for_a_42second_survey/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166nao6/dbt_what_is_it_good_for_a_42second_survey/", "subreddit_subscribers": 126198, "created_utc": 1693516475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nDatabricks: Photon compute is recommended best practice for all streaming DE pipelines\n\nCustomer: OK\n\nDatabricks: Rescue columns are recommended best practice for all streaming DE pipelines\n\nCustomer: OK\n\n&amp;#x200B;\n\n2 weeks later...\n\nCustomer: Hey Databricks, I followed your advice and our costs are way up but performance has not changed.\n\nDatabricks: That's because you can't use Photon with streaming pipelines that use rescue columns. ", "author_fullname": "t2_883tcd85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone else run into these kinds of issues with Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1678m8c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693578361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Databricks: Photon compute is recommended best practice for all streaming DE pipelines&lt;/p&gt;\n\n&lt;p&gt;Customer: OK&lt;/p&gt;\n\n&lt;p&gt;Databricks: Rescue columns are recommended best practice for all streaming DE pipelines&lt;/p&gt;\n\n&lt;p&gt;Customer: OK&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;2 weeks later...&lt;/p&gt;\n\n&lt;p&gt;Customer: Hey Databricks, I followed your advice and our costs are way up but performance has not changed.&lt;/p&gt;\n\n&lt;p&gt;Databricks: That&amp;#39;s because you can&amp;#39;t use Photon with streaming pipelines that use rescue columns. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1678m8c", "is_robot_indexable": true, "report_reasons": null, "author": "Basic_Cucumber_165", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1678m8c/anyone_else_run_into_these_kinds_of_issues_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1678m8c/anyone_else_run_into_these_kinds_of_issues_with/", "subreddit_subscribers": 126198, "created_utc": 1693578361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a new(ish) DataBricks lakehouse with a traditional medallion architecture (bronze -&gt; silver -&gt; gold). \n\nIn a datawarehouse, I would expect to use surrogate keys (rather than natural keys) in the silver layer, to account for things like data coming from two different sources. \n\nWhat is the best practise in lakehouses? Should we still be using surrogate keys or has the practice changed?", "author_fullname": "t2_5cbo0joy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lakehouse Silver Layer - Surrogate or Natural Keys?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166zppb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693551045.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a new(ish) DataBricks lakehouse with a traditional medallion architecture (bronze -&amp;gt; silver -&amp;gt; gold). &lt;/p&gt;\n\n&lt;p&gt;In a datawarehouse, I would expect to use surrogate keys (rather than natural keys) in the silver layer, to account for things like data coming from two different sources. &lt;/p&gt;\n\n&lt;p&gt;What is the best practise in lakehouses? Should we still be using surrogate keys or has the practice changed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166zppb", "is_robot_indexable": true, "report_reasons": null, "author": "Edd037", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166zppb/lakehouse_silver_layer_surrogate_or_natural_keys/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166zppb/lakehouse_silver_layer_surrogate_or_natural_keys/", "subreddit_subscribers": 126198, "created_utc": 1693551045.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "1. Does your team have on-call rotation for any failures after hours or on weekends? Primarily those who work with critical data pipelines. \n2. How frequent do you have issues in a week/month?  \n   1. we have issues maybe twice a month (sometimes just bc we didn't get a file from the client and have to wait for them to resend it, which can be hours later)\n3. Have you encountered issues with knowledge being siloed that multiple have to jump on regardless of who's on-call? We're working on better documentation and knowledge sharing so other people aren't required to jump in but it's a challenge. \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On-call rotation and frequency of support off-hours?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_167eqrd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693592278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;Does your team have on-call rotation for any failures after hours or on weekends? Primarily those who work with critical data pipelines. &lt;/li&gt;\n&lt;li&gt;How frequent do you have issues in a week/month?&lt;br/&gt;\n\n&lt;ol&gt;\n&lt;li&gt;we have issues maybe twice a month (sometimes just bc we didn&amp;#39;t get a file from the client and have to wait for them to resend it, which can be hours later)&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;Have you encountered issues with knowledge being siloed that multiple have to jump on regardless of who&amp;#39;s on-call? We&amp;#39;re working on better documentation and knowledge sharing so other people aren&amp;#39;t required to jump in but it&amp;#39;s a challenge. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167eqrd", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167eqrd/oncall_rotation_and_frequency_of_support_offhours/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167eqrd/oncall_rotation_and_frequency_of_support_offhours/", "subreddit_subscribers": 126198, "created_utc": 1693592278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.\n\n**Community Links:**\n\n* [Monthly newsletter](https://dataengineeringcommunity.substack.com/): community updates, inspiration, and insights\n* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly General Discussion - Sep 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/6278fda2-fad1-4706-9e82-6ddb67d49c0b", "link_ids": ["t3_shzqhy", "t3_t4clgk", "t3_ttu87x", "t3_ug2xqg", "t3_v2ka5e", "t3_vp487n", "t3_wdl07g", "t3_x3bb2b", "t3_xsyy4v", "t3_yjchhi", "t3_z9szlc", "t3_100nsr2", "t3_10qzpp1", "t3_11f8z5h", "t3_128qhe2", "t3_134qgn8", "t3_13xle38", "t3_14nylwl", "t3_15fgn9y", "t3_167b40e"], "description": "", "title": "Monthly General Discussions", "created_at_utc": 1642292653.587, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "6278fda2-fad1-4706-9e82-6ddb67d49c0b", "author_id": "t2_2tv9i42n", "last_update_utc": 1693584094.435, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167b40e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1693584093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are you working on this month?&lt;/li&gt;\n&lt;li&gt;What was something you accomplished?&lt;/li&gt;\n&lt;li&gt;What was something you learned recently?&lt;/li&gt;\n&lt;li&gt;What is something frustrating you currently?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As always, sub rules apply. Please be respectful and stay curious.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Community Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineeringcommunity.substack.com/\"&gt;Monthly newsletter&lt;/a&gt;: community updates, inspiration, and insights&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Events\"&gt;Data Engineering Events&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Get+Involved\"&gt;Get involved in the community&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5hX88p7Zlx995y2AC0Sb_biZAT_nna3SZcxbz_11hhI.jpg?auto=webp&amp;s=f9359dad1d983347db44f2cccbd0b0f99e16a62c", "width": 920, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/5hX88p7Zlx995y2AC0Sb_biZAT_nna3SZcxbz_11hhI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04e1b5b7eb03f5443c87c198a1aa6708e2c52eac", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/5hX88p7Zlx995y2AC0Sb_biZAT_nna3SZcxbz_11hhI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8679e56088c4fe736444659cb5fb4e3ef2fff64f", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/5hX88p7Zlx995y2AC0Sb_biZAT_nna3SZcxbz_11hhI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6a80cc7e920d2be12047018c3175d792aa279ab3", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/5hX88p7Zlx995y2AC0Sb_biZAT_nna3SZcxbz_11hhI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=baa6cc1c0ed69575c177c13d6296ff46f82657bf", "width": 640, "height": 333}], "variants": {}, "id": "Qo9qYKD-7P-sb-46EoJ3ZlaOD05VErGXT0coJg6xpxg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167b40e", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167b40e/monthly_general_discussion_sep_2023/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/167b40e/monthly_general_discussion_sep_2023/", "subreddit_subscribers": 126198, "created_utc": 1693584093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello. Say I have multiple tables. Until now, I've been just deleting the tables completely and uploading all the data again.\n\nNow this is not really feasible, as there are millions of rows and it takes time that's not acceptable compared to before.\n\nSo I've been reading about incremental loading but I still don't understand how I am supposed to implement it.\n\nMy tables are simple accounting records. For example, an invoice with money paid, money owed, date of the invoice, type of document and so on.\n\nThey typically come from some kind of ERP where invoices are loaded and so on. \n\nCurrently I'm using Azure, Python and SQL to do all my work. Save .csvs of the data in blob storage, do some stuff with the data in Python, load them onto an SQL Server database.\n\nNow I would like to only change the database: \n\n1. Add new records that aren't present\n\n2. Delete records that were deleted \n\n3. Update the records that were updated. Although rare, some invoices from 2015 can still be updated\n\nas only a a few hundred, if that, records are changed/added/deleted per day.\n\nHow can I go about this? Is there column requirements I must have to do this? Like timestamps of updates, IDs and so on?\n\nLike I said, I have access to most things Azure, I'm fairly competent at Python (though haven't delved too much into frameworks) and SQL.\n\nAny input appreciated.", "author_fullname": "t2_sfbys1gt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the requirements for incremental loading and what tools are used for this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1679qq9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693580935.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. Say I have multiple tables. Until now, I&amp;#39;ve been just deleting the tables completely and uploading all the data again.&lt;/p&gt;\n\n&lt;p&gt;Now this is not really feasible, as there are millions of rows and it takes time that&amp;#39;s not acceptable compared to before.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;ve been reading about incremental loading but I still don&amp;#39;t understand how I am supposed to implement it.&lt;/p&gt;\n\n&lt;p&gt;My tables are simple accounting records. For example, an invoice with money paid, money owed, date of the invoice, type of document and so on.&lt;/p&gt;\n\n&lt;p&gt;They typically come from some kind of ERP where invoices are loaded and so on. &lt;/p&gt;\n\n&lt;p&gt;Currently I&amp;#39;m using Azure, Python and SQL to do all my work. Save .csvs of the data in blob storage, do some stuff with the data in Python, load them onto an SQL Server database.&lt;/p&gt;\n\n&lt;p&gt;Now I would like to only change the database: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Add new records that aren&amp;#39;t present&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Delete records that were deleted &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Update the records that were updated. Although rare, some invoices from 2015 can still be updated&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;as only a a few hundred, if that, records are changed/added/deleted per day.&lt;/p&gt;\n\n&lt;p&gt;How can I go about this? Is there column requirements I must have to do this? Like timestamps of updates, IDs and so on?&lt;/p&gt;\n\n&lt;p&gt;Like I said, I have access to most things Azure, I&amp;#39;m fairly competent at Python (though haven&amp;#39;t delved too much into frameworks) and SQL.&lt;/p&gt;\n\n&lt;p&gt;Any input appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1679qq9", "is_robot_indexable": true, "report_reasons": null, "author": "necesitorespuestas", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1679qq9/what_are_the_requirements_for_incremental_loading/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1679qq9/what_are_the_requirements_for_incremental_loading/", "subreddit_subscribers": 126198, "created_utc": 1693580935.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We basically have a Databricks Workflow (on dev env) that triggers 20 notebooks in a specific sequence. This is made through the UI of Databricks.\n\nHow can we CICD this so that UAT env or prod env can get that workflow?\n\nIs there some way to version control the Databricks Workflows?", "author_fullname": "t2_3cuv2cgu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Workflows (Jobs) CICD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1678xun", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693579097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We basically have a Databricks Workflow (on dev env) that triggers 20 notebooks in a specific sequence. This is made through the UI of Databricks.&lt;/p&gt;\n\n&lt;p&gt;How can we CICD this so that UAT env or prod env can get that workflow?&lt;/p&gt;\n\n&lt;p&gt;Is there some way to version control the Databricks Workflows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1678xun", "is_robot_indexable": true, "report_reasons": null, "author": "Equivalent-Style6371", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1678xun/databricks_workflows_jobs_cicd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1678xun/databricks_workflows_jobs_cicd/", "subreddit_subscribers": 126198, "created_utc": 1693579097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Complete noob here. How would one go about creating an automated data pipeline with the export of test scores through a test lay form API, to a cloud server capable of running automated data transformation and basic analysis all while maintains privacy?\n\nIs there a place where someone can get higher to accomplish this task and what does that usually cost?", "author_fullname": "t2_gyk9qe35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Pipeline Creation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166ya6w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693546022.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Complete noob here. How would one go about creating an automated data pipeline with the export of test scores through a test lay form API, to a cloud server capable of running automated data transformation and basic analysis all while maintains privacy?&lt;/p&gt;\n\n&lt;p&gt;Is there a place where someone can get higher to accomplish this task and what does that usually cost?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166ya6w", "is_robot_indexable": true, "report_reasons": null, "author": "One-Construction-732", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166ya6w/data_pipeline_creation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166ya6w/data_pipeline_creation/", "subreddit_subscribers": 126198, "created_utc": 1693546022.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm having real trouble with our Google Analytics setup and migration that the marketing team pushed onto my plate.\n\nAs some of you probably know, Google recently forced all users to migrate from Universal Analytics (UA) to the new GA4 platform. There's now a big gap in our analytics history, making it hard to track trends and compare past reports.\n\nOn top of all my existing data engineering work, our marketing team insisted I take this on. I have very little GA knowledge and no time for a deep dive. I'm hitting roadblocks trying to stitch these surprisingly disparate systems together.\n\nThe problem is Google didn't provide an easy way to connect historical data, and there's very limited useful information out there. I could really use any shortcuts or tips from this community.\n\nHas anyone found good ways to map UA to GA4 to maintain continuous analytics? Our small team has limited resources, so I'm struggling.\n\nPS: I'm pretty exasperated by how Google is handling this migration. They seem so detached from their actual customers!", "author_fullname": "t2_43r4n5ss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "google analytics UA-GA4 debacle - dataeng perspective?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167axtw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693583700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m having real trouble with our Google Analytics setup and migration that the marketing team pushed onto my plate.&lt;/p&gt;\n\n&lt;p&gt;As some of you probably know, Google recently forced all users to migrate from Universal Analytics (UA) to the new GA4 platform. There&amp;#39;s now a big gap in our analytics history, making it hard to track trends and compare past reports.&lt;/p&gt;\n\n&lt;p&gt;On top of all my existing data engineering work, our marketing team insisted I take this on. I have very little GA knowledge and no time for a deep dive. I&amp;#39;m hitting roadblocks trying to stitch these surprisingly disparate systems together.&lt;/p&gt;\n\n&lt;p&gt;The problem is Google didn&amp;#39;t provide an easy way to connect historical data, and there&amp;#39;s very limited useful information out there. I could really use any shortcuts or tips from this community.&lt;/p&gt;\n\n&lt;p&gt;Has anyone found good ways to map UA to GA4 to maintain continuous analytics? Our small team has limited resources, so I&amp;#39;m struggling.&lt;/p&gt;\n\n&lt;p&gt;PS: I&amp;#39;m pretty exasperated by how Google is handling this migration. They seem so detached from their actual customers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "167axtw", "is_robot_indexable": true, "report_reasons": null, "author": "colorfulskull", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167axtw/google_analytics_uaga4_debacle_dataeng_perspective/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167axtw/google_analytics_uaga4_debacle_dataeng_perspective/", "subreddit_subscribers": 126198, "created_utc": 1693583700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Get the Most Out of dbt's Built-In Tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1679obh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vy6yNIeHQQe-lVqfxZOUd6R_IBvtRt2epbXLJO8Yx1c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693580783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dbtips.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dbtips.substack.com/p/how-to-get-the-most-out-of-dbts-built", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?auto=webp&amp;s=e343d24090864a36a1d2e5c01f80d493b97f41a5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=981e7a03c2c1587a3b5e11eb6386b658bb916b28", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c365f45355171bb3180853cede6e79af530c006e", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb425634a996da08e7d5b09c0efc8666b3bf0404", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4761a82fd38e5e194ffed69e0dcc3e2f7c018833", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=af713ceeb72ae41ad0ad29af6154e4bb4cd835c9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15968aa11fffce6b7588e14bdeed97fac9d0ce40", "width": 1080, "height": 540}], "variants": {}, "id": "kpD353cJ6z0DO95iVfnPYLYpBHDueASViOyySTM2GKc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1679obh", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1679obh/how_to_get_the_most_out_of_dbts_builtin_tests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dbtips.substack.com/p/how-to-get-the-most-out-of-dbts-built", "subreddit_subscribers": 126198, "created_utc": 1693580783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3g7ch6cj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Complex search, filtering and aggregations on DynamoDB - tech talk by AWS SA &amp; Rockset CTO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_166wh7n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tT3j0WbiyEk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tT3j0WbiyEk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset\"&gt;&lt;/iframe&gt;", "author_name": "Rockset", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/tT3j0WbiyEk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Rockset"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tT3j0WbiyEk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/166wh7n", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gMdeRakrRRLbAmsMYPixywB7jhXsfCwNS7pabrRBiOE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693540310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=tT3j0WbiyEk&amp;feature=youtu.be", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Xqy5BnDITq0PXIQR7G2g9alPeNpi_sGawWqscwjnaXA.jpg?auto=webp&amp;s=171147a81dccb6ef66ba97243782b74e44403160", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Xqy5BnDITq0PXIQR7G2g9alPeNpi_sGawWqscwjnaXA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb6d107ec2cc0f80c942e610775b27f0e75af139", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Xqy5BnDITq0PXIQR7G2g9alPeNpi_sGawWqscwjnaXA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80c57b082e11e86816e1343ec40d30dd90af095e", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Xqy5BnDITq0PXIQR7G2g9alPeNpi_sGawWqscwjnaXA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1681a572fe5a86b42eec8ed0e17d46e5862180c9", "width": 320, "height": 240}], "variants": {}, "id": "ua_5DddNv9ecnN9b6PuF45ubk6q3Y7b-8WWNQI4nPAY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "166wh7n", "is_robot_indexable": true, "report_reasons": null, "author": "ssb61", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166wh7n/complex_search_filtering_and_aggregations_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=tT3j0WbiyEk&amp;feature=youtu.be", "subreddit_subscribers": 126198, "created_utc": 1693540310.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tT3j0WbiyEk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset\"&gt;&lt;/iframe&gt;", "author_name": "Rockset", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/tT3j0WbiyEk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Rockset"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I have a substantial list of IDs that I need to work with. My objective is to retrieve data from various API endpoints for each individual ID, process the data, and then load it into a database. \n\nCurrently, I'm using Prefect with Kubernetes to structure the workflow. This involves organizing the tasks into a flow, with each task responsible for making requests to a distinct API endpoint. The flow follows this pattern:\n\n**Flow**\n\n* Task 1: Fetch data from API endpoint 1, then load the acquired data into table 1 in the Postgres database.\n* Task 2: Retrieve data from API endpoint 2, followed by loading it into table 2 in the Postgres database.\n* ...\n\nThese tasks can be executed concurrently. Given the extensive list of IDs, I divide them into batches of 100. Subsequently, I pass each batch as a parameter to initiate a new instance of the flow. At any given moment, a maximum of 5 batches can be actively running. Each task within the flow can access the list of IDs in a batch and process them in parallel.\n\nHowever, I am encountering several challenges:\n\n* In many cases, I only need to execute the entire workflow for individual IDs rather than batches. Unfortunately, each time the flow is triggered, Prefect creates a dedicated container for the job. This results in unnecessary resource usage. If I initiate 100 flows for 100 individual IDs, it could potentially overwhelm the Kubernetes cluster's resources.\n* The API enforces rate limits, including a concurrent limit and a limit per minute, for each ID. When using Prefect, what I can do is to employ retries, which is not an optimal solution. I am exploring options to implement a request queue mechanism that deal with these rate limits more effectively.\n\nBecause of these challenges, I'm considering alternative approaches. One of them is using Flask on Cloud Run and Cloud Functions to design a custom workflow and tasks, without relying on Prefect. I'd really appreciate any suggestions you might have for tackling this issue. ", "author_fullname": "t2_b23p74kt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing Data Retrieval and Processing Workflow for Multiple API Endpoints", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166pml8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693521945.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I have a substantial list of IDs that I need to work with. My objective is to retrieve data from various API endpoints for each individual ID, process the data, and then load it into a database. &lt;/p&gt;\n\n&lt;p&gt;Currently, I&amp;#39;m using Prefect with Kubernetes to structure the workflow. This involves organizing the tasks into a flow, with each task responsible for making requests to a distinct API endpoint. The flow follows this pattern:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Flow&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Task 1: Fetch data from API endpoint 1, then load the acquired data into table 1 in the Postgres database.&lt;/li&gt;\n&lt;li&gt;Task 2: Retrieve data from API endpoint 2, followed by loading it into table 2 in the Postgres database.&lt;/li&gt;\n&lt;li&gt;...&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;These tasks can be executed concurrently. Given the extensive list of IDs, I divide them into batches of 100. Subsequently, I pass each batch as a parameter to initiate a new instance of the flow. At any given moment, a maximum of 5 batches can be actively running. Each task within the flow can access the list of IDs in a batch and process them in parallel.&lt;/p&gt;\n\n&lt;p&gt;However, I am encountering several challenges:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In many cases, I only need to execute the entire workflow for individual IDs rather than batches. Unfortunately, each time the flow is triggered, Prefect creates a dedicated container for the job. This results in unnecessary resource usage. If I initiate 100 flows for 100 individual IDs, it could potentially overwhelm the Kubernetes cluster&amp;#39;s resources.&lt;/li&gt;\n&lt;li&gt;The API enforces rate limits, including a concurrent limit and a limit per minute, for each ID. When using Prefect, what I can do is to employ retries, which is not an optimal solution. I am exploring options to implement a request queue mechanism that deal with these rate limits more effectively.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Because of these challenges, I&amp;#39;m considering alternative approaches. One of them is using Flask on Cloud Run and Cloud Functions to design a custom workflow and tasks, without relying on Prefect. I&amp;#39;d really appreciate any suggestions you might have for tackling this issue. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166pml8", "is_robot_indexable": true, "report_reasons": null, "author": "kia_ora_st", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166pml8/optimizing_data_retrieval_and_processing_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166pml8/optimizing_data_retrieval_and_processing_workflow/", "subreddit_subscribers": 126198, "created_utc": 1693521945.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. This is the first post I'm creating in this channel. To give a background about myself, I work as a data engineer. I joined a startup company around 1.5-2 years ago as a software engineer. Initially, I used to work as a software engineer and then after few months the need for a data warehouse arose but we didn't had any data engineers back then. So I was asked to work on the DE stuff where I was suppose to build the data warehouse using Python/Airflow/Postgres stack(yes you read right!) My manager's focus was just to get this DWH project started to serve the purpose of analytics and research's model training. Fast forward to 6 months down the line, we noticed some bottlenecks related to some database design decision that we had made earlier and also there were performance bottlenecks coming into the picture as our data size grew in size. We started to think and plan to overcome those issues but till now we have not yet gotten started(it's been more than 5 months since we had done the planning of resolution of these issues). Everytime we start to think of working on the resolution there are ad-hoc requests which keeps in flowing and they term them as blocker/critical. There's very little of data quality checks implemented in these pipelines which keeps us haunting. On top of that I'm expected to deliver them resources which would be useful for them in their analytical purpose. These resources are something which I think should be created by the data analysts as they know the exact metrics what they need. It could be the case that since they don't want to write complex SQL queries they ask me to give them the data in a structured format which would make their life easier. I'm still the only data engineer working in there and we are tight on budget to get more resources. I'm now constantly getting held responsible for data related issues which I don't have control over or there's not enough bandwidth for me to work and get everything fixed at once. I feel like I'm not growing and only resolving the bugs and issues which could have been taken care of if I was given enough time rather than those ad-hoc requests which kept on flowing to me. I need some suggestions and if anyone has gone through this then please let me know what was your experience like. Thanks!", "author_fullname": "t2_bt39kjks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I look lost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_167e18k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693590693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. This is the first post I&amp;#39;m creating in this channel. To give a background about myself, I work as a data engineer. I joined a startup company around 1.5-2 years ago as a software engineer. Initially, I used to work as a software engineer and then after few months the need for a data warehouse arose but we didn&amp;#39;t had any data engineers back then. So I was asked to work on the DE stuff where I was suppose to build the data warehouse using Python/Airflow/Postgres stack(yes you read right!) My manager&amp;#39;s focus was just to get this DWH project started to serve the purpose of analytics and research&amp;#39;s model training. Fast forward to 6 months down the line, we noticed some bottlenecks related to some database design decision that we had made earlier and also there were performance bottlenecks coming into the picture as our data size grew in size. We started to think and plan to overcome those issues but till now we have not yet gotten started(it&amp;#39;s been more than 5 months since we had done the planning of resolution of these issues). Everytime we start to think of working on the resolution there are ad-hoc requests which keeps in flowing and they term them as blocker/critical. There&amp;#39;s very little of data quality checks implemented in these pipelines which keeps us haunting. On top of that I&amp;#39;m expected to deliver them resources which would be useful for them in their analytical purpose. These resources are something which I think should be created by the data analysts as they know the exact metrics what they need. It could be the case that since they don&amp;#39;t want to write complex SQL queries they ask me to give them the data in a structured format which would make their life easier. I&amp;#39;m still the only data engineer working in there and we are tight on budget to get more resources. I&amp;#39;m now constantly getting held responsible for data related issues which I don&amp;#39;t have control over or there&amp;#39;s not enough bandwidth for me to work and get everything fixed at once. I feel like I&amp;#39;m not growing and only resolving the bugs and issues which could have been taken care of if I was given enough time rather than those ad-hoc requests which kept on flowing to me. I need some suggestions and if anyone has gone through this then please let me know what was your experience like. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "167e18k", "is_robot_indexable": true, "report_reasons": null, "author": "tradax", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167e18k/i_look_lost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167e18k/i_look_lost/", "subreddit_subscribers": 126198, "created_utc": 1693590693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've never used dbt core at an organization. I've only used the closed-source versions (dbt cloud, paradime)  \n\n\nIf you've used dbt core at your organization (ex. Used dbt core w/ VS code + Airflow) what are the pros and cons?  \n\n\nOnce dbt core is set up properly in an organization, do you spend a lot of time updating and fixing the setup? Or does it run smoothly for the most part?", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pros &amp; cons of running dbt core at scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167czoc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693588341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve never used dbt core at an organization. I&amp;#39;ve only used the closed-source versions (dbt cloud, paradime)  &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve used dbt core at your organization (ex. Used dbt core w/ VS code + Airflow) what are the pros and cons?  &lt;/p&gt;\n\n&lt;p&gt;Once dbt core is set up properly in an organization, do you spend a lot of time updating and fixing the setup? Or does it run smoothly for the most part?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167czoc", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167czoc/pros_cons_of_running_dbt_core_at_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167czoc/pros_cons_of_running_dbt_core_at_scale/", "subreddit_subscribers": 126198, "created_utc": 1693588341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm preparing for the SnowPro Advanced cert and came across a bunch of sample exams online. Curious to know what the answer to the below question is:\n\n    Rank the following querying and materialziation techniques from least to most performant: \n    \n    A.  Direct query files from stage \n    B.  Materialized Views over an External Table\n    C.  External Table \n    D.  External Table with Partitioning\n    \n\nIs the answer here: A, C, D, B? ", "author_fullname": "t2_6b1hqs16", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SnowPro Certification Exam: Question on Performance Techniques", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167a28d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693581670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m preparing for the SnowPro Advanced cert and came across a bunch of sample exams online. Curious to know what the answer to the below question is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Rank the following querying and materialziation techniques from least to most performant: \n\nA.  Direct query files from stage \nB.  Materialized Views over an External Table\nC.  External Table \nD.  External Table with Partitioning\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Is the answer here: A, C, D, B? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "167a28d", "is_robot_indexable": true, "report_reasons": null, "author": "Background-Proof5402", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167a28d/snowpro_certification_exam_question_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167a28d/snowpro_certification_exam_question_on/", "subreddit_subscribers": 126198, "created_utc": 1693581670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I need to build a pipeline that takes data from an API call, do some data transformation and store it somewhere in Azure and use Power BI to visualize it. Data should be refreshed quite frequently~every 30 seconds. \n\nI'm quite lost in all of services provided by Azure. Does anyone have suggestions what service I can use? \nI'm thinking of azure functions and store in blob, but open to new ideas. \n\nThanks", "author_fullname": "t2_a1bnnkle", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Save data in Azure through API call then visualize in Power BI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1677goe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693575652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I need to build a pipeline that takes data from an API call, do some data transformation and store it somewhere in Azure and use Power BI to visualize it. Data should be refreshed quite frequently~every 30 seconds. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m quite lost in all of services provided by Azure. Does anyone have suggestions what service I can use? \nI&amp;#39;m thinking of azure functions and store in blob, but open to new ideas. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1677goe", "is_robot_indexable": true, "report_reasons": null, "author": "Individual-Sweet-734", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1677goe/save_data_in_azure_through_api_call_then/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1677goe/save_data_in_azure_through_api_call_then/", "subreddit_subscribers": 126198, "created_utc": 1693575652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Where would you guys start. I'm at a new company and the repo is HUGE, so it seems overwhelming.", "author_fullname": "t2_ceoouce2p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to learn about a company through it's GitHub?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1677e3s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693575464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Where would you guys start. I&amp;#39;m at a new company and the repo is HUGE, so it seems overwhelming.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1677e3s", "is_robot_indexable": true, "report_reasons": null, "author": "palomino-ridin-21", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1677e3s/how_to_learn_about_a_company_through_its_github/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1677e3s/how_to_learn_about_a_company_through_its_github/", "subreddit_subscribers": 126198, "created_utc": 1693575464.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time analytics with stream processing and OLAP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167c86t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1693586611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "news.ycombinator.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://news.ycombinator.com/item?id=37353112", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "167c86t", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167c86t/realtime_analytics_with_stream_processing_and_olap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://news.ycombinator.com/item?id=37353112", "subreddit_subscribers": 126198, "created_utc": 1693586611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to data engineering. I am learning ETL.  \nPlease share with me some ETL example use cases.\n\n&amp;#x200B;", "author_fullname": "t2_s4mqzf7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL Examples", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167b8eg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693584353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to data engineering. I am learning ETL.&lt;br/&gt;\nPlease share with me some ETL example use cases.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167b8eg", "is_robot_indexable": true, "report_reasons": null, "author": "small_and_boo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167b8eg/etl_examples/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167b8eg/etl_examples/", "subreddit_subscribers": 126198, "created_utc": 1693584353.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}