{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Was reading an interesting blog post about how instacart migrated to databricks that mysteriously disappeared: https://www.instacart.com/company/how-its-made/how-instacart-ads-modularized-data-pipelines-with-lakehouse-architecture-and-spark/\nI went looking for info and found some twitter threads and it\nturns out instacart had saved about 70% on snowflake costs in 2023 by migrating to databricks. Databricks even advertised the [case study](https://pbs.twimg.com/media/F4s0qIcXYAAuvYD?format=jpg&amp;name=medium) . One problem though, snowflakes CEO sits on instacarts board, which means a normally transparent blog had to delete its findings.\n\n[Quote:](https://twitter.com/GergelyOrosz/status/1697192807801184561)\n&gt;Instacart cut Snowflake spend by 70% in 2023, while starting to migrate ETL loads to Databricks - then deletes blog post detailing migration. I email Instacart press team with questions but Snowlake press team comes back with a comment on behalf of Instacart \ud83e\udd2f. Snowflake\u2019s CEO is on the board of directors for Instacart. The thing that blew my mind is how my email addressed only to Instacart\u2019s press team ended up at Snowflake (who I never contacted) and why Snowflake makes/can make definite statements on behalf of Instacart. Emailed Instacart, and then Snowflake press team landed in my inbox referencing things that I only sent to Instacart, saying they hear I am writing an article and they want to give me facts. Never contacted them. Feels like Instacart pinged them.\n\n\nSo now databricks removed the [case study](https://pbs.twimg.com/media/F4s0qIYXAAArUkz?format=jpg&amp;name=large) and snowflake even posted a [response](https://www.snowflake.com/blog/snowflake-and-instacart-the-facts/) which says its countering 'social media' misinformation but most of the details came from putting two and two together with instacarts own blog post.\n\nStumbled upon some drama just reading a tech blog, I have a feeling Instacarts tech blog team is getting a serious talking to and now will have to pass anything they post by the board. It was a really good post though, detailed and well thought out, I was looking to share the info with my team today.\n\nThreads here: [1](https://twitter.com/GergelyOrosz/status/1696435748071772333) [2](https://twitter.com/modestproposal1/status/1695177654822191184) [3](https://twitter.com/GergelyOrosz/status/1697192807801184561)", "author_fullname": "t2_bvrga", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Instacart, Databricks and Snowflake drama", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166ah28", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 122, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 122, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693489569.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693486694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was reading an interesting blog post about how instacart migrated to databricks that mysteriously disappeared: &lt;a href=\"https://www.instacart.com/company/how-its-made/how-instacart-ads-modularized-data-pipelines-with-lakehouse-architecture-and-spark/\"&gt;https://www.instacart.com/company/how-its-made/how-instacart-ads-modularized-data-pipelines-with-lakehouse-architecture-and-spark/&lt;/a&gt;\nI went looking for info and found some twitter threads and it\nturns out instacart had saved about 70% on snowflake costs in 2023 by migrating to databricks. Databricks even advertised the &lt;a href=\"https://pbs.twimg.com/media/F4s0qIcXYAAuvYD?format=jpg&amp;amp;name=medium\"&gt;case study&lt;/a&gt; . One problem though, snowflakes CEO sits on instacarts board, which means a normally transparent blog had to delete its findings.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/GergelyOrosz/status/1697192807801184561\"&gt;Quote:&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Instacart cut Snowflake spend by 70% in 2023, while starting to migrate ETL loads to Databricks - then deletes blog post detailing migration. I email Instacart press team with questions but Snowlake press team comes back with a comment on behalf of Instacart \ud83e\udd2f. Snowflake\u2019s CEO is on the board of directors for Instacart. The thing that blew my mind is how my email addressed only to Instacart\u2019s press team ended up at Snowflake (who I never contacted) and why Snowflake makes/can make definite statements on behalf of Instacart. Emailed Instacart, and then Snowflake press team landed in my inbox referencing things that I only sent to Instacart, saying they hear I am writing an article and they want to give me facts. Never contacted them. Feels like Instacart pinged them.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;So now databricks removed the &lt;a href=\"https://pbs.twimg.com/media/F4s0qIYXAAArUkz?format=jpg&amp;amp;name=large\"&gt;case study&lt;/a&gt; and snowflake even posted a &lt;a href=\"https://www.snowflake.com/blog/snowflake-and-instacart-the-facts/\"&gt;response&lt;/a&gt; which says its countering &amp;#39;social media&amp;#39; misinformation but most of the details came from putting two and two together with instacarts own blog post.&lt;/p&gt;\n\n&lt;p&gt;Stumbled upon some drama just reading a tech blog, I have a feeling Instacarts tech blog team is getting a serious talking to and now will have to pass anything they post by the board. It was a really good post though, detailed and well thought out, I was looking to share the info with my team today.&lt;/p&gt;\n\n&lt;p&gt;Threads here: &lt;a href=\"https://twitter.com/GergelyOrosz/status/1696435748071772333\"&gt;1&lt;/a&gt; &lt;a href=\"https://twitter.com/modestproposal1/status/1695177654822191184\"&gt;2&lt;/a&gt; &lt;a href=\"https://twitter.com/GergelyOrosz/status/1697192807801184561\"&gt;3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 50, "id": "award_02d9ab2c-162e-4c01-8438-317a016ed3d9", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=16&amp;height=16&amp;auto=webp&amp;s=10034f3fdf8214c8377134bb60c5b832d4bbf588", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=32&amp;height=32&amp;auto=webp&amp;s=100f785bf261fa9452a5d82ee0ef0793369dbfa5", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=48&amp;height=48&amp;auto=webp&amp;s=b15d030fdfbbe4af4a5b34ab9dc90a174df40a23", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=64&amp;height=64&amp;auto=webp&amp;s=601c75be6ee30dc4b47a5c65d64dea9a185502a1", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=128&amp;height=128&amp;auto=webp&amp;s=540f36e65c0e2f1347fe32020e4a1565e3680437", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "I'm in this with you.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Take My Energy", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=16&amp;height=16&amp;auto=webp&amp;s=045db73f47a9513c44823d132b4c393ab9241b6a", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=32&amp;height=32&amp;auto=webp&amp;s=298a02e0edbb5b5e293087eeede63802cbe1d2c7", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=48&amp;height=48&amp;auto=webp&amp;s=7d06d606eb23dbcd6dbe39ee0e60588c5eb89065", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=64&amp;height=64&amp;auto=webp&amp;s=ecd9854b14104a36a210028c43420f0dababd96b", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=128&amp;height=128&amp;auto=webp&amp;s=0d5d7b92c1d66aff435f2ad32e6330ca2b971f6d", "width": 128, "height": 128}], "icon_format": "PNG", "icon_height": 2048, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166ah28", "is_robot_indexable": true, "report_reasons": null, "author": "TerriblyRare", "discussion_type": null, "num_comments": 60, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166ah28/instacart_databricks_and_snowflake_drama/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166ah28/instacart_databricks_and_snowflake_drama/", "subreddit_subscribers": 126104, "created_utc": 1693486694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If not what makes it fun?", "author_fullname": "t2_9vkdidvf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is data engineering job boring?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1663we7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693465828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If not what makes it fun?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1663we7", "is_robot_indexable": true, "report_reasons": null, "author": "Friendly-Change-1078", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1663we7/is_data_engineering_job_boring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1663we7/is_data_engineering_job_boring/", "subreddit_subscribers": 126104, "created_utc": 1693465828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been a data engineer for about 5 years, and I'm starting to realize that I want to switch into devops for my long term career plan.\n\n\n\n\n\nHow easy is it for data engineers to make that career change?  I already do a lot of work in Python and SQL, but I've been able to familiarize myself with the cloud, Terraform, CI/CD, and software engineering best practices over the course of my career.  I also have a degree in computer science, where I took courses in computer networking and I feel that knowledge certainly helps.", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone else want to switch into devops?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16693mr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693482921.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been a data engineer for about 5 years, and I&amp;#39;m starting to realize that I want to switch into devops for my long term career plan.&lt;/p&gt;\n\n&lt;p&gt;How easy is it for data engineers to make that career change?  I already do a lot of work in Python and SQL, but I&amp;#39;ve been able to familiarize myself with the cloud, Terraform, CI/CD, and software engineering best practices over the course of my career.  I also have a degree in computer science, where I took courses in computer networking and I feel that knowledge certainly helps.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16693mr", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16693mr/does_anyone_else_want_to_switch_into_devops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16693mr/does_anyone_else_want_to_switch_into_devops/", "subreddit_subscribers": 126104, "created_utc": 1693482921.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_viq1t90m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipelines with Python and SQL - Part 1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_166rste", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6FoFuooprnsoo_O92yU7D5QwKRgX2jMp3Z9qkn0aDPc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693527254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "stephendavidwilliams.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://stephendavidwilliams.com/data-pipelines-with-python-and-sql-part-1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?auto=webp&amp;s=e69f6e29dcdebd0a3dee751d2b1929e07664b4e9", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6bfac54f8dfdd46f74556e776eaa81a50d7f8842", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=36f68e2eba4566517be4d4229dc9ba76dc515d56", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c70788f4e382f796dcb1f82d6ef8cbcd26bd755", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c9de6060ba8c380b2abe2b5fea8e8cc8c84546ea", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e8ed329f03658b8866641d22bdee2bd47429dcb1", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/zaeER55yM7lvmlr0syU82YfR32ux4962FGb8UOcwLe4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02d1773f6e573fd02e765a244977bac8af1408ef", "width": 1080, "height": 567}], "variants": {}, "id": "lo0sGmJanYdR0vXmZrljanaI_RhUtYEp-GDYPws1knE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "166rste", "is_robot_indexable": true, "report_reasons": null, "author": "sdw_solutions", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166rste/data_pipelines_with_python_and_sql_part_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://stephendavidwilliams.com/data-pipelines-with-python-and-sql-part-1", "subreddit_subscribers": 126104, "created_utc": 1693527254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry about the rant but man it\u2019s frustrating to see no data engineering job opportunities in Canada even at senior level.\n\nThe few that are there have 400-500 applicants. Just saw a job post which I thought was interesting only to find it has close to 900+ applicants. How will you even get a call. \n\nEnd of rant.", "author_fullname": "t2_4ah8jqqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "There are no jobs and it is mildly frustrating!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166nu11", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693517707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry about the rant but man it\u2019s frustrating to see no data engineering job opportunities in Canada even at senior level.&lt;/p&gt;\n\n&lt;p&gt;The few that are there have 400-500 applicants. Just saw a job post which I thought was interesting only to find it has close to 900+ applicants. How will you even get a call. &lt;/p&gt;\n\n&lt;p&gt;End of rant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166nu11", "is_robot_indexable": true, "report_reasons": null, "author": "idreamoffood101", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166nu11/there_are_no_jobs_and_it_is_mildly_frustrating/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166nu11/there_are_no_jobs_and_it_is_mildly_frustrating/", "subreddit_subscribers": 126104, "created_utc": 1693517707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was wondering how other teams use dbt, so I made this very short survey to understand better. I'll share the results later in the comments.  \n\n\n[https://forms.gle/zwwfDAVYGu37nz2W6](https://forms.gle/zwwfDAVYGu37nz2W6)", "author_fullname": "t2_tbjno", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt. What is it good for? A 42-second survey.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166nao6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693516475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering how other teams use dbt, so I made this very short survey to understand better. I&amp;#39;ll share the results later in the comments.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://forms.gle/zwwfDAVYGu37nz2W6\"&gt;https://forms.gle/zwwfDAVYGu37nz2W6&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?auto=webp&amp;s=3e5851ed8f8cbec67e9d3d95ce1a6ab2fd5766a4", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95d563b3730f933218dfe75138324b3840fffa23", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=39f7c8c738ffa952f983e1213eb4a3465e80a6f4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c2cf8cfd3a17b21357728bbed54d062c79cd3d4", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0450a7adb08aa6c7a15f18ba4e1551c53a5a1cc", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c36a1277013d617e183e04aeec20fd0d1b96478b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/59_t0oanrbP_Ijj0k3j3RVHsfyc7DMVKzWc67ql6Hlc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ed8af9722ace6f0c6b022974f642c2b2e4ac3521", "width": 1080, "height": 567}], "variants": {}, "id": "L38pNlP_aAcmNKnS5YBL_J4J7QdBMDhPuBkZmM20ZLY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166nao6", "is_robot_indexable": true, "report_reasons": null, "author": "PrivateH", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166nao6/dbt_what_is_it_good_for_a_42second_survey/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166nao6/dbt_what_is_it_good_for_a_42second_survey/", "subreddit_subscribers": 126104, "created_utc": 1693516475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it really better pragmatically? What justifies for someone to migrate from siloed data lake - warehouse setup to a lakehouse one?", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Data Lakehouse has a strong business proposition?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166fjlo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693498692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it really better pragmatically? What justifies for someone to migrate from siloed data lake - warehouse setup to a lakehouse one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166fjlo", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166fjlo/does_data_lakehouse_has_a_strong_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166fjlo/does_data_lakehouse_has_a_strong_business/", "subreddit_subscribers": 126104, "created_utc": 1693498692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am recently grinding SQL on both platform. \n\nI just found StrataScratch SQL questions are more straightforward, and I can do 10+ medium questions per day. Its difficulty is more focus on how well you understand the basic fundamental commands of SQL, aka how you can use join, subquery and cte altogether. The learning curve is very smooth. Hence the user interface is more friendly, every question has its official solution. \n\nOn the other hand, Leetcode SQL's difficulty is more \"tricky\", it focuses on how well you understand the problem and how you can find a \"trick\" to hack the problem. Honestly, my progress is very slow on Leetcode style SQL, and I have learned very few from it...\n\nAny thoughts? \n\n&amp;#x200B;", "author_fullname": "t2_csk6gf7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some thoughts about Leetcode SQL and StrataScratch SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_166wmef", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693540773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am recently grinding SQL on both platform. &lt;/p&gt;\n\n&lt;p&gt;I just found StrataScratch SQL questions are more straightforward, and I can do 10+ medium questions per day. Its difficulty is more focus on how well you understand the basic fundamental commands of SQL, aka how you can use join, subquery and cte altogether. The learning curve is very smooth. Hence the user interface is more friendly, every question has its official solution. &lt;/p&gt;\n\n&lt;p&gt;On the other hand, Leetcode SQL&amp;#39;s difficulty is more &amp;quot;tricky&amp;quot;, it focuses on how well you understand the problem and how you can find a &amp;quot;trick&amp;quot; to hack the problem. Honestly, my progress is very slow on Leetcode style SQL, and I have learned very few from it...&lt;/p&gt;\n\n&lt;p&gt;Any thoughts? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166wmef", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Astronomer-471", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166wmef/some_thoughts_about_leetcode_sql_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166wmef/some_thoughts_about_leetcode_sql_and/", "subreddit_subscribers": 126104, "created_utc": 1693540773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Folks,  \nAre there people in this community who have properly evaluated Kafka alternatives like RedPanda or others, and/or MQTT alternatives who are willing to share their experience?\n\nContext:  \nI am working on a project involving a handful of clients with edge devices and sensors in different verticals.\n\nThe broad use case is building data pipelines and analytics software for asset monitoring and management and predictive maintenance.\n\nBaseline standards are using MQTT, Kafka, Python and TypeScript code to build the solution.\n\nBut we are facing some hard challenges with the available capacity of edge devices, some issues with the stability of the network, and the edge cloud integration. As we explore alternatives to our stack, we  are looking for insights from practitioners who have experienced challenges with MQTT and Kafka on IoT projects and learn from what they have already explored.\n\nPlease let me know if anyone has this type of experience and is willing to help.\n\nThank You.", "author_fullname": "t2_6pheknqy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring MQTT, Kafka, alternatives for edge sensors and IoT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166bedb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693489009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks,&lt;br/&gt;\nAre there people in this community who have properly evaluated Kafka alternatives like RedPanda or others, and/or MQTT alternatives who are willing to share their experience?&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;br/&gt;\nI am working on a project involving a handful of clients with edge devices and sensors in different verticals.&lt;/p&gt;\n\n&lt;p&gt;The broad use case is building data pipelines and analytics software for asset monitoring and management and predictive maintenance.&lt;/p&gt;\n\n&lt;p&gt;Baseline standards are using MQTT, Kafka, Python and TypeScript code to build the solution.&lt;/p&gt;\n\n&lt;p&gt;But we are facing some hard challenges with the available capacity of edge devices, some issues with the stability of the network, and the edge cloud integration. As we explore alternatives to our stack, we  are looking for insights from practitioners who have experienced challenges with MQTT and Kafka on IoT projects and learn from what they have already explored.&lt;/p&gt;\n\n&lt;p&gt;Please let me know if anyone has this type of experience and is willing to help.&lt;/p&gt;\n\n&lt;p&gt;Thank You.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Product Manager - Data Platform ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166bedb", "is_robot_indexable": true, "report_reasons": null, "author": "drc1728", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/166bedb/exploring_mqtt_kafka_alternatives_for_edge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166bedb/exploring_mqtt_kafka_alternatives_for_edge/", "subreddit_subscribers": 126104, "created_utc": 1693489009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have experiences to share about deploying Airflow as a container using AWS ECS, either using EC2 or Fargate?\n\nWe currently have a self-deployed instance on an EC2. I\u2019m wondering if this can give us a serverless option with less maintenance. I also wonder if this will be cheaper than MWAA, which will cost us about $1000 a month. Not to mention more flexibility.\n\nI\u2019m interested in using Airflow to kick off AWS services in the future, like Lambdas and Glue jobs. Currently we use Airflow to orchestrate Python pipelines and dbt jobs on the EC2.\n\nThank you in advance!", "author_fullname": "t2_b1gt6885", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying Airflow as a container on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1669iuk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693484103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have experiences to share about deploying Airflow as a container using AWS ECS, either using EC2 or Fargate?&lt;/p&gt;\n\n&lt;p&gt;We currently have a self-deployed instance on an EC2. I\u2019m wondering if this can give us a serverless option with less maintenance. I also wonder if this will be cheaper than MWAA, which will cost us about $1000 a month. Not to mention more flexibility.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m interested in using Airflow to kick off AWS services in the future, like Lambdas and Glue jobs. Currently we use Airflow to orchestrate Python pipelines and dbt jobs on the EC2.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1669iuk", "is_robot_indexable": true, "report_reasons": null, "author": "Strange_Upstairs9456", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1669iuk/deploying_airflow_as_a_container_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1669iuk/deploying_airflow_as_a_container_on_aws/", "subreddit_subscribers": 126104, "created_utc": 1693484103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "An often recommended practice is to keep a raw layer of extracted data before transforming/loading, usually in object storage, in order to more easily backfill, reconstruct the DWH, etc...\n\nFor those tables that are going to be dimensional, there are two common approaches, both with tradeoffs:\n\n- snapshots are very easy for querying and to rebuild history but size is going to increase pretty fast\n\n- SCD2 keeps the size smaller but makes it more tricky to rebuild history and querying\n\nWhich approach do you prefer, and why?\n\n[View Poll](https://www.reddit.com/poll/1668q51)", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Daily snapshots or SCD2 for the raw layer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1668q51", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693481841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;An often recommended practice is to keep a raw layer of extracted data before transforming/loading, usually in object storage, in order to more easily backfill, reconstruct the DWH, etc...&lt;/p&gt;\n\n&lt;p&gt;For those tables that are going to be dimensional, there are two common approaches, both with tradeoffs:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;snapshots are very easy for querying and to rebuild history but size is going to increase pretty fast&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;SCD2 keeps the size smaller but makes it more tricky to rebuild history and querying&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Which approach do you prefer, and why?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1668q51\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1668q51", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1693654641833, "options": [{"text": "Daily snapshots", "id": "24576504"}, {"text": "SCD2", "id": "24576505"}, {"text": "see results", "id": "24576506"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 44, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1668q51/daily_snapshots_or_scd2_for_the_raw_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1668q51/daily_snapshots_or_scd2_for_the_raw_layer/", "subreddit_subscribers": 126104, "created_utc": 1693481841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3g7ch6cj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Complex search, filtering and aggregations on DynamoDB - tech talk by AWS SA &amp; Rockset CTO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_166wh7n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tT3j0WbiyEk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tT3j0WbiyEk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset\"&gt;&lt;/iframe&gt;", "author_name": "Rockset", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/tT3j0WbiyEk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Rockset"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tT3j0WbiyEk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/166wh7n", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gMdeRakrRRLbAmsMYPixywB7jhXsfCwNS7pabrRBiOE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693540310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=tT3j0WbiyEk&amp;feature=youtu.be", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Xqy5BnDITq0PXIQR7G2g9alPeNpi_sGawWqscwjnaXA.jpg?auto=webp&amp;s=171147a81dccb6ef66ba97243782b74e44403160", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Xqy5BnDITq0PXIQR7G2g9alPeNpi_sGawWqscwjnaXA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb6d107ec2cc0f80c942e610775b27f0e75af139", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Xqy5BnDITq0PXIQR7G2g9alPeNpi_sGawWqscwjnaXA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80c57b082e11e86816e1343ec40d30dd90af095e", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Xqy5BnDITq0PXIQR7G2g9alPeNpi_sGawWqscwjnaXA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1681a572fe5a86b42eec8ed0e17d46e5862180c9", "width": 320, "height": 240}], "variants": {}, "id": "ua_5DddNv9ecnN9b6PuF45ubk6q3Y7b-8WWNQI4nPAY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "166wh7n", "is_robot_indexable": true, "report_reasons": null, "author": "ssb61", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166wh7n/complex_search_filtering_and_aggregations_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=tT3j0WbiyEk&amp;feature=youtu.be", "subreddit_subscribers": 126104, "created_utc": 1693540310.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tT3j0WbiyEk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Tech Talk: Real-time search, filtering and aggregation on DynamoDB using Rockset\"&gt;&lt;/iframe&gt;", "author_name": "Rockset", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/tT3j0WbiyEk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Rockset"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I have a substantial list of IDs that I need to work with. My objective is to retrieve data from various API endpoints for each individual ID, process the data, and then load it into a database. \n\nCurrently, I'm using Prefect with Kubernetes to structure the workflow. This involves organizing the tasks into a flow, with each task responsible for making requests to a distinct API endpoint. The flow follows this pattern:\n\n**Flow**\n\n* Task 1: Fetch data from API endpoint 1, then load the acquired data into table 1 in the Postgres database.\n* Task 2: Retrieve data from API endpoint 2, followed by loading it into table 2 in the Postgres database.\n* ...\n\nThese tasks can be executed concurrently. Given the extensive list of IDs, I divide them into batches of 100. Subsequently, I pass each batch as a parameter to initiate a new instance of the flow. At any given moment, a maximum of 5 batches can be actively running. Each task within the flow can access the list of IDs in a batch and process them in parallel.\n\nHowever, I am encountering several challenges:\n\n* In many cases, I only need to execute the entire workflow for individual IDs rather than batches. Unfortunately, each time the flow is triggered, Prefect creates a dedicated container for the job. This results in unnecessary resource usage. If I initiate 100 flows for 100 individual IDs, it could potentially overwhelm the Kubernetes cluster's resources.\n* The API enforces rate limits, including a concurrent limit and a limit per minute, for each ID. When using Prefect, what I can do is to employ retries, which is not an optimal solution. I am exploring options to implement a request queue mechanism that deal with these rate limits more effectively.\n\nBecause of these challenges, I'm considering alternative approaches. One of them is using Flask on Cloud Run and Cloud Functions to design a custom workflow and tasks, without relying on Prefect. I'd really appreciate any suggestions you might have for tackling this issue. ", "author_fullname": "t2_b23p74kt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing Data Retrieval and Processing Workflow for Multiple API Endpoints", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166pml8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693521945.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I have a substantial list of IDs that I need to work with. My objective is to retrieve data from various API endpoints for each individual ID, process the data, and then load it into a database. &lt;/p&gt;\n\n&lt;p&gt;Currently, I&amp;#39;m using Prefect with Kubernetes to structure the workflow. This involves organizing the tasks into a flow, with each task responsible for making requests to a distinct API endpoint. The flow follows this pattern:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Flow&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Task 1: Fetch data from API endpoint 1, then load the acquired data into table 1 in the Postgres database.&lt;/li&gt;\n&lt;li&gt;Task 2: Retrieve data from API endpoint 2, followed by loading it into table 2 in the Postgres database.&lt;/li&gt;\n&lt;li&gt;...&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;These tasks can be executed concurrently. Given the extensive list of IDs, I divide them into batches of 100. Subsequently, I pass each batch as a parameter to initiate a new instance of the flow. At any given moment, a maximum of 5 batches can be actively running. Each task within the flow can access the list of IDs in a batch and process them in parallel.&lt;/p&gt;\n\n&lt;p&gt;However, I am encountering several challenges:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In many cases, I only need to execute the entire workflow for individual IDs rather than batches. Unfortunately, each time the flow is triggered, Prefect creates a dedicated container for the job. This results in unnecessary resource usage. If I initiate 100 flows for 100 individual IDs, it could potentially overwhelm the Kubernetes cluster&amp;#39;s resources.&lt;/li&gt;\n&lt;li&gt;The API enforces rate limits, including a concurrent limit and a limit per minute, for each ID. When using Prefect, what I can do is to employ retries, which is not an optimal solution. I am exploring options to implement a request queue mechanism that deal with these rate limits more effectively.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Because of these challenges, I&amp;#39;m considering alternative approaches. One of them is using Flask on Cloud Run and Cloud Functions to design a custom workflow and tasks, without relying on Prefect. I&amp;#39;d really appreciate any suggestions you might have for tackling this issue. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166pml8", "is_robot_indexable": true, "report_reasons": null, "author": "kia_ora_st", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166pml8/optimizing_data_retrieval_and_processing_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166pml8/optimizing_data_retrieval_and_processing_workflow/", "subreddit_subscribers": 126104, "created_utc": 1693521945.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This type of question might not be allowed so I apologize if this is the case. I have a process that generates a list of known network interfaces (identified by MacAddress) in our networks every hour. It is a json array of objects with a hand-full of fields. So every hour we write a list of around 20k network interface objects to S3. For example\n\n    {  \n        \"MacAddress\": \"BF-6E-AB-B9-BA-96\",  \n        \"FirstSeen\": '2022-08-10T102200',  \n        \"LastSeen\": '2022-08-14T110100',  \n       ....  \n    }  \n\nThis data needs to make its way into a postgresql database where a django app uses it. Initially I simply deleted all entries in the table, and wrote the contents of the state file into the table. It should be noted that some MacAddresses from the past are no longer in the current dataset so they would be deleted. \n\nWe later decided we wanted to hold on to things from the past that are not in the current state. The obvious way to do that would be to simply stop deleting old entries an do an UPSERT. But I didn't understand the data well enough at the time to have the confidence that I didn't make a mistake in the ingestion process. Assuming I discovered a bug in the ingestion code I would be left with these options after fixing the bug. \n\n1. Re-running the ingestion from the start (would take around 20 days currently). \n1. Doing nothing about past data and simply accept that it is inaccurate\n1. Deleting the old data and starting fresh\n\nI didn't really want to be stuck with any of these options so I decided to try something else. I created a function that reads all of the files in S3 and merges them. It does this based on the more recent data. So you end up with a state file in json that contains all MacAddresses ever seen with the most recent data. This is cached as well, so if you have the state from an hour ago in S3 you only need to read that and the newest update to produce the latest one. \n\nWhat I like about this approach is that it incorporated the \"restore\" process into the daily data loading operation. So in this case if we discover a data error the fix is to simply re-run the entire state computation (about 2 hours) with the new code. Another important thing to consider is that we will need the state calculated for every day as well. That way we can ingest historical data as well. \n\nBut the problem is that this doesn't scale well. And we have had to change the ingestion process so we have had to rerun it many times. I would ideally like to come up with a solution that can handle 10x growth and 5 years. I think by that point with our current approach it would take days to recalculate the current state. \n\nCurrently I am investigating the database timescaledb to see if that can be beneficial to us. The compression works really well and it seems like we could load all of our S3 data there and do the work of the ingestion process in SQL. That way if something went wrong we could rebuild it in the database (quickly).\n\nBut now I am beginning to wonder about big-data platforms and how they could be used. If we didn't need to produce the daily state file this would be extremely parallelizable I think. Divide the S3 files into N number of slices and have N workers merge the files in their slice and then merge those N results. But I think it wouldn't be too bad to parallelize on MacAddress last digit. That is probably pretty evenly distributed. But you'd end up with 16x the number of downloads from S3 that would normally be required. Maybe there is a way to cache them?\n\nI am thinking of AWS Glue or AWS EMR. But the point is to come up with a solution that can be done hourly and hopefully without the need to recompute everything from scratch. I'd really just love to have a reliable solution that allows for changes in the merge logic to be possible with a much larger amount of data. Also any other suggestions for how to handle this would be welcome!", "author_fullname": "t2_m55mbvoc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to architect sequential state data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166hul7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693504140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This type of question might not be allowed so I apologize if this is the case. I have a process that generates a list of known network interfaces (identified by MacAddress) in our networks every hour. It is a json array of objects with a hand-full of fields. So every hour we write a list of around 20k network interface objects to S3. For example&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{  \n    &amp;quot;MacAddress&amp;quot;: &amp;quot;BF-6E-AB-B9-BA-96&amp;quot;,  \n    &amp;quot;FirstSeen&amp;quot;: &amp;#39;2022-08-10T102200&amp;#39;,  \n    &amp;quot;LastSeen&amp;quot;: &amp;#39;2022-08-14T110100&amp;#39;,  \n   ....  \n}  \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This data needs to make its way into a postgresql database where a django app uses it. Initially I simply deleted all entries in the table, and wrote the contents of the state file into the table. It should be noted that some MacAddresses from the past are no longer in the current dataset so they would be deleted. &lt;/p&gt;\n\n&lt;p&gt;We later decided we wanted to hold on to things from the past that are not in the current state. The obvious way to do that would be to simply stop deleting old entries an do an UPSERT. But I didn&amp;#39;t understand the data well enough at the time to have the confidence that I didn&amp;#39;t make a mistake in the ingestion process. Assuming I discovered a bug in the ingestion code I would be left with these options after fixing the bug. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Re-running the ingestion from the start (would take around 20 days currently). &lt;/li&gt;\n&lt;li&gt;Doing nothing about past data and simply accept that it is inaccurate&lt;/li&gt;\n&lt;li&gt;Deleting the old data and starting fresh&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I didn&amp;#39;t really want to be stuck with any of these options so I decided to try something else. I created a function that reads all of the files in S3 and merges them. It does this based on the more recent data. So you end up with a state file in json that contains all MacAddresses ever seen with the most recent data. This is cached as well, so if you have the state from an hour ago in S3 you only need to read that and the newest update to produce the latest one. &lt;/p&gt;\n\n&lt;p&gt;What I like about this approach is that it incorporated the &amp;quot;restore&amp;quot; process into the daily data loading operation. So in this case if we discover a data error the fix is to simply re-run the entire state computation (about 2 hours) with the new code. Another important thing to consider is that we will need the state calculated for every day as well. That way we can ingest historical data as well. &lt;/p&gt;\n\n&lt;p&gt;But the problem is that this doesn&amp;#39;t scale well. And we have had to change the ingestion process so we have had to rerun it many times. I would ideally like to come up with a solution that can handle 10x growth and 5 years. I think by that point with our current approach it would take days to recalculate the current state. &lt;/p&gt;\n\n&lt;p&gt;Currently I am investigating the database timescaledb to see if that can be beneficial to us. The compression works really well and it seems like we could load all of our S3 data there and do the work of the ingestion process in SQL. That way if something went wrong we could rebuild it in the database (quickly).&lt;/p&gt;\n\n&lt;p&gt;But now I am beginning to wonder about big-data platforms and how they could be used. If we didn&amp;#39;t need to produce the daily state file this would be extremely parallelizable I think. Divide the S3 files into N number of slices and have N workers merge the files in their slice and then merge those N results. But I think it wouldn&amp;#39;t be too bad to parallelize on MacAddress last digit. That is probably pretty evenly distributed. But you&amp;#39;d end up with 16x the number of downloads from S3 that would normally be required. Maybe there is a way to cache them?&lt;/p&gt;\n\n&lt;p&gt;I am thinking of AWS Glue or AWS EMR. But the point is to come up with a solution that can be done hourly and hopefully without the need to recompute everything from scratch. I&amp;#39;d really just love to have a reliable solution that allows for changes in the merge logic to be possible with a much larger amount of data. Also any other suggestions for how to handle this would be welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166hul7", "is_robot_indexable": true, "report_reasons": null, "author": "Fantastic_Search_504", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166hul7/best_way_to_architect_sequential_state_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166hul7/best_way_to_architect_sequential_state_data/", "subreddit_subscribers": 126104, "created_utc": 1693504140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Did someone have an experience with building \u201cDatabricks-like\u201d platform (with open-source technologies) and transitioning to it from Databricks? \n\nI understand that you can do something with integrating Apache Iceberg + YARN cluster + Apache Zeppelin/Jupyter + some cloud object storage + cloud compute + whatever is needed on top, but I am struggling on how to approach it and at least create MVP. \n\nAny advice/guides/materials/success stories are much appreciated!", "author_fullname": "t2_i0q1ptpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up \u201cDatabricks-like\u201d platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166a36n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693485660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Did someone have an experience with building \u201cDatabricks-like\u201d platform (with open-source technologies) and transitioning to it from Databricks? &lt;/p&gt;\n\n&lt;p&gt;I understand that you can do something with integrating Apache Iceberg + YARN cluster + Apache Zeppelin/Jupyter + some cloud object storage + cloud compute + whatever is needed on top, but I am struggling on how to approach it and at least create MVP. &lt;/p&gt;\n\n&lt;p&gt;Any advice/guides/materials/success stories are much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166a36n", "is_robot_indexable": true, "report_reasons": null, "author": "ye11owmonster", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166a36n/setting_up_databrickslike_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166a36n/setting_up_databrickslike_platform/", "subreddit_subscribers": 126104, "created_utc": 1693485660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI am trying to move data from one snowflake table to another using snowflake.\n\nMy first table has 10 columns out of which 2 have Json data.\n\nA, B, C, D, E, F, G, H, I, J where A and B are json columns.\n\nI want to map it to a target table which has 12 columns. \n\nA_key, A_value, B_key, B_value, C, D, E, F, G, H, I, J\n\nI tried using an intelligent structure model where I could parse A as A_key, A_value but I\u2019m unable to map other columns to my target.\n\nI also tried using parser but that didn\u2019t work out great.\n\nHow do I do this using informatica cloud?", "author_fullname": "t2_59ygsznq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[informatica] Parsing a column which has json data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166icik", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693505255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am trying to move data from one snowflake table to another using snowflake.&lt;/p&gt;\n\n&lt;p&gt;My first table has 10 columns out of which 2 have Json data.&lt;/p&gt;\n\n&lt;p&gt;A, B, C, D, E, F, G, H, I, J where A and B are json columns.&lt;/p&gt;\n\n&lt;p&gt;I want to map it to a target table which has 12 columns. &lt;/p&gt;\n\n&lt;p&gt;A_key, A_value, B_key, B_value, C, D, E, F, G, H, I, J&lt;/p&gt;\n\n&lt;p&gt;I tried using an intelligent structure model where I could parse A as A_key, A_value but I\u2019m unable to map other columns to my target.&lt;/p&gt;\n\n&lt;p&gt;I also tried using parser but that didn\u2019t work out great.&lt;/p&gt;\n\n&lt;p&gt;How do I do this using informatica cloud?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166icik", "is_robot_indexable": true, "report_reasons": null, "author": "lnx2n", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166icik/informatica_parsing_a_column_which_has_json_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166icik/informatica_parsing_a_column_which_has_json_data/", "subreddit_subscribers": 126104, "created_utc": 1693505255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy!  \n\n\nSo I know how to implement testing for my models and I can also set the individual tests configuration to set the severity to either warn or error. All good for now. My question is: if the dbt test command does in fact encounter errors of increased severity, the pipeline still passes. Ideally, I would like to not include this broken data further down stream. Additionally I would like to push the broken data to a separate view or something so that I can inspect it. How do I do that. Or better still, how do you actually use this command in production? What do you actually do with the errors? I am a bit lost.", "author_fullname": "t2_genrcwic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I handle errors in DBT tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166fa39", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693498074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy!  &lt;/p&gt;\n\n&lt;p&gt;So I know how to implement testing for my models and I can also set the individual tests configuration to set the severity to either warn or error. All good for now. My question is: if the dbt test command does in fact encounter errors of increased severity, the pipeline still passes. Ideally, I would like to not include this broken data further down stream. Additionally I would like to push the broken data to a separate view or something so that I can inspect it. How do I do that. Or better still, how do you actually use this command in production? What do you actually do with the errors? I am a bit lost.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166fa39", "is_robot_indexable": true, "report_reasons": null, "author": "nacho_biznis", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166fa39/how_do_i_handle_errors_in_dbt_tests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166fa39/how_do_i_handle_errors_in_dbt_tests/", "subreddit_subscribers": 126104, "created_utc": 1693498074.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Very new to this and looking for possible solutions.  Trying to connect to a dba via an ODBC connection to migrate the data out to a mysql server. ODBC is the only way for me to connect. It's a Windows 2012 server/32 bit.  What's the best IDE for this?", "author_fullname": "t2_7hcvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ODBC Connection on 32 bit - best IDE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166bf7j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693489067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Very new to this and looking for possible solutions.  Trying to connect to a dba via an ODBC connection to migrate the data out to a mysql server. ODBC is the only way for me to connect. It&amp;#39;s a Windows 2012 server/32 bit.  What&amp;#39;s the best IDE for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166bf7j", "is_robot_indexable": true, "report_reasons": null, "author": "scrupio", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166bf7j/odbc_connection_on_32_bit_best_ide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166bf7j/odbc_connection_on_32_bit_best_ide/", "subreddit_subscribers": 126104, "created_utc": 1693489067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For context, my background is in data analytics and science. But I\u2019m helping with ETL on an API at my company. And while I\u2019ve dealt with nested data before, I\u2019ve never spent much time with JSON. \n\nDo you all have any resources that would be helpful for doing data analysis and normalization of JSON in either R or Python? And please don\u2019t suggest anything that uses the state and counties example in the json_normalize() documentation, which seems to be used in examples all across the web for some reason. Videos of people working on multi-level nested JSON would be great, but books and articles would also be helpful.\n\nThanks!", "author_fullname": "t2_9ytsa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "JSON learning resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166b6v8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693488499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context, my background is in data analytics and science. But I\u2019m helping with ETL on an API at my company. And while I\u2019ve dealt with nested data before, I\u2019ve never spent much time with JSON. &lt;/p&gt;\n\n&lt;p&gt;Do you all have any resources that would be helpful for doing data analysis and normalization of JSON in either R or Python? And please don\u2019t suggest anything that uses the state and counties example in the json_normalize() documentation, which seems to be used in examples all across the web for some reason. Videos of people working on multi-level nested JSON would be great, but books and articles would also be helpful.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166b6v8", "is_robot_indexable": true, "report_reasons": null, "author": "ursamajorm82", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166b6v8/json_learning_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166b6v8/json_learning_resources/", "subreddit_subscribers": 126104, "created_utc": 1693488499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am also partly new to DE world, but this question has been busying my mind for a long time already. Does any one of you who has got experience in the field think the following super duper god level advanced topics are necessary to learn or have used them and said \u201cnah, not necessarily needed\u201d:\n\nPL/pgSQL;\nPL/Python,Java,whatever language it is;\nFunctions, procedures, routines;\nTriggers;\nRule system;\nRLS, CLS, access security things;\nExtensions;\nIPC;\nPartitioning, Sharding\nand etc.\n\nOr do you think only some of these are a must to learn and others just for show off?", "author_fullname": "t2_rle9nde7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Super advanced SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1666j4y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693474970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am also partly new to DE world, but this question has been busying my mind for a long time already. Does any one of you who has got experience in the field think the following super duper god level advanced topics are necessary to learn or have used them and said \u201cnah, not necessarily needed\u201d:&lt;/p&gt;\n\n&lt;p&gt;PL/pgSQL;\nPL/Python,Java,whatever language it is;\nFunctions, procedures, routines;\nTriggers;\nRule system;\nRLS, CLS, access security things;\nExtensions;\nIPC;\nPartitioning, Sharding\nand etc.&lt;/p&gt;\n\n&lt;p&gt;Or do you think only some of these are a must to learn and others just for show off?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1666j4y", "is_robot_indexable": true, "report_reasons": null, "author": "turalfirst", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1666j4y/super_advanced_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1666j4y/super_advanced_sql/", "subreddit_subscribers": 126104, "created_utc": 1693474970.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,   \n\n\ndlt is making a push towards adding more governance features, starting with **data contracts.**   \n\n\nWe will add the following modes as alternative to data contracts:  \n\u00a0**\\* evolve**: The current standard behavior, adapt the schema of the destination to the incoming data.  \n\u00a0**\\* freeze-and-trim**: Freeze the schema, and trim additional fields and subtables of incoming rows. This will still store all the incoming rows but will discard fields and subtable unknown to the schema.  \n\u00a0**\\* freeze-and-raise**: If any of the incoming rows do not fit the current schema, fail the current load. This will not store any data in the destination for this load and also not change the schema.  \n\u00a0**\\* freeze-and-discard:** Freeze the schema and completely discard any row that does not fit the current schema. Rows that fit the schema will be stored in the destination.  \n\n\nif you want to give feedback to the above and influence direction, please comment here or join out slack (link on page on top)\n\nIn the meantime, I wrote a short article about what row and column lineages are, why we use them, and how to get them at loading with dlt.  \n\n\nI hope this is useful!\n\n[https://dlthub.com/docs/blog/dlt-lineage-support](https://dlthub.com/docs/blog/dlt-lineage-support)", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trust your data - how to do simple row and column level lineage with dlt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166c3n5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693490730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,   &lt;/p&gt;\n\n&lt;p&gt;dlt is making a push towards adding more governance features, starting with &lt;strong&gt;data contracts.&lt;/strong&gt;   &lt;/p&gt;\n\n&lt;p&gt;We will add the following modes as alternative to data contracts:&lt;br/&gt;\n\u00a0&lt;strong&gt;* evolve&lt;/strong&gt;: The current standard behavior, adapt the schema of the destination to the incoming data.&lt;br/&gt;\n\u00a0&lt;strong&gt;* freeze-and-trim&lt;/strong&gt;: Freeze the schema, and trim additional fields and subtables of incoming rows. This will still store all the incoming rows but will discard fields and subtable unknown to the schema.&lt;br/&gt;\n\u00a0&lt;strong&gt;* freeze-and-raise&lt;/strong&gt;: If any of the incoming rows do not fit the current schema, fail the current load. This will not store any data in the destination for this load and also not change the schema.&lt;br/&gt;\n\u00a0&lt;strong&gt;* freeze-and-discard:&lt;/strong&gt; Freeze the schema and completely discard any row that does not fit the current schema. Rows that fit the schema will be stored in the destination.  &lt;/p&gt;\n\n&lt;p&gt;if you want to give feedback to the above and influence direction, please comment here or join out slack (link on page on top)&lt;/p&gt;\n\n&lt;p&gt;In the meantime, I wrote a short article about what row and column lineages are, why we use them, and how to get them at loading with dlt.  &lt;/p&gt;\n\n&lt;p&gt;I hope this is useful!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dlthub.com/docs/blog/dlt-lineage-support\"&gt;https://dlthub.com/docs/blog/dlt-lineage-support&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?auto=webp&amp;s=7990e0b7bda28e0b896446b9feb29a76037878b8", "width": 1200, "height": 996}, "resolutions": [{"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bfefd634af4380c1030e9656e1354749bb4a05bb", "width": 108, "height": 89}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ab83290a6cf23b2ac6974f064275ad182be6cf3", "width": 216, "height": 179}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e87ff109cc91fe7f974d4fb6959e2336f5b10ff2", "width": 320, "height": 265}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7077fbf6f66bee841cd5bb27c0bc8ab6312c4952", "width": 640, "height": 531}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bb59b243274e62869fcca0f37970db9227063740", "width": 960, "height": 796}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fe2a351dc3c7fcfa194482d6e2da298e9c9edce5", "width": 1080, "height": 896}], "variants": {}, "id": "dHy9-9qK5psAYmJuOmTrdz4FUBDbwmI7B4kzzPMoWZk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "166c3n5", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166c3n5/trust_your_data_how_to_do_simple_row_and_column/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166c3n5/trust_your_data_how_to_do_simple_row_and_column/", "subreddit_subscribers": 126104, "created_utc": 1693490730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "People often ask how Spark compares to Dask. This is a hard question to answer  well, since It largely depends on the type of work you're doing. There are a lot of different areas to consider on a case-by-case basis. We try to answer that in an unbiased way in [this blog post](https://medium.com/coiled-hq/spark-vs-dask-27216502b129), but that's hard. Interested to hear what you think.", "author_fullname": "t2_w7crvjmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dask vs Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1665pmu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693472109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People often ask how Spark compares to Dask. This is a hard question to answer  well, since It largely depends on the type of work you&amp;#39;re doing. There are a lot of different areas to consider on a case-by-case basis. We try to answer that in an unbiased way in &lt;a href=\"https://medium.com/coiled-hq/spark-vs-dask-27216502b129\"&gt;this blog post&lt;/a&gt;, but that&amp;#39;s hard. Interested to hear what you think.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?auto=webp&amp;s=8a810acad4e438359457d167b99eeaeee718da35", "width": 1200, "height": 793}, "resolutions": [{"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e982cfadd74561b216a50f08dbdefbb8c732ef84", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4132eef276fd816270df32deebaf7510f9fd36d5", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e099dc72b8e1ad47f7c1c9c0ab1f1f09e83f19d0", "width": 320, "height": 211}, {"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=786db3166fd514c0cdc4dba339eed70be79d0964", "width": 640, "height": 422}, {"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eeb50d0f50e387ad9597aa9155b3922ac4e0e283", "width": 960, "height": 634}, {"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=821986973881548746fa4181c749ccadb5c51a79", "width": 1080, "height": 713}], "variants": {}, "id": "mDCHWSdp9SxRKd1B5w-sZOFIlEffBJpYOtChAipLuGE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1665pmu", "is_robot_indexable": true, "report_reasons": null, "author": "dask-jeeves", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1665pmu/dask_vs_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1665pmu/dask_vs_spark/", "subreddit_subscribers": 126104, "created_utc": 1693472109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cyber security  v/s Data engineer which one would you choose as a career if you are starting out now.", "author_fullname": "t2_9vkdidvf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cyber security v/s Data engineer which one would you choose as a career if you are starting out now in 2023 and why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1663txm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693465591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cyber security  v/s Data engineer which one would you choose as a career if you are starting out now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1663txm", "is_robot_indexable": true, "report_reasons": null, "author": "Friendly-Change-1078", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1663txm/cyber_security_vs_data_engineer_which_one_would/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1663txm/cyber_security_vs_data_engineer_which_one_would/", "subreddit_subscribers": 126104, "created_utc": 1693465591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w07bcus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Kafka Is the New Data Lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 102, "top_awarded_type": null, "hide_score": false, "name": "t3_166b2ic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.23, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ha48ThKEN_w2J9wipekBKlh67100X6d9wXPSwqITa5o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693488193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "risingwave.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.risingwave.com/blog/why-kafka-is-the-new-data-lake/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/i8-zzJXS3o8C0HIq4qProRITPINi_0e1sCZv62XhI3g.jpg?auto=webp&amp;s=df5f70d626df82f0d3dab4c8c805f0afd06d703f", "width": 988, "height": 722}, "resolutions": [{"url": "https://external-preview.redd.it/i8-zzJXS3o8C0HIq4qProRITPINi_0e1sCZv62XhI3g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e9c98375bd457970e031718e0117a61e38280157", "width": 108, "height": 78}, {"url": "https://external-preview.redd.it/i8-zzJXS3o8C0HIq4qProRITPINi_0e1sCZv62XhI3g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e3efcbbee3475dc620b1991c433a6b73af7d74b0", "width": 216, "height": 157}, {"url": "https://external-preview.redd.it/i8-zzJXS3o8C0HIq4qProRITPINi_0e1sCZv62XhI3g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7db0152b96461ec75f3b2815ac8a114398910456", "width": 320, "height": 233}, {"url": "https://external-preview.redd.it/i8-zzJXS3o8C0HIq4qProRITPINi_0e1sCZv62XhI3g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bffc1aff94df3538722c18e1f76d968ce5a236f4", "width": 640, "height": 467}, {"url": "https://external-preview.redd.it/i8-zzJXS3o8C0HIq4qProRITPINi_0e1sCZv62XhI3g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f4a4430538d01c6a842658073df63932657407a9", "width": 960, "height": 701}], "variants": {}, "id": "5Bxhz29v1dCNko_FyUZjSoPYgXVXjo1EsXbPPHeH_LQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "166b2ic", "is_robot_indexable": true, "report_reasons": null, "author": "yingjunwu", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166b2ic/why_kafka_is_the_new_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.risingwave.com/blog/why-kafka-is-the-new-data-lake/", "subreddit_subscribers": 126104, "created_utc": 1693488193.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}