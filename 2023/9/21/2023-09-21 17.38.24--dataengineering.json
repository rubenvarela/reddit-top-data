{"kind": "Listing", "data": {"after": "t3_16nxfk0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": ".", "author_fullname": "t2_bsuu4apm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Pareto Principle - what is the 20% (algos, functions, libraries) that lets you develop 80% of code related to Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o883v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695276875.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16o883v", "is_robot_indexable": true, "report_reasons": null, "author": "CrimsonMentone30", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o883v/python_pareto_principle_what_is_the_20_algos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o883v/python_pareto_principle_what_is_the_20_algos/", "subreddit_subscribers": 129635, "created_utc": 1695276875.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Preparing for an engineering interview can be overwhelming, LeetCode, System Design, Behavioral questions, its a lot to manage. If you are preparing for a **#dataengieering** interview, give my free app a try. It gives you 2 tasks a day for 30 days to help you prepare.\n\nAlso I wanted to learn Swift.\n\nhttps://apps.apple.com/app/interview-ace/id6465748534", "author_fullname": "t2_4fpl974m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made a free app to help you prepare for the data engineering interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o5d6o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695267311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Preparing for an engineering interview can be overwhelming, LeetCode, System Design, Behavioral questions, its a lot to manage. If you are preparing for a &lt;strong&gt;#dataengieering&lt;/strong&gt; interview, give my free app a try. It gives you 2 tasks a day for 30 days to help you prepare.&lt;/p&gt;\n\n&lt;p&gt;Also I wanted to learn Swift.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://apps.apple.com/app/interview-ace/id6465748534\"&gt;https://apps.apple.com/app/interview-ace/id6465748534&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?auto=webp&amp;s=86f11be447465164e06c1895f371f8247b3de78e", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9de58037059431fcc82b29fec49b0b89fd33aaed", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9ca9694b7fac863b314527c7f015ec557450634", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=974fe41036a7cd50cac5039677476f1175904ee7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0a246610bdf304e619347a7ace86b9bacea4e0d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=96029c532b8c99d9d8ae6e66965bfd26dc47cb50", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=48172f9d876603790b702ec84e57e0e6b4cff7da", "width": 1080, "height": 567}], "variants": {}, "id": "5YmFqnaQR8uybNXWtDGGlBlmqp-5bnkiVElQWM1Gehw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16o5d6o", "is_robot_indexable": true, "report_reasons": null, "author": "coyne_operated", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o5d6o/i_made_a_free_app_to_help_you_prepare_for_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o5d6o/i_made_a_free_app_to_help_you_prepare_for_the/", "subreddit_subscribers": 129635, "created_utc": 1695267311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to take about 6 months to really dig into company and code before expressing any strong architectural opinions, but I do have a bit of experience in this niche. The rest of the team is 1-2 years more senior but less experienced in DE (they\u2019re SWEs).\n\nA re-architecture is underway and I see major issues with it already. \n\n- Implementing streaming from scratch, like as an alternative to Kafka, pubsub, or what have you \n- Implementing data transformations in python vs something like dbt. This one is debatable but as someone who\u2019s done both my judgement is that this should be a case of ELT and should be done in the DWH.\n- STRONG coupling of extraction and transformation. On this path, a relatively mild change in data requirements might require a full re-extract of all data just to run the transformation step. And we\u2019d be subject to 3rd party API limits. \n- STRONG coupling of app needs and analytics team needs. Right now, analytics teams are hitting the prod DB freely. And plans are to build solutions that support both, vs separate solutions that support each. \n- No concern or plan for data quality in the re-architecture\n\nThe way I see it i could\n\n- Be the squeaky wheel on all fronts. But it feels like an uphill battle. The planning and implementation has been done by a very early employee that isn\u2019t quite a co-founder but does own more than your average employee and the team lead. The plan (as a focus, not in detail) has already been mentioned to the board as a priority.\n- Go along with the plan, cuz who cares, it\u2019s not my VC money, right? But I feel I would be taking a huge step back in terms of the DE infra I\u2019ll be working with.\n- Try to walk a middle ground where I pick and choose things to push for. Haven\u2019t figured out which hills to die on yet though.\n\nThoughts or prayers appreciated", "author_fullname": "t2_2tu8n7l9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coming into an org just as a flawed re-architecture is underway. Any tips on pushing for changes early in your time at a new place?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nvnp3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695241908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to take about 6 months to really dig into company and code before expressing any strong architectural opinions, but I do have a bit of experience in this niche. The rest of the team is 1-2 years more senior but less experienced in DE (they\u2019re SWEs).&lt;/p&gt;\n\n&lt;p&gt;A re-architecture is underway and I see major issues with it already. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Implementing streaming from scratch, like as an alternative to Kafka, pubsub, or what have you &lt;/li&gt;\n&lt;li&gt;Implementing data transformations in python vs something like dbt. This one is debatable but as someone who\u2019s done both my judgement is that this should be a case of ELT and should be done in the DWH.&lt;/li&gt;\n&lt;li&gt;STRONG coupling of extraction and transformation. On this path, a relatively mild change in data requirements might require a full re-extract of all data just to run the transformation step. And we\u2019d be subject to 3rd party API limits. &lt;/li&gt;\n&lt;li&gt;STRONG coupling of app needs and analytics team needs. Right now, analytics teams are hitting the prod DB freely. And plans are to build solutions that support both, vs separate solutions that support each. &lt;/li&gt;\n&lt;li&gt;No concern or plan for data quality in the re-architecture&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The way I see it i could&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Be the squeaky wheel on all fronts. But it feels like an uphill battle. The planning and implementation has been done by a very early employee that isn\u2019t quite a co-founder but does own more than your average employee and the team lead. The plan (as a focus, not in detail) has already been mentioned to the board as a priority.&lt;/li&gt;\n&lt;li&gt;Go along with the plan, cuz who cares, it\u2019s not my VC money, right? But I feel I would be taking a huge step back in terms of the DE infra I\u2019ll be working with.&lt;/li&gt;\n&lt;li&gt;Try to walk a middle ground where I pick and choose things to push for. Haven\u2019t figured out which hills to die on yet though.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thoughts or prayers appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16nvnp3", "is_robot_indexable": true, "report_reasons": null, "author": "Firm_Bit", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nvnp3/coming_into_an_org_just_as_a_flawed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nvnp3/coming_into_an_org_just_as_a_flawed/", "subreddit_subscribers": 129635, "created_utc": 1695241908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is looking into switching from SSIS-based ETL process to a Python and T-SQL to create data pipelines. We work in a classified environment, which restricts access to non-US based software (no Pycharm) and causes other problems with account privileges. Privileges we can work on, slowly but surely. The scheduling process is where we aren't certain what to do. I mentioned Airflow, but it would require a hefty vetting process to get into each server, even the unclassified environment. These classified environments do not have access to the internet either, for obvious reasons.\n\nDoes anyone have experience doing DE work in a SCIF?\n\nIf so, how did you go about the scheduling process for Python/T-SQL?\n\nI'm welcome to other possibilities and ideas.\n\nEDIT 1: I'm very grateful for the many responses. I'll just summarize some of my responses here. Each environment comes with Anaconda, thus it comes with any modules that are pre-packaged with Anaconda. Additional modules can be tested in our open environment then vetted before moving them to our closed environments. Windows Task Scheduler seems to be a simple solution that \\*should\\* be readily available in each of our networks. Airflow or Databricks would be nice but would likely require a rigorous vetting process. ", "author_fullname": "t2_aiumhm29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Scheduler in a Closed Environment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nzm2s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695315323.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695251381.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is looking into switching from SSIS-based ETL process to a Python and T-SQL to create data pipelines. We work in a classified environment, which restricts access to non-US based software (no Pycharm) and causes other problems with account privileges. Privileges we can work on, slowly but surely. The scheduling process is where we aren&amp;#39;t certain what to do. I mentioned Airflow, but it would require a hefty vetting process to get into each server, even the unclassified environment. These classified environments do not have access to the internet either, for obvious reasons.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience doing DE work in a SCIF?&lt;/p&gt;\n\n&lt;p&gt;If so, how did you go about the scheduling process for Python/T-SQL?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m welcome to other possibilities and ideas.&lt;/p&gt;\n\n&lt;p&gt;EDIT 1: I&amp;#39;m very grateful for the many responses. I&amp;#39;ll just summarize some of my responses here. Each environment comes with Anaconda, thus it comes with any modules that are pre-packaged with Anaconda. Additional modules can be tested in our open environment then vetted before moving them to our closed environments. Windows Task Scheduler seems to be a simple solution that *should* be readily available in each of our networks. Airflow or Databricks would be nice but would likely require a rigorous vetting process. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nzm2s", "is_robot_indexable": true, "report_reasons": null, "author": "Upbeat_Count_7568", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nzm2s/python_scheduler_in_a_closed_environment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nzm2s/python_scheduler_in_a_closed_environment/", "subreddit_subscribers": 129635, "created_utc": 1695251381.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long story short, Im a junior data engineering tasked with moving a (for me) tremendous amount of files. This is the generall architecture:\n\n1. Historic files + daily files in blob container A.\n2. Needs to be moved to blob container B, which is a reload container. Moving files to this container will trigger a reload function, writing output of the files to a staging db.\n3. A dataflow reads from the staging table, and finally writes the output to a dwh-table.\n\nNow, the problem the dataflow cannot handle the ETL load of taking the entire load from the staging table (Will take many days even with maxed out clusters).\n\nI initally thought of having a Select Top(x) as a query in my dataflow, but this means I will also need to make changes to the dataflow after it has done its job in production.\n\nI have used a Az Copy command to move the files from container A to container B, and thought about the possibility of using this in a pipeline. But it looks somewhat complicated.  \n\n\nAny thoughts on the best way to handle this case? best practises or just generall tips/input would be very appreciated!\n\nEdit: \n\n\\- The files in question are XML-files.  \n\\- Our dataflows and pipelines are located in the Azure Synapse environment.  \n\\- The Storage of which container A and container B uses is Azure Storage Explorer.  \n\\- The staging db is  sql database, and the dwh-table is a Dedicated SQL Pool.\n\n&amp;#x200B;", "author_fullname": "t2_ddrknaxf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scipting a ETL job for 200.000 files (200gb)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nr1hc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695231771.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695230599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short, Im a junior data engineering tasked with moving a (for me) tremendous amount of files. This is the generall architecture:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Historic files + daily files in blob container A.&lt;/li&gt;\n&lt;li&gt;Needs to be moved to blob container B, which is a reload container. Moving files to this container will trigger a reload function, writing output of the files to a staging db.&lt;/li&gt;\n&lt;li&gt;A dataflow reads from the staging table, and finally writes the output to a dwh-table.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now, the problem the dataflow cannot handle the ETL load of taking the entire load from the staging table (Will take many days even with maxed out clusters).&lt;/p&gt;\n\n&lt;p&gt;I initally thought of having a Select Top(x) as a query in my dataflow, but this means I will also need to make changes to the dataflow after it has done its job in production.&lt;/p&gt;\n\n&lt;p&gt;I have used a Az Copy command to move the files from container A to container B, and thought about the possibility of using this in a pipeline. But it looks somewhat complicated.  &lt;/p&gt;\n\n&lt;p&gt;Any thoughts on the best way to handle this case? best practises or just generall tips/input would be very appreciated!&lt;/p&gt;\n\n&lt;p&gt;Edit: &lt;/p&gt;\n\n&lt;p&gt;- The files in question are XML-files.&lt;br/&gt;\n- Our dataflows and pipelines are located in the Azure Synapse environment.&lt;br/&gt;\n- The Storage of which container A and container B uses is Azure Storage Explorer.&lt;br/&gt;\n- The staging db is  sql database, and the dwh-table is a Dedicated SQL Pool.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nr1hc", "is_robot_indexable": true, "report_reasons": null, "author": "Alarmed-Royal-2161", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nr1hc/scipting_a_etl_job_for_200000_files_200gb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nr1hc/scipting_a_etl_job_for_200000_files_200gb/", "subreddit_subscribers": 129635, "created_utc": 1695230599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nCurrently building a silver level streaming table from a bronze level streaming source, but not getting the append behavior I'm expecting. When running simple aggregations (min, max, count, etc..), my target silver table keeps being overwritten. For context this DLT pipeline is scheduled to run every 20 minutes, and the expected behavior is to have each 20 minutes worth of data be aggregated and then appended to the target table. Why is my target table being overwritten and not appended to?\n\nThe below query just overwrites \\`example\\_silver\\_table\\` every time the pipeline is run:\n\n    CREATE OR REFRESH STREAMING LIVE TABLE example_silver_table\n    AS \n        SELECT\n            dimension_1,\n            dimension_2,\n            MAX(dimension_3) AS max_dimension_3,\n            SUM(dimension_4) AS dimension_4_total\n        FROM STREAM(LIVE.fact_bronze_table_event)\n        GROUP BY\n            dimension_1,\n            dimension_2\n\nI also tried including a watermark, but not sure I'm implementing it correctly. Every 20 minutes when the pipeline runs, the below query output is 0 records written to \\`example\\_silver\\_table\\` :\n\n    CREATE OR REFRESH STREAMING LIVE TABLE example_silver_table\n    AS \n        SELECT\n            dimension_1,\n            dimension_2,\n            MAX(dimension_3) AS max_dimension_3,\n            SUM(dimension_4) AS dimension_4_total\n        FROM STREAM(LIVE.fact_bronze_table_event) WATERMARK timestamp_field AS event_time DELAY OF INTERVAL 20 MINUTE\n        GROUP BY\n            dimension_1,\n            dimension_2,\n            WINDOW('event_time', '20 minutes')\n\nTLDR;\n\nWhen trying to do aggregations on streaming (20 minute batches) data, I can't get the target table to stop being overwritten every pipeline run. Tried using watermarks, but no luck. Any help or guidance would be much appreciated!!\n\nResources I've tried using, but no luck:\n\n* [Feature Deep Dive: Watermarking in Apache Spark Structured Streaming](https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html)\n* [Apply watermarks to control data processing thresholds](https://docs.databricks.com/en/structured-streaming/watermarks.html)\n* [Event-time Aggregation and Watermarking in Apache Spark\u2019s Structured Streaming](https://www.databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html)", "author_fullname": "t2_3i1tg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Aggregate Streaming Data With Databricks Delta Live Tables (DLT)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o3pat", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695262481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently building a silver level streaming table from a bronze level streaming source, but not getting the append behavior I&amp;#39;m expecting. When running simple aggregations (min, max, count, etc..), my target silver table keeps being overwritten. For context this DLT pipeline is scheduled to run every 20 minutes, and the expected behavior is to have each 20 minutes worth of data be aggregated and then appended to the target table. Why is my target table being overwritten and not appended to?&lt;/p&gt;\n\n&lt;p&gt;The below query just overwrites `example_silver_table` every time the pipeline is run:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE OR REFRESH STREAMING LIVE TABLE example_silver_table\nAS \n    SELECT\n        dimension_1,\n        dimension_2,\n        MAX(dimension_3) AS max_dimension_3,\n        SUM(dimension_4) AS dimension_4_total\n    FROM STREAM(LIVE.fact_bronze_table_event)\n    GROUP BY\n        dimension_1,\n        dimension_2\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I also tried including a watermark, but not sure I&amp;#39;m implementing it correctly. Every 20 minutes when the pipeline runs, the below query output is 0 records written to `example_silver_table` :&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE OR REFRESH STREAMING LIVE TABLE example_silver_table\nAS \n    SELECT\n        dimension_1,\n        dimension_2,\n        MAX(dimension_3) AS max_dimension_3,\n        SUM(dimension_4) AS dimension_4_total\n    FROM STREAM(LIVE.fact_bronze_table_event) WATERMARK timestamp_field AS event_time DELAY OF INTERVAL 20 MINUTE\n    GROUP BY\n        dimension_1,\n        dimension_2,\n        WINDOW(&amp;#39;event_time&amp;#39;, &amp;#39;20 minutes&amp;#39;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;TLDR;&lt;/p&gt;\n\n&lt;p&gt;When trying to do aggregations on streaming (20 minute batches) data, I can&amp;#39;t get the target table to stop being overwritten every pipeline run. Tried using watermarks, but no luck. Any help or guidance would be much appreciated!!&lt;/p&gt;\n\n&lt;p&gt;Resources I&amp;#39;ve tried using, but no luck:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html\"&gt;Feature Deep Dive: Watermarking in Apache Spark Structured Streaming&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://docs.databricks.com/en/structured-streaming/watermarks.html\"&gt;Apply watermarks to control data processing thresholds&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\"&gt;Event-time Aggregation and Watermarking in Apache Spark\u2019s Structured Streaming&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?auto=webp&amp;s=baea0bdbfae95e881ea0a2f0c3b33954b1f12ee9", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5149cb676e163de6e1c8f6c468e674e2e1eedccf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ffe2f685b75ffa31d46101223d2e85d396c11a2c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1e57ce0d9b65813a43715d49944408d42572108", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6218b26dffaec8bb109ecc5069aa1cdf4fe9af22", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=876e8e3df516ab9110f4d59af591fd3b01d003e4", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c649702fc9a45301838bb4e09431f6fd1f4a9ec6", "width": 1080, "height": 565}], "variants": {}, "id": "0LSNAjx4YTAOOrcgDeHl6XfnklsWXU_JL9sKej5JZLo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16o3pat", "is_robot_indexable": true, "report_reasons": null, "author": "Shatonmedeek", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o3pat/how_to_aggregate_streaming_data_with_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o3pat/how_to_aggregate_streaming_data_with_databricks/", "subreddit_subscribers": 129635, "created_utc": 1695262481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, someone recently asked to review their CV and after replying I got a lot of you asking for a review. I did all of you, and now sharing what I think are some useful tips to improve your CV.  Use these if you are applying but nobody is getting back to you.\n\n\nOf course use your own judgment and adapt to your local market and requirements, but here are the top level tips: [Article link](https://dlthub.com/docs/blog/data-engineering-cv) \n\nIf you think there are more useful tips, go ahead and do a PR so others where I distribute this article can also benefit. [Github link](https://github.com/dlt-hub/dlt/blob/devel/docs/website/blog/2023-09-20-data-engineering-cv.md)", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for creating an effective data engineering CV", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o9yib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695283270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, someone recently asked to review their CV and after replying I got a lot of you asking for a review. I did all of you, and now sharing what I think are some useful tips to improve your CV.  Use these if you are applying but nobody is getting back to you.&lt;/p&gt;\n\n&lt;p&gt;Of course use your own judgment and adapt to your local market and requirements, but here are the top level tips: &lt;a href=\"https://dlthub.com/docs/blog/data-engineering-cv\"&gt;Article link&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;If you think there are more useful tips, go ahead and do a PR so others where I distribute this article can also benefit. &lt;a href=\"https://github.com/dlt-hub/dlt/blob/devel/docs/website/blog/2023-09-20-data-engineering-cv.md\"&gt;Github link&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?auto=webp&amp;s=5138f6a644eb11025e1e169629ec855c86ee615c", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=466b5c3fbf6cfffd5c4a2b6d512422f503422991", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe7bb9fc82541622b4e97e8645b8e6da967f65a8", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=97a4d6c5707d25736322abf5b75ad8d2d74851c5", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4081d26b1866e1569fb8d29f0618e191d061d9a8", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1717b579d7702da6f4216eadedad440f2ac2666", "width": 960, "height": 960}], "variants": {}, "id": "n7I77mKH6Q0sG5S_9xXPTJWYmzVs165z-AMNcpkvBPk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16o9yib", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o9yib/tips_for_creating_an_effective_data_engineering_cv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o9yib/tips_for_creating_an_effective_data_engineering_cv/", "subreddit_subscribers": 129635, "created_utc": 1695283270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_74fdrilk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PyJaws v0.1.7: A Pythonic way of Declaring Databricks Jobs and Workflows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocnvb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1695293333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "pypi.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://pypi.org/project/pyjaws", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16ocnvb", "is_robot_indexable": true, "report_reasons": null, "author": "j0selit0342", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocnvb/pyjaws_v017_a_pythonic_way_of_declaring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://pypi.org/project/pyjaws", "subreddit_subscribers": 129635, "created_utc": 1695293333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am storing over 200gb of data on mysql, I am considering  to migrate it into a more effective dbms (or just keep using mysql?):\n- Data durability is optional (this table is defered from another source of truth so I can regeneration it).\n- Not in-memory db (I can not afford to have 200GB memory)\n- Read instensive, infrequently batched update.\n- the query is very simple (select * from table where key1 &gt; key2 and key3 &gt; const ) . don't need to join multiple table or doing aggregation \n- Support range query, index\n- Effective data storage (I don't want it to turn into a 1TB data)\n- Easy to deploy (only need to run on single node via docker)", "author_fullname": "t2_bju0j9jyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choose me a database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocikg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695292819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am storing over 200gb of data on mysql, I am considering  to migrate it into a more effective dbms (or just keep using mysql?):\n- Data durability is optional (this table is defered from another source of truth so I can regeneration it).\n- Not in-memory db (I can not afford to have 200GB memory)\n- Read instensive, infrequently batched update.\n- the query is very simple (select * from table where key1 &amp;gt; key2 and key3 &amp;gt; const ) . don&amp;#39;t need to join multiple table or doing aggregation \n- Support range query, index\n- Effective data storage (I don&amp;#39;t want it to turn into a 1TB data)\n- Easy to deploy (only need to run on single node via docker)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ocikg", "is_robot_indexable": true, "report_reasons": null, "author": "chu_nghia_nam_thang", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocikg/choose_me_a_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ocikg/choose_me_a_database/", "subreddit_subscribers": 129635, "created_utc": 1695292819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I will be having a technical interview for the Data Modeler position. They need someone who is familiar with Data Vault 2.0. Can anyone give me tips on what keywords I should mention? Also if you have any resources to prepare that would be very helpful. Thanks to all redditors.", "author_fullname": "t2_44nfhvhnm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Technical Interview Data Vault 2.0", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o5fkb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695267515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I will be having a technical interview for the Data Modeler position. They need someone who is familiar with Data Vault 2.0. Can anyone give me tips on what keywords I should mention? Also if you have any resources to prepare that would be very helpful. Thanks to all redditors.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16o5fkb", "is_robot_indexable": true, "report_reasons": null, "author": "Personal_Tennis_466", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o5fkb/technical_interview_data_vault_20/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o5fkb/technical_interview_data_vault_20/", "subreddit_subscribers": 129635, "created_utc": 1695267515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm fetching real-time flight data every second and need some way to monitor the script and restart it if it crashes. Script is in python and ideally deploying on a linux server.\n\nIs some sort of process manager what I need, or are there tools better designed for this task?", "author_fullname": "t2_4memz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Robust methods for fetching data from an API every second?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nqx1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695230302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m fetching real-time flight data every second and need some way to monitor the script and restart it if it crashes. Script is in python and ideally deploying on a linux server.&lt;/p&gt;\n\n&lt;p&gt;Is some sort of process manager what I need, or are there tools better designed for this task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nqx1j", "is_robot_indexable": true, "report_reasons": null, "author": "obviouslyCPTobvious", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nqx1j/robust_methods_for_fetching_data_from_an_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nqx1j/robust_methods_for_fetching_data_from_an_api/", "subreddit_subscribers": 129635, "created_utc": 1695230302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mxj2oz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Key Postgres parameters to fine tune", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_16nqw9b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Asd9oQa5sGOUzoOsksnAz_i6SH7by1t_MvrJk27hsog.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695230250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "timescale.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.timescale.com/blog/postgresql-performance-tuning-part-ii-adjusting-key-parameters/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?auto=webp&amp;s=eb7b0c15396d5d0ac7f612355f2f26dcf11fb695", "width": 998, "height": 559}, "resolutions": [{"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c9376299fc42e9760c1dfd29d948abd7995e6d4", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=136b53b9a92ce9bccd00b4cfdacc9ef36bdde4e9", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7fb32af348258c5bd4d7d064e2959cf8beae0198", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=17ba5f1974550257862fc5466fc4f75e25546e7d", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9169b126bda126bcc11777f6d5144efd83ec6a30", "width": 960, "height": 537}], "variants": {}, "id": "9tCt4JMnRg2fVsa7yIYuykSWnRsNyDxO6GjnFhFbT4A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16nqw9b", "is_robot_indexable": true, "report_reasons": null, "author": "carlotasoto", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nqw9b/key_postgres_parameters_to_fine_tune/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.timescale.com/blog/postgresql-performance-tuning-part-ii-adjusting-key-parameters/", "subreddit_subscribers": 129635, "created_utc": 1695230250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just found out my boss used ChatGPT to write our department goals for data engineering. Mainly because she doesn\u2019t actually understand technology at all.\n\nFor 2024 our ChatGPT overlord demands we:\n\n* Implement an Enterprise data lake\n* Move all existing applications to AWS\n* Implement a master data management solution\n* Implement a self-service model with easy-to-use data catalogs\n* Implement an AI solution for self-service (is ChatGPT trying to reproduce?!!!)\n\nTeam size? 4 engineers.\n\nAnyone else dealing with this kind of craziness?", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ChatGPT for goals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16okwxv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695315146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just found out my boss used ChatGPT to write our department goals for data engineering. Mainly because she doesn\u2019t actually understand technology at all.&lt;/p&gt;\n\n&lt;p&gt;For 2024 our ChatGPT overlord demands we:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Implement an Enterprise data lake&lt;/li&gt;\n&lt;li&gt;Move all existing applications to AWS&lt;/li&gt;\n&lt;li&gt;Implement a master data management solution&lt;/li&gt;\n&lt;li&gt;Implement a self-service model with easy-to-use data catalogs&lt;/li&gt;\n&lt;li&gt;Implement an AI solution for self-service (is ChatGPT trying to reproduce?!!!)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Team size? 4 engineers.&lt;/p&gt;\n\n&lt;p&gt;Anyone else dealing with this kind of craziness?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16okwxv", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16okwxv/chatgpt_for_goals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16okwxv/chatgpt_for_goals/", "subreddit_subscribers": 129635, "created_utc": 1695315146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Question:** What are some low-code time series manipulation tools available for excel users who can't be trusted to author production ready transformations using SQL or Python?  \n\n\nI'm a software dev building tools for a financial data analyst team. The analyst team has asked me to build a tool for self-serve authoring of time series transformations so analysts can author hundreds of these transformations that power dashboards we deliver to customers.\n\nI plan on using open-source only modern data stack tools, dbt in particular, as I don't see a point in reinventing the wheel. The main obstacle however is my team doesn't want to use SQL or Python as our analysts can't be trusted to code. They complain that SQL is too verbose and ugly for time series, and python is powerful enough that analyst-authored code has introduced production bugs previously. Our analysts are more like excel analysts. Thus i'm looking for low-code tools that would make panel data manipulations (time series &amp; cross-sectional gap fills, bucketing, panel data) fairly trivial to author **and** productionize.  \n\n\nI did some research on the following approaches:\n\n* **Semantic Layers**: This is promising as it compiles to SQL, but it's not clear from documentation if they implement time series features well yet.\n* **Custom DSL**: Instead of a YAML like DSL in semantic layers, I could write a simple lisp-like DSL for manipulating time series that feels more like excel while not allowing advanced features in python. This gives me most control over the prettiness of the language while also giving me the freedom to compile to either SQL or Python or wtv in the future. Our data sets are small enough that we don't need the SQL optimizer necessarily.\n* **Dashboard w/ Code Gen**: It might be possible for a tool like superset to generate the SQL visually instead.\n* **Raw SQL**: I've been holding my ground that data analysts should be expected to write SQL, as it was literally designed for analysts to do business transformations. The syntax is ugly and can get fairly advanced, but it has a proven track record for several decades, with a large ecosystem of tooling. I don't know if I have the level of influence to change an entire org though. I will keep trying to push this concern to higher level mgmt as i think upskilling your workforce while eliminating engineering work is a big win overall.", "author_fullname": "t2_2naya68b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What simplified language to use for time series (panel) data manipulation in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16ojf6c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695311474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What are some low-code time series manipulation tools available for excel users who can&amp;#39;t be trusted to author production ready transformations using SQL or Python?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a software dev building tools for a financial data analyst team. The analyst team has asked me to build a tool for self-serve authoring of time series transformations so analysts can author hundreds of these transformations that power dashboards we deliver to customers.&lt;/p&gt;\n\n&lt;p&gt;I plan on using open-source only modern data stack tools, dbt in particular, as I don&amp;#39;t see a point in reinventing the wheel. The main obstacle however is my team doesn&amp;#39;t want to use SQL or Python as our analysts can&amp;#39;t be trusted to code. They complain that SQL is too verbose and ugly for time series, and python is powerful enough that analyst-authored code has introduced production bugs previously. Our analysts are more like excel analysts. Thus i&amp;#39;m looking for low-code tools that would make panel data manipulations (time series &amp;amp; cross-sectional gap fills, bucketing, panel data) fairly trivial to author &lt;strong&gt;and&lt;/strong&gt; productionize.  &lt;/p&gt;\n\n&lt;p&gt;I did some research on the following approaches:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Semantic Layers&lt;/strong&gt;: This is promising as it compiles to SQL, but it&amp;#39;s not clear from documentation if they implement time series features well yet.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Custom DSL&lt;/strong&gt;: Instead of a YAML like DSL in semantic layers, I could write a simple lisp-like DSL for manipulating time series that feels more like excel while not allowing advanced features in python. This gives me most control over the prettiness of the language while also giving me the freedom to compile to either SQL or Python or wtv in the future. Our data sets are small enough that we don&amp;#39;t need the SQL optimizer necessarily.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dashboard w/ Code Gen&lt;/strong&gt;: It might be possible for a tool like superset to generate the SQL visually instead.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Raw SQL&lt;/strong&gt;: I&amp;#39;ve been holding my ground that data analysts should be expected to write SQL, as it was literally designed for analysts to do business transformations. The syntax is ugly and can get fairly advanced, but it has a proven track record for several decades, with a large ecosystem of tooling. I don&amp;#39;t know if I have the level of influence to change an entire org though. I will keep trying to push this concern to higher level mgmt as i think upskilling your workforce while eliminating engineering work is a big win overall.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ojf6c", "is_robot_indexable": true, "report_reasons": null, "author": "shuaibot", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ojf6c/what_simplified_language_to_use_for_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ojf6c/what_simplified_language_to_use_for_time_series/", "subreddit_subscribers": 129635, "created_utc": 1695311474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi  I'm new to data engineering.\n\nI want to devise a system that loads tabular data (10-20 columns with 10\\^9/10\\^10/ -billions of data records), that will be ingest from eternal API.\n\nThe system should do processing (filtering subset of the rows and make basic computation analysis)\n\nHow to approach this problem? and what are the recommenced tools\n\nDoes best practice of DB like Spark or Snowflake will be sufficient for that task?\n\nand if there are any good resources to gain more knowledge- it will be awesome!\n\nI'm looking for the best of: latency in inference, easy to develop my system, easy to maintain, minimum server requirement, easy and basic monitor.\n\nThanks!\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_5831rdhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to approach designing a novel system that loads tabular data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oiax5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695308771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi  I&amp;#39;m new to data engineering.&lt;/p&gt;\n\n&lt;p&gt;I want to devise a system that loads tabular data (10-20 columns with 10^9/10^10/ -billions of data records), that will be ingest from eternal API.&lt;/p&gt;\n\n&lt;p&gt;The system should do processing (filtering subset of the rows and make basic computation analysis)&lt;/p&gt;\n\n&lt;p&gt;How to approach this problem? and what are the recommenced tools&lt;/p&gt;\n\n&lt;p&gt;Does best practice of DB like Spark or Snowflake will be sufficient for that task?&lt;/p&gt;\n\n&lt;p&gt;and if there are any good resources to gain more knowledge- it will be awesome!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for the best of: latency in inference, easy to develop my system, easy to maintain, minimum server requirement, easy and basic monitor.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16oiax5", "is_robot_indexable": true, "report_reasons": null, "author": "Expensive_Breakfast6", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16oiax5/how_to_approach_designing_a_novel_system_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16oiax5/how_to_approach_designing_a_novel_system_that/", "subreddit_subscribers": 129635, "created_utc": 1695308771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3i0xn3gy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Motherduck is now free for anyone to sign up for", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_16ofd3b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/d_Y3RLZe_KhTsbyenw-vB_8759rZH3XEztzDvH0aVwk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695301324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "motherduck.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://motherduck.com/blog/motherduck-open-for-all-with-series-b/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?auto=webp&amp;s=8e4e22764c7020df061de59726b1fae1cc0fb7cb", "width": 3200, "height": 1672}, "resolutions": [{"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2636a7232b1fe7525a4d5b1251afa5094b5fa5de", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3903f2fd35776377e83f5d2bf5f292b42daf81c4", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fb04ac59ca8b94c744f0c29a0ca7dbc6a9da4818", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da889478bbaa7a40a819fc79e6ddba2c729545c5", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c3ce0a20370d11fef048e9f7fc5503706c8ba38b", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=69a2759f51a49739e14139f0efc138621dbc9d46", "width": 1080, "height": 564}], "variants": {}, "id": "kv9OeUq5aTu0KwswK_YR0kPeKOsiIRZ02HxYtgwWFKc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16ofd3b", "is_robot_indexable": true, "report_reasons": null, "author": "ddanieltan", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ofd3b/motherduck_is_now_free_for_anyone_to_sign_up_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://motherduck.com/blog/motherduck-open-for-all-with-series-b/", "subreddit_subscribers": 129635, "created_utc": 1695301324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just published an article on how to setup Zero-ETL integration between an Amazon Aurora database and Redshift.\n\nWith the Zero-ETL feature, you do not need to bother about setting up complex data pipelines in your organisation.\n\n[https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift](https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift)", "author_fullname": "t2_k6fldwtv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How you can set up Zero-ETL data pipeline between Amazon Aurora and Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16of8ak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695300955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just published an article on how to setup Zero-ETL integration between an Amazon Aurora database and Redshift.&lt;/p&gt;\n\n&lt;p&gt;With the Zero-ETL feature, you do not need to bother about setting up complex data pipelines in your organisation.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift\"&gt;https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?auto=webp&amp;s=43854de3c536f86fe8edad9982c3583ab380ad09", "width": 681, "height": 370}, "resolutions": [{"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecdcf6fcf7adb0a98d8aa69dc115ea125539065f", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d049f2c1df9c1ec82935472ae7fbeb1ccbd6e8d", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6807b4b492aac7f299db9cd405289e68a67fe5f8", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ee0039bae6a8d6f694eb79595479c445690476e", "width": 640, "height": 347}], "variants": {}, "id": "C3ODgW2FQeNZnu5nJxonIMIBLmXRwIgb3c7YtEsw3sM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16of8ak", "is_robot_indexable": true, "report_reasons": null, "author": "gbxnga", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16of8ak/how_you_can_set_up_zeroetl_data_pipeline_between/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16of8ak/how_you_can_set_up_zeroetl_data_pipeline_between/", "subreddit_subscribers": 129635, "created_utc": 1695300955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are trying to read a big chunk of small files from several buckets in S3 with Spark. The objective is to merge those files into one and write it in another s3 bucket. \n\n&amp;#x200B;\n\nThe code for read:\n\n    val parquetFiles = Seq(\"s3a://...\", \"s3a://.....\" ..)\n    val df = spark.read.format(\"parquet\").load(parquetFiles:_*)\n\n It takes about 10 minutes to execute the following query:\n\n    df.coalesce(1).write.format(\"parquet\").save(\"s3://...\")\n\nWhile a `df.count()` it takes about 2 minutes (which is also not ok, I guess). \n\nWe've tried changing a lot of configurations from `hadoop.fs.s3a`, but no combination seems to alleviate the time. We cannot clearly understand which task is delaying the execution, but from Spark UI we have seen that not much CPU or Memory is consumed. \n\nMy assumption is that HTTP calls to S3 are getting too expensive. But I am not sure. \n\nHas anyone experienced similar issues? \n\nHave you solved them with conf or is it just a known problem? \n\n&amp;#x200B;\n\nThank you!\n\n&amp;#x200B;", "author_fullname": "t2_ffabopog", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reading small files from S3 with Spark is slow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocm37", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695293163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are trying to read a big chunk of small files from several buckets in S3 with Spark. The objective is to merge those files into one and write it in another s3 bucket. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The code for read:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;val parquetFiles = Seq(&amp;quot;s3a://...&amp;quot;, &amp;quot;s3a://.....&amp;quot; ..)\nval df = spark.read.format(&amp;quot;parquet&amp;quot;).load(parquetFiles:_*)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;It takes about 10 minutes to execute the following query:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df.coalesce(1).write.format(&amp;quot;parquet&amp;quot;).save(&amp;quot;s3://...&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;While a &lt;code&gt;df.count()&lt;/code&gt; it takes about 2 minutes (which is also not ok, I guess). &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve tried changing a lot of configurations from &lt;code&gt;hadoop.fs.s3a&lt;/code&gt;, but no combination seems to alleviate the time. We cannot clearly understand which task is delaying the execution, but from Spark UI we have seen that not much CPU or Memory is consumed. &lt;/p&gt;\n\n&lt;p&gt;My assumption is that HTTP calls to S3 are getting too expensive. But I am not sure. &lt;/p&gt;\n\n&lt;p&gt;Has anyone experienced similar issues? &lt;/p&gt;\n\n&lt;p&gt;Have you solved them with conf or is it just a known problem? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ocm37", "is_robot_indexable": true, "report_reasons": null, "author": "paolapardo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocm37/reading_small_files_from_s3_with_spark_is_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ocm37/reading_small_files_from_s3_with_spark_is_slow/", "subreddit_subscribers": 129635, "created_utc": 1695293163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, \n\nI am wondering If someone of you used open source DVT from google github  - [professional-services-data-validator](https://github.com/GoogleCloudPlatform/professional-services-data-validator).\n\nI need to validate data exported from differences system. I have one folder, where all files are stored and via pipelines are loaded to the BigQuery. I can easily transform these files to the .csv and run the validation. But the problem is with the Filesystem connection. Filesystem connection has argument \"file path\" and connection is created only for one file.\n\nMy question is, with your experience, Do you think its better to create new connection for every file (thousands files) or use python script that will transform and load data to the one file work\\_file.csv that is connected and after the validation delete the file content then take another file and transform and load data to the work\\_file.csv...\n\nI do not have enough experience with these technologies and BI processes so I would be glad for every point you have. \n\nThank you very much.  \nM\n\n&amp;#x200B;", "author_fullname": "t2_79da9w8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Validation via professional-services-data-validator from Google", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocb07", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695292088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;I am wondering If someone of you used open source DVT from google github  - &lt;a href=\"https://github.com/GoogleCloudPlatform/professional-services-data-validator\"&gt;professional-services-data-validator&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I need to validate data exported from differences system. I have one folder, where all files are stored and via pipelines are loaded to the BigQuery. I can easily transform these files to the .csv and run the validation. But the problem is with the Filesystem connection. Filesystem connection has argument &amp;quot;file path&amp;quot; and connection is created only for one file.&lt;/p&gt;\n\n&lt;p&gt;My question is, with your experience, Do you think its better to create new connection for every file (thousands files) or use python script that will transform and load data to the one file work_file.csv that is connected and after the validation delete the file content then take another file and transform and load data to the work_file.csv...&lt;/p&gt;\n\n&lt;p&gt;I do not have enough experience with these technologies and BI processes so I would be glad for every point you have. &lt;/p&gt;\n\n&lt;p&gt;Thank you very much.&lt;br/&gt;\nM&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?auto=webp&amp;s=e21a2042737dc223e67be7ea27e40fc2236d12ee", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c766345a5f83563a2848746e21a43c0ad01a8c7e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80407217416d454d1f32b3b0c1fe41d4ba9dbf87", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ab68f2323e57915f688870703aa3b7827922c10", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3167be568b95344dbbaa19877c37d4bfffb3e688", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f988a35b7c0f9d7945888db9d4ec0e48eddb82d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e44e34cb74d2a495765f7b1cc9148eed809bb28", "width": 1080, "height": 540}], "variants": {}, "id": "Y2KMmNmoflDB_TA4VN4tScfKspEdT0Y_43XDwIeYUzA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ocb07", "is_robot_indexable": true, "report_reasons": null, "author": "Illustrious_Union216", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocb07/data_validation_via/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ocb07/data_validation_via/", "subreddit_subscribers": 129635, "created_utc": 1695292088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering about this for a personal project.", "author_fullname": "t2_imajwwpcb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have a good resource to learn how to set up and use NoSQL databases as quickly as possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o91r5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695279809.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering about this for a personal project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16o91r5", "is_robot_indexable": true, "report_reasons": null, "author": "al-hamal", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o91r5/does_anyone_have_a_good_resource_to_learn_how_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o91r5/does_anyone_have_a_good_resource_to_learn_how_to/", "subreddit_subscribers": 129635, "created_utc": 1695279809.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Few days back I have posted a help in this group and got wonderful comments. \nHere is my update: I have successfully implemented airflow dag to run dbt generic tests, but the airflow dag is picking up and running the dbt models before running the generic yml tests. \nHere I dont want dbt models to run because we have separate dags for that. I only want dbt generic tests to run as part of the airflow dag. \nAny suggestions are appreciated.", "author_fullname": "t2_ke7daufi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running dbt tests with Airflow dag", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o8n9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1695278383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Few days back I have posted a help in this group and got wonderful comments. \nHere is my update: I have successfully implemented airflow dag to run dbt generic tests, but the airflow dag is picking up and running the dbt models before running the generic yml tests. \nHere I dont want dbt models to run because we have separate dags for that. I only want dbt generic tests to run as part of the airflow dag. \nAny suggestions are appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://reddit.com/r/dataengineering/s/pHwzT1IgCw", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16o8n9p", "is_robot_indexable": true, "report_reasons": null, "author": "Long-Neighborhood330", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o8n9p/running_dbt_tests_with_airflow_dag/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://reddit.com/r/dataengineering/s/pHwzT1IgCw", "subreddit_subscribers": 129635, "created_utc": 1695278383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm wondering how people scale batch inference? I'm a Senior Data Analyst and I currently just use a massive VM and parallelize it so I can make predictions concurrently. At first that was fine but now I need to make 10-100x more predictions for my job. I don't have much cloud infrastructure experience and would love to hear how people scale their inference models.\n\nFYI) I'm actively trying to develop a Python package for this but still ideating. All recommendations and thoughts are welcomed.", "author_fullname": "t2_7iyeps3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Do You Scale Batch Inference?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o6f96", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695270753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering how people scale batch inference? I&amp;#39;m a Senior Data Analyst and I currently just use a massive VM and parallelize it so I can make predictions concurrently. At first that was fine but now I need to make 10-100x more predictions for my job. I don&amp;#39;t have much cloud infrastructure experience and would love to hear how people scale their inference models.&lt;/p&gt;\n\n&lt;p&gt;FYI) I&amp;#39;m actively trying to develop a Python package for this but still ideating. All recommendations and thoughts are welcomed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16o6f96", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Post_149", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o6f96/how_do_you_scale_batch_inference/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o6f96/how_do_you_scale_batch_inference/", "subreddit_subscribers": 129635, "created_utc": 1695270753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_xf2t5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data-toolset: Upgrade from avro-tools and parquet-tools jars to a more user-friendly Python package.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_16o4m7a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/y-354gCMniyFSuzUKJ6MspXshe8ut3YAzm9X8lfyoRs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695265043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/luminousmen/data-toolset", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?auto=webp&amp;s=0275760653eb6471989e32ee41bdd0f65611dde6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=469ef1a73b2198c9f1e7c7426347172e55b280c1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=61528f26675044fe6e11d8bbbc1fd48591852412", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=78d54701fbb545a3b8b96e5d80f161dd12314208", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac4314944f920db905b28366c67cae5988ece7ba", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a15a31f790cedbd314369c39431f04b33cab1a0b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f8dbb411e6eec14d712db9e91a2aa42c967ebcb3", "width": 1080, "height": 540}], "variants": {}, "id": "CnDlsmlGwvqlEdv_LY44Io94g8FnYa6LctUlcwAVAHo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16o4m7a", "is_robot_indexable": true, "report_reasons": null, "author": "luminoumen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o4m7a/datatoolset_upgrade_from_avrotools_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/luminousmen/data-toolset", "subreddit_subscribers": 129635, "created_utc": 1695265043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, does anyone here work or has worked with Looker Studio connecting to Big Query? I want to know how the data sync between the tools works.\n\nMore specifically, does the Big Query connector have a row limit for returning a table? My goal is to bring a table of 50 million rows from Big Query to Looker Studio and then create visualizations from that table. Would that be possible? I saw that there is a 1 million row limit when connecting with Snowflake, so I don't know if it would be possible with Big Query.\n\nThanks!", "author_fullname": "t2_7iesagha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Row limit when plugging Big Query into Looker Studio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nyg13", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695248430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, does anyone here work or has worked with Looker Studio connecting to Big Query? I want to know how the data sync between the tools works.&lt;/p&gt;\n\n&lt;p&gt;More specifically, does the Big Query connector have a row limit for returning a table? My goal is to bring a table of 50 million rows from Big Query to Looker Studio and then create visualizations from that table. Would that be possible? I saw that there is a 1 million row limit when connecting with Snowflake, so I don&amp;#39;t know if it would be possible with Big Query.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nyg13", "is_robot_indexable": true, "report_reasons": null, "author": "Pop-Huge", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nyg13/row_limit_when_plugging_big_query_into_looker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nyg13/row_limit_when_plugging_big_query_into_looker/", "subreddit_subscribers": 129635, "created_utc": 1695248430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In simple terms, should you organise your code into repos that are by function e.g.:\n - one repo for all your python code for spark / serverless functions\n - one repo for all your sql views (dbt kinda code)\n - one repo for all your orchestration yamls\nwhere you have the benefit that each repo centralises and can benefit from living together e.g. dbt models all centralised and can refer to one another\n\nOr, should you organise it by outcome i.e. all code for a pipeline lives together e.g. in a single repo you'll have:\n - python code for extraction\n - code for sql views\n - python transformation code\n - orchestration yaml\n\nContext: small start up looking to scale from simple beginnings with serverless functions + scheduling to a more robust setup that can handle more complicated pipelines.\n\nThe latter makes more sense to me but want to ask for opinion incase it's a clear mistake e.g. we hit the next stage in scaling up and this current idea results in new complexity I haven't thought about yet.", "author_fullname": "t2_576k5sn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to organise pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nxfk0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695246063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In simple terms, should you organise your code into repos that are by function e.g.:\n - one repo for all your python code for spark / serverless functions\n - one repo for all your sql views (dbt kinda code)\n - one repo for all your orchestration yamls\nwhere you have the benefit that each repo centralises and can benefit from living together e.g. dbt models all centralised and can refer to one another&lt;/p&gt;\n\n&lt;p&gt;Or, should you organise it by outcome i.e. all code for a pipeline lives together e.g. in a single repo you&amp;#39;ll have:\n - python code for extraction\n - code for sql views\n - python transformation code\n - orchestration yaml&lt;/p&gt;\n\n&lt;p&gt;Context: small start up looking to scale from simple beginnings with serverless functions + scheduling to a more robust setup that can handle more complicated pipelines.&lt;/p&gt;\n\n&lt;p&gt;The latter makes more sense to me but want to ask for opinion incase it&amp;#39;s a clear mistake e.g. we hit the next stage in scaling up and this current idea results in new complexity I haven&amp;#39;t thought about yet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nxfk0", "is_robot_indexable": true, "report_reasons": null, "author": "mjam03", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nxfk0/how_to_organise_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nxfk0/how_to_organise_pipelines/", "subreddit_subscribers": 129635, "created_utc": 1695246063.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}