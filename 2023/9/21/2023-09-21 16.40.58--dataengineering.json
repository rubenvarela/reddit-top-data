{"kind": "Listing", "data": {"after": "t3_16nrmdh", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": ".", "author_fullname": "t2_bsuu4apm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Pareto Principle - what is the 20% (algos, functions, libraries) that lets you develop 80% of code related to Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o883v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695276875.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16o883v", "is_robot_indexable": true, "report_reasons": null, "author": "CrimsonMentone30", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o883v/python_pareto_principle_what_is_the_20_algos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o883v/python_pareto_principle_what_is_the_20_algos/", "subreddit_subscribers": 129625, "created_utc": 1695276875.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Preparing for an engineering interview can be overwhelming, LeetCode, System Design, Behavioral questions, its a lot to manage. If you are preparing for a **#dataengieering** interview, give my free app a try. It gives you 2 tasks a day for 30 days to help you prepare.\n\nAlso I wanted to learn Swift.\n\nhttps://apps.apple.com/app/interview-ace/id6465748534", "author_fullname": "t2_4fpl974m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made a free app to help you prepare for the data engineering interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o5d6o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695267311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Preparing for an engineering interview can be overwhelming, LeetCode, System Design, Behavioral questions, its a lot to manage. If you are preparing for a &lt;strong&gt;#dataengieering&lt;/strong&gt; interview, give my free app a try. It gives you 2 tasks a day for 30 days to help you prepare.&lt;/p&gt;\n\n&lt;p&gt;Also I wanted to learn Swift.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://apps.apple.com/app/interview-ace/id6465748534\"&gt;https://apps.apple.com/app/interview-ace/id6465748534&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?auto=webp&amp;s=86f11be447465164e06c1895f371f8247b3de78e", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9de58037059431fcc82b29fec49b0b89fd33aaed", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9ca9694b7fac863b314527c7f015ec557450634", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=974fe41036a7cd50cac5039677476f1175904ee7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0a246610bdf304e619347a7ace86b9bacea4e0d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=96029c532b8c99d9d8ae6e66965bfd26dc47cb50", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=48172f9d876603790b702ec84e57e0e6b4cff7da", "width": 1080, "height": 567}], "variants": {}, "id": "5YmFqnaQR8uybNXWtDGGlBlmqp-5bnkiVElQWM1Gehw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16o5d6o", "is_robot_indexable": true, "report_reasons": null, "author": "coyne_operated", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o5d6o/i_made_a_free_app_to_help_you_prepare_for_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o5d6o/i_made_a_free_app_to_help_you_prepare_for_the/", "subreddit_subscribers": 129625, "created_utc": 1695267311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to take about 6 months to really dig into company and code before expressing any strong architectural opinions, but I do have a bit of experience in this niche. The rest of the team is 1-2 years more senior but less experienced in DE (they\u2019re SWEs).\n\nA re-architecture is underway and I see major issues with it already. \n\n- Implementing streaming from scratch, like as an alternative to Kafka, pubsub, or what have you \n- Implementing data transformations in python vs something like dbt. This one is debatable but as someone who\u2019s done both my judgement is that this should be a case of ELT and should be done in the DWH.\n- STRONG coupling of extraction and transformation. On this path, a relatively mild change in data requirements might require a full re-extract of all data just to run the transformation step. And we\u2019d be subject to 3rd party API limits. \n- STRONG coupling of app needs and analytics team needs. Right now, analytics teams are hitting the prod DB freely. And plans are to build solutions that support both, vs separate solutions that support each. \n- No concern or plan for data quality in the re-architecture\n\nThe way I see it i could\n\n- Be the squeaky wheel on all fronts. But it feels like an uphill battle. The planning and implementation has been done by a very early employee that isn\u2019t quite a co-founder but does own more than your average employee and the team lead. The plan (as a focus, not in detail) has already been mentioned to the board as a priority.\n- Go along with the plan, cuz who cares, it\u2019s not my VC money, right? But I feel I would be taking a huge step back in terms of the DE infra I\u2019ll be working with.\n- Try to walk a middle ground where I pick and choose things to push for. Haven\u2019t figured out which hills to die on yet though.\n\nThoughts or prayers appreciated", "author_fullname": "t2_2tu8n7l9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coming into an org just as a flawed re-architecture is underway. Any tips on pushing for changes early in your time at a new place?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nvnp3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695241908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to take about 6 months to really dig into company and code before expressing any strong architectural opinions, but I do have a bit of experience in this niche. The rest of the team is 1-2 years more senior but less experienced in DE (they\u2019re SWEs).&lt;/p&gt;\n\n&lt;p&gt;A re-architecture is underway and I see major issues with it already. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Implementing streaming from scratch, like as an alternative to Kafka, pubsub, or what have you &lt;/li&gt;\n&lt;li&gt;Implementing data transformations in python vs something like dbt. This one is debatable but as someone who\u2019s done both my judgement is that this should be a case of ELT and should be done in the DWH.&lt;/li&gt;\n&lt;li&gt;STRONG coupling of extraction and transformation. On this path, a relatively mild change in data requirements might require a full re-extract of all data just to run the transformation step. And we\u2019d be subject to 3rd party API limits. &lt;/li&gt;\n&lt;li&gt;STRONG coupling of app needs and analytics team needs. Right now, analytics teams are hitting the prod DB freely. And plans are to build solutions that support both, vs separate solutions that support each. &lt;/li&gt;\n&lt;li&gt;No concern or plan for data quality in the re-architecture&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The way I see it i could&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Be the squeaky wheel on all fronts. But it feels like an uphill battle. The planning and implementation has been done by a very early employee that isn\u2019t quite a co-founder but does own more than your average employee and the team lead. The plan (as a focus, not in detail) has already been mentioned to the board as a priority.&lt;/li&gt;\n&lt;li&gt;Go along with the plan, cuz who cares, it\u2019s not my VC money, right? But I feel I would be taking a huge step back in terms of the DE infra I\u2019ll be working with.&lt;/li&gt;\n&lt;li&gt;Try to walk a middle ground where I pick and choose things to push for. Haven\u2019t figured out which hills to die on yet though.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thoughts or prayers appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16nvnp3", "is_robot_indexable": true, "report_reasons": null, "author": "Firm_Bit", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nvnp3/coming_into_an_org_just_as_a_flawed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nvnp3/coming_into_an_org_just_as_a_flawed/", "subreddit_subscribers": 129625, "created_utc": 1695241908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is looking into switching from SSIS-based ETL process to a Python and T-SQL to create data pipelines. We work in a classified environment, which restricts access to non-US based software (no Pycharm) and causes other problems with account privileges. Privileges we can work on, slowly but surely. The scheduling process is where we aren't certain what to do. I mentioned Airflow, but it would require a hefty vetting process to get into each server, even the unclassified environment. These classified environments do not have access to the internet either, for obvious reasons.\n\nDoes anyone have experience doing DE work in a SCIF? \n\nIf so, how did you go about the scheduling process for Python/T-SQL?\n\nI'm welcome to other possibilities and ideas.", "author_fullname": "t2_aiumhm29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Scheduler in a Closed Environment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nzm2s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695251381.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is looking into switching from SSIS-based ETL process to a Python and T-SQL to create data pipelines. We work in a classified environment, which restricts access to non-US based software (no Pycharm) and causes other problems with account privileges. Privileges we can work on, slowly but surely. The scheduling process is where we aren&amp;#39;t certain what to do. I mentioned Airflow, but it would require a hefty vetting process to get into each server, even the unclassified environment. These classified environments do not have access to the internet either, for obvious reasons.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience doing DE work in a SCIF? &lt;/p&gt;\n\n&lt;p&gt;If so, how did you go about the scheduling process for Python/T-SQL?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m welcome to other possibilities and ideas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nzm2s", "is_robot_indexable": true, "report_reasons": null, "author": "Upbeat_Count_7568", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nzm2s/python_scheduler_in_a_closed_environment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nzm2s/python_scheduler_in_a_closed_environment/", "subreddit_subscribers": 129625, "created_utc": 1695251381.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long story short, Im a junior data engineering tasked with moving a (for me) tremendous amount of files. This is the generall architecture:\n\n1. Historic files + daily files in blob container A.\n2. Needs to be moved to blob container B, which is a reload container. Moving files to this container will trigger a reload function, writing output of the files to a staging db.\n3. A dataflow reads from the staging table, and finally writes the output to a dwh-table.\n\nNow, the problem the dataflow cannot handle the ETL load of taking the entire load from the staging table (Will take many days even with maxed out clusters).\n\nI initally thought of having a Select Top(x) as a query in my dataflow, but this means I will also need to make changes to the dataflow after it has done its job in production.\n\nI have used a Az Copy command to move the files from container A to container B, and thought about the possibility of using this in a pipeline. But it looks somewhat complicated.  \n\n\nAny thoughts on the best way to handle this case? best practises or just generall tips/input would be very appreciated!\n\nEdit: \n\n\\- The files in question are XML-files.  \n\\- Our dataflows and pipelines are located in the Azure Synapse environment.  \n\\- The Storage of which container A and container B uses is Azure Storage Explorer.  \n\\- The staging db is  sql database, and the dwh-table is a Dedicated SQL Pool.\n\n&amp;#x200B;", "author_fullname": "t2_ddrknaxf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scipting a ETL job for 200.000 files (200gb)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nr1hc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695231771.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695230599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short, Im a junior data engineering tasked with moving a (for me) tremendous amount of files. This is the generall architecture:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Historic files + daily files in blob container A.&lt;/li&gt;\n&lt;li&gt;Needs to be moved to blob container B, which is a reload container. Moving files to this container will trigger a reload function, writing output of the files to a staging db.&lt;/li&gt;\n&lt;li&gt;A dataflow reads from the staging table, and finally writes the output to a dwh-table.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now, the problem the dataflow cannot handle the ETL load of taking the entire load from the staging table (Will take many days even with maxed out clusters).&lt;/p&gt;\n\n&lt;p&gt;I initally thought of having a Select Top(x) as a query in my dataflow, but this means I will also need to make changes to the dataflow after it has done its job in production.&lt;/p&gt;\n\n&lt;p&gt;I have used a Az Copy command to move the files from container A to container B, and thought about the possibility of using this in a pipeline. But it looks somewhat complicated.  &lt;/p&gt;\n\n&lt;p&gt;Any thoughts on the best way to handle this case? best practises or just generall tips/input would be very appreciated!&lt;/p&gt;\n\n&lt;p&gt;Edit: &lt;/p&gt;\n\n&lt;p&gt;- The files in question are XML-files.&lt;br/&gt;\n- Our dataflows and pipelines are located in the Azure Synapse environment.&lt;br/&gt;\n- The Storage of which container A and container B uses is Azure Storage Explorer.&lt;br/&gt;\n- The staging db is  sql database, and the dwh-table is a Dedicated SQL Pool.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nr1hc", "is_robot_indexable": true, "report_reasons": null, "author": "Alarmed-Royal-2161", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nr1hc/scipting_a_etl_job_for_200000_files_200gb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nr1hc/scipting_a_etl_job_for_200000_files_200gb/", "subreddit_subscribers": 129625, "created_utc": 1695230599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Git Repo:**\n\n[Fluvio](https://github.com/infinyon/fluvio)\n\nThere is of decent amount of conversation in the community on Rust for Data Engineering. This project has been a work in progress for nearly 5 years and would love feedback from this amazing community on the project.\n\n**Premise:**\n\nThe initial creators of the project who were running the service mesh at Nginx were not satisfied with the available solutions. They needed a better solution 5 years ago in the context of building data pipelines that simplified the spaghetti data stack working to syndicate data from monoliths and microservices with a variety of databases and models along with third party APIs. They tried SQS, Kinesis, NATS, as alternatives to Kafka.\n\nThere are a bunch of data streaming platforms that have come up over the last few years. The goal is to make a small dent in the data streaming universe by offering our honest attempt at solving the complexity of stateful data streaming and making it accessible and generally available for developers and engineers building data driven solutions. We are working to provide a legit alternative for JVM based tools like Kafka, and Flink based on Rust and WebAssembly.\n\nJVM is the standard and people are used to it, there are many Goliath's in the data platforms space. The smooth stones that the Fluvio project is betting on include the efficiency and performance of Rust, the flexibility and growth trajectory of Web Assembly, the rise of data products, the push for better overall economics of data pipelines, the demand for asynchronous and real time insights.\n\n**Current Project Status:**\n\nIn it's current state with [Fluvio](https://github.com/infinyon/fluvio) you can build data pipelines that would offer extreme throughput. You can apply on stream transformations using web assembly (they are called [Fluvio Smart Modules](https://www.fluvio.io/smartmodules/)).\n\nWith delivery guarantees, deduplication, robust caching and mirroring the project has matured significantly in 2023.\n\nThe project supports at least once, at most once, exactly once delivery semantic that can be configured.\n\nThe cloud install is a single binary that is less than 150 MB and the edge version is a 15 MB binary can be deployed on ARMv7 devices with as low as 256 MB memory. Needless to say that it is lightweight.\n\nWith this tiny footprint user tests show 5X throughput along with 10X less CPU utilization and 50X less memory utilization with some of our active installs.\n\n[Fluvio Connectors](https://www.fluvio.io/connectors/) connect at protocol level to http endpoints, BLOB stores, databases, application APIs using supported Rust, Python, Node client, or write to a webhook gateway for polling type applications.\n\nWith smart modules you can currently do unbounded transformation operations like JSON to JSON transformation, maps, array maps, RSS to JSON conversion, REGEX operations and more.\n\n**Future improvements:**\n\nThe tradeoff that was made is to front load a bunch of software engineering and design work and build a somewhat over-engineered solution. The project is nearing it's biggest milestone in a couple of months with on stream materialized views using stateful computation on time windows.\n\nThere are a few areas where the project makes deliberate trade-offs:\n\n1. Kubernetes dependency, even our open source project needs K8 and that is something that the community has been asking us to change. This is on the verge of being solved.\n2. Supported languages: Currently the project supports Rust, Python, Node, and there are some requests to support Go that is being shaped.\n3. No support for SQL transformations: This is a big one based on the conversations in this community, while our domain specific language construct is based on YAML and you can express operations that look and feel like SQL in many cases it is still YAML, and if you are needing custom Smart Modules for transformations you can write custom ones using the smart module development kit.\n4. Hybrid deployment to use whatever cloud you'd like - The project is are currently deployed as a cloud native fully managed turn key system on AWS - [InfinyOn Cloud](https://infinyon.cloud/account). There is a gap where folks are looking for a clear separation of control and data plane and run the core platform within their network to maintain data sovereignty, minimize ingress egress over network which is a big cost driver etc. As a workaround we have helm charts to support the deployment on kubernetes and with the completion of the first point above have the ability to support docker compose and other patterns\n\n&amp;#x200B;\n\n**Feedback request:**\n\nAppreciate feedback from folks in the community share feedback about the experience that you have building data platforms building event streaming use cases on how our project compares with what you are used to.\n\nThanks in advance for any insights that you share.", "author_fullname": "t2_6pheknqy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fluvio OSS[v0.10.15]: Kafka + Flink built using Rust + WASM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nqizp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695229377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Git Repo:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/infinyon/fluvio\"&gt;Fluvio&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There is of decent amount of conversation in the community on Rust for Data Engineering. This project has been a work in progress for nearly 5 years and would love feedback from this amazing community on the project.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Premise:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The initial creators of the project who were running the service mesh at Nginx were not satisfied with the available solutions. They needed a better solution 5 years ago in the context of building data pipelines that simplified the spaghetti data stack working to syndicate data from monoliths and microservices with a variety of databases and models along with third party APIs. They tried SQS, Kinesis, NATS, as alternatives to Kafka.&lt;/p&gt;\n\n&lt;p&gt;There are a bunch of data streaming platforms that have come up over the last few years. The goal is to make a small dent in the data streaming universe by offering our honest attempt at solving the complexity of stateful data streaming and making it accessible and generally available for developers and engineers building data driven solutions. We are working to provide a legit alternative for JVM based tools like Kafka, and Flink based on Rust and WebAssembly.&lt;/p&gt;\n\n&lt;p&gt;JVM is the standard and people are used to it, there are many Goliath&amp;#39;s in the data platforms space. The smooth stones that the Fluvio project is betting on include the efficiency and performance of Rust, the flexibility and growth trajectory of Web Assembly, the rise of data products, the push for better overall economics of data pipelines, the demand for asynchronous and real time insights.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Current Project Status:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In it&amp;#39;s current state with &lt;a href=\"https://github.com/infinyon/fluvio\"&gt;Fluvio&lt;/a&gt; you can build data pipelines that would offer extreme throughput. You can apply on stream transformations using web assembly (they are called &lt;a href=\"https://www.fluvio.io/smartmodules/\"&gt;Fluvio Smart Modules&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;With delivery guarantees, deduplication, robust caching and mirroring the project has matured significantly in 2023.&lt;/p&gt;\n\n&lt;p&gt;The project supports at least once, at most once, exactly once delivery semantic that can be configured.&lt;/p&gt;\n\n&lt;p&gt;The cloud install is a single binary that is less than 150 MB and the edge version is a 15 MB binary can be deployed on ARMv7 devices with as low as 256 MB memory. Needless to say that it is lightweight.&lt;/p&gt;\n\n&lt;p&gt;With this tiny footprint user tests show 5X throughput along with 10X less CPU utilization and 50X less memory utilization with some of our active installs.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.fluvio.io/connectors/\"&gt;Fluvio Connectors&lt;/a&gt; connect at protocol level to http endpoints, BLOB stores, databases, application APIs using supported Rust, Python, Node client, or write to a webhook gateway for polling type applications.&lt;/p&gt;\n\n&lt;p&gt;With smart modules you can currently do unbounded transformation operations like JSON to JSON transformation, maps, array maps, RSS to JSON conversion, REGEX operations and more.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Future improvements:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The tradeoff that was made is to front load a bunch of software engineering and design work and build a somewhat over-engineered solution. The project is nearing it&amp;#39;s biggest milestone in a couple of months with on stream materialized views using stateful computation on time windows.&lt;/p&gt;\n\n&lt;p&gt;There are a few areas where the project makes deliberate trade-offs:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Kubernetes dependency, even our open source project needs K8 and that is something that the community has been asking us to change. This is on the verge of being solved.&lt;/li&gt;\n&lt;li&gt;Supported languages: Currently the project supports Rust, Python, Node, and there are some requests to support Go that is being shaped.&lt;/li&gt;\n&lt;li&gt;No support for SQL transformations: This is a big one based on the conversations in this community, while our domain specific language construct is based on YAML and you can express operations that look and feel like SQL in many cases it is still YAML, and if you are needing custom Smart Modules for transformations you can write custom ones using the smart module development kit.&lt;/li&gt;\n&lt;li&gt;Hybrid deployment to use whatever cloud you&amp;#39;d like - The project is are currently deployed as a cloud native fully managed turn key system on AWS - &lt;a href=\"https://infinyon.cloud/account\"&gt;InfinyOn Cloud&lt;/a&gt;. There is a gap where folks are looking for a clear separation of control and data plane and run the core platform within their network to maintain data sovereignty, minimize ingress egress over network which is a big cost driver etc. As a workaround we have helm charts to support the deployment on kubernetes and with the completion of the first point above have the ability to support docker compose and other patterns&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Feedback request:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Appreciate feedback from folks in the community share feedback about the experience that you have building data platforms building event streaming use cases on how our project compares with what you are used to.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any insights that you share.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?auto=webp&amp;s=520d6eb9e3127fc61ea12beb6c628a7ac01126bf", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8b2d34eea58832a825d918f152a82b79af8c434", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=19a34b16fad6ae4f0d23b0303a670f94a47da06d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3c2b308bc6209a315ab6214f481a64fbbabc748", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d9c8a8d74aff977a447682ab1ec267414c6a446", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f977101c367113c571ea99c758181058571c0de", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=870e21320abfa5d890ea9120345d80f67f1970a5", "width": 1080, "height": 540}], "variants": {}, "id": "whUAUNos2msWHtiLmyar3aMQwJS9FMcp7R_jyGXzp-s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Head of Product - Data Platform ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16nqizp", "is_robot_indexable": true, "report_reasons": null, "author": "drc1728", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16nqizp/fluvio_ossv01015_kafka_flink_built_using_rust_wasm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nqizp/fluvio_ossv01015_kafka_flink_built_using_rust_wasm/", "subreddit_subscribers": 129625, "created_utc": 1695229377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nCurrently building a silver level streaming table from a bronze level streaming source, but not getting the append behavior I'm expecting. When running simple aggregations (min, max, count, etc..), my target silver table keeps being overwritten. For context this DLT pipeline is scheduled to run every 20 minutes, and the expected behavior is to have each 20 minutes worth of data be aggregated and then appended to the target table. Why is my target table being overwritten and not appended to?\n\nThe below query just overwrites \\`example\\_silver\\_table\\` every time the pipeline is run:\n\n    CREATE OR REFRESH STREAMING LIVE TABLE example_silver_table\n    AS \n        SELECT\n            dimension_1,\n            dimension_2,\n            MAX(dimension_3) AS max_dimension_3,\n            SUM(dimension_4) AS dimension_4_total\n        FROM STREAM(LIVE.fact_bronze_table_event)\n        GROUP BY\n            dimension_1,\n            dimension_2\n\nI also tried including a watermark, but not sure I'm implementing it correctly. Every 20 minutes when the pipeline runs, the below query output is 0 records written to \\`example\\_silver\\_table\\` :\n\n    CREATE OR REFRESH STREAMING LIVE TABLE example_silver_table\n    AS \n        SELECT\n            dimension_1,\n            dimension_2,\n            MAX(dimension_3) AS max_dimension_3,\n            SUM(dimension_4) AS dimension_4_total\n        FROM STREAM(LIVE.fact_bronze_table_event) WATERMARK timestamp_field AS event_time DELAY OF INTERVAL 20 MINUTE\n        GROUP BY\n            dimension_1,\n            dimension_2,\n            WINDOW('event_time', '20 minutes')\n\nTLDR;\n\nWhen trying to do aggregations on streaming (20 minute batches) data, I can't get the target table to stop being overwritten every pipeline run. Tried using watermarks, but no luck. Any help or guidance would be much appreciated!!\n\nResources I've tried using, but no luck:\n\n* [Feature Deep Dive: Watermarking in Apache Spark Structured Streaming](https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html)\n* [Apply watermarks to control data processing thresholds](https://docs.databricks.com/en/structured-streaming/watermarks.html)\n* [Event-time Aggregation and Watermarking in Apache Spark\u2019s Structured Streaming](https://www.databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html)", "author_fullname": "t2_3i1tg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Aggregate Streaming Data With Databricks Delta Live Tables (DLT)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o3pat", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695262481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently building a silver level streaming table from a bronze level streaming source, but not getting the append behavior I&amp;#39;m expecting. When running simple aggregations (min, max, count, etc..), my target silver table keeps being overwritten. For context this DLT pipeline is scheduled to run every 20 minutes, and the expected behavior is to have each 20 minutes worth of data be aggregated and then appended to the target table. Why is my target table being overwritten and not appended to?&lt;/p&gt;\n\n&lt;p&gt;The below query just overwrites `example_silver_table` every time the pipeline is run:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE OR REFRESH STREAMING LIVE TABLE example_silver_table\nAS \n    SELECT\n        dimension_1,\n        dimension_2,\n        MAX(dimension_3) AS max_dimension_3,\n        SUM(dimension_4) AS dimension_4_total\n    FROM STREAM(LIVE.fact_bronze_table_event)\n    GROUP BY\n        dimension_1,\n        dimension_2\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I also tried including a watermark, but not sure I&amp;#39;m implementing it correctly. Every 20 minutes when the pipeline runs, the below query output is 0 records written to `example_silver_table` :&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE OR REFRESH STREAMING LIVE TABLE example_silver_table\nAS \n    SELECT\n        dimension_1,\n        dimension_2,\n        MAX(dimension_3) AS max_dimension_3,\n        SUM(dimension_4) AS dimension_4_total\n    FROM STREAM(LIVE.fact_bronze_table_event) WATERMARK timestamp_field AS event_time DELAY OF INTERVAL 20 MINUTE\n    GROUP BY\n        dimension_1,\n        dimension_2,\n        WINDOW(&amp;#39;event_time&amp;#39;, &amp;#39;20 minutes&amp;#39;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;TLDR;&lt;/p&gt;\n\n&lt;p&gt;When trying to do aggregations on streaming (20 minute batches) data, I can&amp;#39;t get the target table to stop being overwritten every pipeline run. Tried using watermarks, but no luck. Any help or guidance would be much appreciated!!&lt;/p&gt;\n\n&lt;p&gt;Resources I&amp;#39;ve tried using, but no luck:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html\"&gt;Feature Deep Dive: Watermarking in Apache Spark Structured Streaming&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://docs.databricks.com/en/structured-streaming/watermarks.html\"&gt;Apply watermarks to control data processing thresholds&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\"&gt;Event-time Aggregation and Watermarking in Apache Spark\u2019s Structured Streaming&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?auto=webp&amp;s=baea0bdbfae95e881ea0a2f0c3b33954b1f12ee9", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5149cb676e163de6e1c8f6c468e674e2e1eedccf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ffe2f685b75ffa31d46101223d2e85d396c11a2c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1e57ce0d9b65813a43715d49944408d42572108", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6218b26dffaec8bb109ecc5069aa1cdf4fe9af22", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=876e8e3df516ab9110f4d59af591fd3b01d003e4", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c649702fc9a45301838bb4e09431f6fd1f4a9ec6", "width": 1080, "height": 565}], "variants": {}, "id": "0LSNAjx4YTAOOrcgDeHl6XfnklsWXU_JL9sKej5JZLo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16o3pat", "is_robot_indexable": true, "report_reasons": null, "author": "Shatonmedeek", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o3pat/how_to_aggregate_streaming_data_with_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o3pat/how_to_aggregate_streaming_data_with_databricks/", "subreddit_subscribers": 129625, "created_utc": 1695262481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, someone recently asked to review their CV and after replying I got a lot of you asking for a review. I did all of you, and now sharing what I think are some useful tips to improve your CV.  Use these if you are applying but nobody is getting back to you.\n\n\nOf course use your own judgment and adapt to your local market and requirements, but here are the top level tips: [Article link](https://dlthub.com/docs/blog/data-engineering-cv) \n\nIf you think there are more useful tips, go ahead and do a PR so others where I distribute this article can also benefit. [Github link](https://github.com/dlt-hub/dlt/blob/devel/docs/website/blog/2023-09-20-data-engineering-cv.md)", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for creating an effective data engineering CV", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o9yib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695283270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, someone recently asked to review their CV and after replying I got a lot of you asking for a review. I did all of you, and now sharing what I think are some useful tips to improve your CV.  Use these if you are applying but nobody is getting back to you.&lt;/p&gt;\n\n&lt;p&gt;Of course use your own judgment and adapt to your local market and requirements, but here are the top level tips: &lt;a href=\"https://dlthub.com/docs/blog/data-engineering-cv\"&gt;Article link&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;If you think there are more useful tips, go ahead and do a PR so others where I distribute this article can also benefit. &lt;a href=\"https://github.com/dlt-hub/dlt/blob/devel/docs/website/blog/2023-09-20-data-engineering-cv.md\"&gt;Github link&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?auto=webp&amp;s=5138f6a644eb11025e1e169629ec855c86ee615c", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=466b5c3fbf6cfffd5c4a2b6d512422f503422991", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe7bb9fc82541622b4e97e8645b8e6da967f65a8", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=97a4d6c5707d25736322abf5b75ad8d2d74851c5", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4081d26b1866e1569fb8d29f0618e191d061d9a8", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1717b579d7702da6f4216eadedad440f2ac2666", "width": 960, "height": 960}], "variants": {}, "id": "n7I77mKH6Q0sG5S_9xXPTJWYmzVs165z-AMNcpkvBPk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16o9yib", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o9yib/tips_for_creating_an_effective_data_engineering_cv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o9yib/tips_for_creating_an_effective_data_engineering_cv/", "subreddit_subscribers": 129625, "created_utc": 1695283270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_74fdrilk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PyJaws v0.1.7: A Pythonic way of Declaring Databricks Jobs and Workflows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocnvb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1695293333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "pypi.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://pypi.org/project/pyjaws", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16ocnvb", "is_robot_indexable": true, "report_reasons": null, "author": "j0selit0342", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocnvb/pyjaws_v017_a_pythonic_way_of_declaring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://pypi.org/project/pyjaws", "subreddit_subscribers": 129625, "created_utc": 1695293333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm fetching real-time flight data every second and need some way to monitor the script and restart it if it crashes. Script is in python and ideally deploying on a linux server.\n\nIs some sort of process manager what I need, or are there tools better designed for this task?", "author_fullname": "t2_4memz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Robust methods for fetching data from an API every second?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nqx1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695230302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m fetching real-time flight data every second and need some way to monitor the script and restart it if it crashes. Script is in python and ideally deploying on a linux server.&lt;/p&gt;\n\n&lt;p&gt;Is some sort of process manager what I need, or are there tools better designed for this task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nqx1j", "is_robot_indexable": true, "report_reasons": null, "author": "obviouslyCPTobvious", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nqx1j/robust_methods_for_fetching_data_from_an_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nqx1j/robust_methods_for_fetching_data_from_an_api/", "subreddit_subscribers": 129625, "created_utc": 1695230302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mxj2oz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Key Postgres parameters to fine tune", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_16nqw9b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Asd9oQa5sGOUzoOsksnAz_i6SH7by1t_MvrJk27hsog.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695230250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "timescale.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.timescale.com/blog/postgresql-performance-tuning-part-ii-adjusting-key-parameters/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?auto=webp&amp;s=eb7b0c15396d5d0ac7f612355f2f26dcf11fb695", "width": 998, "height": 559}, "resolutions": [{"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c9376299fc42e9760c1dfd29d948abd7995e6d4", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=136b53b9a92ce9bccd00b4cfdacc9ef36bdde4e9", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7fb32af348258c5bd4d7d064e2959cf8beae0198", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=17ba5f1974550257862fc5466fc4f75e25546e7d", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9169b126bda126bcc11777f6d5144efd83ec6a30", "width": 960, "height": 537}], "variants": {}, "id": "9tCt4JMnRg2fVsa7yIYuykSWnRsNyDxO6GjnFhFbT4A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16nqw9b", "is_robot_indexable": true, "report_reasons": null, "author": "carlotasoto", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nqw9b/key_postgres_parameters_to_fine_tune/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.timescale.com/blog/postgresql-performance-tuning-part-ii-adjusting-key-parameters/", "subreddit_subscribers": 129625, "created_utc": 1695230250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Question:** What are some low-code time series manipulation tools available for excel users who can't be trusted to author production ready transformations using SQL or Python?  \n\n\nI'm a software dev building tools for a financial data analyst team. The analyst team has asked me to build a tool for self-serve authoring of time series transformations so analysts can author hundreds of these transformations that power dashboards we deliver to customers.\n\nI plan on using open-source only modern data stack tools, dbt in particular, as I don't see a point in reinventing the wheel. The main obstacle however is my team doesn't want to use SQL or Python as our analysts can't be trusted to code. They complain that SQL is too verbose and ugly for time series, and python is powerful enough that analyst-authored code has introduced production bugs previously. Our analysts are more like excel analysts. Thus i'm looking for low-code tools that would make panel data manipulations (time series &amp; cross-sectional gap fills, bucketing, panel data) fairly trivial to author **and** productionize.  \n\n\nI did some research on the following approaches:\n\n* **Semantic Layers**: This is promising as it compiles to SQL, but it's not clear from documentation if they implement time series features well yet.\n* **Custom DSL**: Instead of a YAML like DSL in semantic layers, I could write a simple lisp-like DSL for manipulating time series that feels more like excel while not allowing advanced features in python. This gives me most control over the prettiness of the language while also giving me the freedom to compile to either SQL or Python or wtv in the future. Our data sets are small enough that we don't need the SQL optimizer necessarily.\n* **Dashboard w/ Code Gen**: It might be possible for a tool like superset to generate the SQL visually instead.\n* **Raw SQL**: I've been holding my ground that data analysts should be expected to write SQL, as it was literally designed for analysts to do business transformations. The syntax is ugly and can get fairly advanced, but it has a proven track record for several decades, with a large ecosystem of tooling. I don't know if I have the level of influence to change an entire org though. I will keep trying to push this concern to higher level mgmt as i think upskilling your workforce while eliminating engineering work is a big win overall.", "author_fullname": "t2_2naya68b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What simplified language to use for time series (panel) data manipulation in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16ojf6c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695311474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What are some low-code time series manipulation tools available for excel users who can&amp;#39;t be trusted to author production ready transformations using SQL or Python?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a software dev building tools for a financial data analyst team. The analyst team has asked me to build a tool for self-serve authoring of time series transformations so analysts can author hundreds of these transformations that power dashboards we deliver to customers.&lt;/p&gt;\n\n&lt;p&gt;I plan on using open-source only modern data stack tools, dbt in particular, as I don&amp;#39;t see a point in reinventing the wheel. The main obstacle however is my team doesn&amp;#39;t want to use SQL or Python as our analysts can&amp;#39;t be trusted to code. They complain that SQL is too verbose and ugly for time series, and python is powerful enough that analyst-authored code has introduced production bugs previously. Our analysts are more like excel analysts. Thus i&amp;#39;m looking for low-code tools that would make panel data manipulations (time series &amp;amp; cross-sectional gap fills, bucketing, panel data) fairly trivial to author &lt;strong&gt;and&lt;/strong&gt; productionize.  &lt;/p&gt;\n\n&lt;p&gt;I did some research on the following approaches:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Semantic Layers&lt;/strong&gt;: This is promising as it compiles to SQL, but it&amp;#39;s not clear from documentation if they implement time series features well yet.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Custom DSL&lt;/strong&gt;: Instead of a YAML like DSL in semantic layers, I could write a simple lisp-like DSL for manipulating time series that feels more like excel while not allowing advanced features in python. This gives me most control over the prettiness of the language while also giving me the freedom to compile to either SQL or Python or wtv in the future. Our data sets are small enough that we don&amp;#39;t need the SQL optimizer necessarily.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dashboard w/ Code Gen&lt;/strong&gt;: It might be possible for a tool like superset to generate the SQL visually instead.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Raw SQL&lt;/strong&gt;: I&amp;#39;ve been holding my ground that data analysts should be expected to write SQL, as it was literally designed for analysts to do business transformations. The syntax is ugly and can get fairly advanced, but it has a proven track record for several decades, with a large ecosystem of tooling. I don&amp;#39;t know if I have the level of influence to change an entire org though. I will keep trying to push this concern to higher level mgmt as i think upskilling your workforce while eliminating engineering work is a big win overall.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ojf6c", "is_robot_indexable": true, "report_reasons": null, "author": "shuaibot", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ojf6c/what_simplified_language_to_use_for_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ojf6c/what_simplified_language_to_use_for_time_series/", "subreddit_subscribers": 129625, "created_utc": 1695311474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi  I'm new to data engineering.\n\nI want to devise a system that loads tabular data (10-20 columns with 10\\^9/10\\^10/ -billions of data records), that will be ingest from eternal API.\n\nThe system should do processing (filtering subset of the rows and make basic computation analysis)\n\nHow to approach this problem? and what are the recommenced tools\n\nDoes best practice of DB like Spark or Snowflake will be sufficient for that task?\n\nand if there are any good resources to gain more knowledge- it will be awesome!\n\nI'm looking for the best of: latency in inference, easy to develop my system, easy to maintain, minimum server requirement, easy and basic monitor.\n\nThanks!\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_5831rdhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to approach designing a novel system that loads tabular data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16oiax5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695308771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi  I&amp;#39;m new to data engineering.&lt;/p&gt;\n\n&lt;p&gt;I want to devise a system that loads tabular data (10-20 columns with 10^9/10^10/ -billions of data records), that will be ingest from eternal API.&lt;/p&gt;\n\n&lt;p&gt;The system should do processing (filtering subset of the rows and make basic computation analysis)&lt;/p&gt;\n\n&lt;p&gt;How to approach this problem? and what are the recommenced tools&lt;/p&gt;\n\n&lt;p&gt;Does best practice of DB like Spark or Snowflake will be sufficient for that task?&lt;/p&gt;\n\n&lt;p&gt;and if there are any good resources to gain more knowledge- it will be awesome!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for the best of: latency in inference, easy to develop my system, easy to maintain, minimum server requirement, easy and basic monitor.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16oiax5", "is_robot_indexable": true, "report_reasons": null, "author": "Expensive_Breakfast6", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16oiax5/how_to_approach_designing_a_novel_system_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16oiax5/how_to_approach_designing_a_novel_system_that/", "subreddit_subscribers": 129625, "created_utc": 1695308771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just published an article on how to setup Zero-ETL integration between an Amazon Aurora database and Redshift.\n\nWith the Zero-ETL feature, you do not need to bother about setting up complex data pipelines in your organisation.\n\n[https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift](https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift)", "author_fullname": "t2_k6fldwtv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How you can set up Zero-ETL data pipeline between Amazon Aurora and Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16of8ak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695300955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just published an article on how to setup Zero-ETL integration between an Amazon Aurora database and Redshift.&lt;/p&gt;\n\n&lt;p&gt;With the Zero-ETL feature, you do not need to bother about setting up complex data pipelines in your organisation.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift\"&gt;https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?auto=webp&amp;s=43854de3c536f86fe8edad9982c3583ab380ad09", "width": 681, "height": 370}, "resolutions": [{"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecdcf6fcf7adb0a98d8aa69dc115ea125539065f", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d049f2c1df9c1ec82935472ae7fbeb1ccbd6e8d", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6807b4b492aac7f299db9cd405289e68a67fe5f8", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ee0039bae6a8d6f694eb79595479c445690476e", "width": 640, "height": 347}], "variants": {}, "id": "C3ODgW2FQeNZnu5nJxonIMIBLmXRwIgb3c7YtEsw3sM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16of8ak", "is_robot_indexable": true, "report_reasons": null, "author": "gbxnga", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16of8ak/how_you_can_set_up_zeroetl_data_pipeline_between/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16of8ak/how_you_can_set_up_zeroetl_data_pipeline_between/", "subreddit_subscribers": 129625, "created_utc": 1695300955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are trying to read a big chunk of small files from several buckets in S3 with Spark. The objective is to merge those files into one and write it in another s3 bucket. \n\n&amp;#x200B;\n\nThe code for read:\n\n    val parquetFiles = Seq(\"s3a://...\", \"s3a://.....\" ..)\n    val df = spark.read.format(\"parquet\").load(parquetFiles:_*)\n\n It takes about 10 minutes to execute the following query:\n\n    df.coalesce(1).write.format(\"parquet\").save(\"s3://...\")\n\nWhile a `df.count()` it takes about 2 minutes (which is also not ok, I guess). \n\nWe've tried changing a lot of configurations from `hadoop.fs.s3a`, but no combination seems to alleviate the time. We cannot clearly understand which task is delaying the execution, but from Spark UI we have seen that not much CPU or Memory is consumed. \n\nMy assumption is that HTTP calls to S3 are getting too expensive. But I am not sure. \n\nHas anyone experienced similar issues? \n\nHave you solved them with conf or is it just a known problem? \n\n&amp;#x200B;\n\nThank you!\n\n&amp;#x200B;", "author_fullname": "t2_ffabopog", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reading small files from S3 with Spark is slow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocm37", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695293163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are trying to read a big chunk of small files from several buckets in S3 with Spark. The objective is to merge those files into one and write it in another s3 bucket. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The code for read:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;val parquetFiles = Seq(&amp;quot;s3a://...&amp;quot;, &amp;quot;s3a://.....&amp;quot; ..)\nval df = spark.read.format(&amp;quot;parquet&amp;quot;).load(parquetFiles:_*)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;It takes about 10 minutes to execute the following query:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df.coalesce(1).write.format(&amp;quot;parquet&amp;quot;).save(&amp;quot;s3://...&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;While a &lt;code&gt;df.count()&lt;/code&gt; it takes about 2 minutes (which is also not ok, I guess). &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve tried changing a lot of configurations from &lt;code&gt;hadoop.fs.s3a&lt;/code&gt;, but no combination seems to alleviate the time. We cannot clearly understand which task is delaying the execution, but from Spark UI we have seen that not much CPU or Memory is consumed. &lt;/p&gt;\n\n&lt;p&gt;My assumption is that HTTP calls to S3 are getting too expensive. But I am not sure. &lt;/p&gt;\n\n&lt;p&gt;Has anyone experienced similar issues? &lt;/p&gt;\n\n&lt;p&gt;Have you solved them with conf or is it just a known problem? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ocm37", "is_robot_indexable": true, "report_reasons": null, "author": "paolapardo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocm37/reading_small_files_from_s3_with_spark_is_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ocm37/reading_small_files_from_s3_with_spark_is_slow/", "subreddit_subscribers": 129625, "created_utc": 1695293163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, \n\nI am wondering If someone of you used open source DVT from google github  - [professional-services-data-validator](https://github.com/GoogleCloudPlatform/professional-services-data-validator).\n\nI need to validate data exported from differences system. I have one folder, where all files are stored and via pipelines are loaded to the BigQuery. I can easily transform these files to the .csv and run the validation. But the problem is with the Filesystem connection. Filesystem connection has argument \"file path\" and connection is created only for one file.\n\nMy question is, with your experience, Do you think its better to create new connection for every file (thousands files) or use python script that will transform and load data to the one file work\\_file.csv that is connected and after the validation delete the file content then take another file and transform and load data to the work\\_file.csv...\n\nI do not have enough experience with these technologies and BI processes so I would be glad for every point you have. \n\nThank you very much.  \nM\n\n&amp;#x200B;", "author_fullname": "t2_79da9w8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Validation via professional-services-data-validator from Google", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocb07", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695292088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;I am wondering If someone of you used open source DVT from google github  - &lt;a href=\"https://github.com/GoogleCloudPlatform/professional-services-data-validator\"&gt;professional-services-data-validator&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I need to validate data exported from differences system. I have one folder, where all files are stored and via pipelines are loaded to the BigQuery. I can easily transform these files to the .csv and run the validation. But the problem is with the Filesystem connection. Filesystem connection has argument &amp;quot;file path&amp;quot; and connection is created only for one file.&lt;/p&gt;\n\n&lt;p&gt;My question is, with your experience, Do you think its better to create new connection for every file (thousands files) or use python script that will transform and load data to the one file work_file.csv that is connected and after the validation delete the file content then take another file and transform and load data to the work_file.csv...&lt;/p&gt;\n\n&lt;p&gt;I do not have enough experience with these technologies and BI processes so I would be glad for every point you have. &lt;/p&gt;\n\n&lt;p&gt;Thank you very much.&lt;br/&gt;\nM&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?auto=webp&amp;s=e21a2042737dc223e67be7ea27e40fc2236d12ee", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c766345a5f83563a2848746e21a43c0ad01a8c7e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80407217416d454d1f32b3b0c1fe41d4ba9dbf87", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ab68f2323e57915f688870703aa3b7827922c10", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3167be568b95344dbbaa19877c37d4bfffb3e688", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f988a35b7c0f9d7945888db9d4ec0e48eddb82d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e44e34cb74d2a495765f7b1cc9148eed809bb28", "width": 1080, "height": 540}], "variants": {}, "id": "Y2KMmNmoflDB_TA4VN4tScfKspEdT0Y_43XDwIeYUzA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ocb07", "is_robot_indexable": true, "report_reasons": null, "author": "Illustrious_Union216", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocb07/data_validation_via/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ocb07/data_validation_via/", "subreddit_subscribers": 129625, "created_utc": 1695292088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering about this for a personal project.", "author_fullname": "t2_imajwwpcb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have a good resource to learn how to set up and use NoSQL databases as quickly as possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o91r5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695279809.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering about this for a personal project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16o91r5", "is_robot_indexable": true, "report_reasons": null, "author": "al-hamal", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o91r5/does_anyone_have_a_good_resource_to_learn_how_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o91r5/does_anyone_have_a_good_resource_to_learn_how_to/", "subreddit_subscribers": 129625, "created_utc": 1695279809.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Few days back I have posted a help in this group and got wonderful comments. \nHere is my update: I have successfully implemented airflow dag to run dbt generic tests, but the airflow dag is picking up and running the dbt models before running the generic yml tests. \nHere I dont want dbt models to run because we have separate dags for that. I only want dbt generic tests to run as part of the airflow dag. \nAny suggestions are appreciated.", "author_fullname": "t2_ke7daufi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running dbt tests with Airflow dag", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o8n9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1695278383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Few days back I have posted a help in this group and got wonderful comments. \nHere is my update: I have successfully implemented airflow dag to run dbt generic tests, but the airflow dag is picking up and running the dbt models before running the generic yml tests. \nHere I dont want dbt models to run because we have separate dags for that. I only want dbt generic tests to run as part of the airflow dag. \nAny suggestions are appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://reddit.com/r/dataengineering/s/pHwzT1IgCw", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16o8n9p", "is_robot_indexable": true, "report_reasons": null, "author": "Long-Neighborhood330", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o8n9p/running_dbt_tests_with_airflow_dag/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://reddit.com/r/dataengineering/s/pHwzT1IgCw", "subreddit_subscribers": 129625, "created_utc": 1695278383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm wondering how people scale batch inference? I'm a Senior Data Analyst and I currently just use a massive VM and parallelize it so I can make predictions concurrently. At first that was fine but now I need to make 10-100x more predictions for my job. I don't have much cloud infrastructure experience and would love to hear how people scale their inference models.\n\nFYI) I'm actively trying to develop a Python package for this but still ideating. All recommendations and thoughts are welcomed.", "author_fullname": "t2_7iyeps3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Do You Scale Batch Inference?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o6f96", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695270753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering how people scale batch inference? I&amp;#39;m a Senior Data Analyst and I currently just use a massive VM and parallelize it so I can make predictions concurrently. At first that was fine but now I need to make 10-100x more predictions for my job. I don&amp;#39;t have much cloud infrastructure experience and would love to hear how people scale their inference models.&lt;/p&gt;\n\n&lt;p&gt;FYI) I&amp;#39;m actively trying to develop a Python package for this but still ideating. All recommendations and thoughts are welcomed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16o6f96", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Post_149", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o6f96/how_do_you_scale_batch_inference/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o6f96/how_do_you_scale_batch_inference/", "subreddit_subscribers": 129625, "created_utc": 1695270753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I will be having a technical interview for the Data Modeler position. They need someone who is familiar with Data Vault 2.0. Can anyone give me tips on what keywords I should mention? Also if you have any resources to prepare that would be very helpful. Thanks to all redditors.", "author_fullname": "t2_44nfhvhnm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Technical Interview Data Vault 2.0", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o5fkb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695267515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I will be having a technical interview for the Data Modeler position. They need someone who is familiar with Data Vault 2.0. Can anyone give me tips on what keywords I should mention? Also if you have any resources to prepare that would be very helpful. Thanks to all redditors.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16o5fkb", "is_robot_indexable": true, "report_reasons": null, "author": "Personal_Tennis_466", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o5fkb/technical_interview_data_vault_20/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o5fkb/technical_interview_data_vault_20/", "subreddit_subscribers": 129625, "created_utc": 1695267515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_xf2t5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data-toolset: Upgrade from avro-tools and parquet-tools jars to a more user-friendly Python package.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_16o4m7a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/y-354gCMniyFSuzUKJ6MspXshe8ut3YAzm9X8lfyoRs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695265043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/luminousmen/data-toolset", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?auto=webp&amp;s=0275760653eb6471989e32ee41bdd0f65611dde6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=469ef1a73b2198c9f1e7c7426347172e55b280c1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=61528f26675044fe6e11d8bbbc1fd48591852412", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=78d54701fbb545a3b8b96e5d80f161dd12314208", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac4314944f920db905b28366c67cae5988ece7ba", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a15a31f790cedbd314369c39431f04b33cab1a0b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/YSZxgn7xHCFC9pn7x26TePyRFKszYukqyxe9rMjW52Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f8dbb411e6eec14d712db9e91a2aa42c967ebcb3", "width": 1080, "height": 540}], "variants": {}, "id": "CnDlsmlGwvqlEdv_LY44Io94g8FnYa6LctUlcwAVAHo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16o4m7a", "is_robot_indexable": true, "report_reasons": null, "author": "luminoumen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o4m7a/datatoolset_upgrade_from_avrotools_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/luminousmen/data-toolset", "subreddit_subscribers": 129625, "created_utc": 1695265043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, does anyone here work or has worked with Looker Studio connecting to Big Query? I want to know how the data sync between the tools works.\n\nMore specifically, does the Big Query connector have a row limit for returning a table? My goal is to bring a table of 50 million rows from Big Query to Looker Studio and then create visualizations from that table. Would that be possible? I saw that there is a 1 million row limit when connecting with Snowflake, so I don't know if it would be possible with Big Query.\n\nThanks!", "author_fullname": "t2_7iesagha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Row limit when plugging Big Query into Looker Studio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nyg13", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695248430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, does anyone here work or has worked with Looker Studio connecting to Big Query? I want to know how the data sync between the tools works.&lt;/p&gt;\n\n&lt;p&gt;More specifically, does the Big Query connector have a row limit for returning a table? My goal is to bring a table of 50 million rows from Big Query to Looker Studio and then create visualizations from that table. Would that be possible? I saw that there is a 1 million row limit when connecting with Snowflake, so I don&amp;#39;t know if it would be possible with Big Query.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nyg13", "is_robot_indexable": true, "report_reasons": null, "author": "Pop-Huge", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nyg13/row_limit_when_plugging_big_query_into_looker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nyg13/row_limit_when_plugging_big_query_into_looker/", "subreddit_subscribers": 129625, "created_utc": 1695248430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In simple terms, should you organise your code into repos that are by function e.g.:\n - one repo for all your python code for spark / serverless functions\n - one repo for all your sql views (dbt kinda code)\n - one repo for all your orchestration yamls\nwhere you have the benefit that each repo centralises and can benefit from living together e.g. dbt models all centralised and can refer to one another\n\nOr, should you organise it by outcome i.e. all code for a pipeline lives together e.g. in a single repo you'll have:\n - python code for extraction\n - code for sql views\n - python transformation code\n - orchestration yaml\n\nContext: small start up looking to scale from simple beginnings with serverless functions + scheduling to a more robust setup that can handle more complicated pipelines.\n\nThe latter makes more sense to me but want to ask for opinion incase it's a clear mistake e.g. we hit the next stage in scaling up and this current idea results in new complexity I haven't thought about yet.", "author_fullname": "t2_576k5sn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to organise pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nxfk0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695246063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In simple terms, should you organise your code into repos that are by function e.g.:\n - one repo for all your python code for spark / serverless functions\n - one repo for all your sql views (dbt kinda code)\n - one repo for all your orchestration yamls\nwhere you have the benefit that each repo centralises and can benefit from living together e.g. dbt models all centralised and can refer to one another&lt;/p&gt;\n\n&lt;p&gt;Or, should you organise it by outcome i.e. all code for a pipeline lives together e.g. in a single repo you&amp;#39;ll have:\n - python code for extraction\n - code for sql views\n - python transformation code\n - orchestration yaml&lt;/p&gt;\n\n&lt;p&gt;Context: small start up looking to scale from simple beginnings with serverless functions + scheduling to a more robust setup that can handle more complicated pipelines.&lt;/p&gt;\n\n&lt;p&gt;The latter makes more sense to me but want to ask for opinion incase it&amp;#39;s a clear mistake e.g. we hit the next stage in scaling up and this current idea results in new complexity I haven&amp;#39;t thought about yet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nxfk0", "is_robot_indexable": true, "report_reasons": null, "author": "mjam03", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nxfk0/how_to_organise_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nxfk0/how_to_organise_pipelines/", "subreddit_subscribers": 129625, "created_utc": 1695246063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a big data app, that transforms a dozen of companies data (through the same pipeline) that I wrote in Python. I want to schedule and orchestrate the app now.\n\nRn I am just using windows scheduler for scheduling which is very dumb and naive. I thought of orchestrating it manually, essentially making a table in SQL which keeps track of each days run and downstream tasks would query the table to know the progress and run accordingly. But I was thinking of learning a new tool to do it, instead of doing this way.\n\nMy situation:\n- App is deployed in an Azure VM running windows(could move to Linux)\n- Org have the complete Azure suite so can leverage that\n- I am open to solutions from other languages, not sticking hard to Python.\n\n\nWhich tool would you recommend for it (feel free to suggest others as well):\n\n[View Poll](https://www.reddit.com/poll/16nx5py)", "author_fullname": "t2_7g84dv87", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions for Orchestration tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nx5py", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695245457.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a big data app, that transforms a dozen of companies data (through the same pipeline) that I wrote in Python. I want to schedule and orchestrate the app now.&lt;/p&gt;\n\n&lt;p&gt;Rn I am just using windows scheduler for scheduling which is very dumb and naive. I thought of orchestrating it manually, essentially making a table in SQL which keeps track of each days run and downstream tasks would query the table to know the progress and run accordingly. But I was thinking of learning a new tool to do it, instead of doing this way.&lt;/p&gt;\n\n&lt;p&gt;My situation:\n- App is deployed in an Azure VM running windows(could move to Linux)\n- Org have the complete Azure suite so can leverage that\n- I am open to solutions from other languages, not sticking hard to Python.&lt;/p&gt;\n\n&lt;p&gt;Which tool would you recommend for it (feel free to suggest others as well):&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/16nx5py\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nx5py", "is_robot_indexable": true, "report_reasons": null, "author": "sepiolGoddess", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1695763857749, "options": [{"text": "Airflow", "id": "24887402"}, {"text": "Prefect", "id": "24887403"}, {"text": "Dagster", "id": "24887404"}, {"text": "ADF", "id": "24887405"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 141, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nx5py/suggestions_for_orchestration_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/16nx5py/suggestions_for_orchestration_tools/", "subreddit_subscribers": 129625, "created_utc": 1695245457.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are thinking about using FastAPI in order to get a more event driven data orch. system. \n\nWhen a pipeline really starts with f.ex. a CSV file being posted to FastAPI service, I would want the Dagster logger to reflect that. Can we? ", "author_fullname": "t2_7no9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to write to the Dagster logger", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nrmdh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695232026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are thinking about using FastAPI in order to get a more event driven data orch. system. &lt;/p&gt;\n\n&lt;p&gt;When a pipeline really starts with f.ex. a CSV file being posted to FastAPI service, I would want the Dagster logger to reflect that. Can we? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nrmdh", "is_robot_indexable": true, "report_reasons": null, "author": "YourOldBuddy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nrmdh/how_to_write_to_the_dagster_logger/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nrmdh/how_to_write_to_the_dagster_logger/", "subreddit_subscribers": 129625, "created_utc": 1695232026.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}