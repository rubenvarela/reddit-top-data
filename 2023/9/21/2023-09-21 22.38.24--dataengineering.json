{"kind": "Listing", "data": {"after": "t3_16o8n9p", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": ".", "author_fullname": "t2_bsuu4apm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Pareto Principle - what is the 20% (algos, functions, libraries) that lets you develop 80% of code related to Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o883v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 75, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 75, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695276875.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16o883v", "is_robot_indexable": true, "report_reasons": null, "author": "CrimsonMentone30", "discussion_type": null, "num_comments": 58, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o883v/python_pareto_principle_what_is_the_20_algos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o883v/python_pareto_principle_what_is_the_20_algos/", "subreddit_subscribers": 129696, "created_utc": 1695276875.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Preparing for an engineering interview can be overwhelming, LeetCode, System Design, Behavioral questions, its a lot to manage. If you are preparing for a **#dataengieering** interview, give my free app a try. It gives you 2 tasks a day for 30 days to help you prepare.\n\nAlso I wanted to learn Swift.\n\nhttps://apps.apple.com/app/interview-ace/id6465748534", "author_fullname": "t2_4fpl974m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made a free app to help you prepare for the data engineering interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o5d6o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695267311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Preparing for an engineering interview can be overwhelming, LeetCode, System Design, Behavioral questions, its a lot to manage. If you are preparing for a &lt;strong&gt;#dataengieering&lt;/strong&gt; interview, give my free app a try. It gives you 2 tasks a day for 30 days to help you prepare.&lt;/p&gt;\n\n&lt;p&gt;Also I wanted to learn Swift.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://apps.apple.com/app/interview-ace/id6465748534\"&gt;https://apps.apple.com/app/interview-ace/id6465748534&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?auto=webp&amp;s=86f11be447465164e06c1895f371f8247b3de78e", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9de58037059431fcc82b29fec49b0b89fd33aaed", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9ca9694b7fac863b314527c7f015ec557450634", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=974fe41036a7cd50cac5039677476f1175904ee7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0a246610bdf304e619347a7ace86b9bacea4e0d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=96029c532b8c99d9d8ae6e66965bfd26dc47cb50", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/YFzT_CWNwiPVZMNnMgLWlWd8XiMOqYpJoPJ86b8Cdzs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=48172f9d876603790b702ec84e57e0e6b4cff7da", "width": 1080, "height": 567}], "variants": {}, "id": "5YmFqnaQR8uybNXWtDGGlBlmqp-5bnkiVElQWM1Gehw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16o5d6o", "is_robot_indexable": true, "report_reasons": null, "author": "coyne_operated", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o5d6o/i_made_a_free_app_to_help_you_prepare_for_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o5d6o/i_made_a_free_app_to_help_you_prepare_for_the/", "subreddit_subscribers": 129696, "created_utc": 1695267311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like every new trend in Data it has become the norm for people to invent terms. This one is the latest one that people are talking about.\n\nSchema management is not new and nobody in a team does so frequent changes that you need a GitHub like infrastructure to settle on a schema.\n\nIf we want versioning, as you would do in a REST world with breaking changes, that\u2019s a better approach and more formalized way to do change. I feel like the principles of SWE are jammed into the data world with no forethought.\n\nThis concept has been made way too complicated than it needs to be. I think we need some level setting here. \n\nWhat do you think?", "author_fullname": "t2_fy6g58f7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data contracts is a buzzword", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16on32m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695320376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like every new trend in Data it has become the norm for people to invent terms. This one is the latest one that people are talking about.&lt;/p&gt;\n\n&lt;p&gt;Schema management is not new and nobody in a team does so frequent changes that you need a GitHub like infrastructure to settle on a schema.&lt;/p&gt;\n\n&lt;p&gt;If we want versioning, as you would do in a REST world with breaking changes, that\u2019s a better approach and more formalized way to do change. I feel like the principles of SWE are jammed into the data world with no forethought.&lt;/p&gt;\n\n&lt;p&gt;This concept has been made way too complicated than it needs to be. I think we need some level setting here. &lt;/p&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16on32m", "is_robot_indexable": true, "report_reasons": null, "author": "WarriorData", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16on32m/data_contracts_is_a_buzzword/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16on32m/data_contracts_is_a_buzzword/", "subreddit_subscribers": 129696, "created_utc": 1695320376.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is looking into switching from SSIS-based ETL process to a Python and T-SQL to create data pipelines. We work in a classified environment, which restricts access to non-US based software (no Pycharm) and causes other problems with account privileges. Privileges we can work on, slowly but surely. The scheduling process is where we aren't certain what to do. I mentioned Airflow, but it would require a hefty vetting process to get into each server, even the unclassified environment. These classified environments do not have access to the internet either, for obvious reasons.\n\nDoes anyone have experience doing DE work in a SCIF?\n\nIf so, how did you go about the scheduling process for Python/T-SQL?\n\nI'm welcome to other possibilities and ideas.\n\nEDIT 1: I'm very grateful for the many responses. I'll just summarize some of my responses here. Each environment comes with Anaconda, thus it comes with any modules that are pre-packaged with Anaconda. Additional modules can be tested in our open environment then vetted before moving them to our closed environments. Windows Task Scheduler seems to be a simple solution that \\*should\\* be readily available in each of our networks. Airflow or Databricks would be nice but would likely require a rigorous vetting process. ", "author_fullname": "t2_aiumhm29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Scheduler in a Closed Environment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nzm2s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695315323.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695251381.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is looking into switching from SSIS-based ETL process to a Python and T-SQL to create data pipelines. We work in a classified environment, which restricts access to non-US based software (no Pycharm) and causes other problems with account privileges. Privileges we can work on, slowly but surely. The scheduling process is where we aren&amp;#39;t certain what to do. I mentioned Airflow, but it would require a hefty vetting process to get into each server, even the unclassified environment. These classified environments do not have access to the internet either, for obvious reasons.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience doing DE work in a SCIF?&lt;/p&gt;\n\n&lt;p&gt;If so, how did you go about the scheduling process for Python/T-SQL?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m welcome to other possibilities and ideas.&lt;/p&gt;\n\n&lt;p&gt;EDIT 1: I&amp;#39;m very grateful for the many responses. I&amp;#39;ll just summarize some of my responses here. Each environment comes with Anaconda, thus it comes with any modules that are pre-packaged with Anaconda. Additional modules can be tested in our open environment then vetted before moving them to our closed environments. Windows Task Scheduler seems to be a simple solution that *should* be readily available in each of our networks. Airflow or Databricks would be nice but would likely require a rigorous vetting process. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nzm2s", "is_robot_indexable": true, "report_reasons": null, "author": "Upbeat_Count_7568", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nzm2s/python_scheduler_in_a_closed_environment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nzm2s/python_scheduler_in_a_closed_environment/", "subreddit_subscribers": 129696, "created_utc": 1695251381.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am storing over 200gb of data on mysql, I am considering  to migrate it into a more effective dbms (or just keep using mysql?):\n- Data durability is optional (this table is defered from another source of truth so I can regeneration it).\n- Not in-memory db (I can not afford to have 200GB memory)\n- Read instensive, infrequently batched update.\n- the query is very simple (select * from table where key1 &gt; key2 and key3 &gt; const ) . don't need to join multiple table or doing aggregation \n- Support range query, index\n- Effective data storage (I don't want it to turn into a 1TB data)\n- Easy to deploy (only need to run on single node via docker)", "author_fullname": "t2_bju0j9jyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choose me a database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocikg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695292819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am storing over 200gb of data on mysql, I am considering  to migrate it into a more effective dbms (or just keep using mysql?):\n- Data durability is optional (this table is defered from another source of truth so I can regeneration it).\n- Not in-memory db (I can not afford to have 200GB memory)\n- Read instensive, infrequently batched update.\n- the query is very simple (select * from table where key1 &amp;gt; key2 and key3 &amp;gt; const ) . don&amp;#39;t need to join multiple table or doing aggregation \n- Support range query, index\n- Effective data storage (I don&amp;#39;t want it to turn into a 1TB data)\n- Easy to deploy (only need to run on single node via docker)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ocikg", "is_robot_indexable": true, "report_reasons": null, "author": "chu_nghia_nam_thang", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocikg/choose_me_a_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ocikg/choose_me_a_database/", "subreddit_subscribers": 129696, "created_utc": 1695292819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just found out my boss used ChatGPT to write our department goals for data engineering. Mainly because she doesn\u2019t actually understand technology at all.\n\nFor 2024 our ChatGPT overlord demands we:\n\n* Implement an Enterprise data lake\n* Move all existing applications to AWS\n* Implement a master data management solution\n* Implement a self-service model with easy-to-use data catalogs\n* Implement an AI solution for self-service (is ChatGPT trying to reproduce?!!!)\n\nTeam size? 4 engineers.\n\nAnyone else dealing with this kind of craziness?", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ChatGPT for goals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16okwxv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695315146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just found out my boss used ChatGPT to write our department goals for data engineering. Mainly because she doesn\u2019t actually understand technology at all.&lt;/p&gt;\n\n&lt;p&gt;For 2024 our ChatGPT overlord demands we:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Implement an Enterprise data lake&lt;/li&gt;\n&lt;li&gt;Move all existing applications to AWS&lt;/li&gt;\n&lt;li&gt;Implement a master data management solution&lt;/li&gt;\n&lt;li&gt;Implement a self-service model with easy-to-use data catalogs&lt;/li&gt;\n&lt;li&gt;Implement an AI solution for self-service (is ChatGPT trying to reproduce?!!!)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Team size? 4 engineers.&lt;/p&gt;\n\n&lt;p&gt;Anyone else dealing with this kind of craziness?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16okwxv", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16okwxv/chatgpt_for_goals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16okwxv/chatgpt_for_goals/", "subreddit_subscribers": 129696, "created_utc": 1695315146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3i0xn3gy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Motherduck is now free for anyone to sign up for", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_16ofd3b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/d_Y3RLZe_KhTsbyenw-vB_8759rZH3XEztzDvH0aVwk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695301324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "motherduck.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://motherduck.com/blog/motherduck-open-for-all-with-series-b/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?auto=webp&amp;s=8e4e22764c7020df061de59726b1fae1cc0fb7cb", "width": 3200, "height": 1672}, "resolutions": [{"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2636a7232b1fe7525a4d5b1251afa5094b5fa5de", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3903f2fd35776377e83f5d2bf5f292b42daf81c4", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fb04ac59ca8b94c744f0c29a0ca7dbc6a9da4818", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da889478bbaa7a40a819fc79e6ddba2c729545c5", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c3ce0a20370d11fef048e9f7fc5503706c8ba38b", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=69a2759f51a49739e14139f0efc138621dbc9d46", "width": 1080, "height": 564}], "variants": {}, "id": "kv9OeUq5aTu0KwswK_YR0kPeKOsiIRZ02HxYtgwWFKc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16ofd3b", "is_robot_indexable": true, "report_reasons": null, "author": "ddanieltan", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ofd3b/motherduck_is_now_free_for_anyone_to_sign_up_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://motherduck.com/blog/motherduck-open-for-all-with-series-b/", "subreddit_subscribers": 129696, "created_utc": 1695301324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, someone recently asked to review their CV and after replying I got a lot of you asking for a review. I did all of you, and now sharing what I think are some useful tips to improve your CV.  Use these if you are applying but nobody is getting back to you.\n\n\nOf course use your own judgment and adapt to your local market and requirements, but here are the top level tips: [Article link](https://dlthub.com/docs/blog/data-engineering-cv) \n\nIf you think there are more useful tips, go ahead and do a PR so others where I distribute this article can also benefit. [Github link](https://github.com/dlt-hub/dlt/blob/devel/docs/website/blog/2023-09-20-data-engineering-cv.md)", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for creating an effective data engineering CV", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o9yib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695283270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, someone recently asked to review their CV and after replying I got a lot of you asking for a review. I did all of you, and now sharing what I think are some useful tips to improve your CV.  Use these if you are applying but nobody is getting back to you.&lt;/p&gt;\n\n&lt;p&gt;Of course use your own judgment and adapt to your local market and requirements, but here are the top level tips: &lt;a href=\"https://dlthub.com/docs/blog/data-engineering-cv\"&gt;Article link&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;If you think there are more useful tips, go ahead and do a PR so others where I distribute this article can also benefit. &lt;a href=\"https://github.com/dlt-hub/dlt/blob/devel/docs/website/blog/2023-09-20-data-engineering-cv.md\"&gt;Github link&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?auto=webp&amp;s=5138f6a644eb11025e1e169629ec855c86ee615c", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=466b5c3fbf6cfffd5c4a2b6d512422f503422991", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe7bb9fc82541622b4e97e8645b8e6da967f65a8", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=97a4d6c5707d25736322abf5b75ad8d2d74851c5", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4081d26b1866e1569fb8d29f0618e191d061d9a8", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/nkUXwhmgV4Z5B2to9N9ic5VUtbBfT1UW7Xv0wTwnOMQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1717b579d7702da6f4216eadedad440f2ac2666", "width": 960, "height": 960}], "variants": {}, "id": "n7I77mKH6Q0sG5S_9xXPTJWYmzVs165z-AMNcpkvBPk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16o9yib", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o9yib/tips_for_creating_an_effective_data_engineering_cv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o9yib/tips_for_creating_an_effective_data_engineering_cv/", "subreddit_subscribers": 129696, "created_utc": 1695283270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have 4 YOE. Current company is great but I\u2019ve plateau\u2019d. Is there a list of employers in the states that is heavily focused on Python, Docker, AWS Snowflake, Airflow? I\u2019ve had experience with many technologies and languages. Over the years I learned that I LOVE writing Python or anything Python related. Any way I can find employers that seek stack heavily focused on Python, AWS, Snowflake, Airflow, Docker? Preferably in the U.S?\nWhich data engineering titles would help me find what I\u2019m looking for? Or how can I find such roles/employers?", "author_fullname": "t2_83p02r6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good Companies that use: Python, AWS, Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oocc3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695323297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have 4 YOE. Current company is great but I\u2019ve plateau\u2019d. Is there a list of employers in the states that is heavily focused on Python, Docker, AWS Snowflake, Airflow? I\u2019ve had experience with many technologies and languages. Over the years I learned that I LOVE writing Python or anything Python related. Any way I can find employers that seek stack heavily focused on Python, AWS, Snowflake, Airflow, Docker? Preferably in the U.S?\nWhich data engineering titles would help me find what I\u2019m looking for? Or how can I find such roles/employers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16oocc3", "is_robot_indexable": true, "report_reasons": null, "author": "1337codethrow", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16oocc3/good_companies_that_use_python_aws_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16oocc3/good_companies_that_use_python_aws_snowflake/", "subreddit_subscribers": 129696, "created_utc": 1695323297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nCurrently building a silver level streaming table from a bronze level streaming source, but not getting the append behavior I'm expecting. When running simple aggregations (min, max, count, etc..), my target silver table keeps being overwritten. For context this DLT pipeline is scheduled to run every 20 minutes, and the expected behavior is to have each 20 minutes worth of data be aggregated and then appended to the target table. Why is my target table being overwritten and not appended to?\n\nThe below query just overwrites \\`example\\_silver\\_table\\` every time the pipeline is run:\n\n    CREATE OR REFRESH STREAMING LIVE TABLE example_silver_table\n    AS \n        SELECT\n            dimension_1,\n            dimension_2,\n            MAX(dimension_3) AS max_dimension_3,\n            SUM(dimension_4) AS dimension_4_total\n        FROM STREAM(LIVE.fact_bronze_table_event)\n        GROUP BY\n            dimension_1,\n            dimension_2\n\nI also tried including a watermark, but not sure I'm implementing it correctly. Every 20 minutes when the pipeline runs, the below query output is 0 records written to \\`example\\_silver\\_table\\` :\n\n    CREATE OR REFRESH STREAMING LIVE TABLE example_silver_table\n    AS \n        SELECT\n            dimension_1,\n            dimension_2,\n            MAX(dimension_3) AS max_dimension_3,\n            SUM(dimension_4) AS dimension_4_total\n        FROM STREAM(LIVE.fact_bronze_table_event) WATERMARK timestamp_field AS event_time DELAY OF INTERVAL 20 MINUTE\n        GROUP BY\n            dimension_1,\n            dimension_2,\n            WINDOW('event_time', '20 minutes')\n\nTLDR;\n\nWhen trying to do aggregations on streaming (20 minute batches) data, I can't get the target table to stop being overwritten every pipeline run. Tried using watermarks, but no luck. Any help or guidance would be much appreciated!!\n\nResources I've tried using, but no luck:\n\n* [Feature Deep Dive: Watermarking in Apache Spark Structured Streaming](https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html)\n* [Apply watermarks to control data processing thresholds](https://docs.databricks.com/en/structured-streaming/watermarks.html)\n* [Event-time Aggregation and Watermarking in Apache Spark\u2019s Structured Streaming](https://www.databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html)", "author_fullname": "t2_3i1tg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Aggregate Streaming Data With Databricks Delta Live Tables (DLT)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o3pat", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695262481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently building a silver level streaming table from a bronze level streaming source, but not getting the append behavior I&amp;#39;m expecting. When running simple aggregations (min, max, count, etc..), my target silver table keeps being overwritten. For context this DLT pipeline is scheduled to run every 20 minutes, and the expected behavior is to have each 20 minutes worth of data be aggregated and then appended to the target table. Why is my target table being overwritten and not appended to?&lt;/p&gt;\n\n&lt;p&gt;The below query just overwrites `example_silver_table` every time the pipeline is run:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE OR REFRESH STREAMING LIVE TABLE example_silver_table\nAS \n    SELECT\n        dimension_1,\n        dimension_2,\n        MAX(dimension_3) AS max_dimension_3,\n        SUM(dimension_4) AS dimension_4_total\n    FROM STREAM(LIVE.fact_bronze_table_event)\n    GROUP BY\n        dimension_1,\n        dimension_2\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I also tried including a watermark, but not sure I&amp;#39;m implementing it correctly. Every 20 minutes when the pipeline runs, the below query output is 0 records written to `example_silver_table` :&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE OR REFRESH STREAMING LIVE TABLE example_silver_table\nAS \n    SELECT\n        dimension_1,\n        dimension_2,\n        MAX(dimension_3) AS max_dimension_3,\n        SUM(dimension_4) AS dimension_4_total\n    FROM STREAM(LIVE.fact_bronze_table_event) WATERMARK timestamp_field AS event_time DELAY OF INTERVAL 20 MINUTE\n    GROUP BY\n        dimension_1,\n        dimension_2,\n        WINDOW(&amp;#39;event_time&amp;#39;, &amp;#39;20 minutes&amp;#39;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;TLDR;&lt;/p&gt;\n\n&lt;p&gt;When trying to do aggregations on streaming (20 minute batches) data, I can&amp;#39;t get the target table to stop being overwritten every pipeline run. Tried using watermarks, but no luck. Any help or guidance would be much appreciated!!&lt;/p&gt;\n\n&lt;p&gt;Resources I&amp;#39;ve tried using, but no luck:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html\"&gt;Feature Deep Dive: Watermarking in Apache Spark Structured Streaming&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://docs.databricks.com/en/structured-streaming/watermarks.html\"&gt;Apply watermarks to control data processing thresholds&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\"&gt;Event-time Aggregation and Watermarking in Apache Spark\u2019s Structured Streaming&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?auto=webp&amp;s=baea0bdbfae95e881ea0a2f0c3b33954b1f12ee9", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5149cb676e163de6e1c8f6c468e674e2e1eedccf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ffe2f685b75ffa31d46101223d2e85d396c11a2c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1e57ce0d9b65813a43715d49944408d42572108", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6218b26dffaec8bb109ecc5069aa1cdf4fe9af22", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=876e8e3df516ab9110f4d59af591fd3b01d003e4", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/V7LspbF8vR6EPa39oesKrpVZPNCH_5j3hdrUiK8ilx8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c649702fc9a45301838bb4e09431f6fd1f4a9ec6", "width": 1080, "height": 565}], "variants": {}, "id": "0LSNAjx4YTAOOrcgDeHl6XfnklsWXU_JL9sKej5JZLo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16o3pat", "is_robot_indexable": true, "report_reasons": null, "author": "Shatonmedeek", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o3pat/how_to_aggregate_streaming_data_with_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o3pat/how_to_aggregate_streaming_data_with_databricks/", "subreddit_subscribers": 129696, "created_utc": 1695262481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_74fdrilk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PyJaws v0.1.7: A Pythonic way of Declaring Databricks Jobs and Workflows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocnvb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1695293333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "pypi.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://pypi.org/project/pyjaws", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16ocnvb", "is_robot_indexable": true, "report_reasons": null, "author": "j0selit0342", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocnvb/pyjaws_v017_a_pythonic_way_of_declaring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://pypi.org/project/pyjaws", "subreddit_subscribers": 129696, "created_utc": 1695293333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I will be having a technical interview for the Data Modeler position. They need someone who is familiar with Data Vault 2.0. Can anyone give me tips on what keywords I should mention? Also if you have any resources to prepare that would be very helpful. Thanks to all redditors.", "author_fullname": "t2_44nfhvhnm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Technical Interview Data Vault 2.0", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o5fkb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695267515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I will be having a technical interview for the Data Modeler position. They need someone who is familiar with Data Vault 2.0. Can anyone give me tips on what keywords I should mention? Also if you have any resources to prepare that would be very helpful. Thanks to all redditors.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16o5fkb", "is_robot_indexable": true, "report_reasons": null, "author": "Personal_Tennis_466", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o5fkb/technical_interview_data_vault_20/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16o5fkb/technical_interview_data_vault_20/", "subreddit_subscribers": 129696, "created_utc": 1695267515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to start learning how to use sql through python along with pandas, etc. I was hoping you guys could tell me whats the most common option for this in the industry as of now. I saw some say SQLalchemy.", "author_fullname": "t2_8mn3m0sl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tool/framework to pick up for SQL in python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16or26x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695329653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to start learning how to use sql through python along with pandas, etc. I was hoping you guys could tell me whats the most common option for this in the industry as of now. I saw some say SQLalchemy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16or26x", "is_robot_indexable": true, "report_reasons": null, "author": "PurpVan", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16or26x/best_toolframework_to_pick_up_for_sql_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16or26x/best_toolframework_to_pick_up_for_sql_in_python/", "subreddit_subscribers": 129696, "created_utc": 1695329653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any good recommendations on learning how to design a data architecture for a data science project or an enterprise?", "author_fullname": "t2_61mc4jcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Designing a Data Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16or0j9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695329543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any good recommendations on learning how to design a data architecture for a data science project or an enterprise?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16or0j9", "is_robot_indexable": true, "report_reasons": null, "author": "prtkkr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16or0j9/designing_a_data_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16or0j9/designing_a_data_architecture/", "subreddit_subscribers": 129696, "created_utc": 1695329543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've gained experience with Google Cloud Platform (GCP) and explored its fundamental services. However, I've noticed that many job opportunities in this field prefer or require AWS expertise. As a result, I've recently set up a free tier AWS account.\n\nPreviously, I had been using GCP with the free credits provided by Google for three months, following tutorials tailored for GCP. This made my experience smooth and easy. The challenge now is that AWS is entirely new to me. I'm seeking guidance on finding resources or platforms where I can discover equivalent tools and services to what I used in GCP.\n\nI'm not completely shifting from GCP to AWS; rather, I want to gain a similar level of exposure to AWS to enhance my qualifications. Any advice on this transition would be greatly appreciated.", "author_fullname": "t2_430i2d0p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switch from GCP to AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16omzke", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695320144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve gained experience with Google Cloud Platform (GCP) and explored its fundamental services. However, I&amp;#39;ve noticed that many job opportunities in this field prefer or require AWS expertise. As a result, I&amp;#39;ve recently set up a free tier AWS account.&lt;/p&gt;\n\n&lt;p&gt;Previously, I had been using GCP with the free credits provided by Google for three months, following tutorials tailored for GCP. This made my experience smooth and easy. The challenge now is that AWS is entirely new to me. I&amp;#39;m seeking guidance on finding resources or platforms where I can discover equivalent tools and services to what I used in GCP.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not completely shifting from GCP to AWS; rather, I want to gain a similar level of exposure to AWS to enhance my qualifications. Any advice on this transition would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16omzke", "is_robot_indexable": true, "report_reasons": null, "author": "Blanco04", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16omzke/switch_from_gcp_to_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16omzke/switch_from_gcp_to_aws/", "subreddit_subscribers": 129696, "created_utc": 1695320144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently saw a job for a Golang Data Engineer and I was surprised because I don\u2019t remember hearing about any libraries in Go for handling data like how Python has Pandas, Polars, etc. If you\u2019ve used Go for data engineering how are you using it? \n\nI\u2019m a Data Engineer and I wanna learn Go for backend development but if theres some libraries like Polars or Pandas in Go for dealing with data-frames I would love to learn that as well.", "author_fullname": "t2_8wpw0e1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Golang in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16omrba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695319588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently saw a job for a Golang Data Engineer and I was surprised because I don\u2019t remember hearing about any libraries in Go for handling data like how Python has Pandas, Polars, etc. If you\u2019ve used Go for data engineering how are you using it? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m a Data Engineer and I wanna learn Go for backend development but if theres some libraries like Polars or Pandas in Go for dealing with data-frames I would love to learn that as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16omrba", "is_robot_indexable": true, "report_reasons": null, "author": "khaili109", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16omrba/golang_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16omrba/golang_in_data_engineering/", "subreddit_subscribers": 129696, "created_utc": 1695319588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Question:** What are some low-code time series manipulation tools available for excel users who can't be trusted to author production ready transformations using SQL or Python?  \n\n\nI'm a software dev building tools for a financial data analyst team. The analyst team has asked me to build a tool for self-serve authoring of time series transformations so analysts can author hundreds of these transformations that power dashboards we deliver to customers.\n\nI plan on using open-source only modern data stack tools, dbt in particular, as I don't see a point in reinventing the wheel. The main obstacle however is my team doesn't want to use SQL or Python as our analysts can't be trusted to code. They complain that SQL is too verbose and ugly for time series, and python is powerful enough that analyst-authored code has introduced production bugs previously. Our analysts are more like excel analysts. Thus i'm looking for low-code tools that would make panel data manipulations (time series &amp; cross-sectional gap fills, bucketing, panel data) fairly trivial to author **and** productionize.  \n\n\nI did some research on the following approaches:\n\n* **Semantic Layers**: This is promising as it compiles to SQL, but it's not clear from documentation if they implement time series features well yet.\n* **Custom DSL**: Instead of a YAML like DSL in semantic layers, I could write a simple lisp-like DSL for manipulating time series that feels more like excel while not allowing advanced features in python. This gives me most control over the prettiness of the language while also giving me the freedom to compile to either SQL or Python or wtv in the future. Our data sets are small enough that we don't need the SQL optimizer necessarily.\n* **Dashboard w/ Code Gen**: It might be possible for a tool like superset to generate the SQL visually instead.\n* **Raw SQL**: I've been holding my ground that data analysts should be expected to write SQL, as it was literally designed for analysts to do business transformations. The syntax is ugly and can get fairly advanced, but it has a proven track record for several decades, with a large ecosystem of tooling. I don't know if I have the level of influence to change an entire org though. I will keep trying to push this concern to higher level mgmt as i think upskilling your workforce while eliminating engineering work is a big win overall.", "author_fullname": "t2_2naya68b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What simplified language to use for time series (panel) data manipulation in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ojf6c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695311474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What are some low-code time series manipulation tools available for excel users who can&amp;#39;t be trusted to author production ready transformations using SQL or Python?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a software dev building tools for a financial data analyst team. The analyst team has asked me to build a tool for self-serve authoring of time series transformations so analysts can author hundreds of these transformations that power dashboards we deliver to customers.&lt;/p&gt;\n\n&lt;p&gt;I plan on using open-source only modern data stack tools, dbt in particular, as I don&amp;#39;t see a point in reinventing the wheel. The main obstacle however is my team doesn&amp;#39;t want to use SQL or Python as our analysts can&amp;#39;t be trusted to code. They complain that SQL is too verbose and ugly for time series, and python is powerful enough that analyst-authored code has introduced production bugs previously. Our analysts are more like excel analysts. Thus i&amp;#39;m looking for low-code tools that would make panel data manipulations (time series &amp;amp; cross-sectional gap fills, bucketing, panel data) fairly trivial to author &lt;strong&gt;and&lt;/strong&gt; productionize.  &lt;/p&gt;\n\n&lt;p&gt;I did some research on the following approaches:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Semantic Layers&lt;/strong&gt;: This is promising as it compiles to SQL, but it&amp;#39;s not clear from documentation if they implement time series features well yet.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Custom DSL&lt;/strong&gt;: Instead of a YAML like DSL in semantic layers, I could write a simple lisp-like DSL for manipulating time series that feels more like excel while not allowing advanced features in python. This gives me most control over the prettiness of the language while also giving me the freedom to compile to either SQL or Python or wtv in the future. Our data sets are small enough that we don&amp;#39;t need the SQL optimizer necessarily.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dashboard w/ Code Gen&lt;/strong&gt;: It might be possible for a tool like superset to generate the SQL visually instead.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Raw SQL&lt;/strong&gt;: I&amp;#39;ve been holding my ground that data analysts should be expected to write SQL, as it was literally designed for analysts to do business transformations. The syntax is ugly and can get fairly advanced, but it has a proven track record for several decades, with a large ecosystem of tooling. I don&amp;#39;t know if I have the level of influence to change an entire org though. I will keep trying to push this concern to higher level mgmt as i think upskilling your workforce while eliminating engineering work is a big win overall.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ojf6c", "is_robot_indexable": true, "report_reasons": null, "author": "shuaibot", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ojf6c/what_simplified_language_to_use_for_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ojf6c/what_simplified_language_to_use_for_time_series/", "subreddit_subscribers": 129696, "created_utc": 1695311474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are trying to read a big chunk of small files from several buckets in S3 with Spark. The objective is to merge those files into one and write it in another s3 bucket. \n\n&amp;#x200B;\n\nThe code for read:\n\n    val parquetFiles = Seq(\"s3a://...\", \"s3a://.....\" ..)\n    val df = spark.read.format(\"parquet\").load(parquetFiles:_*)\n\n It takes about 10 minutes to execute the following query:\n\n    df.coalesce(1).write.format(\"parquet\").save(\"s3://...\")\n\nWhile a `df.count()` it takes about 2 minutes (which is also not ok, I guess). \n\nWe've tried changing a lot of configurations from `hadoop.fs.s3a`, but no combination seems to alleviate the time. We cannot clearly understand which task is delaying the execution, but from Spark UI we have seen that not much CPU or Memory is consumed. \n\nMy assumption is that HTTP calls to S3 are getting too expensive. But I am not sure. \n\nHas anyone experienced similar issues? \n\nHave you solved them with conf or is it just a known problem? \n\n&amp;#x200B;\n\nThank you!\n\n&amp;#x200B;", "author_fullname": "t2_ffabopog", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reading small files from S3 with Spark is slow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocm37", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695293163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are trying to read a big chunk of small files from several buckets in S3 with Spark. The objective is to merge those files into one and write it in another s3 bucket. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The code for read:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;val parquetFiles = Seq(&amp;quot;s3a://...&amp;quot;, &amp;quot;s3a://.....&amp;quot; ..)\nval df = spark.read.format(&amp;quot;parquet&amp;quot;).load(parquetFiles:_*)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;It takes about 10 minutes to execute the following query:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df.coalesce(1).write.format(&amp;quot;parquet&amp;quot;).save(&amp;quot;s3://...&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;While a &lt;code&gt;df.count()&lt;/code&gt; it takes about 2 minutes (which is also not ok, I guess). &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve tried changing a lot of configurations from &lt;code&gt;hadoop.fs.s3a&lt;/code&gt;, but no combination seems to alleviate the time. We cannot clearly understand which task is delaying the execution, but from Spark UI we have seen that not much CPU or Memory is consumed. &lt;/p&gt;\n\n&lt;p&gt;My assumption is that HTTP calls to S3 are getting too expensive. But I am not sure. &lt;/p&gt;\n\n&lt;p&gt;Has anyone experienced similar issues? &lt;/p&gt;\n\n&lt;p&gt;Have you solved them with conf or is it just a known problem? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ocm37", "is_robot_indexable": true, "report_reasons": null, "author": "paolapardo", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocm37/reading_small_files_from_s3_with_spark_is_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ocm37/reading_small_files_from_s3_with_spark_is_slow/", "subreddit_subscribers": 129696, "created_utc": 1695293163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nMy company's data analysis workflow relies solely on Google Cloud's Looker. Currently, we manually export data from the company's main system into an Excel file and then upload it to Looker to demonstrate the charts and visualizations. However, this process is slow due to the large amounts of data. Therefore, I have been tasked with creating an automated process from scratch to analyze the data and display visualizations similar to Looker's interface, but with more advanced data analysis tools like Python, Pandas, Jupyter, or SQL to make the system faster.\n\nI already have experience using Python, Pandas, Jupyter, and SQL in previous projects, but I need suggestions on how to utilize these tools for my current project. I am also willing to learn new skills if it will help me complete the project successfully.", "author_fullname": "t2_jkvo7dqy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need assistance in developing an automated analysis system. Can you help me with that?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16osu1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695333757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company&amp;#39;s data analysis workflow relies solely on Google Cloud&amp;#39;s Looker. Currently, we manually export data from the company&amp;#39;s main system into an Excel file and then upload it to Looker to demonstrate the charts and visualizations. However, this process is slow due to the large amounts of data. Therefore, I have been tasked with creating an automated process from scratch to analyze the data and display visualizations similar to Looker&amp;#39;s interface, but with more advanced data analysis tools like Python, Pandas, Jupyter, or SQL to make the system faster.&lt;/p&gt;\n\n&lt;p&gt;I already have experience using Python, Pandas, Jupyter, and SQL in previous projects, but I need suggestions on how to utilize these tools for my current project. I am also willing to learn new skills if it will help me complete the project successfully.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16osu1j", "is_robot_indexable": true, "report_reasons": null, "author": "CanSecret8665", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16osu1j/i_need_assistance_in_developing_an_automated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16osu1j/i_need_assistance_in_developing_an_automated/", "subreddit_subscribers": 129696, "created_utc": 1695333757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w07bcus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming Databases: Everything You Wanted to Know", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": true, "name": "t3_16osaaa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GR1WqvPIIOLXNrUUsA-uQuEaf6gcpeSwaZmjjPetkBs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695332501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "risingwave.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.risingwave.com/blog/streaming-databases-everything-you-wanted-to-know/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?auto=webp&amp;s=b7e4dcc20f21eb08532be4f44bcb2afd83a3598f", "width": 2560, "height": 1600}, "resolutions": [{"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da32a5d91a292bffb6d119660b638c9e590567e6", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7353fe828c48d0f4db3187b3034190456a3dec1f", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdb2196149d90817950922af7342f2f08e62780a", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e8121c7e1905cecc6c23cc5660a7c426e5bb4ae2", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=62cc77e8c5b58f12cdccc038f3cd16ebdf21ced9", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=60b72dad7898861032ecfe89103ca54e6a5d96e2", "width": 1080, "height": 675}], "variants": {}, "id": "hFCRvLcZo01Mnja6CckUi4AgVCLM2MZOpfNWBh4FS5U"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16osaaa", "is_robot_indexable": true, "report_reasons": null, "author": "yingjunwu", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16osaaa/streaming_databases_everything_you_wanted_to_know/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.risingwave.com/blog/streaming-databases-everything-you-wanted-to-know/", "subreddit_subscribers": 129696, "created_utc": 1695332501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "can some one explain how to ingest downloaded data from kaggle incrementally using python. you can use aws glue  and store data in s3 ?", "author_fullname": "t2_8vmwwx5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16onc3z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695320978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;can some one explain how to ingest downloaded data from kaggle incrementally using python. you can use aws glue  and store data in s3 ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16onc3z", "is_robot_indexable": true, "report_reasons": null, "author": "lifealtering111", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16onc3z/data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16onc3z/data_ingestion/", "subreddit_subscribers": 129696, "created_utc": 1695320978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_agd3b25og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple personal finance data engineering project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_16omopv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/qAfYIWzd97XHeRbSSy9Bti07lV5qAIIu4eeql_n5Kr8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695319410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/sam-wright-1/personal-finance-automation", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?auto=webp&amp;s=2bfb8cac5bfb9409447c65c5b2e8e5944295f808", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3fd795baaad4e5c64facc3c9e2d1f59c66386a7d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c4831f914f843e0eb0f349575c65e58636bd0af", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8614c9e967090b21dd6049af4d683c83388492f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fef6f995a2c998faba20c8c794cb30b76ba5fb4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=170e686546300424996ce707e93c461e87788af2", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d51f9a396d2b197fe1be96da7ed54ada899634f", "width": 1080, "height": 540}], "variants": {}, "id": "p3Ivo_RgfDav9G-934-9Ik7yNegAsq2eYHKKp5kjpvA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "16omopv", "is_robot_indexable": true, "report_reasons": null, "author": "data-partner", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16omopv/simple_personal_finance_data_engineering_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/sam-wright-1/personal-finance-automation", "subreddit_subscribers": 129696, "created_utc": 1695319410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just published an article on how to setup Zero-ETL integration between an Amazon Aurora database and Redshift.\n\nWith the Zero-ETL feature, you do not need to bother about setting up complex data pipelines in your organisation.\n\n[https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift](https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift)", "author_fullname": "t2_k6fldwtv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How you can set up Zero-ETL data pipeline between Amazon Aurora and Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16of8ak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695300955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just published an article on how to setup Zero-ETL integration between an Amazon Aurora database and Redshift.&lt;/p&gt;\n\n&lt;p&gt;With the Zero-ETL feature, you do not need to bother about setting up complex data pipelines in your organisation.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift\"&gt;https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?auto=webp&amp;s=43854de3c536f86fe8edad9982c3583ab380ad09", "width": 681, "height": 370}, "resolutions": [{"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecdcf6fcf7adb0a98d8aa69dc115ea125539065f", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d049f2c1df9c1ec82935472ae7fbeb1ccbd6e8d", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6807b4b492aac7f299db9cd405289e68a67fe5f8", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ee0039bae6a8d6f694eb79595479c445690476e", "width": 640, "height": 347}], "variants": {}, "id": "C3ODgW2FQeNZnu5nJxonIMIBLmXRwIgb3c7YtEsw3sM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16of8ak", "is_robot_indexable": true, "report_reasons": null, "author": "gbxnga", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16of8ak/how_you_can_set_up_zeroetl_data_pipeline_between/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16of8ak/how_you_can_set_up_zeroetl_data_pipeline_between/", "subreddit_subscribers": 129696, "created_utc": 1695300955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, \n\nI am wondering If someone of you used open source DVT from google github  - [professional-services-data-validator](https://github.com/GoogleCloudPlatform/professional-services-data-validator).\n\nI need to validate data exported from differences system. I have one folder, where all files are stored and via pipelines are loaded to the BigQuery. I can easily transform these files to the .csv and run the validation. But the problem is with the Filesystem connection. Filesystem connection has argument \"file path\" and connection is created only for one file.\n\nMy question is, with your experience, Do you think its better to create new connection for every file (thousands files) or use python script that will transform and load data to the one file work\\_file.csv that is connected and after the validation delete the file content then take another file and transform and load data to the work\\_file.csv...\n\nI do not have enough experience with these technologies and BI processes so I would be glad for every point you have. \n\nThank you very much.  \nM\n\n&amp;#x200B;", "author_fullname": "t2_79da9w8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Validation via professional-services-data-validator from Google", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocb07", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695292088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;I am wondering If someone of you used open source DVT from google github  - &lt;a href=\"https://github.com/GoogleCloudPlatform/professional-services-data-validator\"&gt;professional-services-data-validator&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I need to validate data exported from differences system. I have one folder, where all files are stored and via pipelines are loaded to the BigQuery. I can easily transform these files to the .csv and run the validation. But the problem is with the Filesystem connection. Filesystem connection has argument &amp;quot;file path&amp;quot; and connection is created only for one file.&lt;/p&gt;\n\n&lt;p&gt;My question is, with your experience, Do you think its better to create new connection for every file (thousands files) or use python script that will transform and load data to the one file work_file.csv that is connected and after the validation delete the file content then take another file and transform and load data to the work_file.csv...&lt;/p&gt;\n\n&lt;p&gt;I do not have enough experience with these technologies and BI processes so I would be glad for every point you have. &lt;/p&gt;\n\n&lt;p&gt;Thank you very much.&lt;br/&gt;\nM&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?auto=webp&amp;s=e21a2042737dc223e67be7ea27e40fc2236d12ee", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c766345a5f83563a2848746e21a43c0ad01a8c7e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80407217416d454d1f32b3b0c1fe41d4ba9dbf87", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ab68f2323e57915f688870703aa3b7827922c10", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3167be568b95344dbbaa19877c37d4bfffb3e688", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f988a35b7c0f9d7945888db9d4ec0e48eddb82d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e44e34cb74d2a495765f7b1cc9148eed809bb28", "width": 1080, "height": 540}], "variants": {}, "id": "Y2KMmNmoflDB_TA4VN4tScfKspEdT0Y_43XDwIeYUzA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ocb07", "is_robot_indexable": true, "report_reasons": null, "author": "Illustrious_Union216", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocb07/data_validation_via/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ocb07/data_validation_via/", "subreddit_subscribers": 129696, "created_utc": 1695292088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Few days back I have posted a help in this group and got wonderful comments. \nHere is my update: I have successfully implemented airflow dag to run dbt generic tests, but the airflow dag is picking up and running the dbt models before running the generic yml tests. \nHere I dont want dbt models to run because we have separate dags for that. I only want dbt generic tests to run as part of the airflow dag. \nAny suggestions are appreciated.", "author_fullname": "t2_ke7daufi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running dbt tests with Airflow dag", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o8n9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1695278383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Few days back I have posted a help in this group and got wonderful comments. \nHere is my update: I have successfully implemented airflow dag to run dbt generic tests, but the airflow dag is picking up and running the dbt models before running the generic yml tests. \nHere I dont want dbt models to run because we have separate dags for that. I only want dbt generic tests to run as part of the airflow dag. \nAny suggestions are appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://reddit.com/r/dataengineering/s/pHwzT1IgCw", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16o8n9p", "is_robot_indexable": true, "report_reasons": null, "author": "Long-Neighborhood330", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16o8n9p/running_dbt_tests_with_airflow_dag/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://reddit.com/r/dataengineering/s/pHwzT1IgCw", "subreddit_subscribers": 129696, "created_utc": 1695278383.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}