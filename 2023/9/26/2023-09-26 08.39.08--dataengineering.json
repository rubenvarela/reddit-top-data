{"kind": "Listing", "data": {"after": null, "dist": 20, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've read \"Fundamentals of Data Engineering\" by Reis. However, as the name says that book covers the fundamentals. There are loads of books on software engineering design patterns in general. Not for data engineering in my knowledge.\n\nI'm looking for a great book that goes through the popular data architecture patterns end-to-end. With code samples. Googled, but didn't find anything particularly exciting. Just a few blog posts. Which books do you folks suggest as essential reading for a data engineer?", "author_fullname": "t2_pblux6w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a great book on design patterns in data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rxj7v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 77, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 77, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695661129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve read &amp;quot;Fundamentals of Data Engineering&amp;quot; by Reis. However, as the name says that book covers the fundamentals. There are loads of books on software engineering design patterns in general. Not for data engineering in my knowledge.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a great book that goes through the popular data architecture patterns end-to-end. With code samples. Googled, but didn&amp;#39;t find anything particularly exciting. Just a few blog posts. Which books do you folks suggest as essential reading for a data engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16rxj7v", "is_robot_indexable": true, "report_reasons": null, "author": "newplayer12345", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rxj7v/is_there_a_great_book_on_design_patterns_in_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rxj7v/is_there_a_great_book_on_design_patterns_in_data/", "subreddit_subscribers": 130447, "created_utc": 1695661129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, as part of my thesis I am examining, among other things, the question of whether TDD or BDD can be used to develop ETL processes and if so, then how. It shouldn't go into too much detail and should be answered more theoretically.\n\n&amp;#x200B;\n\nAccording to my understanding so far, I need unit tests for development according to TDD. Now I ask myself how unit test cases can be set up for ETL development. Several sources say that you can only check the 'T' i.e. the transformations with unit testing. The steps 'E' and 'L' require external resources such as data sources or data warehouse to test them and therefore they are automatically integration tests, right? So can unit tests only be used for the transformation process? Does anyone have experience with TDD or even BDD in ETL development?\n\n&amp;#x200B;\n\nThank you for the answers", "author_fullname": "t2_hrqrf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TDD for ETL-Development", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rp3uv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695640321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, as part of my thesis I am examining, among other things, the question of whether TDD or BDD can be used to develop ETL processes and if so, then how. It shouldn&amp;#39;t go into too much detail and should be answered more theoretically.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;According to my understanding so far, I need unit tests for development according to TDD. Now I ask myself how unit test cases can be set up for ETL development. Several sources say that you can only check the &amp;#39;T&amp;#39; i.e. the transformations with unit testing. The steps &amp;#39;E&amp;#39; and &amp;#39;L&amp;#39; require external resources such as data sources or data warehouse to test them and therefore they are automatically integration tests, right? So can unit tests only be used for the transformation process? Does anyone have experience with TDD or even BDD in ETL development?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for the answers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16rp3uv", "is_robot_indexable": true, "report_reasons": null, "author": "m3xx4", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rp3uv/tdd_for_etldevelopment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rp3uv/tdd_for_etldevelopment/", "subreddit_subscribers": 130447, "created_utc": 1695640321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a great use case for Debezium but we don't run Kafka. Any alternatives for change data capture with mysql out there? The end goal is replication of certain changes in 1 DB to another in \"real time\".", "author_fullname": "t2_gua18k7sg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any alternatives to Debezium that don't rely on Kafka?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16sagq5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695692419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a great use case for Debezium but we don&amp;#39;t run Kafka. Any alternatives for change data capture with mysql out there? The end goal is replication of certain changes in 1 DB to another in &amp;quot;real time&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16sagq5", "is_robot_indexable": true, "report_reasons": null, "author": "alexcontrerasdppl", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16sagq5/any_alternatives_to_debezium_that_dont_rely_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16sagq5/any_alternatives_to_debezium_that_dont_rely_on/", "subreddit_subscribers": 130447, "created_utc": 1695692419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Last time I had to worry about such a tool was almost 7 years ago, and back then I relied on Flyway for schema change management. What type of schema change management tools do you use? My requirements are basic: change, and roll back if needed. Thx", "author_fullname": "t2_5tktq9ud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What type of database versioning/schema change management tool do you use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16sej06", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695704436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last time I had to worry about such a tool was almost 7 years ago, and back then I relied on Flyway for schema change management. What type of schema change management tools do you use? My requirements are basic: change, and roll back if needed. Thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16sej06", "is_robot_indexable": true, "report_reasons": null, "author": "zoobeezee", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16sej06/what_type_of_database_versioningschema_change/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16sej06/what_type_of_database_versioningschema_change/", "subreddit_subscribers": 130447, "created_utc": 1695704436.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I am doing a capstone on Log Anomaly detection for a digital sign board manufacturer. We have 17,000 log files(300GB) from over 2500 devices. The sponsor would like us to cluster them based on log patterns. how should I think modeling such data? Unfortunately I don't get access to any paid tools or services. Your suggestions would be valuable. thanks.", "author_fullname": "t2_c9dcr4wt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modeling 17k log files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16s25nl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695671889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am doing a capstone on Log Anomaly detection for a digital sign board manufacturer. We have 17,000 log files(300GB) from over 2500 devices. The sponsor would like us to cluster them based on log patterns. how should I think modeling such data? Unfortunately I don&amp;#39;t get access to any paid tools or services. Your suggestions would be valuable. thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16s25nl", "is_robot_indexable": true, "report_reasons": null, "author": "sigapuranger", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16s25nl/modeling_17k_log_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16s25nl/modeling_17k_log_files/", "subreddit_subscribers": 130447, "created_utc": 1695671889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hello!\n\nAt my workplace, we have a data warehouse, and we're looking to begin using the DBT tool to manage our transformations. However, from what I've gathered, DBT seems to be primarily designed for ELT pipelines. Currently, our pipeline involves a Python script managed via Airflow, which executes some SQL directly on our production DBT.\n\nMy question is, how can I implement a form of ELT with PostgreSQL as our data warehouse? I'm not sure if there's an effective way to organize this structure within PostgreSQL, or if there's something I might be overlooking.", "author_fullname": "t2_8jc0mwfh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using postgresql data warehouse with ELT pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rwtvd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695659481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;At my workplace, we have a data warehouse, and we&amp;#39;re looking to begin using the DBT tool to manage our transformations. However, from what I&amp;#39;ve gathered, DBT seems to be primarily designed for ELT pipelines. Currently, our pipeline involves a Python script managed via Airflow, which executes some SQL directly on our production DBT.&lt;/p&gt;\n\n&lt;p&gt;My question is, how can I implement a form of ELT with PostgreSQL as our data warehouse? I&amp;#39;m not sure if there&amp;#39;s an effective way to organize this structure within PostgreSQL, or if there&amp;#39;s something I might be overlooking.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16rwtvd", "is_robot_indexable": true, "report_reasons": null, "author": "Andremallmann", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rwtvd/using_postgresql_data_warehouse_with_elt_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rwtvd/using_postgresql_data_warehouse_with_elt_pipelines/", "subreddit_subscribers": 130447, "created_utc": 1695659481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I saw Apache AirFlow, but i think it a bit overkill and not Computer vision oriented.\n\nLooking for something that is: scalabe, monitored, easy to use, and there is (FOSS) community behind it.\n\nThanks!", "author_fullname": "t2_5831rdhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any reccomendation of python library for handling pipeline workflow for Computer vision?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16s62sf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695681075.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695680889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw Apache AirFlow, but i think it a bit overkill and not Computer vision oriented.&lt;/p&gt;\n\n&lt;p&gt;Looking for something that is: scalabe, monitored, easy to use, and there is (FOSS) community behind it.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16s62sf", "is_robot_indexable": true, "report_reasons": null, "author": "Expensive_Breakfast6", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16s62sf/any_reccomendation_of_python_library_for_handling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16s62sf/any_reccomendation_of_python_library_for_handling/", "subreddit_subscribers": 130447, "created_utc": 1695680889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For those of us who believe in the power of event-driven architectures and data streaming, you might be interested in our new open-source project: LangStream. It is an open-source framework for building event-driven Gen AI applications that combines LLMs, vector databases, Kubernetes, and Apache Kafka.  \n\n\nLangStream supports all Kafka connectors out of the box. You can just point to the JAR for a connector and use it as part of a Gen AI pipeline.\n\nFind out more here:\n\n[https://langstream.ai/2023/09/13/introducing-langstream/](https://langstream.ai/2023/09/13/introducing-langstream/)\n\nIf you find it interesting, please star the repo: [https://github.com/LangStream/langstream](https://github.com/LangStream/langstream)", "author_fullname": "t2_8nsm8c43", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New project: LangStream for building and running event-driven LLM applications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rxyye", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695662103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those of us who believe in the power of event-driven architectures and data streaming, you might be interested in our new open-source project: LangStream. It is an open-source framework for building event-driven Gen AI applications that combines LLMs, vector databases, Kubernetes, and Apache Kafka.  &lt;/p&gt;\n\n&lt;p&gt;LangStream supports all Kafka connectors out of the box. You can just point to the JAR for a connector and use it as part of a Gen AI pipeline.&lt;/p&gt;\n\n&lt;p&gt;Find out more here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://langstream.ai/2023/09/13/introducing-langstream/\"&gt;https://langstream.ai/2023/09/13/introducing-langstream/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you find it interesting, please star the repo: &lt;a href=\"https://github.com/LangStream/langstream\"&gt;https://github.com/LangStream/langstream&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NG_01PXp8iHDS_CdGx1Yq8HNtjMg7HUOlNjPoHNF0O8.jpg?auto=webp&amp;s=7d52f2f9bf64f49aae414fb6550354d29f1df798", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/NG_01PXp8iHDS_CdGx1Yq8HNtjMg7HUOlNjPoHNF0O8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8201a0b8f9ad3dfaa0dd0562f736ae1f1110d64b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/NG_01PXp8iHDS_CdGx1Yq8HNtjMg7HUOlNjPoHNF0O8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cdf3e5c6350961e9b3000a9b5f1c27665490c0e5", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/NG_01PXp8iHDS_CdGx1Yq8HNtjMg7HUOlNjPoHNF0O8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b0c34ddc1ab9d49bd200f7fcac070c9b39339de0", "width": 320, "height": 320}], "variants": {}, "id": "HwttWOvzTbyp1qu4I_z2JJSOGfJjcyFvmlF5NnjzRKc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16rxyye", "is_robot_indexable": true, "report_reasons": null, "author": "Head_Reaction_6242", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rxyye/new_project_langstream_for_building_and_running/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rxyye/new_project_langstream_for_building_and_running/", "subreddit_subscribers": 130447, "created_utc": 1695662103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TLDR;  checking if \"streaming\" with Postgres is a reasonable idea, given the throughput ( &lt; 10 updates / second) on a small amount of data (&lt; 100 mb).  \n\n\n# Functional Requirements:\n\nMy team has a custom compiled binary that is subscribed to stream of events, computes some outputs, and dumps them to a data sink process. The business logic for the streaming computation is written in a custom DSL and represents a time series transformation. This was written way before things like flink, kafka streams etc existed and has served production well over the years, but could use a little face lift, especially in terms of provenance to understand what inputs when into calculation outputs.  \n\n\n# Non-Functional Requirements:\n\nIt's a very small workload, hardly what someone would call streaming.\n\n1. 10 events / second\n2. 10mb of data\n3. 1s p99 update delay tolerable\n4. \\~ 10 large queries (100 numeric inputs), continuously refreshing. Each query only has 1-2 inputs that are actually constantly changing.\n5. No lock contention amongst the queries. They're independent of each other.  \n\n\n# Postgres Streaming solution:\n\nMy idea is to push the computation and state into Postgres as Flink and Kafka seem like overkill. The data will be modeled as a temporal table to keep a full snapshot of the history.  That way I get the following:\n\n1. High availability out of the box instead of handwritten leader election code\n2. All of the state is persisted. I can inspect the history of the state for provenance purposes using a temporal query.\n3. Rely on mature SQL computation engine instead of an esoteric, unoptimized DSL and runtime\n4. Time partitioning to keep the hot data ( &lt;100 kb ) in memory.\n5. My event processor is now stateless and has a simple job of triggering a postgres insert + update. The query will be compiled to avoid parsing and interpretation overhead.\n6. (Bonus) I can throw away a lot of custom code for a bunch of simple dbt templates as postgres now does all the heavy lifting.\n\nIt does feel weird to use postgres as a streaming engine, but people have pushed databases to handle way more concurrency on way larger data sets. Based on my napkin math, I think this barely qualifies as streaming, as the database would spend most of it's time waiting for work.", "author_fullname": "t2_2naya68b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming on Postgres idea check", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16sazes", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695693821.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR;  checking if &amp;quot;streaming&amp;quot; with Postgres is a reasonable idea, given the throughput ( &amp;lt; 10 updates / second) on a small amount of data (&amp;lt; 100 mb).  &lt;/p&gt;\n\n&lt;h1&gt;Functional Requirements:&lt;/h1&gt;\n\n&lt;p&gt;My team has a custom compiled binary that is subscribed to stream of events, computes some outputs, and dumps them to a data sink process. The business logic for the streaming computation is written in a custom DSL and represents a time series transformation. This was written way before things like flink, kafka streams etc existed and has served production well over the years, but could use a little face lift, especially in terms of provenance to understand what inputs when into calculation outputs.  &lt;/p&gt;\n\n&lt;h1&gt;Non-Functional Requirements:&lt;/h1&gt;\n\n&lt;p&gt;It&amp;#39;s a very small workload, hardly what someone would call streaming.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;10 events / second&lt;/li&gt;\n&lt;li&gt;10mb of data&lt;/li&gt;\n&lt;li&gt;1s p99 update delay tolerable&lt;/li&gt;\n&lt;li&gt;~ 10 large queries (100 numeric inputs), continuously refreshing. Each query only has 1-2 inputs that are actually constantly changing.&lt;/li&gt;\n&lt;li&gt;No lock contention amongst the queries. They&amp;#39;re independent of each other.&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Postgres Streaming solution:&lt;/h1&gt;\n\n&lt;p&gt;My idea is to push the computation and state into Postgres as Flink and Kafka seem like overkill. The data will be modeled as a temporal table to keep a full snapshot of the history.  That way I get the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;High availability out of the box instead of handwritten leader election code&lt;/li&gt;\n&lt;li&gt;All of the state is persisted. I can inspect the history of the state for provenance purposes using a temporal query.&lt;/li&gt;\n&lt;li&gt;Rely on mature SQL computation engine instead of an esoteric, unoptimized DSL and runtime&lt;/li&gt;\n&lt;li&gt;Time partitioning to keep the hot data ( &amp;lt;100 kb ) in memory.&lt;/li&gt;\n&lt;li&gt;My event processor is now stateless and has a simple job of triggering a postgres insert + update. The query will be compiled to avoid parsing and interpretation overhead.&lt;/li&gt;\n&lt;li&gt;(Bonus) I can throw away a lot of custom code for a bunch of simple dbt templates as postgres now does all the heavy lifting.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;It does feel weird to use postgres as a streaming engine, but people have pushed databases to handle way more concurrency on way larger data sets. Based on my napkin math, I think this barely qualifies as streaming, as the database would spend most of it&amp;#39;s time waiting for work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16sazes", "is_robot_indexable": true, "report_reasons": null, "author": "shuaibot", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16sazes/streaming_on_postgres_idea_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16sazes/streaming_on_postgres_idea_check/", "subreddit_subscribers": 130447, "created_utc": 1695693821.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Data Engineering friends,\n\nI would like to ask for your opinions on something that has been bothering me at work.  I work at a start up that hosts an application that uses DynamoDB as our transactional database and snowflake as our reporting/analytics engine.  We currently only have one instance of snowflake that points to our production AWS account/DynamoDb database and I am trying to convince my managers that we absolutely need a dev snowflake instance to point to our Non Prod AWS account to allow us to do end to end regression testing including our reports/analytics products.  I am basically being told that the overhead cost of replicating production to our lower environments will be too expensive, but at the same time we are being told that we need to have a highly available system and we have to meet SLA's.  I don't  find it sufficient to just do code reviews, I don't think we will ever scale if we cannot perform a set of regression tests in a safe space. \n\nAm I right here or are my standards of how development should be done outdated?  Would anyone else here find this as a red flag for management?", "author_fullname": "t2_osq93qer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Development Ops Separation for Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rrecb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695646833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Data Engineering friends,&lt;/p&gt;\n\n&lt;p&gt;I would like to ask for your opinions on something that has been bothering me at work.  I work at a start up that hosts an application that uses DynamoDB as our transactional database and snowflake as our reporting/analytics engine.  We currently only have one instance of snowflake that points to our production AWS account/DynamoDb database and I am trying to convince my managers that we absolutely need a dev snowflake instance to point to our Non Prod AWS account to allow us to do end to end regression testing including our reports/analytics products.  I am basically being told that the overhead cost of replicating production to our lower environments will be too expensive, but at the same time we are being told that we need to have a highly available system and we have to meet SLA&amp;#39;s.  I don&amp;#39;t  find it sufficient to just do code reviews, I don&amp;#39;t think we will ever scale if we cannot perform a set of regression tests in a safe space. &lt;/p&gt;\n\n&lt;p&gt;Am I right here or are my standards of how development should be done outdated?  Would anyone else here find this as a red flag for management?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16rrecb", "is_robot_indexable": true, "report_reasons": null, "author": "nsq116", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rrecb/development_ops_separation_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rrecb/development_ops_separation_for_data_engineering/", "subreddit_subscribers": 130447, "created_utc": 1695646833.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Alright friends, I've got a client who is dead-set on using Informatics for MDM. Anyone know off the top of their head what makes up an IPU for these guys?\n\nAlso, how angry does it make you when you try to get an answer to a question like that and their official page tells you to call a sales rep?", "author_fullname": "t2_713dpi97", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Informatica and IPU", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rrn5k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695647428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Alright friends, I&amp;#39;ve got a client who is dead-set on using Informatics for MDM. Anyone know off the top of their head what makes up an IPU for these guys?&lt;/p&gt;\n\n&lt;p&gt;Also, how angry does it make you when you try to get an answer to a question like that and their official page tells you to call a sales rep?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16rrn5k", "is_robot_indexable": true, "report_reasons": null, "author": "rexicusmaximus", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16rrn5k/informatica_and_ipu/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rrn5k/informatica_and_ipu/", "subreddit_subscribers": 130447, "created_utc": 1695647428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working in one of the WITCHR and never had a data related role. But I want to be a data engineer, and I am targeting for role of ETL developer for same.\n\nI have learnt python, SQL in previous 8 months and now I am currently learning informatica as it is mostly used in etl projects in my company. I want to get into a etl dev role in my company and that's why I am learning informatica.\nI also have learnt basic of Hadoop and spark. I have AZ-900 and DP-900, and basic understanding of cloud.\n\nI keep doubting that I am either spreading myself too thin or not focusing on right technology. I am doing DSA too in python from last few months on leetcode and I am not sure if that is required for data engineers.\n\nWhich technology should I choose? Should I be looking for data related roles other than ETL developer too like data analyst? Should I be doing DSA or not? Is informatica good or should I quit it?", "author_fullname": "t2_uz5me90b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am i on the right track?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16sgz0q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695713053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working in one of the WITCHR and never had a data related role. But I want to be a data engineer, and I am targeting for role of ETL developer for same.&lt;/p&gt;\n\n&lt;p&gt;I have learnt python, SQL in previous 8 months and now I am currently learning informatica as it is mostly used in etl projects in my company. I want to get into a etl dev role in my company and that&amp;#39;s why I am learning informatica.\nI also have learnt basic of Hadoop and spark. I have AZ-900 and DP-900, and basic understanding of cloud.&lt;/p&gt;\n\n&lt;p&gt;I keep doubting that I am either spreading myself too thin or not focusing on right technology. I am doing DSA too in python from last few months on leetcode and I am not sure if that is required for data engineers.&lt;/p&gt;\n\n&lt;p&gt;Which technology should I choose? Should I be looking for data related roles other than ETL developer too like data analyst? Should I be doing DSA or not? Is informatica good or should I quit it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16sgz0q", "is_robot_indexable": true, "report_reasons": null, "author": "iamthatmadman", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16sgz0q/am_i_on_the_right_track/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16sgz0q/am_i_on_the_right_track/", "subreddit_subscribers": 130447, "created_utc": 1695713053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hello\n\ni am trying to keep my table updated with forex currency pairs from yahoo finance\n\nwhen i run the table with no etl checkpoint, the code runs and can get the data\n\nbut when i run the second etl checkpoint, i get this:\n\n    1 Failed download:\n    - GBPJPY=X: Data doesn't exist for startDate = 1695718121, endDate = 1695718124\n    error in function read_source_df_insert_dest=\"['Datetime'] not in index\"\n\nwhen i print startdate and enddate, i get this:\n\n    2023-09-26 09:48:41 2023-09-26 09:48:44\n\nso idk where the values startDate = 1695718121, endDate = 1695718124 are coming from\n\nthis is my insert statement, but it works perfectly on all datatypes so i dont think its the problem:\n\n    def return_insert_into_sql_statement_from_df(dataframe, schema_name, table_name):\n        columns = ', '.join(dataframe.columns)\n        insert_statement_list = []\n        \n        for _, row in dataframe.iterrows():\n            value_strs = []\n            for col, val in row.items():\n    \n                if dataframe[col].dtype == 'datetime64[ns]':\n    \n                    value_strs.append(f\"'{val}'\")\n                    \n                elif isinstance(val, str):\n                    value_strs.append(f\"'{val}'\")\n                else:\n                    value_strs.append(str(val))\n                    \n            values = ', '.join(value_strs)\n            insert_statement = f\"INSERT INTO {schema_name}.{table_name} ({columns}) VALUES ({values});\"\n            insert_statement_list.append(insert_statement)\n        \n        return insert_statement_list\n\nthanks for help", "author_fullname": "t2_a8ditcldc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "help on my first ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16sgl85", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695711625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hello&lt;/p&gt;\n\n&lt;p&gt;i am trying to keep my table updated with forex currency pairs from yahoo finance&lt;/p&gt;\n\n&lt;p&gt;when i run the table with no etl checkpoint, the code runs and can get the data&lt;/p&gt;\n\n&lt;p&gt;but when i run the second etl checkpoint, i get this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;1 Failed download:\n- GBPJPY=X: Data doesn&amp;#39;t exist for startDate = 1695718121, endDate = 1695718124\nerror in function read_source_df_insert_dest=&amp;quot;[&amp;#39;Datetime&amp;#39;] not in index&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;when i print startdate and enddate, i get this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;2023-09-26 09:48:41 2023-09-26 09:48:44\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;so idk where the values startDate = 1695718121, endDate = 1695718124 are coming from&lt;/p&gt;\n\n&lt;p&gt;this is my insert statement, but it works perfectly on all datatypes so i dont think its the problem:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def return_insert_into_sql_statement_from_df(dataframe, schema_name, table_name):\n    columns = &amp;#39;, &amp;#39;.join(dataframe.columns)\n    insert_statement_list = []\n\n    for _, row in dataframe.iterrows():\n        value_strs = []\n        for col, val in row.items():\n\n            if dataframe[col].dtype == &amp;#39;datetime64[ns]&amp;#39;:\n\n                value_strs.append(f&amp;quot;&amp;#39;{val}&amp;#39;&amp;quot;)\n\n            elif isinstance(val, str):\n                value_strs.append(f&amp;quot;&amp;#39;{val}&amp;#39;&amp;quot;)\n            else:\n                value_strs.append(str(val))\n\n        values = &amp;#39;, &amp;#39;.join(value_strs)\n        insert_statement = f&amp;quot;INSERT INTO {schema_name}.{table_name} ({columns}) VALUES ({values});&amp;quot;\n        insert_statement_list.append(insert_statement)\n\n    return insert_statement_list\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;thanks for help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "16sgl85", "is_robot_indexable": true, "report_reasons": null, "author": "qhelspil", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16sgl85/help_on_my_first_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16sgl85/help_on_my_first_etl/", "subreddit_subscribers": 130447, "created_utc": 1695711625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious to hear your insights and experiences with how you may handle data discrepancies that crop up between different databases in your projects?\n\nE.g. I have one excel spreadsheet full of vendors' names and contact info and a mailchimp mailing list with their info but notice from time to time that emails between both may be different. How could I sync these without manually going through thousands of entries? \n\nDo you have any specific techniques for data reconciliation or data validation? Are there any tools that have been particularly helpful? How do you maintain data consistency and accuracy as your data ecosystem grows and evolves?\n\nHoping to find answers/tools to the above q\u2019s that would be usable by a non-technical person or don't require a dedicated programmer. ", "author_fullname": "t2_kgr62d3c0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices for Dealing with Data Discrepancies Between Databases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rza4k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695665142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious to hear your insights and experiences with how you may handle data discrepancies that crop up between different databases in your projects?&lt;/p&gt;\n\n&lt;p&gt;E.g. I have one excel spreadsheet full of vendors&amp;#39; names and contact info and a mailchimp mailing list with their info but notice from time to time that emails between both may be different. How could I sync these without manually going through thousands of entries? &lt;/p&gt;\n\n&lt;p&gt;Do you have any specific techniques for data reconciliation or data validation? Are there any tools that have been particularly helpful? How do you maintain data consistency and accuracy as your data ecosystem grows and evolves?&lt;/p&gt;\n\n&lt;p&gt;Hoping to find answers/tools to the above q\u2019s that would be usable by a non-technical person or don&amp;#39;t require a dedicated programmer. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16rza4k", "is_robot_indexable": true, "report_reasons": null, "author": "Stat58372", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rza4k/best_practices_for_dealing_with_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rza4k/best_practices_for_dealing_with_data/", "subreddit_subscribers": 130447, "created_utc": 1695665142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\nI am a data engineer in Amsterdam, NL and having 2-3 years of experience. Half of it was on data warehousing in gcp and other half was on spark/scala. I am considering to work for a IT consultancy company, but I moved here 6 months ago and don\u2019t know the sector. \n\nBasically, I want to learn as much as possible in my early stage of career and have a decent income in a non-toxic environment.\n\nWhich consultancy companies do you recommend or definitely not recommend to work as a data engineer?", "author_fullname": "t2_knked3pz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Consultancy company in Netherlands", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rsv5k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695650288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,\nI am a data engineer in Amsterdam, NL and having 2-3 years of experience. Half of it was on data warehousing in gcp and other half was on spark/scala. I am considering to work for a IT consultancy company, but I moved here 6 months ago and don\u2019t know the sector. &lt;/p&gt;\n\n&lt;p&gt;Basically, I want to learn as much as possible in my early stage of career and have a decent income in a non-toxic environment.&lt;/p&gt;\n\n&lt;p&gt;Which consultancy companies do you recommend or definitely not recommend to work as a data engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16rsv5k", "is_robot_indexable": true, "report_reasons": null, "author": "Naive-Treacle2355", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rsv5k/consultancy_company_in_netherlands/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rsv5k/consultancy_company_in_netherlands/", "subreddit_subscribers": 130447, "created_utc": 1695650288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[\ud83c\udf0d Spatial Analysis of Population Shifts: A Deep Dive into Raster-based Exploration \ud83c\udf0d](https://i.redd.it/k2sa7mconjqb1.gif)\n\n[\ud83c\udf0d Spatial Analysis of Population Shifts: A Deep Dive into Raster-based Exploration \ud83c\udf0d](https://spatial-dev.guru/2023/09/19/spatial-analysis-of-population-shifts-a-raster-based-exploration/)  \nDive into a comprehensive geospatial analysis of population shifts in Slovakia from 2006 to 2021. This tutorial showcases the power of raster data in identifying significant population changes over time. \ud83d\udcc8  \nKey Takeaways:  \n\ud83d\udd0d Why rasterizing 1KM Grid Census Data is a game-changer.  \n\ud83d\udee0\ufe0f Step-by-step guide using Python libraries like geopandas, geocube, and xarray.  \n\ud83d\udccc Pinpointing areas with the most significant population shifts.  \n\ud83d\udcca Organizing, reprojecting, and saving results for further insights.", "author_fullname": "t2_avt84u4i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83c\udf0d Spatial Analysis of Population Shifts: A Deep Dive into Raster-based Exploration \ud83c\udf0d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 63, "top_awarded_type": null, "hide_score": false, "media_metadata": {"k2sa7mconjqb1": {"status": "valid", "e": "AnimatedImage", "m": "image/gif", "p": [{"y": 49, "x": 108, "u": "https://preview.redd.it/k2sa7mconjqb1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=e70208346fdf52737440431a3782e132274792dd"}, {"y": 98, "x": 216, "u": "https://preview.redd.it/k2sa7mconjqb1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=510a4d9fe0bfd5a71979f2f23594ab0ae629d3c4"}, {"y": 145, "x": 320, "u": "https://preview.redd.it/k2sa7mconjqb1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=70ce85bbde247fddce69c6f758e20ffc32792bc5"}, {"y": 290, "x": 640, "u": "https://preview.redd.it/k2sa7mconjqb1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=ab4c12472443479fbc44c4ab080ad313810de3d1"}, {"y": 435, "x": 960, "u": "https://preview.redd.it/k2sa7mconjqb1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=17edb1f4ef4d12ee0d2bbe20336e3c0f7f1764dc"}, {"y": 490, "x": 1080, "u": "https://preview.redd.it/k2sa7mconjqb1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=d6b8e29162c559b903e636726efe0c6e72705d31"}], "s": {"y": 869, "gif": "https://i.redd.it/k2sa7mconjqb1.gif", "mp4": "https://preview.redd.it/k2sa7mconjqb1.gif?format=mp4&amp;s=313ac783cd6b92be5232a3fe20b6aced995e54e0", "x": 1914}, "id": "k2sa7mconjqb1"}}, "name": "t3_16sg35m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/CWVPJMXsHjZ9EEELr5tTQMOPoNnvfY4oJVFScBR2uf0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695709768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://i.redd.it/k2sa7mconjqb1.gif\"&gt;\ud83c\udf0d Spatial Analysis of Population Shifts: A Deep Dive into Raster-based Exploration \ud83c\udf0d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://spatial-dev.guru/2023/09/19/spatial-analysis-of-population-shifts-a-raster-based-exploration/\"&gt;\ud83c\udf0d Spatial Analysis of Population Shifts: A Deep Dive into Raster-based Exploration \ud83c\udf0d&lt;/a&gt;&lt;br/&gt;\nDive into a comprehensive geospatial analysis of population shifts in Slovakia from 2006 to 2021. This tutorial showcases the power of raster data in identifying significant population changes over time. \ud83d\udcc8&lt;br/&gt;\nKey Takeaways:&lt;br/&gt;\n\ud83d\udd0d Why rasterizing 1KM Grid Census Data is a game-changer.&lt;br/&gt;\n\ud83d\udee0\ufe0f Step-by-step guide using Python libraries like geopandas, geocube, and xarray.&lt;br/&gt;\n\ud83d\udccc Pinpointing areas with the most significant population shifts.&lt;br/&gt;\n\ud83d\udcca Organizing, reprojecting, and saving results for further insights.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16sg35m", "is_robot_indexable": true, "report_reasons": null, "author": "iamgeoknight", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16sg35m/spatial_analysis_of_population_shifts_a_deep_dive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16sg35m/spatial_analysis_of_population_shifts_a_deep_dive/", "subreddit_subscribers": 130447, "created_utc": 1695709768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Which degree would you choose &amp; why?\n\nI\u2019m currently working as a jr swe. I\u2019m looking to go back to school for a masters. I\u2019m thinking of either going for an MBA or M.S. in either data science or cybersecurity. I don\u2019t think I want to work as a swe the rest of my life and would like to use my masters to pivot into other areas when the time comes. I\u2019m also looking for something with job security/stability and would like to keep income over six figures. \n\nAlso, does the school matter for the masters? I\u2019m thinking WGU for now but still exploring other schools. I like how affordable WGU is. Which degree would you choose and why? Would love to hear people\u2019s input.", "author_fullname": "t2_mgoqraub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which masters degree would you choose?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16sdnxz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695701674.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which degree would you choose &amp;amp; why?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently working as a jr swe. I\u2019m looking to go back to school for a masters. I\u2019m thinking of either going for an MBA or M.S. in either data science or cybersecurity. I don\u2019t think I want to work as a swe the rest of my life and would like to use my masters to pivot into other areas when the time comes. I\u2019m also looking for something with job security/stability and would like to keep income over six figures. &lt;/p&gt;\n\n&lt;p&gt;Also, does the school matter for the masters? I\u2019m thinking WGU for now but still exploring other schools. I like how affordable WGU is. Which degree would you choose and why? Would love to hear people\u2019s input.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16sdnxz", "is_robot_indexable": true, "report_reasons": null, "author": "That-Wolverine1781", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16sdnxz/which_masters_degree_would_you_choose/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16sdnxz/which_masters_degree_would_you_choose/", "subreddit_subscribers": 130447, "created_utc": 1695701674.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have always been curious has to how different it is to be a DE in the sports industry when compared to a tech/consulting company? Do you view the usage of tools and overall architecture similarly? Do you have opportunities to learn and grow? How are your peers around you and do you have a WLB?", "author_fullname": "t2_u0ewxlrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is it working as a DE in the sports industry?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16sdlq8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695701474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have always been curious has to how different it is to be a DE in the sports industry when compared to a tech/consulting company? Do you view the usage of tools and overall architecture similarly? Do you have opportunities to learn and grow? How are your peers around you and do you have a WLB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16sdlq8", "is_robot_indexable": true, "report_reasons": null, "author": "millenialMonkk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16sdlq8/how_is_it_working_as_a_de_in_the_sports_industry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16sdlq8/how_is_it_working_as_a_de_in_the_sports_industry/", "subreddit_subscribers": 130447, "created_utc": 1695701474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm looking into setting up an on prem Lake House with Kubernetes compute (&gt;1k nodes) and S3 compatible object store (multi PB).  Not interested in a Hadoop based stack or cloud based.   Must be able to run Apache Spark workloads and have a Metastore (like Hive Metastore or [Nessie](https://projectnessie.org/))  bonus if it can run multiple data processing engines (Trino, Flink...)  Apache 2 License is prefered, but I'm willing to pay if I find something to solve my needs. \n\n[DataRoaster](https://github.com/cloudcheflabs/dataroaster)  looks like a great start, but I'm looking for something that is actively being developed and maybe a little bit further along.  \n\nThanks for any suggestions!\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_ind8p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for On Prem Lake House software stack using k8s and s3 compat obj.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ruspl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695654793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m looking into setting up an on prem Lake House with Kubernetes compute (&amp;gt;1k nodes) and S3 compatible object store (multi PB).  Not interested in a Hadoop based stack or cloud based.   Must be able to run Apache Spark workloads and have a Metastore (like Hive Metastore or &lt;a href=\"https://projectnessie.org/\"&gt;Nessie&lt;/a&gt;)  bonus if it can run multiple data processing engines (Trino, Flink...)  Apache 2 License is prefered, but I&amp;#39;m willing to pay if I find something to solve my needs. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/cloudcheflabs/dataroaster\"&gt;DataRoaster&lt;/a&gt;  looks like a great start, but I&amp;#39;m looking for something that is actively being developed and maybe a little bit further along.  &lt;/p&gt;\n\n&lt;p&gt;Thanks for any suggestions!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ruspl", "is_robot_indexable": true, "report_reasons": null, "author": "6nop_", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ruspl/looking_for_on_prem_lake_house_software_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ruspl/looking_for_on_prem_lake_house_software_stack/", "subreddit_subscribers": 130447, "created_utc": 1695654793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently been working on an easy way to spin up dashboards, scheduling, alerting, silencing, logs, etc. around some of my custom data quality Python checks. Decided this past weekend to spin that out as a standalone and simple solution that anyone can use for their checks. If it's interesting, DM me and we can talk further", "author_fullname": "t2_dcdt0mh5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data quality checks and dashboards with a lines of Python code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16s4pxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695677718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently been working on an easy way to spin up dashboards, scheduling, alerting, silencing, logs, etc. around some of my custom data quality Python checks. Decided this past weekend to spin that out as a standalone and simple solution that anyone can use for their checks. If it&amp;#39;s interesting, DM me and we can talk further&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16s4pxk", "is_robot_indexable": true, "report_reasons": null, "author": "gnahznavia", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16s4pxk/data_quality_checks_and_dashboards_with_a_lines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16s4pxk/data_quality_checks_and_dashboards_with_a_lines/", "subreddit_subscribers": 130447, "created_utc": 1695677718.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}