{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've read \"Fundamentals of Data Engineering\" by Reis. However, as the name says that book covers the fundamentals. There are loads of books on software engineering design patterns in general. Not for data engineering in my knowledge.\n\nI'm looking for a great book that goes through the popular data architecture patterns end-to-end. With code samples. Googled, but didn't find anything particularly exciting. Just a few blog posts. Which books do you folks suggest as essential reading for a data engineer?", "author_fullname": "t2_pblux6w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a great book on design patterns in data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rxj7v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 58, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 58, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695661129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve read &amp;quot;Fundamentals of Data Engineering&amp;quot; by Reis. However, as the name says that book covers the fundamentals. There are loads of books on software engineering design patterns in general. Not for data engineering in my knowledge.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a great book that goes through the popular data architecture patterns end-to-end. With code samples. Googled, but didn&amp;#39;t find anything particularly exciting. Just a few blog posts. Which books do you folks suggest as essential reading for a data engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16rxj7v", "is_robot_indexable": true, "report_reasons": null, "author": "newplayer12345", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rxj7v/is_there_a_great_book_on_design_patterns_in_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rxj7v/is_there_a_great_book_on_design_patterns_in_data/", "subreddit_subscribers": 130420, "created_utc": 1695661129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, as part of my thesis I am examining, among other things, the question of whether TDD or BDD can be used to develop ETL processes and if so, then how. It shouldn't go into too much detail and should be answered more theoretically.\n\n&amp;#x200B;\n\nAccording to my understanding so far, I need unit tests for development according to TDD. Now I ask myself how unit test cases can be set up for ETL development. Several sources say that you can only check the 'T' i.e. the transformations with unit testing. The steps 'E' and 'L' require external resources such as data sources or data warehouse to test them and therefore they are automatically integration tests, right? So can unit tests only be used for the transformation process? Does anyone have experience with TDD or even BDD in ETL development?\n\n&amp;#x200B;\n\nThank you for the answers", "author_fullname": "t2_hrqrf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TDD for ETL-Development", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rp3uv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695640321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, as part of my thesis I am examining, among other things, the question of whether TDD or BDD can be used to develop ETL processes and if so, then how. It shouldn&amp;#39;t go into too much detail and should be answered more theoretically.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;According to my understanding so far, I need unit tests for development according to TDD. Now I ask myself how unit test cases can be set up for ETL development. Several sources say that you can only check the &amp;#39;T&amp;#39; i.e. the transformations with unit testing. The steps &amp;#39;E&amp;#39; and &amp;#39;L&amp;#39; require external resources such as data sources or data warehouse to test them and therefore they are automatically integration tests, right? So can unit tests only be used for the transformation process? Does anyone have experience with TDD or even BDD in ETL development?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for the answers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16rp3uv", "is_robot_indexable": true, "report_reasons": null, "author": "m3xx4", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rp3uv/tdd_for_etldevelopment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rp3uv/tdd_for_etldevelopment/", "subreddit_subscribers": 130420, "created_utc": 1695640321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,I'm new and trying to do POC of CDC for streaming data from mysql to Kafka via debezium. What can I do in the correct way when initialize a snapshot from MySQL? If a slave or debezium connectors are down, how can I manage the Kafka cluster to recovery debezium instances from other MySQL slave or master? Do you have any specified whole pictures of CDC in the real world?Thanks for your recommendation,\n\nhttps://preview.redd.it/hzrcstb8ibqb1.png?width=1057&amp;format=png&amp;auto=webp&amp;s=ddc4b951a7a53c5fbaa804c7c75db791df143bd8", "author_fullname": "t2_kwjgg3p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC MySQL with debezium", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 55, "top_awarded_type": null, "hide_score": false, "media_metadata": {"hzrcstb8ibqb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 42, "x": 108, "u": "https://preview.redd.it/hzrcstb8ibqb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f8fb0a779962adcfb683216cfc7339411d99f29f"}, {"y": 85, "x": 216, "u": "https://preview.redd.it/hzrcstb8ibqb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=005e10db26a7c9233c4f0d800cf36b29ffc6bda1"}, {"y": 126, "x": 320, "u": "https://preview.redd.it/hzrcstb8ibqb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39c6218748851df4499992c78693909d4b27df32"}, {"y": 253, "x": 640, "u": "https://preview.redd.it/hzrcstb8ibqb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=95f12d39a610f400ab5a0a4c98d7f3033126e3ba"}, {"y": 379, "x": 960, "u": "https://preview.redd.it/hzrcstb8ibqb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=83f6ddd8ebafb395897d525493915cd397f72422"}], "s": {"y": 418, "x": 1057, "u": "https://preview.redd.it/hzrcstb8ibqb1.png?width=1057&amp;format=png&amp;auto=webp&amp;s=ddc4b951a7a53c5fbaa804c7c75db791df143bd8"}, "id": "hzrcstb8ibqb1"}}, "name": "t3_16rgu36", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/p0onLUeK1KG8N58AlD59dqhZxHEM-uHC6adOsdvVqL0.jpg", "edited": 1695615842.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695611253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,I&amp;#39;m new and trying to do POC of CDC for streaming data from mysql to Kafka via debezium. What can I do in the correct way when initialize a snapshot from MySQL? If a slave or debezium connectors are down, how can I manage the Kafka cluster to recovery debezium instances from other MySQL slave or master? Do you have any specified whole pictures of CDC in the real world?Thanks for your recommendation,&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hzrcstb8ibqb1.png?width=1057&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddc4b951a7a53c5fbaa804c7c75db791df143bd8\"&gt;https://preview.redd.it/hzrcstb8ibqb1.png?width=1057&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddc4b951a7a53c5fbaa804c7c75db791df143bd8&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16rgu36", "is_robot_indexable": true, "report_reasons": null, "author": "phamtanvinhme", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rgu36/cdc_mysql_with_debezium/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rgu36/cdc_mysql_with_debezium/", "subreddit_subscribers": 130420, "created_utc": 1695611253.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I am doing a capstone on Log Anomaly detection for a digital sign board manufacturer. We have 17,000 log files(300GB) from over 2500 devices. The sponsor would like us to cluster them based on log patterns. how should I think modeling such data? Unfortunately I don't get access to any paid tools or services. Your suggestions would be valuable. thanks.", "author_fullname": "t2_c9dcr4wt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modeling 17k log files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16s25nl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695671889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am doing a capstone on Log Anomaly detection for a digital sign board manufacturer. We have 17,000 log files(300GB) from over 2500 devices. The sponsor would like us to cluster them based on log patterns. how should I think modeling such data? Unfortunately I don&amp;#39;t get access to any paid tools or services. Your suggestions would be valuable. thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16s25nl", "is_robot_indexable": true, "report_reasons": null, "author": "sigapuranger", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16s25nl/modeling_17k_log_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16s25nl/modeling_17k_log_files/", "subreddit_subscribers": 130420, "created_utc": 1695671889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hello!\n\nAt my workplace, we have a data warehouse, and we're looking to begin using the DBT tool to manage our transformations. However, from what I've gathered, DBT seems to be primarily designed for ELT pipelines. Currently, our pipeline involves a Python script managed via Airflow, which executes some SQL directly on our production DBT.\n\nMy question is, how can I implement a form of ELT with PostgreSQL as our data warehouse? I'm not sure if there's an effective way to organize this structure within PostgreSQL, or if there's something I might be overlooking.", "author_fullname": "t2_8jc0mwfh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using postgresql data warehouse with ELT pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rwtvd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695659481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;At my workplace, we have a data warehouse, and we&amp;#39;re looking to begin using the DBT tool to manage our transformations. However, from what I&amp;#39;ve gathered, DBT seems to be primarily designed for ELT pipelines. Currently, our pipeline involves a Python script managed via Airflow, which executes some SQL directly on our production DBT.&lt;/p&gt;\n\n&lt;p&gt;My question is, how can I implement a form of ELT with PostgreSQL as our data warehouse? I&amp;#39;m not sure if there&amp;#39;s an effective way to organize this structure within PostgreSQL, or if there&amp;#39;s something I might be overlooking.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16rwtvd", "is_robot_indexable": true, "report_reasons": null, "author": "Andremallmann", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rwtvd/using_postgresql_data_warehouse_with_elt_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rwtvd/using_postgresql_data_warehouse_with_elt_pipelines/", "subreddit_subscribers": 130420, "created_utc": 1695659481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For those of us who believe in the power of event-driven architectures and data streaming, you might be interested in our new open-source project: LangStream. It is an open-source framework for building event-driven Gen AI applications that combines LLMs, vector databases, Kubernetes, and Apache Kafka.  \n\n\nLangStream supports all Kafka connectors out of the box. You can just point to the JAR for a connector and use it as part of a Gen AI pipeline.\n\nFind out more here:\n\n[https://langstream.ai/2023/09/13/introducing-langstream/](https://langstream.ai/2023/09/13/introducing-langstream/)\n\nIf you find it interesting, please star the repo: [https://github.com/LangStream/langstream](https://github.com/LangStream/langstream)", "author_fullname": "t2_8nsm8c43", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New project: LangStream for building and running event-driven LLM applications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rxyye", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695662103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those of us who believe in the power of event-driven architectures and data streaming, you might be interested in our new open-source project: LangStream. It is an open-source framework for building event-driven Gen AI applications that combines LLMs, vector databases, Kubernetes, and Apache Kafka.  &lt;/p&gt;\n\n&lt;p&gt;LangStream supports all Kafka connectors out of the box. You can just point to the JAR for a connector and use it as part of a Gen AI pipeline.&lt;/p&gt;\n\n&lt;p&gt;Find out more here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://langstream.ai/2023/09/13/introducing-langstream/\"&gt;https://langstream.ai/2023/09/13/introducing-langstream/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you find it interesting, please star the repo: &lt;a href=\"https://github.com/LangStream/langstream\"&gt;https://github.com/LangStream/langstream&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NG_01PXp8iHDS_CdGx1Yq8HNtjMg7HUOlNjPoHNF0O8.jpg?auto=webp&amp;s=7d52f2f9bf64f49aae414fb6550354d29f1df798", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/NG_01PXp8iHDS_CdGx1Yq8HNtjMg7HUOlNjPoHNF0O8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8201a0b8f9ad3dfaa0dd0562f736ae1f1110d64b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/NG_01PXp8iHDS_CdGx1Yq8HNtjMg7HUOlNjPoHNF0O8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cdf3e5c6350961e9b3000a9b5f1c27665490c0e5", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/NG_01PXp8iHDS_CdGx1Yq8HNtjMg7HUOlNjPoHNF0O8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b0c34ddc1ab9d49bd200f7fcac070c9b39339de0", "width": 320, "height": 320}], "variants": {}, "id": "HwttWOvzTbyp1qu4I_z2JJSOGfJjcyFvmlF5NnjzRKc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16rxyye", "is_robot_indexable": true, "report_reasons": null, "author": "Head_Reaction_6242", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rxyye/new_project_langstream_for_building_and_running/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rxyye/new_project_langstream_for_building_and_running/", "subreddit_subscribers": 130420, "created_utc": 1695662103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Data Engineering friends,\n\nI would like to ask for your opinions on something that has been bothering me at work.  I work at a start up that hosts an application that uses DynamoDB as our transactional database and snowflake as our reporting/analytics engine.  We currently only have one instance of snowflake that points to our production AWS account/DynamoDb database and I am trying to convince my managers that we absolutely need a dev snowflake instance to point to our Non Prod AWS account to allow us to do end to end regression testing including our reports/analytics products.  I am basically being told that the overhead cost of replicating production to our lower environments will be too expensive, but at the same time we are being told that we need to have a highly available system and we have to meet SLA's.  I don't  find it sufficient to just do code reviews, I don't think we will ever scale if we cannot perform a set of regression tests in a safe space. \n\nAm I right here or are my standards of how development should be done outdated?  Would anyone else here find this as a red flag for management?", "author_fullname": "t2_osq93qer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Development Ops Separation for Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rrecb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695646833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Data Engineering friends,&lt;/p&gt;\n\n&lt;p&gt;I would like to ask for your opinions on something that has been bothering me at work.  I work at a start up that hosts an application that uses DynamoDB as our transactional database and snowflake as our reporting/analytics engine.  We currently only have one instance of snowflake that points to our production AWS account/DynamoDb database and I am trying to convince my managers that we absolutely need a dev snowflake instance to point to our Non Prod AWS account to allow us to do end to end regression testing including our reports/analytics products.  I am basically being told that the overhead cost of replicating production to our lower environments will be too expensive, but at the same time we are being told that we need to have a highly available system and we have to meet SLA&amp;#39;s.  I don&amp;#39;t  find it sufficient to just do code reviews, I don&amp;#39;t think we will ever scale if we cannot perform a set of regression tests in a safe space. &lt;/p&gt;\n\n&lt;p&gt;Am I right here or are my standards of how development should be done outdated?  Would anyone else here find this as a red flag for management?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16rrecb", "is_robot_indexable": true, "report_reasons": null, "author": "nsq116", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rrecb/development_ops_separation_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rrecb/development_ops_separation_for_data_engineering/", "subreddit_subscribers": 130420, "created_utc": 1695646833.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a great use case for Debezium but we don't run Kafka. Any alternatives for change data capture with mysql out there? The end goal is replication of certain changes in 1 DB to another in \"real time\".", "author_fullname": "t2_gua18k7sg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any alternatives to Debezium that don't rely on Kafka?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16sagq5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695692419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a great use case for Debezium but we don&amp;#39;t run Kafka. Any alternatives for change data capture with mysql out there? The end goal is replication of certain changes in 1 DB to another in &amp;quot;real time&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16sagq5", "is_robot_indexable": true, "report_reasons": null, "author": "alexcontrerasdppl", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16sagq5/any_alternatives_to_debezium_that_dont_rely_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16sagq5/any_alternatives_to_debezium_that_dont_rely_on/", "subreddit_subscribers": 130420, "created_utc": 1695692419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I saw Apache AirFlow, but i think it a bit overkill and not Computer vision oriented.\n\nLooking for something that is: scalabe, monitored, easy to use, and there is (FOSS) community behind it.\n\nThanks!", "author_fullname": "t2_5831rdhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any reccomendation of python library for handling pipeline workflow for Computer vision?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16s62sf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695681075.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695680889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw Apache AirFlow, but i think it a bit overkill and not Computer vision oriented.&lt;/p&gt;\n\n&lt;p&gt;Looking for something that is: scalabe, monitored, easy to use, and there is (FOSS) community behind it.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16s62sf", "is_robot_indexable": true, "report_reasons": null, "author": "Expensive_Breakfast6", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16s62sf/any_reccomendation_of_python_library_for_handling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16s62sf/any_reccomendation_of_python_library_for_handling/", "subreddit_subscribers": 130420, "created_utc": 1695680889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TLDR;  checking if \"streaming\" with Postgres is a reasonable idea, given the throughput ( &lt; 10 updates / second) on a small amount of data (&lt; 100 mb).  \n\n\n# Functional Requirements:\n\nMy team has a custom compiled binary that is subscribed to stream of events, computes some outputs, and dumps them to a data sink process. The business logic for the streaming computation is written in a custom DSL and represents a time series transformation. This was written way before things like flink, kafka streams etc existed and has served production well over the years, but could use a little face lift, especially in terms of provenance to understand what inputs when into calculation outputs.  \n\n\n# Non-Functional Requirements:\n\nIt's a very small workload, hardly what someone would call streaming.\n\n1. 10 events / second\n2. 10mb of data\n3. 1s p99 update delay tolerable\n4. \\~ 10 large queries (100 numeric inputs), continuously refreshing. Each query only has 1-2 inputs that are actually constantly changing.\n5. No lock contention amongst the queries. They're independent of each other.  \n\n\n# Postgres Streaming solution:\n\nMy idea is to push the computation and state into Postgres as Flink and Kafka seem like overkill. The data will be modeled as a temporal table to keep a full snapshot of the history.  That way I get the following:\n\n1. High availability out of the box instead of handwritten leader election code\n2. All of the state is persisted. I can inspect the history of the state for provenance purposes using a temporal query.\n3. Rely on mature SQL computation engine instead of an esoteric, unoptimized DSL and runtime\n4. Time partitioning to keep the hot data ( &lt;100 kb ) in memory.\n5. My event processor is now stateless and has a simple job of triggering a postgres insert + update. The query will be compiled to avoid parsing and interpretation overhead.\n6. (Bonus) I can throw away a lot of custom code for a bunch of simple dbt templates as postgres now does all the heavy lifting.\n\nIt does feel weird to use postgres as a streaming engine, but people have pushed databases to handle way more concurrency on way larger data sets. Based on my napkin math, I think this barely qualifies as streaming, as the database would spend most of it's time waiting for work.", "author_fullname": "t2_2naya68b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming on Postgres idea check", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16sazes", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695693821.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR;  checking if &amp;quot;streaming&amp;quot; with Postgres is a reasonable idea, given the throughput ( &amp;lt; 10 updates / second) on a small amount of data (&amp;lt; 100 mb).  &lt;/p&gt;\n\n&lt;h1&gt;Functional Requirements:&lt;/h1&gt;\n\n&lt;p&gt;My team has a custom compiled binary that is subscribed to stream of events, computes some outputs, and dumps them to a data sink process. The business logic for the streaming computation is written in a custom DSL and represents a time series transformation. This was written way before things like flink, kafka streams etc existed and has served production well over the years, but could use a little face lift, especially in terms of provenance to understand what inputs when into calculation outputs.  &lt;/p&gt;\n\n&lt;h1&gt;Non-Functional Requirements:&lt;/h1&gt;\n\n&lt;p&gt;It&amp;#39;s a very small workload, hardly what someone would call streaming.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;10 events / second&lt;/li&gt;\n&lt;li&gt;10mb of data&lt;/li&gt;\n&lt;li&gt;1s p99 update delay tolerable&lt;/li&gt;\n&lt;li&gt;~ 10 large queries (100 numeric inputs), continuously refreshing. Each query only has 1-2 inputs that are actually constantly changing.&lt;/li&gt;\n&lt;li&gt;No lock contention amongst the queries. They&amp;#39;re independent of each other.&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Postgres Streaming solution:&lt;/h1&gt;\n\n&lt;p&gt;My idea is to push the computation and state into Postgres as Flink and Kafka seem like overkill. The data will be modeled as a temporal table to keep a full snapshot of the history.  That way I get the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;High availability out of the box instead of handwritten leader election code&lt;/li&gt;\n&lt;li&gt;All of the state is persisted. I can inspect the history of the state for provenance purposes using a temporal query.&lt;/li&gt;\n&lt;li&gt;Rely on mature SQL computation engine instead of an esoteric, unoptimized DSL and runtime&lt;/li&gt;\n&lt;li&gt;Time partitioning to keep the hot data ( &amp;lt;100 kb ) in memory.&lt;/li&gt;\n&lt;li&gt;My event processor is now stateless and has a simple job of triggering a postgres insert + update. The query will be compiled to avoid parsing and interpretation overhead.&lt;/li&gt;\n&lt;li&gt;(Bonus) I can throw away a lot of custom code for a bunch of simple dbt templates as postgres now does all the heavy lifting.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;It does feel weird to use postgres as a streaming engine, but people have pushed databases to handle way more concurrency on way larger data sets. Based on my napkin math, I think this barely qualifies as streaming, as the database would spend most of it&amp;#39;s time waiting for work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16sazes", "is_robot_indexable": true, "report_reasons": null, "author": "shuaibot", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16sazes/streaming_on_postgres_idea_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16sazes/streaming_on_postgres_idea_check/", "subreddit_subscribers": 130420, "created_utc": 1695693821.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious to hear your insights and experiences with how you may handle data discrepancies that crop up between different databases in your projects?\n\nE.g. I have one excel spreadsheet full of vendors' names and contact info and a mailchimp mailing list with their info but notice from time to time that emails between both may be different. How could I sync these without manually going through thousands of entries? \n\nDo you have any specific techniques for data reconciliation or data validation? Are there any tools that have been particularly helpful? How do you maintain data consistency and accuracy as your data ecosystem grows and evolves?\n\nHoping to find answers/tools to the above q\u2019s that would be usable by a non-technical person or don't require a dedicated programmer. ", "author_fullname": "t2_kgr62d3c0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices for Dealing with Data Discrepancies Between Databases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rza4k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695665142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious to hear your insights and experiences with how you may handle data discrepancies that crop up between different databases in your projects?&lt;/p&gt;\n\n&lt;p&gt;E.g. I have one excel spreadsheet full of vendors&amp;#39; names and contact info and a mailchimp mailing list with their info but notice from time to time that emails between both may be different. How could I sync these without manually going through thousands of entries? &lt;/p&gt;\n\n&lt;p&gt;Do you have any specific techniques for data reconciliation or data validation? Are there any tools that have been particularly helpful? How do you maintain data consistency and accuracy as your data ecosystem grows and evolves?&lt;/p&gt;\n\n&lt;p&gt;Hoping to find answers/tools to the above q\u2019s that would be usable by a non-technical person or don&amp;#39;t require a dedicated programmer. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16rza4k", "is_robot_indexable": true, "report_reasons": null, "author": "Stat58372", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rza4k/best_practices_for_dealing_with_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rza4k/best_practices_for_dealing_with_data/", "subreddit_subscribers": 130420, "created_utc": 1695665142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\nI am a data engineer in Amsterdam, NL and having 2-3 years of experience. Half of it was on data warehousing in gcp and other half was on spark/scala. I am considering to work for a IT consultancy company, but I moved here 6 months ago and don\u2019t know the sector. \n\nBasically, I want to learn as much as possible in my early stage of career and have a decent income in a non-toxic environment.\n\nWhich consultancy companies do you recommend or definitely not recommend to work as a data engineer?", "author_fullname": "t2_knked3pz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Consultancy company in Netherlands", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rsv5k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695650288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,\nI am a data engineer in Amsterdam, NL and having 2-3 years of experience. Half of it was on data warehousing in gcp and other half was on spark/scala. I am considering to work for a IT consultancy company, but I moved here 6 months ago and don\u2019t know the sector. &lt;/p&gt;\n\n&lt;p&gt;Basically, I want to learn as much as possible in my early stage of career and have a decent income in a non-toxic environment.&lt;/p&gt;\n\n&lt;p&gt;Which consultancy companies do you recommend or definitely not recommend to work as a data engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16rsv5k", "is_robot_indexable": true, "report_reasons": null, "author": "Naive-Treacle2355", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16rsv5k/consultancy_company_in_netherlands/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rsv5k/consultancy_company_in_netherlands/", "subreddit_subscribers": 130420, "created_utc": 1695650288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Alright friends, I've got a client who is dead-set on using Informatics for MDM. Anyone know off the top of their head what makes up an IPU for these guys?\n\nAlso, how angry does it make you when you try to get an answer to a question like that and their official page tells you to call a sales rep?", "author_fullname": "t2_713dpi97", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Informatica and IPU", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16rrn5k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695647428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Alright friends, I&amp;#39;ve got a client who is dead-set on using Informatics for MDM. Anyone know off the top of their head what makes up an IPU for these guys?&lt;/p&gt;\n\n&lt;p&gt;Also, how angry does it make you when you try to get an answer to a question like that and their official page tells you to call a sales rep?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16rrn5k", "is_robot_indexable": true, "report_reasons": null, "author": "rexicusmaximus", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16rrn5k/informatica_and_ipu/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16rrn5k/informatica_and_ipu/", "subreddit_subscribers": 130420, "created_utc": 1695647428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm looking into setting up an on prem Lake House with Kubernetes compute (&gt;1k nodes) and S3 compatible object store (multi PB).  Not interested in a Hadoop based stack or cloud based.   Must be able to run Apache Spark workloads and have a Metastore (like Hive Metastore or [Nessie](https://projectnessie.org/))  bonus if it can run multiple data processing engines (Trino, Flink...)  Apache 2 License is prefered, but I'm willing to pay if I find something to solve my needs. \n\n[DataRoaster](https://github.com/cloudcheflabs/dataroaster)  looks like a great start, but I'm looking for something that is actively being developed and maybe a little bit further along.  \n\nThanks for any suggestions!\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_ind8p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for On Prem Lake House software stack using k8s and s3 compat obj.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ruspl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695654793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m looking into setting up an on prem Lake House with Kubernetes compute (&amp;gt;1k nodes) and S3 compatible object store (multi PB).  Not interested in a Hadoop based stack or cloud based.   Must be able to run Apache Spark workloads and have a Metastore (like Hive Metastore or &lt;a href=\"https://projectnessie.org/\"&gt;Nessie&lt;/a&gt;)  bonus if it can run multiple data processing engines (Trino, Flink...)  Apache 2 License is prefered, but I&amp;#39;m willing to pay if I find something to solve my needs. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/cloudcheflabs/dataroaster\"&gt;DataRoaster&lt;/a&gt;  looks like a great start, but I&amp;#39;m looking for something that is actively being developed and maybe a little bit further along.  &lt;/p&gt;\n\n&lt;p&gt;Thanks for any suggestions!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ruspl", "is_robot_indexable": true, "report_reasons": null, "author": "6nop_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ruspl/looking_for_on_prem_lake_house_software_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ruspl/looking_for_on_prem_lake_house_software_stack/", "subreddit_subscribers": 130420, "created_utc": 1695654793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently been working on an easy way to spin up dashboards, scheduling, alerting, silencing, logs, etc. around some of my custom data quality Python checks. Decided this past weekend to spin that out as a standalone and simple solution that anyone can use for their checks. If it's interesting, DM me and we can talk further", "author_fullname": "t2_dcdt0mh5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data quality checks and dashboards with a lines of Python code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16s4pxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695677718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently been working on an easy way to spin up dashboards, scheduling, alerting, silencing, logs, etc. around some of my custom data quality Python checks. Decided this past weekend to spin that out as a standalone and simple solution that anyone can use for their checks. If it&amp;#39;s interesting, DM me and we can talk further&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16s4pxk", "is_robot_indexable": true, "report_reasons": null, "author": "gnahznavia", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16s4pxk/data_quality_checks_and_dashboards_with_a_lines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16s4pxk/data_quality_checks_and_dashboards_with_a_lines/", "subreddit_subscribers": 130420, "created_utc": 1695677718.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}