{"kind": "Listing", "data": {"after": null, "dist": 12, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nThere are many DE project posts out there. But they don't explain exactly why a specific approach was chosen. If you are trying to improve your data engineering skills or are the sole data person in your company, it can be hard to know how your technical skills are developing.\n\nWith this in mind, I wrote an article that explains high-level best practices (with links to specific approaches and a project), such as:\n\n1. Using established data processing patterns\n\n2. Data quality checks &amp; code testing\n\n3. Approach to make pipelines Idempotent\n\n4. Metadata for debugging and tracking\n\nI hope the posts explain the underlying concepts behind best practices and when to use them.\n\nBlog: [https://www.startdataengineering.com/post/de\\_best\\_practices/](https://www.startdataengineering.com/post/de_best_practices/)\n\nGitHub Code: [https://github.com/josephmachado/data\\_engineering\\_best\\_practices](https://github.com/josephmachado/data_engineering_best_practices)\n\nI appreciate any questions, feedback, or comments. I hope this helps someone.\n\n&amp;#x200B;", "author_fullname": "t2_5srxspj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering project: Apache Spark, Delta Lake, &amp; Great Expectations running on Docker; Explaining some best practices!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16e4d6v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 118, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 118, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1694262027.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;There are many DE project posts out there. But they don&amp;#39;t explain exactly why a specific approach was chosen. If you are trying to improve your data engineering skills or are the sole data person in your company, it can be hard to know how your technical skills are developing.&lt;/p&gt;\n\n&lt;p&gt;With this in mind, I wrote an article that explains high-level best practices (with links to specific approaches and a project), such as:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Using established data processing patterns&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Data quality checks &amp;amp; code testing&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Approach to make pipelines Idempotent&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Metadata for debugging and tracking&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I hope the posts explain the underlying concepts behind best practices and when to use them.&lt;/p&gt;\n\n&lt;p&gt;Blog: &lt;a href=\"https://www.startdataengineering.com/post/de_best_practices/\"&gt;https://www.startdataengineering.com/post/de_best_practices/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GitHub Code: &lt;a href=\"https://github.com/josephmachado/data_engineering_best_practices\"&gt;https://github.com/josephmachado/data_engineering_best_practices&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I appreciate any questions, feedback, or comments. I hope this helps someone.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wsl6YZRW6CI-kerPHLeCEYh-yPLQLYy8fcBAKEkjUc0.jpg?auto=webp&amp;s=c4451d47eef4b4c6375320f1576acbc392ccb35a", "width": 1270, "height": 716}, "resolutions": [{"url": "https://external-preview.redd.it/wsl6YZRW6CI-kerPHLeCEYh-yPLQLYy8fcBAKEkjUc0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8417a2bbe453577d5bd14f0fdfeba8e7d2f4854b", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/wsl6YZRW6CI-kerPHLeCEYh-yPLQLYy8fcBAKEkjUc0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=68639fcddff9d247782ede6857b9ed0eaabb347c", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/wsl6YZRW6CI-kerPHLeCEYh-yPLQLYy8fcBAKEkjUc0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=827ee60bc82429ecea18367d295486940ef0e4cd", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/wsl6YZRW6CI-kerPHLeCEYh-yPLQLYy8fcBAKEkjUc0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a6e981978c85a35359f14f34064a9a7aab63554d", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/wsl6YZRW6CI-kerPHLeCEYh-yPLQLYy8fcBAKEkjUc0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=76e74236c097c86237ec30fd14f2820b9b99d4bb", "width": 960, "height": 541}, {"url": "https://external-preview.redd.it/wsl6YZRW6CI-kerPHLeCEYh-yPLQLYy8fcBAKEkjUc0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4f54ae7a9decb33043ce95e131b589093ac562d4", "width": 1080, "height": 608}], "variants": {}, "id": "ZRzmjlwD29o6y4DYt1M7s52D2E2DPvb1Q28Ck4Kl-zw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16e4d6v", "is_robot_indexable": true, "report_reasons": null, "author": "joseph_machado", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16e4d6v/data_engineering_project_apache_spark_delta_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16e4d6v/data_engineering_project_apache_spark_delta_lake/", "subreddit_subscribers": 127728, "created_utc": 1694262027.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently a data analyst working in healthcare and looking to move to data engineering. For those of you who moved into the field from something else:\n\n1. What was your previous role?\n2. Why did you move to data engineering\n3. Pros and cons of the move compared to your previous job\n4. Do you regret moving to DE and why if you did\n\nAny info would be much appreciated", "author_fullname": "t2_1eqrbzj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone regret moving to Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16e9iqw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 59, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694275721.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a data analyst working in healthcare and looking to move to data engineering. For those of you who moved into the field from something else:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What was your previous role?&lt;/li&gt;\n&lt;li&gt;Why did you move to data engineering&lt;/li&gt;\n&lt;li&gt;Pros and cons of the move compared to your previous job&lt;/li&gt;\n&lt;li&gt;Do you regret moving to DE and why if you did&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any info would be much appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16e9iqw", "is_robot_indexable": true, "report_reasons": null, "author": "xyzabc123410000", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16e9iqw/does_anyone_regret_moving_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16e9iqw/does_anyone_regret_moving_to_data_engineering/", "subreddit_subscribers": 127728, "created_utc": 1694275721.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a DE at an AWS shop writing Pyspark pipelines all day long and very much enjoy it. DEs here mostly just develop ETL code and push up their docker images. We have a separate Infra,DevOps and Developer Experience team that has set up end-to-end platform over the years. \n\nNow I want to expand my skillset into being able to write end-to-end Data platforms. Do you have good learning resource recommendations Or perhaps a project showcasing how to do this at production level? If not, I'd at least love to get a learning plan.", "author_fullname": "t2_g4v8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good resources to learn core Data platform engineering and DataOps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16eltza", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694305963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a DE at an AWS shop writing Pyspark pipelines all day long and very much enjoy it. DEs here mostly just develop ETL code and push up their docker images. We have a separate Infra,DevOps and Developer Experience team that has set up end-to-end platform over the years. &lt;/p&gt;\n\n&lt;p&gt;Now I want to expand my skillset into being able to write end-to-end Data platforms. Do you have good learning resource recommendations Or perhaps a project showcasing how to do this at production level? If not, I&amp;#39;d at least love to get a learning plan.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16eltza", "is_robot_indexable": true, "report_reasons": null, "author": "swapripper", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16eltza/what_are_some_good_resources_to_learn_core_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16eltza/what_are_some_good_resources_to_learn_core_data/", "subreddit_subscribers": 127728, "created_utc": 1694305963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m new to the data field, and I was looking for resources/books that could help tie in how different tools and concepts can work together in one data project. I hear a lot about dbt, airflow, apache spark, apache hadoop, kubernetes and other tools &amp; technologies, but never how each can work hand-in-hand in one project to achieve different outcomes. \n\nAny recommendations on where to get started understanding how they\u2019re each used and integrated together in the industry in one system.", "author_fullname": "t2_5i2tn33k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "System Design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16erfno", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694322450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m new to the data field, and I was looking for resources/books that could help tie in how different tools and concepts can work together in one data project. I hear a lot about dbt, airflow, apache spark, apache hadoop, kubernetes and other tools &amp;amp; technologies, but never how each can work hand-in-hand in one project to achieve different outcomes. &lt;/p&gt;\n\n&lt;p&gt;Any recommendations on where to get started understanding how they\u2019re each used and integrated together in the industry in one system.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16erfno", "is_robot_indexable": true, "report_reasons": null, "author": "Head-Opportunity7328", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16erfno/system_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16erfno/system_design/", "subreddit_subscribers": 127728, "created_utc": 1694322450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, I'm a data analyst trying to strengthen my pipeline skills. Setting up an environment is tough, so I figured I'd use this as a chance to leverage docker.\n\nThe problem I'm having is looking for a good docker image to use for a data eng environment. I was hoping to do a project where I access Spotify data via and API and store it into json files in an S3 bucket and have a separate script to read clean and import it into a DW. Then have it all orchestrated by Dagster.\n\nBut I don't really know what image to use or trust. Any recommendations?", "author_fullname": "t2_qcvr2y7j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Docker Images are you using for your local development", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ej31k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694299046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;m a data analyst trying to strengthen my pipeline skills. Setting up an environment is tough, so I figured I&amp;#39;d use this as a chance to leverage docker.&lt;/p&gt;\n\n&lt;p&gt;The problem I&amp;#39;m having is looking for a good docker image to use for a data eng environment. I was hoping to do a project where I access Spotify data via and API and store it into json files in an S3 bucket and have a separate script to read clean and import it into a DW. Then have it all orchestrated by Dagster.&lt;/p&gt;\n\n&lt;p&gt;But I don&amp;#39;t really know what image to use or trust. Any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ej31k", "is_robot_indexable": true, "report_reasons": null, "author": "Own_Efficiency_1443", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ej31k/what_docker_images_are_you_using_for_your_local/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ej31k/what_docker_images_are_you_using_for_your_local/", "subreddit_subscribers": 127728, "created_utc": 1694299046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey fellow DE's. I've recently started a position as a senior DE and I'm tasked with designing and building the data infrastructure of my company. The benefit is that it's almost a greenfield project, there is very little in the way of DWH and data modeling and I have a lot of decision power. I have a proposal ready for my team, but I was hoping to get some feedback from other professionals first. I'm the only actual DE on the team, the others are Data Scientists who had to take up engineering tasks for their work, so they're not going to be the best soundboard I'm afraid.\n\nAnother reason I ask is because I don't have that much experience with the companies' chosen cloud provider (Azure). I have 5 YoE with most of my focus in AWS, Snowflake, dbt and python/PySpark. It's not like I feel I can't handle Azure or the task, I just know that I might be overlooking some intricacies that come with experience with the platform.\n\nSome background on the project: my company is an industrial producer of certain materials. The data flows I will be handling will mostly be IoT data from factory lines. They currently have a setup that drops the raw data in Azure Storage in hourly batches and one process that transforms the binary data into parquet. The scripts that move the data into storage are maintained by another team and not my responsibility. I've had a look at it already and it all looks clean and stable, so I'm happy with the current setup. Beyond this they have very little (read: nothing) in terms of data infrastructure. Scientists pick up their input data directly from storage and do ad-hoc transformations when they need them. I am currently unaware on how business operates their dashboards.\n\nThe goal of my infrastructure would be to create a DWH where consumers (business and the scientists) can pick up their transformed datasets and do their thing, without having to worry about spending their time wrangling data.\n\nMy proposal consists of using Azure Synapse Analytics (ASA) as the datalake/DWH. The raw parquet files would be loaded into ASA in a 'RAW' schema and picked up by transformation processes and dropped in a data mart schema to be consumed. For my transformation process I would like to use dbt. I would containerize the project and use Github Actions to push it to Azure Container Storage. I'm thinking of setting it up such that deployment would happen when a merge happens on the test/prod branch, given that the source branch has a certain naming convention (e.g. starts with 'feature-{project-code}-'). I would then use Azure Data Factory as a scheduler to run the code in either Azure Kubernetes Service or in Azure Container Instance (I'm leaning towards AKS though).\n\n----------------------------------------------------\n\nSome feedback I can foresee and get ahead of:\n\n- *'Don't use dbt, it's bad because {insert-hater-reasons}!'.* I know there's some people on here that don't like it. I've used it plenty and I've seen it used well. I know the pitfalls of model bloat, etc, so I will keep an eye out on that stuff. I like the tech, it's powerful and it does what it does very well.\n\n- *'Why not use Databricks?'* The only benefit I can see for Databricks would be that it integrates  more easily into the Azure ecosystem. Currently the volumes we're talking about do not require PySpark. Factory lines are fully independent and have separate resource groups. The hourly batches are in the order of 10-100MBs. Nothing a well organized incremental dbt model can't handle. Another problem is that I don't know of a good way to create a true/proper CI/CD pipeline with Databricks. Notebooks in production is an 'over my dead body' type of thing. I've once had to clean up a production 'pipeline' like that and I'll fight a motherfucker before I introduce that nonsense in my project.\n\n------------------------------------------------------\nTL;DR:\n\nI want to make a data platform using:\n\n- Containerized dbt code for my transformations\n\n- Github Actions for my CI/CD pipeline\n\n- Azure Data Factory as orchestrator\n\n- Azure Kubernetes Service as executors (alternatively maybe Azure Container Instance)\n\n- Azure Synapse Analytics as datalake/DWH\n\nI'm happy to hear your thoughts!", "author_fullname": "t2_ptrv3fcx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data architecture proposal feedback", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ew9af", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694339448.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow DE&amp;#39;s. I&amp;#39;ve recently started a position as a senior DE and I&amp;#39;m tasked with designing and building the data infrastructure of my company. The benefit is that it&amp;#39;s almost a greenfield project, there is very little in the way of DWH and data modeling and I have a lot of decision power. I have a proposal ready for my team, but I was hoping to get some feedback from other professionals first. I&amp;#39;m the only actual DE on the team, the others are Data Scientists who had to take up engineering tasks for their work, so they&amp;#39;re not going to be the best soundboard I&amp;#39;m afraid.&lt;/p&gt;\n\n&lt;p&gt;Another reason I ask is because I don&amp;#39;t have that much experience with the companies&amp;#39; chosen cloud provider (Azure). I have 5 YoE with most of my focus in AWS, Snowflake, dbt and python/PySpark. It&amp;#39;s not like I feel I can&amp;#39;t handle Azure or the task, I just know that I might be overlooking some intricacies that come with experience with the platform.&lt;/p&gt;\n\n&lt;p&gt;Some background on the project: my company is an industrial producer of certain materials. The data flows I will be handling will mostly be IoT data from factory lines. They currently have a setup that drops the raw data in Azure Storage in hourly batches and one process that transforms the binary data into parquet. The scripts that move the data into storage are maintained by another team and not my responsibility. I&amp;#39;ve had a look at it already and it all looks clean and stable, so I&amp;#39;m happy with the current setup. Beyond this they have very little (read: nothing) in terms of data infrastructure. Scientists pick up their input data directly from storage and do ad-hoc transformations when they need them. I am currently unaware on how business operates their dashboards.&lt;/p&gt;\n\n&lt;p&gt;The goal of my infrastructure would be to create a DWH where consumers (business and the scientists) can pick up their transformed datasets and do their thing, without having to worry about spending their time wrangling data.&lt;/p&gt;\n\n&lt;p&gt;My proposal consists of using Azure Synapse Analytics (ASA) as the datalake/DWH. The raw parquet files would be loaded into ASA in a &amp;#39;RAW&amp;#39; schema and picked up by transformation processes and dropped in a data mart schema to be consumed. For my transformation process I would like to use dbt. I would containerize the project and use Github Actions to push it to Azure Container Storage. I&amp;#39;m thinking of setting it up such that deployment would happen when a merge happens on the test/prod branch, given that the source branch has a certain naming convention (e.g. starts with &amp;#39;feature-{project-code}-&amp;#39;). I would then use Azure Data Factory as a scheduler to run the code in either Azure Kubernetes Service or in Azure Container Instance (I&amp;#39;m leaning towards AKS though).&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Some feedback I can foresee and get ahead of:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;em&gt;&amp;#39;Don&amp;#39;t use dbt, it&amp;#39;s bad because {insert-hater-reasons}!&amp;#39;.&lt;/em&gt; I know there&amp;#39;s some people on here that don&amp;#39;t like it. I&amp;#39;ve used it plenty and I&amp;#39;ve seen it used well. I know the pitfalls of model bloat, etc, so I will keep an eye out on that stuff. I like the tech, it&amp;#39;s powerful and it does what it does very well.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;em&gt;&amp;#39;Why not use Databricks?&amp;#39;&lt;/em&gt; The only benefit I can see for Databricks would be that it integrates  more easily into the Azure ecosystem. Currently the volumes we&amp;#39;re talking about do not require PySpark. Factory lines are fully independent and have separate resource groups. The hourly batches are in the order of 10-100MBs. Nothing a well organized incremental dbt model can&amp;#39;t handle. Another problem is that I don&amp;#39;t know of a good way to create a true/proper CI/CD pipeline with Databricks. Notebooks in production is an &amp;#39;over my dead body&amp;#39; type of thing. I&amp;#39;ve once had to clean up a production &amp;#39;pipeline&amp;#39; like that and I&amp;#39;ll fight a motherfucker before I introduce that nonsense in my project.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;TL;DR:&lt;/p&gt;\n\n&lt;p&gt;I want to make a data platform using:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Containerized dbt code for my transformations&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Github Actions for my CI/CD pipeline&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Azure Data Factory as orchestrator&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Azure Kubernetes Service as executors (alternatively maybe Azure Container Instance)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Azure Synapse Analytics as datalake/DWH&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m happy to hear your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ew9af", "is_robot_indexable": true, "report_reasons": null, "author": "ilikedmatrixiv", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ew9af/data_architecture_proposal_feedback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ew9af/data_architecture_proposal_feedback/", "subreddit_subscribers": 127728, "created_utc": 1694339448.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was testing this AWS Zero-ETL Integration from Aurora-MySQL to Redshift and I am so confused and disappointed! My Aurora-MySQL db is 10TB and it would be nice if it could be pushed to Redshift through this zero-etl integration. I tried to follow the documentation (set up a redshift preview cluster and zero-etl integration) and I think I got it right. But when I tried to run the zero-etl integration, it just kept spinning and I don't know why. It is just frustrating and then I could not even delete both the zero-etl integration and redshift cluster. So, has anybody gotten this feature to work?  \n\n\n&amp;#x200B;", "author_fullname": "t2_bgia4b7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Zero-ETL Integration Feedback", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ebg51", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694280520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was testing this AWS Zero-ETL Integration from Aurora-MySQL to Redshift and I am so confused and disappointed! My Aurora-MySQL db is 10TB and it would be nice if it could be pushed to Redshift through this zero-etl integration. I tried to follow the documentation (set up a redshift preview cluster and zero-etl integration) and I think I got it right. But when I tried to run the zero-etl integration, it just kept spinning and I don&amp;#39;t know why. It is just frustrating and then I could not even delete both the zero-etl integration and redshift cluster. So, has anybody gotten this feature to work?  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ebg51", "is_robot_indexable": true, "report_reasons": null, "author": "chiphelsea4", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ebg51/aws_zeroetl_integration_feedback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ebg51/aws_zeroetl_integration_feedback/", "subreddit_subscribers": 127728, "created_utc": 1694280520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI want to learn the MLOps best practices on GCP.  \nI have gone over multiples resources Coursera/YouTube/Blogs about Tensorflow Extended and Kubeflow.  \nI feel a bit confused.\n\nI understand that the GCP way is to define the ML pipeline with TFX by using the predefined modules (such as CsvExampleGen, StatisticsGen, ExampleValidator, etc...) and then orchestrate the whole pipeline with Kubeflow.\n\nBy doing so, doesn't it mean as a MLOps person I would need to re-write all the code written by the data scientists (in Pandas) using TFX modules?   \nWouldn't it be more convenient to use \"raw\" Kubeflow and simply refactor the code break it down into Kubeflow components?   \nDoes TFX running on Vertex AI bring a lot of value compared to \"raw\" Kubeflow?\n\nI guess my question is would you suggest TFX + Kubeflow or only Kubeflow.\n\nThanks in advance!", "author_fullname": "t2_3wj092gm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MLOps on GCP Vertex AI -TFX &amp; Kubeflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ejvrw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694300979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I want to learn the MLOps best practices on GCP.&lt;br/&gt;\nI have gone over multiples resources Coursera/YouTube/Blogs about Tensorflow Extended and Kubeflow.&lt;br/&gt;\nI feel a bit confused.&lt;/p&gt;\n\n&lt;p&gt;I understand that the GCP way is to define the ML pipeline with TFX by using the predefined modules (such as CsvExampleGen, StatisticsGen, ExampleValidator, etc...) and then orchestrate the whole pipeline with Kubeflow.&lt;/p&gt;\n\n&lt;p&gt;By doing so, doesn&amp;#39;t it mean as a MLOps person I would need to re-write all the code written by the data scientists (in Pandas) using TFX modules?&lt;br/&gt;\nWouldn&amp;#39;t it be more convenient to use &amp;quot;raw&amp;quot; Kubeflow and simply refactor the code break it down into Kubeflow components?&lt;br/&gt;\nDoes TFX running on Vertex AI bring a lot of value compared to &amp;quot;raw&amp;quot; Kubeflow?&lt;/p&gt;\n\n&lt;p&gt;I guess my question is would you suggest TFX + Kubeflow or only Kubeflow.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ejvrw", "is_robot_indexable": true, "report_reasons": null, "author": "yinshangyi", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ejvrw/mlops_on_gcp_vertex_ai_tfx_kubeflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ejvrw/mlops_on_gcp_vertex_ai_tfx_kubeflow/", "subreddit_subscribers": 127728, "created_utc": 1694300979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New to the channel, maybe my probation period is over as I have a question.\n\nSimply my question is:  \n1) Has anybody experience of using Dagster Cloud and also having a backup/plan-B for on-prem?\n\n2) How do you cost running on-prem yourself compared to Cloud?\n\nMore details:\n\nI am building a new pipeline to process thousands of cXML files into a DW.  We are mostly Airflow user, but for the new project we decided to upgrade and are using Dagster. We found Dagster to be amazing for the Data Asset approach, and our team are heavily using Dagster deployments for CI/CD. The costs for Dagster Cloud right now are very small but would grow with usage.\n\nWe are thinking once the main development is done, we might not need such a powerful CI/CD tool, and then we could move Dagster to on-prem with open source and host the system ourselves. Also with EU regulations we might need to move our project to our private cloud as we scale. Also all vendor tool pricing seems unpredictable (like dbt).\n\nHas anybody done this? What would you really miss in daily work not having Dagster Cloud?  \nI need to write a cost comparison of Cloud compared to self-hosting.  How to calculate all hidden cost (time) or managing Dagster ourself?", "author_fullname": "t2_i2z3iuo98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster Cloud &amp; Plan B on-prem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ec0dd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694281897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New to the channel, maybe my probation period is over as I have a question.&lt;/p&gt;\n\n&lt;p&gt;Simply my question is:&lt;br/&gt;\n1) Has anybody experience of using Dagster Cloud and also having a backup/plan-B for on-prem?&lt;/p&gt;\n\n&lt;p&gt;2) How do you cost running on-prem yourself compared to Cloud?&lt;/p&gt;\n\n&lt;p&gt;More details:&lt;/p&gt;\n\n&lt;p&gt;I am building a new pipeline to process thousands of cXML files into a DW.  We are mostly Airflow user, but for the new project we decided to upgrade and are using Dagster. We found Dagster to be amazing for the Data Asset approach, and our team are heavily using Dagster deployments for CI/CD. The costs for Dagster Cloud right now are very small but would grow with usage.&lt;/p&gt;\n\n&lt;p&gt;We are thinking once the main development is done, we might not need such a powerful CI/CD tool, and then we could move Dagster to on-prem with open source and host the system ourselves. Also with EU regulations we might need to move our project to our private cloud as we scale. Also all vendor tool pricing seems unpredictable (like dbt).&lt;/p&gt;\n\n&lt;p&gt;Has anybody done this? What would you really miss in daily work not having Dagster Cloud?&lt;br/&gt;\nI need to write a cost comparison of Cloud compared to self-hosting.  How to calculate all hidden cost (time) or managing Dagster ourself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ec0dd", "is_robot_indexable": true, "report_reasons": null, "author": "vbnotthecity", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ec0dd/dagster_cloud_plan_b_onprem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ec0dd/dagster_cloud_plan_b_onprem/", "subreddit_subscribers": 127728, "created_utc": 1694281897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone here who bought his course can tell if it's worth it?\nhttps://learn.datawithdarshil.com/courses/Data-Warehouse-for-Data-Engineers-with-Snowflake-64919790e4b0192d60c75c19\n\nHe's a YouTuber and I just came across this course from his video.\n\nThere is only other course on DWH in Udemy but he used Pentaho and Postgres.", "author_fullname": "t2_r509bej6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone took Darshil Parmar DWH course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16ey4bc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1694345983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone here who bought his course can tell if it&amp;#39;s worth it?\n&lt;a href=\"https://learn.datawithdarshil.com/courses/Data-Warehouse-for-Data-Engineers-with-Snowflake-64919790e4b0192d60c75c19\"&gt;https://learn.datawithdarshil.com/courses/Data-Warehouse-for-Data-Engineers-with-Snowflake-64919790e4b0192d60c75c19&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;He&amp;#39;s a YouTuber and I just came across this course from his video.&lt;/p&gt;\n\n&lt;p&gt;There is only other course on DWH in Udemy but he used Pentaho and Postgres.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3-5D8EmbO89IAXyY--ThfkAneH7vLqFnjkgEzphSBQ8.jpg?auto=webp&amp;s=9735c1af5d2122ea96a4436e0e00fcc2354bcd06", "width": 600, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/3-5D8EmbO89IAXyY--ThfkAneH7vLqFnjkgEzphSBQ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e115f0c345e559180570376f77e871a9f3249afe", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/3-5D8EmbO89IAXyY--ThfkAneH7vLqFnjkgEzphSBQ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c0c14c5d2719c0e7a669ee43f3577f78651f17b", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/3-5D8EmbO89IAXyY--ThfkAneH7vLqFnjkgEzphSBQ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0921baf0bdfc8f892db8ef770cc36df51ee88665", "width": 320, "height": 213}], "variants": {}, "id": "op2-poNOn7heUMO8WwMZY9VAhi3eebmETc0d84s3MtA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ey4bc", "is_robot_indexable": true, "report_reasons": null, "author": "mediocrX", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ey4bc/anyone_took_darshil_parmar_dwh_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ey4bc/anyone_took_darshil_parmar_dwh_course/", "subreddit_subscribers": 127728, "created_utc": 1694345983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've got a lambda function (py310) and I use pyscopg2 which I installed using pip install aws-psycopg2 and added the layer successfully.\n\n&amp;#x200B;\n\nI'm able to import the library in my lambda function which is good, but when i try to actually connect to the postgres database that I've got in RDS I get the following error:\n\nDatabase connection failed due to SCRAM authentication requires libpq version 10 or above\n\nI never received this error before and I used py310 and the layer on a previous lambda function. I'm not sure but I think that lambda function might've gotten messed up too now... I don't know how exactly. I did tinker around a bit with VPC, subnets, security groups and all that stuff so is it possible that might've messed it up?\n\n&amp;#x200B;\n\nBecause upon searching a bit on google I found that it's the libraries problem (psycopg2) not necessarily anything related to networking. Any ideas?", "author_fullname": "t2_5692by6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Issue connecting AWS Lambda function to Postgres hosted in AWS RDS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ehrab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694295868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a lambda function (py310) and I use pyscopg2 which I installed using pip install aws-psycopg2 and added the layer successfully.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m able to import the library in my lambda function which is good, but when i try to actually connect to the postgres database that I&amp;#39;ve got in RDS I get the following error:&lt;/p&gt;\n\n&lt;p&gt;Database connection failed due to SCRAM authentication requires libpq version 10 or above&lt;/p&gt;\n\n&lt;p&gt;I never received this error before and I used py310 and the layer on a previous lambda function. I&amp;#39;m not sure but I think that lambda function might&amp;#39;ve gotten messed up too now... I don&amp;#39;t know how exactly. I did tinker around a bit with VPC, subnets, security groups and all that stuff so is it possible that might&amp;#39;ve messed it up?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Because upon searching a bit on google I found that it&amp;#39;s the libraries problem (psycopg2) not necessarily anything related to networking. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ehrab", "is_robot_indexable": true, "report_reasons": null, "author": "anasp1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ehrab/issue_connecting_aws_lambda_function_to_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ehrab/issue_connecting_aws_lambda_function_to_postgres/", "subreddit_subscribers": 127728, "created_utc": 1694295868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Blog post is out, sharing how to solve challenges related to spark testing, including both unit and regression testing. Special Mention of spark open source testing libraries by MrPowers.\n\nhttps://www.junaideffendi.com/blog/testing-data-in-apache-spark/\n\nLet me know how you perform testing.", "author_fullname": "t2_dhgy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing Data in Apache Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16eav23", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1694279047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog post is out, sharing how to solve challenges related to spark testing, including both unit and regression testing. Special Mention of spark open source testing libraries by MrPowers.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.junaideffendi.com/blog/testing-data-in-apache-spark/\"&gt;https://www.junaideffendi.com/blog/testing-data-in-apache-spark/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know how you perform testing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/G3LkfMFa-ZT0-Ghctl_HbWIkBfQaHzYTtceghJ3WXSU.jpg?auto=webp&amp;s=8ba655d6b4e9e5ad6b6de7dc2fd5cab5f47d1ed8", "width": 2000, "height": 1333}, "resolutions": [{"url": "https://external-preview.redd.it/G3LkfMFa-ZT0-Ghctl_HbWIkBfQaHzYTtceghJ3WXSU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=97180977b6417e7cd07ed72006fc32fcbe1d8aed", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/G3LkfMFa-ZT0-Ghctl_HbWIkBfQaHzYTtceghJ3WXSU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=719cb846a641f15f6bb3eb6649340634dc0ec07c", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/G3LkfMFa-ZT0-Ghctl_HbWIkBfQaHzYTtceghJ3WXSU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a4c7fa198c08ad34c9ae552bf107abea61bbc8d", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/G3LkfMFa-ZT0-Ghctl_HbWIkBfQaHzYTtceghJ3WXSU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e29e03a933a928ddaa6fe3b2ab0993b53a89a7db", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/G3LkfMFa-ZT0-Ghctl_HbWIkBfQaHzYTtceghJ3WXSU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=37da2d6f9ebe4e519121831c35879f978c317424", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/G3LkfMFa-ZT0-Ghctl_HbWIkBfQaHzYTtceghJ3WXSU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=439666adb80b44debf93914aa21c5cf6639574db", "width": 1080, "height": 719}], "variants": {}, "id": "s2VvemkhdM8TjWxv2oJo1YYMHh7a89FogLjqj6pTUYg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16eav23", "is_robot_indexable": true, "report_reasons": null, "author": "mjfnd", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16eav23/testing_data_in_apache_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16eav23/testing_data_in_apache_spark/", "subreddit_subscribers": 127728, "created_utc": 1694279047.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}