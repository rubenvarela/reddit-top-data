{"kind": "Listing", "data": {"after": null, "dist": 12, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey fellow DE's. I've recently started a position as a senior DE and I'm tasked with designing and building the data infrastructure of my company. The benefit is that it's almost a greenfield project, there is very little in the way of DWH and data modeling and I have a lot of decision power. I have a proposal ready for my team, but I was hoping to get some feedback from other professionals first. I'm the only actual DE on the team, the others are Data Scientists who had to take up engineering tasks for their work, so they're not going to be the best soundboard I'm afraid.\n\nAnother reason I ask is because I don't have that much experience with the companies' chosen cloud provider (Azure). I have 5 YoE with most of my focus in AWS, Snowflake, dbt and python/PySpark. It's not like I feel I can't handle Azure or the task, I just know that I might be overlooking some intricacies that come with experience with the platform.\n\nSome background on the project: my company is an industrial producer of certain materials. The data flows I will be handling will mostly be IoT data from factory lines. They currently have a setup that drops the raw data in Azure Storage in hourly batches and one process that transforms the binary data into parquet. The scripts that move the data into storage are maintained by another team and not my responsibility. I've had a look at it already and it all looks clean and stable, so I'm happy with the current setup. Beyond this they have very little (read: nothing) in terms of data infrastructure. Scientists pick up their input data directly from storage and do ad-hoc transformations when they need them. I am currently unaware on how business operates their dashboards.\n\nThe goal of my infrastructure would be to create a DWH where consumers (business and the scientists) can pick up their transformed datasets and do their thing, without having to worry about spending their time wrangling data.\n\nMy proposal consists of using Azure Synapse Analytics (ASA) as the datalake/DWH. The raw parquet files would be loaded into ASA in a 'RAW' schema and picked up by transformation processes and dropped in a data mart schema to be consumed. For my transformation process I would like to use dbt. I would containerize the project and use Github Actions to push it to Azure Container Storage. I'm thinking of setting it up such that deployment would happen when a merge happens on the test/prod branch, given that the source branch has a certain naming convention (e.g. starts with 'feature-{project-code}-'). I would then use Azure Data Factory as a scheduler to run the code in either Azure Kubernetes Service or in Azure Container Instance (I'm leaning towards AKS though).\n\n----------------------------------------------------\n\nSome feedback I can foresee and get ahead of:\n\n- *'Don't use dbt, it's bad because {insert-hater-reasons}!'.* I know there's some people on here that don't like it. I've used it plenty and I've seen it used well. I know the pitfalls of model bloat, etc, so I will keep an eye out on that stuff. I like the tech, it's powerful and it does what it does very well.\n\n- *'Why not use Databricks?'* The only benefit I can see for Databricks would be that it integrates  more easily into the Azure ecosystem. Currently the volumes we're talking about do not require PySpark. Factory lines are fully independent and have separate resource groups. The hourly batches are in the order of 10-100MBs. Nothing a well organized incremental dbt model can't handle. Another problem is that I don't know of a good way to create a true/proper CI/CD pipeline with Databricks. Notebooks in production is an 'over my dead body' type of thing. I've once had to clean up a production 'pipeline' like that and I'll fight a motherfucker before I introduce that nonsense in my project.\n\n------------------------------------------------------\nTL;DR:\n\nI want to make a data platform using:\n\n- Containerized dbt code for my transformations\n\n- Github Actions for my CI/CD pipeline\n\n- Azure Data Factory as orchestrator\n\n- Azure Kubernetes Service as executors (alternatively maybe Azure Container Instance)\n\n- Azure Synapse Analytics as datalake/DWH\n\nI'm happy to hear your thoughts!", "author_fullname": "t2_ptrv3fcx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data architecture proposal feedback", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ew9af", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694339448.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow DE&amp;#39;s. I&amp;#39;ve recently started a position as a senior DE and I&amp;#39;m tasked with designing and building the data infrastructure of my company. The benefit is that it&amp;#39;s almost a greenfield project, there is very little in the way of DWH and data modeling and I have a lot of decision power. I have a proposal ready for my team, but I was hoping to get some feedback from other professionals first. I&amp;#39;m the only actual DE on the team, the others are Data Scientists who had to take up engineering tasks for their work, so they&amp;#39;re not going to be the best soundboard I&amp;#39;m afraid.&lt;/p&gt;\n\n&lt;p&gt;Another reason I ask is because I don&amp;#39;t have that much experience with the companies&amp;#39; chosen cloud provider (Azure). I have 5 YoE with most of my focus in AWS, Snowflake, dbt and python/PySpark. It&amp;#39;s not like I feel I can&amp;#39;t handle Azure or the task, I just know that I might be overlooking some intricacies that come with experience with the platform.&lt;/p&gt;\n\n&lt;p&gt;Some background on the project: my company is an industrial producer of certain materials. The data flows I will be handling will mostly be IoT data from factory lines. They currently have a setup that drops the raw data in Azure Storage in hourly batches and one process that transforms the binary data into parquet. The scripts that move the data into storage are maintained by another team and not my responsibility. I&amp;#39;ve had a look at it already and it all looks clean and stable, so I&amp;#39;m happy with the current setup. Beyond this they have very little (read: nothing) in terms of data infrastructure. Scientists pick up their input data directly from storage and do ad-hoc transformations when they need them. I am currently unaware on how business operates their dashboards.&lt;/p&gt;\n\n&lt;p&gt;The goal of my infrastructure would be to create a DWH where consumers (business and the scientists) can pick up their transformed datasets and do their thing, without having to worry about spending their time wrangling data.&lt;/p&gt;\n\n&lt;p&gt;My proposal consists of using Azure Synapse Analytics (ASA) as the datalake/DWH. The raw parquet files would be loaded into ASA in a &amp;#39;RAW&amp;#39; schema and picked up by transformation processes and dropped in a data mart schema to be consumed. For my transformation process I would like to use dbt. I would containerize the project and use Github Actions to push it to Azure Container Storage. I&amp;#39;m thinking of setting it up such that deployment would happen when a merge happens on the test/prod branch, given that the source branch has a certain naming convention (e.g. starts with &amp;#39;feature-{project-code}-&amp;#39;). I would then use Azure Data Factory as a scheduler to run the code in either Azure Kubernetes Service or in Azure Container Instance (I&amp;#39;m leaning towards AKS though).&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Some feedback I can foresee and get ahead of:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;em&gt;&amp;#39;Don&amp;#39;t use dbt, it&amp;#39;s bad because {insert-hater-reasons}!&amp;#39;.&lt;/em&gt; I know there&amp;#39;s some people on here that don&amp;#39;t like it. I&amp;#39;ve used it plenty and I&amp;#39;ve seen it used well. I know the pitfalls of model bloat, etc, so I will keep an eye out on that stuff. I like the tech, it&amp;#39;s powerful and it does what it does very well.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;em&gt;&amp;#39;Why not use Databricks?&amp;#39;&lt;/em&gt; The only benefit I can see for Databricks would be that it integrates  more easily into the Azure ecosystem. Currently the volumes we&amp;#39;re talking about do not require PySpark. Factory lines are fully independent and have separate resource groups. The hourly batches are in the order of 10-100MBs. Nothing a well organized incremental dbt model can&amp;#39;t handle. Another problem is that I don&amp;#39;t know of a good way to create a true/proper CI/CD pipeline with Databricks. Notebooks in production is an &amp;#39;over my dead body&amp;#39; type of thing. I&amp;#39;ve once had to clean up a production &amp;#39;pipeline&amp;#39; like that and I&amp;#39;ll fight a motherfucker before I introduce that nonsense in my project.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;TL;DR:&lt;/p&gt;\n\n&lt;p&gt;I want to make a data platform using:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Containerized dbt code for my transformations&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Github Actions for my CI/CD pipeline&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Azure Data Factory as orchestrator&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Azure Kubernetes Service as executors (alternatively maybe Azure Container Instance)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Azure Synapse Analytics as datalake/DWH&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m happy to hear your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ew9af", "is_robot_indexable": true, "report_reasons": null, "author": "ilikedmatrixiv", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ew9af/data_architecture_proposal_feedback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ew9af/data_architecture_proposal_feedback/", "subreddit_subscribers": 127847, "created_utc": 1694339448.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m new to the data field, and I was looking for resources/books that could help tie in how different tools and concepts can work together in one data project. I hear a lot about dbt, airflow, apache spark, apache hadoop, kubernetes and other tools &amp; technologies, but never how each can work hand-in-hand in one project to achieve different outcomes. \n\nAny recommendations on where to get started understanding how they\u2019re each used and integrated together in the industry in one system.", "author_fullname": "t2_5i2tn33k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "System Design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16erfno", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694322450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m new to the data field, and I was looking for resources/books that could help tie in how different tools and concepts can work together in one data project. I hear a lot about dbt, airflow, apache spark, apache hadoop, kubernetes and other tools &amp;amp; technologies, but never how each can work hand-in-hand in one project to achieve different outcomes. &lt;/p&gt;\n\n&lt;p&gt;Any recommendations on where to get started understanding how they\u2019re each used and integrated together in the industry in one system.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16erfno", "is_robot_indexable": true, "report_reasons": null, "author": "Head-Opportunity7328", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16erfno/system_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16erfno/system_design/", "subreddit_subscribers": 127847, "created_utc": 1694322450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel stagnated at my current job. I'm an architect in the biotech industry, but my actual responsibilities are closer to a product manager if anything else. In the past 2 years I've been hardly exposed to any technical challenges and when I am it's always at such a high-level that I don't feel any growth or satisfaction when completing projects.\n\nIn general I want to leave the biotech industry for more technically mature companies like Google or Microsoft, but I feel like I need to step backward to step forward. I'm interested in joining a new company as an engineer, be exposed to new ideas and best practices, and then continue from there. However, because I don't work hands-on in my day job, it's a catch-22 to actually pass the interviews!\n\n I'd love to have hands-on experience with tools and technologies like Iceberg, Hudi, Trino, ect. that I don't get to use in my day job, and ideally I could build some sort of personal project around them. But data engineering is different than something like frontend engineering. There's more startup and cost associated with building a data project, and I'm not sure if I can actually master some of these technologies without working with *big data*. I'm curious if anyone here has any recommendations on potential data engineering personal projects? Or recommendations on switching industries when you feel overleveled in your current one :)  ", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for data engineering personal project ideas for technical growth?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16f9z4h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694375941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel stagnated at my current job. I&amp;#39;m an architect in the biotech industry, but my actual responsibilities are closer to a product manager if anything else. In the past 2 years I&amp;#39;ve been hardly exposed to any technical challenges and when I am it&amp;#39;s always at such a high-level that I don&amp;#39;t feel any growth or satisfaction when completing projects.&lt;/p&gt;\n\n&lt;p&gt;In general I want to leave the biotech industry for more technically mature companies like Google or Microsoft, but I feel like I need to step backward to step forward. I&amp;#39;m interested in joining a new company as an engineer, be exposed to new ideas and best practices, and then continue from there. However, because I don&amp;#39;t work hands-on in my day job, it&amp;#39;s a catch-22 to actually pass the interviews!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to have hands-on experience with tools and technologies like Iceberg, Hudi, Trino, ect. that I don&amp;#39;t get to use in my day job, and ideally I could build some sort of personal project around them. But data engineering is different than something like frontend engineering. There&amp;#39;s more startup and cost associated with building a data project, and I&amp;#39;m not sure if I can actually master some of these technologies without working with &lt;em&gt;big data&lt;/em&gt;. I&amp;#39;m curious if anyone here has any recommendations on potential data engineering personal projects? Or recommendations on switching industries when you feel overleveled in your current one :)  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16f9z4h", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16f9z4h/looking_for_data_engineering_personal_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16f9z4h/looking_for_data_engineering_personal_project/", "subreddit_subscribers": 127847, "created_utc": 1694375941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I believe the core cloud concepts are global, like :\n\n- storage vs processing separation\n- serveles functions\n- IAM\n- managed DBs\n- HA \n- VPCs\n- cost management\n- etc\n\nSo how dificult would be taking an AWS job being a GCP guy? Im my opinion should not be a problem since most of our work is figuring things on the fly, but I do acknowledge that AWS is more complex so not sure what my chances are \n\nIm taking de AWS asociate(SAA-C03) cert btw", "author_fullname": "t2_4bchq4zo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can a GCP DE take a AWS job comfortably?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16f08vb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694352328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I believe the core cloud concepts are global, like :&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;storage vs processing separation&lt;/li&gt;\n&lt;li&gt;serveles functions&lt;/li&gt;\n&lt;li&gt;IAM&lt;/li&gt;\n&lt;li&gt;managed DBs&lt;/li&gt;\n&lt;li&gt;HA &lt;/li&gt;\n&lt;li&gt;VPCs&lt;/li&gt;\n&lt;li&gt;cost management&lt;/li&gt;\n&lt;li&gt;etc&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So how dificult would be taking an AWS job being a GCP guy? Im my opinion should not be a problem since most of our work is figuring things on the fly, but I do acknowledge that AWS is more complex so not sure what my chances are &lt;/p&gt;\n\n&lt;p&gt;Im taking de AWS asociate(SAA-C03) cert btw&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16f08vb", "is_robot_indexable": true, "report_reasons": null, "author": "Grand-Theory", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16f08vb/can_a_gcp_de_take_a_aws_job_comfortably/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16f08vb/can_a_gcp_de_take_a_aws_job_comfortably/", "subreddit_subscribers": 127847, "created_utc": 1694352328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's suppose you have an ELT process: If you want to use a data lake just to dump ingested data before modeling them in a data warehouse, is there any benefit of using Databricks Delta Lake just for the landing zone instead of just a datalake?\n\nI understand that the vast majority of their features are tailored towards structured data (delta tables etc.), so for the ingestion layer (landing zone) of this process, what is the extra benefit of using Databrikcs over some storage (Azure ADLS2, AWS S3 etc)?", "author_fullname": "t2_3cuv2cgu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Databricks just for the ingestion of data and nothing else (?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16eylw4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1694348149.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694347484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s suppose you have an ELT process: If you want to use a data lake just to dump ingested data before modeling them in a data warehouse, is there any benefit of using Databricks Delta Lake just for the landing zone instead of just a datalake?&lt;/p&gt;\n\n&lt;p&gt;I understand that the vast majority of their features are tailored towards structured data (delta tables etc.), so for the ingestion layer (landing zone) of this process, what is the extra benefit of using Databrikcs over some storage (Azure ADLS2, AWS S3 etc)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16eylw4", "is_robot_indexable": true, "report_reasons": null, "author": "Equivalent-Style6371", "discussion_type": null, "num_comments": 14, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16eylw4/using_databricks_just_for_the_ingestion_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16eylw4/using_databricks_just_for_the_ingestion_of_data/", "subreddit_subscribers": 127847, "created_utc": 1694347484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I am developing and testing a set of data quality guidelines for semi-structured data during the transformation layer of a data warehouse. I would like to test the proposed guidelines on a dataset. In particular, I am looking for a sample database in 3NF, along with  the raw json and xml files before it was converted into a structured database format. I have searched online but I am having a hard time finding exactly this. Does anyone know of  any open source databases resources that meets my requirements ?", "author_fullname": "t2_6g5ryjg5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mock database in 3NF along with raw semi-structured files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16f2lrh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694358531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I am developing and testing a set of data quality guidelines for semi-structured data during the transformation layer of a data warehouse. I would like to test the proposed guidelines on a dataset. In particular, I am looking for a sample database in 3NF, along with  the raw json and xml files before it was converted into a structured database format. I have searched online but I am having a hard time finding exactly this. Does anyone know of  any open source databases resources that meets my requirements ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16f2lrh", "is_robot_indexable": true, "report_reasons": null, "author": "yasteel11", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16f2lrh/mock_database_in_3nf_along_with_raw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16f2lrh/mock_database_in_3nf_along_with_raw/", "subreddit_subscribers": 127847, "created_utc": 1694358531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any one using Airflow dag to run the dbt tests for dbt models? Please suggest any online references? I have been trying to create/schedule a dag to run the dbt tests in dbt core. #airflow #dbt #dbtcore", "author_fullname": "t2_ke7daufi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow dags to run the dbt tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16f0gxh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694352965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any one using Airflow dag to run the dbt tests for dbt models? Please suggest any online references? I have been trying to create/schedule a dag to run the dbt tests in dbt core. #airflow #dbt #dbtcore&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16f0gxh", "is_robot_indexable": true, "report_reasons": null, "author": "Long-Neighborhood330", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16f0gxh/airflow_dags_to_run_the_dbt_tests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16f0gxh/airflow_dags_to_run_the_dbt_tests/", "subreddit_subscribers": 127847, "created_utc": 1694352965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our current architecture is pulling all marketing data in S3 from different sources: salesforce, oracle db, FTP, APIs of marketing tools. We currently pull in all of this using talend which is pretty inefficient. One of the reasons is it takes way too much time pull all this in. What better options can I use to pull all this data in?. P.S : We also want to make this real time", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data ingestion tools/tech", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16f8w3p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694373456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our current architecture is pulling all marketing data in S3 from different sources: salesforce, oracle db, FTP, APIs of marketing tools. We currently pull in all of this using talend which is pretty inefficient. One of the reasons is it takes way too much time pull all this in. What better options can I use to pull all this data in?. P.S : We also want to make this real time&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16f8w3p", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16f8w3p/data_ingestion_toolstech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16f8w3p/data_ingestion_toolstech/", "subreddit_subscribers": 127847, "created_utc": 1694373456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I have a problem, I'm migrating mssql data to a data lake based on HDFS and I wanted to write a simple tool that will be able to check if the migration was successful. \n\nI have to use PySpark for this task - and here I wanted to ask for advice on how would you approached it.  \n   \nFirst, I think I'll check if the columns in one table are the same as the columns in the other, and then whether the data types in are identical \n\n  \nThis seems simple, but I wanted to ask how you would approach the very issue of comparing rows and individual values \u200b\u200b- do you have any idea what function methods to use?\n\nMaybe some hashing of records, or some kind of joins on two dataframes ?\n\nThe goal is to check what are differences, and catch them up and show in some summary or table. \n\nI think dataframes can be huge, so would be great to do it in pretty smart/lazy way. Would appreciate any help. Thanks\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n ", "author_fullname": "t2_835lb23v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark data migration - how to compare 2 data frames without using any external tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16f3w5e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694361656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I have a problem, I&amp;#39;m migrating mssql data to a data lake based on HDFS and I wanted to write a simple tool that will be able to check if the migration was successful. &lt;/p&gt;\n\n&lt;p&gt;I have to use PySpark for this task - and here I wanted to ask for advice on how would you approached it.  &lt;/p&gt;\n\n&lt;p&gt;First, I think I&amp;#39;ll check if the columns in one table are the same as the columns in the other, and then whether the data types in are identical &lt;/p&gt;\n\n&lt;p&gt;This seems simple, but I wanted to ask how you would approach the very issue of comparing rows and individual values \u200b\u200b- do you have any idea what function methods to use?&lt;/p&gt;\n\n&lt;p&gt;Maybe some hashing of records, or some kind of joins on two dataframes ?&lt;/p&gt;\n\n&lt;p&gt;The goal is to check what are differences, and catch them up and show in some summary or table. &lt;/p&gt;\n\n&lt;p&gt;I think dataframes can be huge, so would be great to do it in pretty smart/lazy way. Would appreciate any help. Thanks&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16f3w5e", "is_robot_indexable": true, "report_reasons": null, "author": "Purple_Wrap9596", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16f3w5e/pyspark_data_migration_how_to_compare_2_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16f3w5e/pyspark_data_migration_how_to_compare_2_data/", "subreddit_subscribers": 127847, "created_utc": 1694361656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview with Seth Wiesman (Materialize)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_16fgtic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xtVs5y6S5k2xhwvSZ0dScc6hk4NytwMuA0NO--wjUgo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694392229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/interview-with-seth-wiesman-materialize?utm_campaign=post&amp;utm_medium=web", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zjx7Uc1hEd0B-Vwrx62AKaofgXZkZ3-6lYvhobEaHU8.jpg?auto=webp&amp;s=edb9b27fcfcb55c4b515b0a71a043b46a6b35e85", "width": 170, "height": 170}, "resolutions": [{"url": "https://external-preview.redd.it/zjx7Uc1hEd0B-Vwrx62AKaofgXZkZ3-6lYvhobEaHU8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=100dfb12fdcad3ee0349d8e78770266269a066ab", "width": 108, "height": 108}], "variants": {}, "id": "XPfoqna9Yj25waJh0iDNlNWPjCX87lqjgkiMP5ZiH5k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16fgtic", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fgtic/interview_with_seth_wiesman_materialize/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/interview-with-seth-wiesman-materialize?utm_campaign=post&amp;utm_medium=web", "subreddit_subscribers": 127847, "created_utc": 1694392229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://open.substack.com/pub/hubertdulay/p/interview-with-mike-and-tun-at-quix?utm_campaign=post&amp;utm_medium=web", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview with Quix podcast", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ezxfi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1694351403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://open.substack.com/pub/hubertdulay/p/interview-with-mike-and-tun-at-quix?utm_campaign=post&amp;amp;utm_medium=web\"&gt;https://open.substack.com/pub/hubertdulay/p/interview-with-mike-and-tun-at-quix?utm_campaign=post&amp;amp;utm_medium=web&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q2CSzi0CESqOGOrRkn5yqMCAVJ7z5Vn6qic8mRVKfrk.jpg?auto=webp&amp;s=a9b992b87b28dfd64951bfe2859e3c5ec1355a59", "width": 170, "height": 170}, "resolutions": [{"url": "https://external-preview.redd.it/q2CSzi0CESqOGOrRkn5yqMCAVJ7z5Vn6qic8mRVKfrk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dcffb928983812e0d32553554da64eb750ce3b98", "width": 108, "height": 108}], "variants": {}, "id": "daNPfDq2yn2U76chGE0wzuAehU1nLg3gpprPOyyJ74g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16ezxfi", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ezxfi/interview_with_quix_podcast/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ezxfi/interview_with_quix_podcast/", "subreddit_subscribers": 127847, "created_utc": 1694351403.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google vs Databricks - Who wins the Lakehouse/open table formats race ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16f0msa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694353400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16f0msa", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16f0msa/google_vs_databricks_who_wins_the_lakehouseopen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16f0msa/google_vs_databricks_who_wins_the_lakehouseopen/", "subreddit_subscribers": 127847, "created_utc": 1694353400.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}