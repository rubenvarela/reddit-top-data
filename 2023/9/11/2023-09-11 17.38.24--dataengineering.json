{"kind": "Listing", "data": {"after": "t3_16flqt7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_s4te90xk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The state of data content on LinkedIn: you can reduce costs by just doing less! Game changing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "name": "t3_16frhic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 110, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 110, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/b05wesGJLhREGk_kcRzO6_TvWwka0_rZJAHtbY5pAW4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694427062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/cg4yomx0plnb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/cg4yomx0plnb1.jpg?auto=webp&amp;s=57018188065745b50bd77364efd06d5a59e22c62", "width": 1079, "height": 844}, "resolutions": [{"url": "https://preview.redd.it/cg4yomx0plnb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=70d104ac81fb242540debb4b8978f475c921e006", "width": 108, "height": 84}, {"url": "https://preview.redd.it/cg4yomx0plnb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4de7359dc36ec56eacf247b1a61cf7ce344091c", "width": 216, "height": 168}, {"url": "https://preview.redd.it/cg4yomx0plnb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ea27aadc46510ddb42a20b39801c07b3705283f", "width": 320, "height": 250}, {"url": "https://preview.redd.it/cg4yomx0plnb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4f0c95004fbd47faf6d469a92d4e36022250cc01", "width": 640, "height": 500}, {"url": "https://preview.redd.it/cg4yomx0plnb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f2289b4534674760fb9d2e425e56e848d323536", "width": 960, "height": 750}], "variants": {}, "id": "PDNj3pC_3dM6xzgIhUmUiJViAwv2G7rUWS8pN56jM20"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "16frhic", "is_robot_indexable": true, "report_reasons": null, "author": "dataxp-community", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16frhic/the_state_of_data_content_on_linkedin_you_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/cg4yomx0plnb1.jpg", "subreddit_subscribers": 127934, "created_utc": 1694427062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel stagnated at my current job. I'm an architect in the biotech industry, but my actual responsibilities are closer to a product manager if anything else. In the past 2 years I've been hardly exposed to any technical challenges and when I am it's always at such a high-level that I don't feel any growth or satisfaction when completing projects.\n\nIn general I want to leave the biotech industry for more technically mature companies like Google or Microsoft, but I feel like I need to step backward to step forward. I'm interested in joining a new company as an engineer, be exposed to new ideas and best practices, and then continue from there. However, because I don't work hands-on in my day job, it's a catch-22 to actually pass the interviews!\n\n I'd love to have hands-on experience with tools and technologies like Iceberg, Hudi, Trino, ect. that I don't get to use in my day job, and ideally I could build some sort of personal project around them. But data engineering is different than something like frontend engineering. There's more startup and cost associated with building a data project, and I'm not sure if I can actually master some of these technologies without working with *big data*. I'm curious if anyone here has any recommendations on potential data engineering personal projects? Or recommendations on switching industries when you feel overleveled in your current one :)  ", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for data engineering personal project ideas for technical growth?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16f9z4h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694375941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel stagnated at my current job. I&amp;#39;m an architect in the biotech industry, but my actual responsibilities are closer to a product manager if anything else. In the past 2 years I&amp;#39;ve been hardly exposed to any technical challenges and when I am it&amp;#39;s always at such a high-level that I don&amp;#39;t feel any growth or satisfaction when completing projects.&lt;/p&gt;\n\n&lt;p&gt;In general I want to leave the biotech industry for more technically mature companies like Google or Microsoft, but I feel like I need to step backward to step forward. I&amp;#39;m interested in joining a new company as an engineer, be exposed to new ideas and best practices, and then continue from there. However, because I don&amp;#39;t work hands-on in my day job, it&amp;#39;s a catch-22 to actually pass the interviews!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to have hands-on experience with tools and technologies like Iceberg, Hudi, Trino, ect. that I don&amp;#39;t get to use in my day job, and ideally I could build some sort of personal project around them. But data engineering is different than something like frontend engineering. There&amp;#39;s more startup and cost associated with building a data project, and I&amp;#39;m not sure if I can actually master some of these technologies without working with &lt;em&gt;big data&lt;/em&gt;. I&amp;#39;m curious if anyone here has any recommendations on potential data engineering personal projects? Or recommendations on switching industries when you feel overleveled in your current one :)  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16f9z4h", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16f9z4h/looking_for_data_engineering_personal_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16f9z4h/looking_for_data_engineering_personal_project/", "subreddit_subscribers": 127934, "created_utc": 1694375941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long story short I\u2019m being put in charge of building an entire database and our entire backend infrastructure. Is this within the realm of data engineering?", "author_fullname": "t2_e0rep", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are data engineers in charge of creating database infrastructure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fmgwu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694408547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I\u2019m being put in charge of building an entire database and our entire backend infrastructure. Is this within the realm of data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fmgwu", "is_robot_indexable": true, "report_reasons": null, "author": "BiggyDeeKay", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fmgwu/are_data_engineers_in_charge_of_creating_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fmgwu/are_data_engineers_in_charge_of_creating_database/", "subreddit_subscribers": 127934, "created_utc": 1694408547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone! We currently have a series of .py files with ELT logic (load and write functions). However, we find that maintaining SQL code within these files is not elegant / best solution. What's a more elegant / best way to approach handling Spark SQL inside our elt .py files? Should we keep embedding SQL logic (say select * from table where date) inside the .py files or have .sql files in our repo and execute such .sql files inside our python elt files? Any advice/comments would be helpful!", "author_fullname": "t2_56ltry44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Elegant way of writing SQL inside of ETL .py files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fkudt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694403417.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! We currently have a series of .py files with ELT logic (load and write functions). However, we find that maintaining SQL code within these files is not elegant / best solution. What&amp;#39;s a more elegant / best way to approach handling Spark SQL inside our elt .py files? Should we keep embedding SQL logic (say select * from table where date) inside the .py files or have .sql files in our repo and execute such .sql files inside our python elt files? Any advice/comments would be helpful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fkudt", "is_robot_indexable": true, "report_reasons": null, "author": "Specific-Passage", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fkudt/elegant_way_of_writing_sql_inside_of_etl_py_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fkudt/elegant_way_of_writing_sql_inside_of_etl_py_files/", "subreddit_subscribers": 127934, "created_utc": 1694403417.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our current architecture is pulling all marketing data in S3 from different sources: salesforce, oracle db, FTP, APIs of marketing tools. We currently pull in all of this using talend which is pretty inefficient. One of the reasons is it takes way too much time pull all this in. What better options can I use to pull all this data in?. P.S : We also want to make this real time", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data ingestion tools/tech", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16f8w3p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694373456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our current architecture is pulling all marketing data in S3 from different sources: salesforce, oracle db, FTP, APIs of marketing tools. We currently pull in all of this using talend which is pretty inefficient. One of the reasons is it takes way too much time pull all this in. What better options can I use to pull all this data in?. P.S : We also want to make this real time&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16f8w3p", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16f8w3p/data_ingestion_toolstech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16f8w3p/data_ingestion_toolstech/", "subreddit_subscribers": 127934, "created_utc": 1694373456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hey, I have a quick question, please.\n\nI'm into the type of jobs that have more engineering side than charts and analysis for business cases\n\n So just need to know if I can get a junior and first job as a machine learning engineer or data engineer if I learn what it takes for that.\n\n I see people who say you can't work in these roles unless you come from previous data-related roles such as data analyst.", "author_fullname": "t2_8sxs7l2s1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE and ML junior roles without DA road", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fp08c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694417639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I have a quick question, please.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m into the type of jobs that have more engineering side than charts and analysis for business cases&lt;/p&gt;\n\n&lt;p&gt;So just need to know if I can get a junior and first job as a machine learning engineer or data engineer if I learn what it takes for that.&lt;/p&gt;\n\n&lt;p&gt;I see people who say you can&amp;#39;t work in these roles unless you come from previous data-related roles such as data analyst.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16fp08c", "is_robot_indexable": true, "report_reasons": null, "author": "Guru_202399", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fp08c/de_and_ml_junior_roles_without_da_road/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fp08c/de_and_ml_junior_roles_without_da_road/", "subreddit_subscribers": 127934, "created_utc": 1694417639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,   \nI am new DE, working on Azure and got asked if its possible to establish dynamic folder path in sync using DataFlow in ADF. I was working all the time on copy activity and though it is possible, answered positively and now I struggle to establish that. Is there any possibility? I've been chat gpt'd, youtube and stackoverflow and seems its impossible or out of the box solution? I am worried that I might be invited to some unpleasnt conversation tomorrow  \n", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have I made fool of myself?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fx672", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694443580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nI am new DE, working on Azure and got asked if its possible to establish dynamic folder path in sync using DataFlow in ADF. I was working all the time on copy activity and though it is possible, answered positively and now I struggle to establish that. Is there any possibility? I&amp;#39;ve been chat gpt&amp;#39;d, youtube and stackoverflow and seems its impossible or out of the box solution? I am worried that I might be invited to some unpleasnt conversation tomorrow  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16fx672", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fx672/have_i_made_fool_of_myself/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fx672/have_i_made_fool_of_myself/", "subreddit_subscribers": 127934, "created_utc": 1694443580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there \ud83d\udc4b\n\nWhat questions you're usually asked in interviews about Airflow or what do you ask candidates?\n\nThank you for your help!  \n", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions during DE interviews about Apache Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fw7t8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694441186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there \ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;What questions you&amp;#39;re usually asked in interviews about Airflow or what do you ask candidates?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your help!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16fw7t8", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fw7t8/questions_during_de_interviews_about_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fw7t8/questions_during_de_interviews_about_apache/", "subreddit_subscribers": 127934, "created_utc": 1694441186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nHoping to spark some healthy debate over Git workflows.\n\nDo you think experienced engineers (e.g. Senior/Lead/Principal) should be allowed to merge from `branch` --&gt; `master`/`main`** without review?\n\n**or whatever you call your production branch\n\nI'll give a couple of examples below, for context:\n- Engineer opens a hotfix merge/pull request to fix an Airflow pipeline issue which is causing a production job to fail\n- Engineer opens a maintenance/BAU change to dbt reporting model\n- Engineer opens a new feature request which is \"urgent\"\n\nWe're currently having this discussion internally, and my feeling is that every MR/PR should be reviewed and approved by another engineer before it is merged into `master`. My reasoning behind this is:\n- It's possible that mistakes have been made and the engineer who has made those changes might overlook them\n  - Often times it takes a fresh pair of eyes to see issues or give a different perspective on changes which can help to triage issues before they occur\n- Any changes that can affect production pipelines should be treated with caution either because they can break said pipelines or the data outcomes are not what was expected\n  - I see production jobs/pipelines/code as something that needs to be managed carefully given that it can cause data to be late, business-critical reports to be incorrect and data consumers to become agitated or downright pissed-off\n\nI'm curious to hear what other engineers think on this topic and get some fresh perspectives that I hadn't considered before.", "author_fullname": "t2_pctu6cby", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Git - Merging to Master", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fsf8x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1694430642.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694430317.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Hoping to spark some healthy debate over Git workflows.&lt;/p&gt;\n\n&lt;p&gt;Do you think experienced engineers (e.g. Senior/Lead/Principal) should be allowed to merge from &lt;code&gt;branch&lt;/code&gt; --&amp;gt; &lt;code&gt;master&lt;/code&gt;/&lt;code&gt;main&lt;/code&gt;** without review?&lt;/p&gt;\n\n&lt;p&gt;**or whatever you call your production branch&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll give a couple of examples below, for context:\n- Engineer opens a hotfix merge/pull request to fix an Airflow pipeline issue which is causing a production job to fail\n- Engineer opens a maintenance/BAU change to dbt reporting model\n- Engineer opens a new feature request which is &amp;quot;urgent&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re currently having this discussion internally, and my feeling is that every MR/PR should be reviewed and approved by another engineer before it is merged into &lt;code&gt;master&lt;/code&gt;. My reasoning behind this is:\n- It&amp;#39;s possible that mistakes have been made and the engineer who has made those changes might overlook them\n  - Often times it takes a fresh pair of eyes to see issues or give a different perspective on changes which can help to triage issues before they occur\n- Any changes that can affect production pipelines should be treated with caution either because they can break said pipelines or the data outcomes are not what was expected\n  - I see production jobs/pipelines/code as something that needs to be managed carefully given that it can cause data to be late, business-critical reports to be incorrect and data consumers to become agitated or downright pissed-off&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious to hear what other engineers think on this topic and get some fresh perspectives that I hadn&amp;#39;t considered before.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fsf8x", "is_robot_indexable": true, "report_reasons": null, "author": "arthur_mills", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fsf8x/git_merging_to_master/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fsf8x/git_merging_to_master/", "subreddit_subscribers": 127934, "created_utc": 1694430317.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Build Data Products? - Learn about Metric-Targeting, Semantic Engineering, Model Validation, and More.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fql3e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1694423786.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/how-to-build-data-products-design", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16fql3e", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fql3e/how_to_build_data_products_learn_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/how-to-build-data-products-design", "subreddit_subscribers": 127934, "created_utc": 1694423786.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview with Seth Wiesman (Materialize)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_16fgtic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xtVs5y6S5k2xhwvSZ0dScc6hk4NytwMuA0NO--wjUgo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694392229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/interview-with-seth-wiesman-materialize?utm_campaign=post&amp;utm_medium=web", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zjx7Uc1hEd0B-Vwrx62AKaofgXZkZ3-6lYvhobEaHU8.jpg?auto=webp&amp;s=edb9b27fcfcb55c4b515b0a71a043b46a6b35e85", "width": 170, "height": 170}, "resolutions": [{"url": "https://external-preview.redd.it/zjx7Uc1hEd0B-Vwrx62AKaofgXZkZ3-6lYvhobEaHU8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=100dfb12fdcad3ee0349d8e78770266269a066ab", "width": 108, "height": 108}], "variants": {}, "id": "XPfoqna9Yj25waJh0iDNlNWPjCX87lqjgkiMP5ZiH5k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16fgtic", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fgtic/interview_with_seth_wiesman_materialize/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/interview-with-seth-wiesman-materialize?utm_campaign=post&amp;utm_medium=web", "subreddit_subscribers": 127934, "created_utc": 1694392229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In a behavioral health data warehouse, we're looking to house demographic data which is specific to an encounter (aka episode of care).\n\nSummary of existing tables:\n\n* FactEncounter table which is technically at a client-program grain (one row for each program attended during an encounter). Note that some demographics link to dimensions (i.e. EducationFacility) while others are calculated and would be stored here (i.e. veteran status Y/N) if that approach is taken.\n* Encounter (dimension) table (which is largely just used for bridging between facts based on the EncounterID)\n* Various other facts, of course, which do not currently have foreign key IDs to link to demographic data\n\nI'd like to add demographics properties (age at admission, veteran status, etc.) which are specific to to each encounter. We'll often be referencing these demographics from other fact tables within their various models (i.e. revenue summaries grouped or filtered by age groups, veteran status, educational status).\n\nMy options, as I see them, along with perceived cons of each:\n\n1. Add all encounter-specific demographics to FactEncounter and use FactEncounter-based summaries to reference those demographics when reporting on other dimensions  \n*\\*I have to use summaries (i.e. Min/Max) to force data to retrieve into a visual that's using another fact and including these demographics from FactEncounter (otherwise we get many-many type behavior), and performance is therefore bad.*\n2. Add all encounter-specific demographics to FactEncounter and also add those demographics to all other dimensions whose reporting may need to reference them  \n*\\*Repeated use of similar code makes model maintenance cumbersome and could lead to mismatched summaries if code is updated in some but not all facts later.*\n3. Add all encounter-specific demographics to Encounter dimension  \n*\\*Relationship from FactEncounter (and other facts) to the dimension(s) referenced by the demographics changes from star to snowflake*\n\nWhat is the best practice to follow in this case? These properties feel like dimension attributes to me, but I've always seen strong guidance away from snowflaking, so I'm unsure. TIA for any advice you all can offer.", "author_fullname": "t2_cknqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Demographics - Fact vs. Dimension", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fxc7r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694443984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In a behavioral health data warehouse, we&amp;#39;re looking to house demographic data which is specific to an encounter (aka episode of care).&lt;/p&gt;\n\n&lt;p&gt;Summary of existing tables:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;FactEncounter table which is technically at a client-program grain (one row for each program attended during an encounter). Note that some demographics link to dimensions (i.e. EducationFacility) while others are calculated and would be stored here (i.e. veteran status Y/N) if that approach is taken.&lt;/li&gt;\n&lt;li&gt;Encounter (dimension) table (which is largely just used for bridging between facts based on the EncounterID)&lt;/li&gt;\n&lt;li&gt;Various other facts, of course, which do not currently have foreign key IDs to link to demographic data&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d like to add demographics properties (age at admission, veteran status, etc.) which are specific to to each encounter. We&amp;#39;ll often be referencing these demographics from other fact tables within their various models (i.e. revenue summaries grouped or filtered by age groups, veteran status, educational status).&lt;/p&gt;\n\n&lt;p&gt;My options, as I see them, along with perceived cons of each:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Add all encounter-specific demographics to FactEncounter and use FactEncounter-based summaries to reference those demographics when reporting on other dimensions&lt;br/&gt;\n&lt;em&gt;\\&lt;/em&gt;I have to use summaries (i.e. Min/Max) to force data to retrieve into a visual that&amp;#39;s using another fact and including these demographics from FactEncounter (otherwise we get many-many type behavior), and performance is therefore bad.*&lt;/li&gt;\n&lt;li&gt;Add all encounter-specific demographics to FactEncounter and also add those demographics to all other dimensions whose reporting may need to reference them&lt;br/&gt;\n&lt;em&gt;\\&lt;/em&gt;Repeated use of similar code makes model maintenance cumbersome and could lead to mismatched summaries if code is updated in some but not all facts later.*&lt;/li&gt;\n&lt;li&gt;Add all encounter-specific demographics to Encounter dimension&lt;br/&gt;\n&lt;em&gt;\\&lt;/em&gt;Relationship from FactEncounter (and other facts) to the dimension(s) referenced by the demographics changes from star to snowflake*&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What is the best practice to follow in this case? These properties feel like dimension attributes to me, but I&amp;#39;ve always seen strong guidance away from snowflaking, so I&amp;#39;m unsure. TIA for any advice you all can offer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fxc7r", "is_robot_indexable": true, "report_reasons": null, "author": "Sub1ime14", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fxc7r/demographics_fact_vs_dimension/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fxc7r/demographics_fact_vs_dimension/", "subreddit_subscribers": 127934, "created_utc": 1694443984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, as the title says I am the only one at my company that is responsible, well I would not say responsible but the one who should bring some strong solutions for the data lineage of the company. The deal is that everyone at the company uses google sheets and they like that because they can manage whatever they want to manage there, most of our data sources come from third parties like GIS web apps, ARP softwares and some other project management apps. And putting this into spreadsheet and cross data in a pain. The process right now is a pull everything I need put it in s3 bucket it, read it in a lambda an return the url for the report (ya they want report, don\u2019t like visuals ). And this is not the way for many reasons I have been learning through this data journey. So I am planning implementing something that empowers teams with data and do their own reports. Because we already have google env. Was thinking keeping the lambdas and s3 and create a data warehouse in google big query which connect with google spreadsheets and looker (try to encourage to use it). But problem comes with stack tech I am the only responsible for this to come up with a solution and don\u2019t want to make a mess and generate a costly solution that no one wanted to use. What are your recommendations and thoughts? I have seen databricks to do pre-processing then DBT to help to create data structures in big query? Or just use all google environment to all this? Yeah basically I little lost here and, alone lol", "author_fullname": "t2_42yrzhea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Overcome spreadsheet company for a one man dataeng team", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fl51g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694404364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, as the title says I am the only one at my company that is responsible, well I would not say responsible but the one who should bring some strong solutions for the data lineage of the company. The deal is that everyone at the company uses google sheets and they like that because they can manage whatever they want to manage there, most of our data sources come from third parties like GIS web apps, ARP softwares and some other project management apps. And putting this into spreadsheet and cross data in a pain. The process right now is a pull everything I need put it in s3 bucket it, read it in a lambda an return the url for the report (ya they want report, don\u2019t like visuals ). And this is not the way for many reasons I have been learning through this data journey. So I am planning implementing something that empowers teams with data and do their own reports. Because we already have google env. Was thinking keeping the lambdas and s3 and create a data warehouse in google big query which connect with google spreadsheets and looker (try to encourage to use it). But problem comes with stack tech I am the only responsible for this to come up with a solution and don\u2019t want to make a mess and generate a costly solution that no one wanted to use. What are your recommendations and thoughts? I have seen databricks to do pre-processing then DBT to help to create data structures in big query? Or just use all google environment to all this? Yeah basically I little lost here and, alone lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16fl51g", "is_robot_indexable": true, "report_reasons": null, "author": "josejo9423", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fl51g/overcome_spreadsheet_company_for_a_one_man/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fl51g/overcome_spreadsheet_company_for_a_one_man/", "subreddit_subscribers": 127934, "created_utc": 1694404364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We run big data mart queries in dbt that usually take ~1hr for the 20-30 upstream builds then another 30mins-1hr for the model itself. Repeat this for another 2 models and they all run sequentially so end to end, it takes really long.\n\nI want to shorten the whole thing as much as I can, but its also hard now to review the model queries because there are select statements within the Join statements, and lots of business logic/case-when/NVLs scattered throughout as patch fixes for messy data in the company. Im sure thats one area we have to clean up anyway.\n\nOverall how do you optimize your data mart queries? Whenever I search on Google , they really just say the same \"select specific columns, avoid distinct, use when instead of having for filters\". But curious for how you've done it in your work.", "author_fullname": "t2_7ki1otgv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to optimize data mart queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16g0kfb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694451370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We run big data mart queries in dbt that usually take ~1hr for the 20-30 upstream builds then another 30mins-1hr for the model itself. Repeat this for another 2 models and they all run sequentially so end to end, it takes really long.&lt;/p&gt;\n\n&lt;p&gt;I want to shorten the whole thing as much as I can, but its also hard now to review the model queries because there are select statements within the Join statements, and lots of business logic/case-when/NVLs scattered throughout as patch fixes for messy data in the company. Im sure thats one area we have to clean up anyway.&lt;/p&gt;\n\n&lt;p&gt;Overall how do you optimize your data mart queries? Whenever I search on Google , they really just say the same &amp;quot;select specific columns, avoid distinct, use when instead of having for filters&amp;quot;. But curious for how you&amp;#39;ve done it in your work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16g0kfb", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Soup4733", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16g0kfb/how_to_optimize_data_mart_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16g0kfb/how_to_optimize_data_mart_queries/", "subreddit_subscribers": 127934, "created_utc": 1694451370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All,\n\nI was wondering if handling multiple fact tables, which includes the below:\n\n1. Modeling the data warehouse, integration layer and data mart tables.\n2. Handling these fact tables (more than one) inside a reporting tool, power bi in my case\n\nwould be good enough to answer the typical question \"what is the toughest challenge you faced technically in your career\" for a data engineer II/III interview or is it very basic.", "author_fullname": "t2_uqu7iar6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Question - Toughest Challenge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16g0i23", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694451222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\n\n&lt;p&gt;I was wondering if handling multiple fact tables, which includes the below:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Modeling the data warehouse, integration layer and data mart tables.&lt;/li&gt;\n&lt;li&gt;Handling these fact tables (more than one) inside a reporting tool, power bi in my case&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;would be good enough to answer the typical question &amp;quot;what is the toughest challenge you faced technically in your career&amp;quot; for a data engineer II/III interview or is it very basic.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16g0i23", "is_robot_indexable": true, "report_reasons": null, "author": "InvestigatorMuted622", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16g0i23/interview_question_toughest_challenge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16g0i23/interview_question_toughest_challenge/", "subreddit_subscribers": 127934, "created_utc": 1694451222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For those who have dealt with the pain of SAP ERP (r/3 or s/4), how have you gone about data extraction? The scope for me being SAP ERP -&gt; delta extraction -&gt; Azure Data Lake Landing Zone.\n\nWe currently use SAP Business Warehouse (SAP BW) for data warehousing, but are looking to migrate as we deal with a lot of data from external sources (cameras, API's, IoT, and various local Manufacturing Systems). I am not a part of the BW team, but they are a group within my direct team (same manager). I deal with all of the other sources, working in Azure.\n\nI like CDC methods, so something like Debezium or even Azure Data Factory has an SAP CDC connector. This way, we could capture each mutation on the individual tables. However, my manager mentioned the use SAP standard extractors, since that is what we use for our current SAP Business Warehouse.\n\nTo me, the standard extractors only seem relevant if we wanted to handle business rules and logic at the point of extraction (more ETL). However for real CDC (more ELT), the standard extractors don't make since as the rules and logic would be handled downstream. Therefore, I'd rather just have a 1:1 mapping between source and target tables.\n\nJust curious about anyone's experience dealing with SAP extraction and what approach works or didn't work for you.", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SAP ERP Data Extraction Methods?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16fzp8w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694449420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those who have dealt with the pain of SAP ERP (r/3 or s/4), how have you gone about data extraction? The scope for me being SAP ERP -&amp;gt; delta extraction -&amp;gt; Azure Data Lake Landing Zone.&lt;/p&gt;\n\n&lt;p&gt;We currently use SAP Business Warehouse (SAP BW) for data warehousing, but are looking to migrate as we deal with a lot of data from external sources (cameras, API&amp;#39;s, IoT, and various local Manufacturing Systems). I am not a part of the BW team, but they are a group within my direct team (same manager). I deal with all of the other sources, working in Azure.&lt;/p&gt;\n\n&lt;p&gt;I like CDC methods, so something like Debezium or even Azure Data Factory has an SAP CDC connector. This way, we could capture each mutation on the individual tables. However, my manager mentioned the use SAP standard extractors, since that is what we use for our current SAP Business Warehouse.&lt;/p&gt;\n\n&lt;p&gt;To me, the standard extractors only seem relevant if we wanted to handle business rules and logic at the point of extraction (more ETL). However for real CDC (more ELT), the standard extractors don&amp;#39;t make since as the rules and logic would be handled downstream. Therefore, I&amp;#39;d rather just have a 1:1 mapping between source and target tables.&lt;/p&gt;\n\n&lt;p&gt;Just curious about anyone&amp;#39;s experience dealing with SAP extraction and what approach works or didn&amp;#39;t work for you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fzp8w", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fzp8w/sap_erp_data_extraction_methods/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fzp8w/sap_erp_data_extraction_methods/", "subreddit_subscribers": 127934, "created_utc": 1694449420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone! Hope everyone's doing good.\n\nI got my first (yes first ever!) job as a data engineer in an MNC. My proposed tech stack will be Neo4j, BigQuery, Terraform, Cloud Composer and GKE (all in Python). No DataFlow/DataProc. The tech stack looks cool but I'm just wondering if this is actually a real data engineering project or should I ask for a more traditional Java ETL type role. The tech stack I mentioned, is it in demand and will it be helpful in case I switch? \n\n\nLooking for guidance from experienced folks.", "author_fullname": "t2_m8ebmzsu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confused over tech stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16fzjfb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694449061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! Hope everyone&amp;#39;s doing good.&lt;/p&gt;\n\n&lt;p&gt;I got my first (yes first ever!) job as a data engineer in an MNC. My proposed tech stack will be Neo4j, BigQuery, Terraform, Cloud Composer and GKE (all in Python). No DataFlow/DataProc. The tech stack looks cool but I&amp;#39;m just wondering if this is actually a real data engineering project or should I ask for a more traditional Java ETL type role. The tech stack I mentioned, is it in demand and will it be helpful in case I switch? &lt;/p&gt;\n\n&lt;p&gt;Looking for guidance from experienced folks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16fzjfb", "is_robot_indexable": true, "report_reasons": null, "author": "riderx65", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fzjfb/confused_over_tech_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fzjfb/confused_over_tech_stack/", "subreddit_subscribers": 127934, "created_utc": 1694449061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "i have this stored procedure that i call when loading flat files into a snowflake table. It looks at the primary keys and checks if any records that changed from source and target and if the record does not exist it updates, and if record exists but changed, it updates.\n\nThe problem however is, i put in \"Load\\_date\" column in ADF. So then when it goes to insert/update the records, it sees that the load\\_date column is different and updates the record even though the records are exactly the same. I'd like it to stop doing that. How would i modify the stored procedure below to ignore the load\\_date field if it exists. I want the load date field to be either at the time of insert or the time of record being updated.\n\n    CREATE OR REPLACE PROCEDURE @{activity('LookupSchemaName').output.firstRow.TARGET_SCHEMA}.BLOBTOSNOWFLAKE(STG_NAME VARCHAR, FILE_PATH VARCHAR, FILE_FRMT VARCHAR, TBL_SCHEMA_NAME VARCHAR, SF_TBL VARCHAR)\n    RETURNS VARCHAR\n    LANGUAGE JAVASCRIPT\n    AS\n    $$\n    \n    // Intialize parameters\n    var single_quote='\\'';\n    var SCEHMA=TBL_SCHEMA_NAME;\n    var Destination_Table=SF_TBL;\n    var Source_Table=\"@\"+STG_NAME+\"/\"+FILE_PATH+\" (file_format =&gt;\"+single_quote+FILE_FRMT+single_quote+\")  \";\n    \n    // First script to count number of columns and append $ to support quering external stage files\n    \n    var Blob_Columns_Query=\"SELECT CONCAT(\"+single_quote+\"$\"+single_quote\n                +\",LISTAGG(CONCAT(ORDINAL_POSITION,\"+single_quote+\" AS \"+single_quote+\",COLUMN_NAME),\"+single_quote+\",$\"+single_quote\n                +\") WITHIN GROUP (ORDER BY ORDINAL_POSITION) ) \\n FROM INFORMATION_SCHEMA.COLUMNS \\nWHERE TABLE_NAME=\"\n                +single_quote+Destination_Table+single_quote+\" AND TABLE_SCHEMA=\"+single_quote+SCEHMA+single_quote+\";\";\n    var Blob_Columns = snowflake.createStatement( {sqlText: Blob_Columns_Query}  ).execute();\n    Blob_Columns.next();\n    \n    //Final Code Preparation\n    var primary_key=[];\n    var update_columns=[];\n    var No_Update_Records=[];\n    var Columns_with_table_name=[];\n    var All_Columns=[];\n    var cmd = \"DESC TABLE \"+Destination_Table;\n    var stmt = snowflake.createStatement( {sqlText: cmd}  );\n    var result1 = stmt.execute();\n    while(result1.next() )\n    {\n      if(result1.getColumnValue(6)=='Y')\n      {\n            primary_key.push( 'A.'+result1.getColumnValue(1)+'=B.'+result1.getColumnValue(1) );\n      }\n      else\n      {\n            update_columns.push( 'A.'+result1.getColumnValue(1)+'=B.'+result1.getColumnValue(1) );\n            No_Update_Records.push( 'A.'+result1.getColumnValue(1)+'&lt;&gt;B.'+result1.getColumnValue(1) );\n      }\n      Columns_with_table_name.push( 'B.'+result1.getColumnValue(1) );\n      All_Columns.push( result1.getColumnValue(1) );\n    }\n    // combine\n    All_Columns=All_Columns.join(\",\");\n    Columns_with_table_name=Columns_with_table_name.join(\",\");\n    update_columns=update_columns.join(\",\");\n    primary_key=primary_key.join(\" AND \");\n    No_Update_Records=No_Update_Records.join(\" OR \");\n    \n    var Final_Command=  \"\\nMERGE INTO \"+Destination_Table+\" AS A USING (SELECT \"+Blob_Columns.getColumnValue(1)+\" FROM \"+Source_Table+\") AS B ON \"+primary_key+\n                        \"\\nWHEN MATCHED AND \"+No_Update_Records+\" THEN UPDATE SET \"+update_columns +\n                        \"\\nWHEN NOT MATCHED THEN INSERT ( \"+All_Columns+\" ) VALUES ( \"+ Columns_with_table_name +\" );\"\n    var Deleta_Load=snowflake.createStatement( {sqlText: Final_Command}  ).execute();   \n    Deleta_Load.next();\n    return \"Rows_Inserted:- \"+Deleta_Load.getColumnValue(1)+\"\\tRows_Updated:- \"+Deleta_Load.getColumnValue(2);\n    \n    \n    $$;\n\nOne of the problems i have now is that when i pull from the Source, i append a column called LOAD\\_DATE and populate with utc(now). It then takes the stored procedure above and loads it into snowflake. If the records are new based on the primary keys, it inserts it as new, but if the records are updated it updates the existing records (i'm doing incremental load). The problem is that since the LOAD\\_DATE is created at the run of the pipeline, it things that the records are updated and so does a merge and updates said records in snowflake, even though they are the same records.\n\nI removed the LOAD\\_DATE from the ADF pipeline and tried to modify the stored procedure to insert the LOAD\\_DATE as it's doing the load into snowflake with this updated code\n\n    CREATE OR REPLACE PROCEDURE @{activity('LookupSchemaName').output.firstRow.TARGET_SCHEMA}.BLOBTOSNOWFLAKE(STG_NAME VARCHAR, FILE_PATH VARCHAR, FILE_FRMT VARCHAR, TBL_SCHEMA_NAME VARCHAR, SF_TBL VARCHAR)\n    RETURNS VARCHAR\n    LANGUAGE JAVASCRIPT\n    AS\n    $$\n    \n    // Initialize parameters\n    var single_quote = '\\'';\n    var SCEHMA = TBL_SCHEMA_NAME;\n    var Destination_Table = SF_TBL;\n    var Source_Table = \"@\" + STG_NAME + \"/\" + FILE_PATH + \" (file_format =&gt; \" + single_quote + FILE_FRMT + single_quote + \")\";\n    \n    // First script to count number of columns and append $ to support querying external stage files\n    var Blob_Columns_Query = \"SELECT CONCAT(\" + single_quote + \"$\" + single_quote +\n        \", LISTAGG(CONCAT(ORDINAL_POSITION, \" + single_quote + \" AS \" + single_quote + \", COLUMN_NAME), \" + single_quote + \",$\" + single_quote +\n        \") WITHIN GROUP (ORDER BY ORDINAL_POSITION)) \\n FROM INFORMATION_SCHEMA.COLUMNS \\n WHERE TABLE_NAME = \" +\n        single_quote + Destination_Table + single_quote + \" AND TABLE_SCHEMA = \" + single_quote + SCEHMA + single_quote + \";\";\n    var Blob_Columns = snowflake.createStatement({sqlText: Blob_Columns_Query}).execute();\n    Blob_Columns.next();\n    \n    // Final Code Preparation\n    var primary_key = [];\n    var update_columns = [];\n    var No_Update_Records = [];\n    var Columns_with_table_name = [];\n    var All_Columns = [];\n    var cmd = \"DESC TABLE \" + Destination_Table;\n    var stmt = snowflake.createStatement({sqlText: cmd});\n    var result1 = stmt.execute();\n    while (result1.next()) {\n        if (result1.getColumnValue(6) == 'Y') {\n            primary_key.push('A.' + result1.getColumnValue(1) + '=B.' + result1.getColumnValue(1));\n        } else {\n            update_columns.push('A.' + result1.getColumnValue(1) + '=B.' + result1.getColumnValue(1));\n            No_Update_Records.push('A.' + result1.getColumnValue(1) + '&lt;&gt;B.' + result1.getColumnValue(1));\n        }\n        Columns_with_table_name.push('B.' + result1.getColumnValue(1));\n        All_Columns.push(result1.getColumnValue(1));\n    }\n    \n    // Check if LOAD_DATE column exists in All_Columns\n    var hasLoadDateColumn = All_Columns.includes('LOAD_DATE');\n    \n    // Append LOAD_DATE column if it doesn't exist\n    if (!hasLoadDateColumn) {\n        All_Columns.push(\"LOAD_DATE\");\n        Columns_with_table_name.push(\"CURRENT_TIMESTAMP() AS LOAD_DATE\");\n    }\n    \n    // Combine\n    All_Columns = All_Columns.join(\",\");\n    Columns_with_table_name = Columns_with_table_name.join(\",\");\n    update_columns = update_columns.join(\",\");\n    primary_key = primary_key.join(\" AND \");\n    No_Update_Records = No_Update_Records.join(\" OR \");\n    \n    var Final_Command = `\n    MERGE INTO ${Destination_Table} AS A \n    USING (\n        SELECT ${Blob_Columns.getColumnValue(1)}, CURRENT_TIMESTAMP() AS LOAD_DATE \n        FROM ${Source_Table}\n    ) AS B \n    ON ${primary_key}\n    WHEN MATCHED AND ${No_Update_Records} THEN UPDATE SET ${update_columns}\n    WHEN NOT MATCHED THEN INSERT (${All_Columns}) VALUES (${Columns_with_table_name});\n    `;\n    \n    var Deleta_Load = snowflake.createStatement({sqlText: Final_Command}).execute();\n    Deleta_Load.next();\n    \n    return \"Rows_Inserted:- \" + Deleta_Load.getColumnValue(1) + \"\\tRows_Updated:- \" + Deleta_Load.getColumnValue(2);\n    \n    $$;\n\nHowever, i get this error:\n\n    Operation on target ADL_Snowflake failed: Failure happened on 'Source' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [42601] Execution error in store procedure BLOBTOSNOWFLAKE:\n    SQL compilation error:\n    ambiguous column name 'LOAD_DATE'\n    At Statement.execute, line 64 position 70,Source=Microsoft.DataTransfer.Runtime.GenericOdbcConnectors,''Type=System.Data.Odbc.OdbcException,Message=ERROR [42601] Execution error in store procedure BLOBTOSNOWFLAKE:\n    SQL compilation error:\n    ambiguous column name 'LOAD_DATE'\n    At Statement.execute, line 64 position 70,Source=SnowflakeODBC_sb64.dll,'\n\nmeaning it doesn't seem to know what do with the column.\n\nI also tried to give the snowflake a default value of  CURRENT\\_TIMESTAMP() so that if nothing is inserted it will give it the timestamp at insert. However that does not seem to be working either. The data loads fine, but the LOAD\\_DATE column is just null records.\n\nI'm not quite sure what i'm doing wrong. If there are other approaches to doing this process outside a stored procedure, i'm absolutely open to it since i have no experience in writing javascript besides what chatGPT helps me with.\n\nBonus question is this: when i add a new source or additional tables for consumption, i have to manually write the DDL. is there a way for me to automate this process and have incremental or full loads?", "author_fullname": "t2_k89p9s2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get this stored procedure to ignore Load_date for records that are not being updated?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16fz9aw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694448405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i have this stored procedure that i call when loading flat files into a snowflake table. It looks at the primary keys and checks if any records that changed from source and target and if the record does not exist it updates, and if record exists but changed, it updates.&lt;/p&gt;\n\n&lt;p&gt;The problem however is, i put in &amp;quot;Load_date&amp;quot; column in ADF. So then when it goes to insert/update the records, it sees that the load_date column is different and updates the record even though the records are exactly the same. I&amp;#39;d like it to stop doing that. How would i modify the stored procedure below to ignore the load_date field if it exists. I want the load date field to be either at the time of insert or the time of record being updated.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE OR REPLACE PROCEDURE @{activity(&amp;#39;LookupSchemaName&amp;#39;).output.firstRow.TARGET_SCHEMA}.BLOBTOSNOWFLAKE(STG_NAME VARCHAR, FILE_PATH VARCHAR, FILE_FRMT VARCHAR, TBL_SCHEMA_NAME VARCHAR, SF_TBL VARCHAR)\nRETURNS VARCHAR\nLANGUAGE JAVASCRIPT\nAS\n$$\n\n// Intialize parameters\nvar single_quote=&amp;#39;\\&amp;#39;&amp;#39;;\nvar SCEHMA=TBL_SCHEMA_NAME;\nvar Destination_Table=SF_TBL;\nvar Source_Table=&amp;quot;@&amp;quot;+STG_NAME+&amp;quot;/&amp;quot;+FILE_PATH+&amp;quot; (file_format =&amp;gt;&amp;quot;+single_quote+FILE_FRMT+single_quote+&amp;quot;)  &amp;quot;;\n\n// First script to count number of columns and append $ to support quering external stage files\n\nvar Blob_Columns_Query=&amp;quot;SELECT CONCAT(&amp;quot;+single_quote+&amp;quot;$&amp;quot;+single_quote\n            +&amp;quot;,LISTAGG(CONCAT(ORDINAL_POSITION,&amp;quot;+single_quote+&amp;quot; AS &amp;quot;+single_quote+&amp;quot;,COLUMN_NAME),&amp;quot;+single_quote+&amp;quot;,$&amp;quot;+single_quote\n            +&amp;quot;) WITHIN GROUP (ORDER BY ORDINAL_POSITION) ) \\n FROM INFORMATION_SCHEMA.COLUMNS \\nWHERE TABLE_NAME=&amp;quot;\n            +single_quote+Destination_Table+single_quote+&amp;quot; AND TABLE_SCHEMA=&amp;quot;+single_quote+SCEHMA+single_quote+&amp;quot;;&amp;quot;;\nvar Blob_Columns = snowflake.createStatement( {sqlText: Blob_Columns_Query}  ).execute();\nBlob_Columns.next();\n\n//Final Code Preparation\nvar primary_key=[];\nvar update_columns=[];\nvar No_Update_Records=[];\nvar Columns_with_table_name=[];\nvar All_Columns=[];\nvar cmd = &amp;quot;DESC TABLE &amp;quot;+Destination_Table;\nvar stmt = snowflake.createStatement( {sqlText: cmd}  );\nvar result1 = stmt.execute();\nwhile(result1.next() )\n{\n  if(result1.getColumnValue(6)==&amp;#39;Y&amp;#39;)\n  {\n        primary_key.push( &amp;#39;A.&amp;#39;+result1.getColumnValue(1)+&amp;#39;=B.&amp;#39;+result1.getColumnValue(1) );\n  }\n  else\n  {\n        update_columns.push( &amp;#39;A.&amp;#39;+result1.getColumnValue(1)+&amp;#39;=B.&amp;#39;+result1.getColumnValue(1) );\n        No_Update_Records.push( &amp;#39;A.&amp;#39;+result1.getColumnValue(1)+&amp;#39;&amp;lt;&amp;gt;B.&amp;#39;+result1.getColumnValue(1) );\n  }\n  Columns_with_table_name.push( &amp;#39;B.&amp;#39;+result1.getColumnValue(1) );\n  All_Columns.push( result1.getColumnValue(1) );\n}\n// combine\nAll_Columns=All_Columns.join(&amp;quot;,&amp;quot;);\nColumns_with_table_name=Columns_with_table_name.join(&amp;quot;,&amp;quot;);\nupdate_columns=update_columns.join(&amp;quot;,&amp;quot;);\nprimary_key=primary_key.join(&amp;quot; AND &amp;quot;);\nNo_Update_Records=No_Update_Records.join(&amp;quot; OR &amp;quot;);\n\nvar Final_Command=  &amp;quot;\\nMERGE INTO &amp;quot;+Destination_Table+&amp;quot; AS A USING (SELECT &amp;quot;+Blob_Columns.getColumnValue(1)+&amp;quot; FROM &amp;quot;+Source_Table+&amp;quot;) AS B ON &amp;quot;+primary_key+\n                    &amp;quot;\\nWHEN MATCHED AND &amp;quot;+No_Update_Records+&amp;quot; THEN UPDATE SET &amp;quot;+update_columns +\n                    &amp;quot;\\nWHEN NOT MATCHED THEN INSERT ( &amp;quot;+All_Columns+&amp;quot; ) VALUES ( &amp;quot;+ Columns_with_table_name +&amp;quot; );&amp;quot;\nvar Deleta_Load=snowflake.createStatement( {sqlText: Final_Command}  ).execute();   \nDeleta_Load.next();\nreturn &amp;quot;Rows_Inserted:- &amp;quot;+Deleta_Load.getColumnValue(1)+&amp;quot;\\tRows_Updated:- &amp;quot;+Deleta_Load.getColumnValue(2);\n\n\n$$;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;One of the problems i have now is that when i pull from the Source, i append a column called LOAD_DATE and populate with utc(now). It then takes the stored procedure above and loads it into snowflake. If the records are new based on the primary keys, it inserts it as new, but if the records are updated it updates the existing records (i&amp;#39;m doing incremental load). The problem is that since the LOAD_DATE is created at the run of the pipeline, it things that the records are updated and so does a merge and updates said records in snowflake, even though they are the same records.&lt;/p&gt;\n\n&lt;p&gt;I removed the LOAD_DATE from the ADF pipeline and tried to modify the stored procedure to insert the LOAD_DATE as it&amp;#39;s doing the load into snowflake with this updated code&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE OR REPLACE PROCEDURE @{activity(&amp;#39;LookupSchemaName&amp;#39;).output.firstRow.TARGET_SCHEMA}.BLOBTOSNOWFLAKE(STG_NAME VARCHAR, FILE_PATH VARCHAR, FILE_FRMT VARCHAR, TBL_SCHEMA_NAME VARCHAR, SF_TBL VARCHAR)\nRETURNS VARCHAR\nLANGUAGE JAVASCRIPT\nAS\n$$\n\n// Initialize parameters\nvar single_quote = &amp;#39;\\&amp;#39;&amp;#39;;\nvar SCEHMA = TBL_SCHEMA_NAME;\nvar Destination_Table = SF_TBL;\nvar Source_Table = &amp;quot;@&amp;quot; + STG_NAME + &amp;quot;/&amp;quot; + FILE_PATH + &amp;quot; (file_format =&amp;gt; &amp;quot; + single_quote + FILE_FRMT + single_quote + &amp;quot;)&amp;quot;;\n\n// First script to count number of columns and append $ to support querying external stage files\nvar Blob_Columns_Query = &amp;quot;SELECT CONCAT(&amp;quot; + single_quote + &amp;quot;$&amp;quot; + single_quote +\n    &amp;quot;, LISTAGG(CONCAT(ORDINAL_POSITION, &amp;quot; + single_quote + &amp;quot; AS &amp;quot; + single_quote + &amp;quot;, COLUMN_NAME), &amp;quot; + single_quote + &amp;quot;,$&amp;quot; + single_quote +\n    &amp;quot;) WITHIN GROUP (ORDER BY ORDINAL_POSITION)) \\n FROM INFORMATION_SCHEMA.COLUMNS \\n WHERE TABLE_NAME = &amp;quot; +\n    single_quote + Destination_Table + single_quote + &amp;quot; AND TABLE_SCHEMA = &amp;quot; + single_quote + SCEHMA + single_quote + &amp;quot;;&amp;quot;;\nvar Blob_Columns = snowflake.createStatement({sqlText: Blob_Columns_Query}).execute();\nBlob_Columns.next();\n\n// Final Code Preparation\nvar primary_key = [];\nvar update_columns = [];\nvar No_Update_Records = [];\nvar Columns_with_table_name = [];\nvar All_Columns = [];\nvar cmd = &amp;quot;DESC TABLE &amp;quot; + Destination_Table;\nvar stmt = snowflake.createStatement({sqlText: cmd});\nvar result1 = stmt.execute();\nwhile (result1.next()) {\n    if (result1.getColumnValue(6) == &amp;#39;Y&amp;#39;) {\n        primary_key.push(&amp;#39;A.&amp;#39; + result1.getColumnValue(1) + &amp;#39;=B.&amp;#39; + result1.getColumnValue(1));\n    } else {\n        update_columns.push(&amp;#39;A.&amp;#39; + result1.getColumnValue(1) + &amp;#39;=B.&amp;#39; + result1.getColumnValue(1));\n        No_Update_Records.push(&amp;#39;A.&amp;#39; + result1.getColumnValue(1) + &amp;#39;&amp;lt;&amp;gt;B.&amp;#39; + result1.getColumnValue(1));\n    }\n    Columns_with_table_name.push(&amp;#39;B.&amp;#39; + result1.getColumnValue(1));\n    All_Columns.push(result1.getColumnValue(1));\n}\n\n// Check if LOAD_DATE column exists in All_Columns\nvar hasLoadDateColumn = All_Columns.includes(&amp;#39;LOAD_DATE&amp;#39;);\n\n// Append LOAD_DATE column if it doesn&amp;#39;t exist\nif (!hasLoadDateColumn) {\n    All_Columns.push(&amp;quot;LOAD_DATE&amp;quot;);\n    Columns_with_table_name.push(&amp;quot;CURRENT_TIMESTAMP() AS LOAD_DATE&amp;quot;);\n}\n\n// Combine\nAll_Columns = All_Columns.join(&amp;quot;,&amp;quot;);\nColumns_with_table_name = Columns_with_table_name.join(&amp;quot;,&amp;quot;);\nupdate_columns = update_columns.join(&amp;quot;,&amp;quot;);\nprimary_key = primary_key.join(&amp;quot; AND &amp;quot;);\nNo_Update_Records = No_Update_Records.join(&amp;quot; OR &amp;quot;);\n\nvar Final_Command = `\nMERGE INTO ${Destination_Table} AS A \nUSING (\n    SELECT ${Blob_Columns.getColumnValue(1)}, CURRENT_TIMESTAMP() AS LOAD_DATE \n    FROM ${Source_Table}\n) AS B \nON ${primary_key}\nWHEN MATCHED AND ${No_Update_Records} THEN UPDATE SET ${update_columns}\nWHEN NOT MATCHED THEN INSERT (${All_Columns}) VALUES (${Columns_with_table_name});\n`;\n\nvar Deleta_Load = snowflake.createStatement({sqlText: Final_Command}).execute();\nDeleta_Load.next();\n\nreturn &amp;quot;Rows_Inserted:- &amp;quot; + Deleta_Load.getColumnValue(1) + &amp;quot;\\tRows_Updated:- &amp;quot; + Deleta_Load.getColumnValue(2);\n\n$$;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;However, i get this error:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Operation on target ADL_Snowflake failed: Failure happened on &amp;#39;Source&amp;#39; side. ErrorCode=UserErrorOdbcOperationFailed,&amp;#39;Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [42601] Execution error in store procedure BLOBTOSNOWFLAKE:\nSQL compilation error:\nambiguous column name &amp;#39;LOAD_DATE&amp;#39;\nAt Statement.execute, line 64 position 70,Source=Microsoft.DataTransfer.Runtime.GenericOdbcConnectors,&amp;#39;&amp;#39;Type=System.Data.Odbc.OdbcException,Message=ERROR [42601] Execution error in store procedure BLOBTOSNOWFLAKE:\nSQL compilation error:\nambiguous column name &amp;#39;LOAD_DATE&amp;#39;\nAt Statement.execute, line 64 position 70,Source=SnowflakeODBC_sb64.dll,&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;meaning it doesn&amp;#39;t seem to know what do with the column.&lt;/p&gt;\n\n&lt;p&gt;I also tried to give the snowflake a default value of  CURRENT_TIMESTAMP() so that if nothing is inserted it will give it the timestamp at insert. However that does not seem to be working either. The data loads fine, but the LOAD_DATE column is just null records.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not quite sure what i&amp;#39;m doing wrong. If there are other approaches to doing this process outside a stored procedure, i&amp;#39;m absolutely open to it since i have no experience in writing javascript besides what chatGPT helps me with.&lt;/p&gt;\n\n&lt;p&gt;Bonus question is this: when i add a new source or additional tables for consumption, i have to manually write the DDL. is there a way for me to automate this process and have incremental or full loads?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16fz9aw", "is_robot_indexable": true, "report_reasons": null, "author": "TheSnowWorm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fz9aw/how_to_get_this_stored_procedure_to_ignore_load/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fz9aw/how_to_get_this_stored_procedure_to_ignore_load/", "subreddit_subscribers": 127934, "created_utc": 1694448405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a team that transforms data to fit into our data warehouse. We use Databricks notebooks to perform the transformations and Airflow DAG for orchestration.\n\nTypically the notebooks have a set of parameters (source schema, source table, target schema, target table) that are fed to the notebook from Airflow. (When we create the tasks, we add the notebook path and parameters)\n\nA new team has proposed storing all the parameters in a table, instead of in the Airflow script. So Airflow would just have the notebook name and would look to a table for all the specific parameters related to the notebook. I'm wondering if this is a good idea or not.\n\nPersonally, I'm leaning towards not. The Databricks notebooks and Airflow scripts are all stored in GitHub, so all changes are tracked and easily monitored. Tracking changes to a table is not as easy. In addition to this, adding a new table that needs to be maintained adds a new layer of complexity to the data flow that doesn't really seem necessary.\n\nMaybe I'm missing something but it seems like it will complicate things more than simplify them. Please let me know what you all think.", "author_fullname": "t2_arc3347", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should parameters be stored in tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16fz3v6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694448080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a team that transforms data to fit into our data warehouse. We use Databricks notebooks to perform the transformations and Airflow DAG for orchestration.&lt;/p&gt;\n\n&lt;p&gt;Typically the notebooks have a set of parameters (source schema, source table, target schema, target table) that are fed to the notebook from Airflow. (When we create the tasks, we add the notebook path and parameters)&lt;/p&gt;\n\n&lt;p&gt;A new team has proposed storing all the parameters in a table, instead of in the Airflow script. So Airflow would just have the notebook name and would look to a table for all the specific parameters related to the notebook. I&amp;#39;m wondering if this is a good idea or not.&lt;/p&gt;\n\n&lt;p&gt;Personally, I&amp;#39;m leaning towards not. The Databricks notebooks and Airflow scripts are all stored in GitHub, so all changes are tracked and easily monitored. Tracking changes to a table is not as easy. In addition to this, adding a new table that needs to be maintained adds a new layer of complexity to the data flow that doesn&amp;#39;t really seem necessary.&lt;/p&gt;\n\n&lt;p&gt;Maybe I&amp;#39;m missing something but it seems like it will complicate things more than simplify them. Please let me know what you all think.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fz3v6", "is_robot_indexable": true, "report_reasons": null, "author": "odzihodo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16fz3v6/should_parameters_be_stored_in_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fz3v6/should_parameters_be_stored_in_tables/", "subreddit_subscribers": 127934, "created_utc": 1694448080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Which services on AWS that you use the most in your daily work?\n\nMy company doesn't use any cloud solutions at all, I don't have much chance to work on it. When I look around, a lot of companies require exp in AWS/GCP/Azure. I'm studying AWS solutions and found that there are so many solutions on AWS that can be served in data engineering major. Which services that I should pay more attention on than others?", "author_fullname": "t2_9j8dvbm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS solutions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fyat4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694446256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which services on AWS that you use the most in your daily work?&lt;/p&gt;\n\n&lt;p&gt;My company doesn&amp;#39;t use any cloud solutions at all, I don&amp;#39;t have much chance to work on it. When I look around, a lot of companies require exp in AWS/GCP/Azure. I&amp;#39;m studying AWS solutions and found that there are so many solutions on AWS that can be served in data engineering major. Which services that I should pay more attention on than others?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16fyat4", "is_robot_indexable": true, "report_reasons": null, "author": "nalsman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fyat4/aws_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fyat4/aws_solutions/", "subreddit_subscribers": 127934, "created_utc": 1694446256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title says, what kind of questions would everyone ask about snowflake to a data engineer?", "author_fullname": "t2_5on7s1v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview questions for snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fxtt6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694445127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says, what kind of questions would everyone ask about snowflake to a data engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16fxtt6", "is_robot_indexable": true, "report_reasons": null, "author": "xander800", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fxtt6/interview_questions_for_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fxtt6/interview_questions_for_snowflake/", "subreddit_subscribers": 127934, "created_utc": 1694445127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Was reading up on data management in Fundamentals of Data Engineering. \n\nFoDE is praised for exactly that, the fundamentals. So I was worried some references might me outdated. \n\nIn the book DAMA-DMBOK *(Data Management Assosciation International - Data Management Body of Knowledge)* is mentioned, Is the book outdated or are there other resources for data management? Or is it as relevant as ever?", "author_fullname": "t2_n9pcs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is DAMA-DMBOK still relevant? (Data Management)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fxmzc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694444683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was reading up on data management in Fundamentals of Data Engineering. &lt;/p&gt;\n\n&lt;p&gt;FoDE is praised for exactly that, the fundamentals. So I was worried some references might me outdated. &lt;/p&gt;\n\n&lt;p&gt;In the book DAMA-DMBOK &lt;em&gt;(Data Management Assosciation International - Data Management Body of Knowledge)&lt;/em&gt; is mentioned, Is the book outdated or are there other resources for data management? Or is it as relevant as ever?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fxmzc", "is_robot_indexable": true, "report_reasons": null, "author": "Zer0designs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fxmzc/is_damadmbok_still_relevant_data_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fxmzc/is_damadmbok_still_relevant_data_management/", "subreddit_subscribers": 127934, "created_utc": 1694444683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a grand plan to move our ML feature engineering from spark to polars (easier for data scientists to use as they don't have to worry about defining and tuning a spark cluster). One sticking point is using a bigquery table as a large out of core data source that polars can use directly, rather than manually loading the data in chunks from bigquery.\n\nIs this possible, and has anyone done it?", "author_fullname": "t2_7aoha4r0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use bigquery as data source for polars out of core processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fx7u2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694443694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a grand plan to move our ML feature engineering from spark to polars (easier for data scientists to use as they don&amp;#39;t have to worry about defining and tuning a spark cluster). One sticking point is using a bigquery table as a large out of core data source that polars can use directly, rather than manually loading the data in chunks from bigquery.&lt;/p&gt;\n\n&lt;p&gt;Is this possible, and has anyone done it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16fx7u2", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Set-711", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fx7u2/use_bigquery_as_data_source_for_polars_out_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fx7u2/use_bigquery_as_data_source_for_polars_out_of/", "subreddit_subscribers": 127934, "created_utc": 1694443694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Based on \\[this article\\]([https://medium.com/@mikldd/the-struggles-scaling-data-teams-face-95ca8eb874f3](https://medium.com/@mikldd/the-struggles-scaling-data-teams-face-95ca8eb874f3))\n\nhttps://preview.redd.it/dc4fmwk1jlnb1.png?width=1418&amp;format=png&amp;auto=webp&amp;s=2966492f8a82b2d094c374dda5b7f3ac3d76b053\n\nI was curious to know what everyone else setup is.\n\nExample: \n\n* **Team size**: 10-50 members.\n* **Characteristics**: \n   * **Data models:** 500-3000\n   * **Commit** **frequency**: 11-100 commits per week (per team)\n   * **Data volume**: 100GB-1TB.\n   * **Environments**: 4 (e.g., development, qa, staging, and production)\n   * **Deployments**: 10 times per month.\n   * **Rollbacks**: 2 times per month.\n   * **Data warehouse**: Snowflake\n   * **Cost**: $$ ", "author_fullname": "t2_4ymkgdql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Size of the DWH and number of operations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"dc4fmwk1jlnb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 28, "x": 108, "u": "https://preview.redd.it/dc4fmwk1jlnb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0ae3989c1bfcd21bafaebb8a760d2973beea9ab"}, {"y": 56, "x": 216, "u": "https://preview.redd.it/dc4fmwk1jlnb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9f78ccebbc5db3b406dcd403d185044f12cad985"}, {"y": 83, "x": 320, "u": "https://preview.redd.it/dc4fmwk1jlnb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c8fcc5aecec7689054b452cd1bd159f7e6e8323"}, {"y": 167, "x": 640, "u": "https://preview.redd.it/dc4fmwk1jlnb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=11688041ce9e4ad224bcd3d10a1f5ae2e1aa9a16"}, {"y": 251, "x": 960, "u": "https://preview.redd.it/dc4fmwk1jlnb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ac99cecd92538f38e66dc2ab05d4f9565b8824fe"}, {"y": 283, "x": 1080, "u": "https://preview.redd.it/dc4fmwk1jlnb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=426651cff33440f79f12478bb47a765c58778dac"}], "s": {"y": 372, "x": 1418, "u": "https://preview.redd.it/dc4fmwk1jlnb1.png?width=1418&amp;format=png&amp;auto=webp&amp;s=2966492f8a82b2d094c374dda5b7f3ac3d76b053"}, "id": "dc4fmwk1jlnb1"}}, "name": "t3_16fqx41", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/g8IwRgufFCCil9M4dDlvCpSIvpO0eewfM65_6ZDG97M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1694425016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based on [this article](&lt;a href=\"https://medium.com/@mikldd/the-struggles-scaling-data-teams-face-95ca8eb874f3\"&gt;https://medium.com/@mikldd/the-struggles-scaling-data-teams-face-95ca8eb874f3&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dc4fmwk1jlnb1.png?width=1418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2966492f8a82b2d094c374dda5b7f3ac3d76b053\"&gt;https://preview.redd.it/dc4fmwk1jlnb1.png?width=1418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2966492f8a82b2d094c374dda5b7f3ac3d76b053&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I was curious to know what everyone else setup is.&lt;/p&gt;\n\n&lt;p&gt;Example: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Team size&lt;/strong&gt;: 10-50 members.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Characteristics&lt;/strong&gt;: \n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Data models:&lt;/strong&gt; 500-3000&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Commit&lt;/strong&gt; &lt;strong&gt;frequency&lt;/strong&gt;: 11-100 commits per week (per team)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data volume&lt;/strong&gt;: 100GB-1TB.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Environments&lt;/strong&gt;: 4 (e.g., development, qa, staging, and production)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Deployments&lt;/strong&gt;: 10 times per month.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Rollbacks&lt;/strong&gt;: 2 times per month.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data warehouse&lt;/strong&gt;: Snowflake&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: $$ &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1a2zgm6pcCFgOHIQc8qGAeXQjCiJX2ASfEKeFQAm_AU.jpg?auto=webp&amp;s=0107b8993ffe23cfdad945484ae508a9c1502ea8", "width": 1200, "height": 607}, "resolutions": [{"url": "https://external-preview.redd.it/1a2zgm6pcCFgOHIQc8qGAeXQjCiJX2ASfEKeFQAm_AU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a8687063420b81c7db2f239c702bd2849132ff06", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/1a2zgm6pcCFgOHIQc8qGAeXQjCiJX2ASfEKeFQAm_AU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=987eb3284fb6b755d4b30faa6a76052e0ff8cdfc", "width": 216, "height": 109}, {"url": "https://external-preview.redd.it/1a2zgm6pcCFgOHIQc8qGAeXQjCiJX2ASfEKeFQAm_AU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b570c29ad76464592b737582e60374f936cfd210", "width": 320, "height": 161}, {"url": "https://external-preview.redd.it/1a2zgm6pcCFgOHIQc8qGAeXQjCiJX2ASfEKeFQAm_AU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ffa1738bcd06eae375f3300e67f5e78acf0c4c43", "width": 640, "height": 323}, {"url": "https://external-preview.redd.it/1a2zgm6pcCFgOHIQc8qGAeXQjCiJX2ASfEKeFQAm_AU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=34bca452a598e0255d5734170fe2024b0bd448e0", "width": 960, "height": 485}, {"url": "https://external-preview.redd.it/1a2zgm6pcCFgOHIQc8qGAeXQjCiJX2ASfEKeFQAm_AU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c53f750942604715cbb415da6db3cc2237cac8f1", "width": 1080, "height": 546}], "variants": {}, "id": "nstjHH3XBaaV-9upMu5IKd0vAuvQuxju1l7E3O6dn_4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fqx41", "is_robot_indexable": true, "report_reasons": null, "author": "SnooBeans3890", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fqx41/size_of_the_dwh_and_number_of_operations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fqx41/size_of_the_dwh_and_number_of_operations/", "subreddit_subscribers": 127934, "created_utc": 1694425016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow DE's: As someone who's dedicated over a decade to Data Engineering, I'm enthusiastic about disseminating my expertise on other platforms for the benefit of fellow professionals. To support this endeavor, I actively maintain a presence on Medium ([https://medium.com/@balachandar-paulraj](https://medium.com/@balachandar-paulraj)) by writing data engineering blogs and imparting my expertise to coworkers  and college students intrigued by the world of Data Engineering. However, my passion for contributing persists, and I'm excited about  participating in volunteer activities with NGOs or instructing free college-level courses. Can someone please guide me if you have details on this?", "author_fullname": "t2_jeh8zfh9o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking guidelines for taking data engineering courses or helping NGOs for data related requirements", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16flqt7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1694406248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow DE&amp;#39;s: As someone who&amp;#39;s dedicated over a decade to Data Engineering, I&amp;#39;m enthusiastic about disseminating my expertise on other platforms for the benefit of fellow professionals. To support this endeavor, I actively maintain a presence on Medium (&lt;a href=\"https://medium.com/@balachandar-paulraj\"&gt;https://medium.com/@balachandar-paulraj&lt;/a&gt;) by writing data engineering blogs and imparting my expertise to coworkers  and college students intrigued by the world of Data Engineering. However, my passion for contributing persists, and I&amp;#39;m excited about  participating in volunteer activities with NGOs or instructing free college-level courses. Can someone please guide me if you have details on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aODSq92pV7ezA7Ow8q6keExMBApORzHZRq4GQ6jiR98.jpg?auto=webp&amp;s=0da6557a73141b3b398950052650bcabce075540", "width": 2320, "height": 3088}, "resolutions": [{"url": "https://external-preview.redd.it/aODSq92pV7ezA7Ow8q6keExMBApORzHZRq4GQ6jiR98.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd4686be82246c8eb501e831ef69a14aefd3b20c", "width": 108, "height": 143}, {"url": "https://external-preview.redd.it/aODSq92pV7ezA7Ow8q6keExMBApORzHZRq4GQ6jiR98.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7107631c4de90a486d124454cd7168a4b80b42b0", "width": 216, "height": 287}, {"url": "https://external-preview.redd.it/aODSq92pV7ezA7Ow8q6keExMBApORzHZRq4GQ6jiR98.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=affe37aedacd103bf8c18030c5dd840367781716", "width": 320, "height": 425}, {"url": "https://external-preview.redd.it/aODSq92pV7ezA7Ow8q6keExMBApORzHZRq4GQ6jiR98.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=77c2223fcefe622b1b0848e24668f8abe1fa6354", "width": 640, "height": 851}, {"url": "https://external-preview.redd.it/aODSq92pV7ezA7Ow8q6keExMBApORzHZRq4GQ6jiR98.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4ef8d7707b7ae19174a0a8581662445681ad757c", "width": 960, "height": 1277}, {"url": "https://external-preview.redd.it/aODSq92pV7ezA7Ow8q6keExMBApORzHZRq4GQ6jiR98.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=20e202976fd847b227ad36f3c3f7393668265696", "width": 1080, "height": 1437}], "variants": {}, "id": "Em4pFOPXwrEVR_nva81YFYYzWvLyRywkIAsgAq-JG1I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16flqt7", "is_robot_indexable": true, "report_reasons": null, "author": "balachandar_paulraj", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16flqt7/seeking_guidelines_for_taking_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16flqt7/seeking_guidelines_for_taking_data_engineering/", "subreddit_subscribers": 127934, "created_utc": 1694406248.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}