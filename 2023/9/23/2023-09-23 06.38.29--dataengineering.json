{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was working as a software developer for a mid size company for around 3 years as a PHP developer. The tech stack was pretty outdated and all I did for three years was code patches to an existing code base. Did not do a lot of new development and didn\u2019t learn much. \nThere was an opening for a Data Engineer in my company which I applied for. I have past BI experience, which helped me get this position. \nI\u2019ve been at the Data Engineering position for around an year now. And my team lead had been constantly giving me negative feedback on my code quality and coding standards. I\u2019ve tried to improve but she still seems unsatisfied with the work I\u2019ve been doing. I would like to know what are the different ways that I can improve my code quality. Are there any websites I can use to learn to get better at programming? Learning better coding standards and improving my overall code quality? Mentorship or peer review learning options would also be helpful.", "author_fullname": "t2_1reibdu0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coding standards and code quality", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16phdqw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695406692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was working as a software developer for a mid size company for around 3 years as a PHP developer. The tech stack was pretty outdated and all I did for three years was code patches to an existing code base. Did not do a lot of new development and didn\u2019t learn much. \nThere was an opening for a Data Engineer in my company which I applied for. I have past BI experience, which helped me get this position. \nI\u2019ve been at the Data Engineering position for around an year now. And my team lead had been constantly giving me negative feedback on my code quality and coding standards. I\u2019ve tried to improve but she still seems unsatisfied with the work I\u2019ve been doing. I would like to know what are the different ways that I can improve my code quality. Are there any websites I can use to learn to get better at programming? Learning better coding standards and improving my overall code quality? Mentorship or peer review learning options would also be helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16phdqw", "is_robot_indexable": true, "report_reasons": null, "author": "luckykanwar", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16phdqw/coding_standards_and_code_quality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16phdqw/coding_standards_and_code_quality/", "subreddit_subscribers": 129918, "created_utc": 1695406692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all first time poster little while lurker.\n\nI see alot of people talk about snowflake and AWS as these seem to be very popular. I don't know if there is a competitive edge or it's just happens to be the go to tool in this industry. Can I ask if many of you out there use Google Cloud Platform and Big Query or a combination of on premises and GCP or different cloud providers?", "author_fullname": "t2_4r6g4urx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p66k3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695376617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all first time poster little while lurker.&lt;/p&gt;\n\n&lt;p&gt;I see alot of people talk about snowflake and AWS as these seem to be very popular. I don&amp;#39;t know if there is a competitive edge or it&amp;#39;s just happens to be the go to tool in this industry. Can I ask if many of you out there use Google Cloud Platform and Big Query or a combination of on premises and GCP or different cloud providers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p66k3", "is_robot_indexable": true, "report_reasons": null, "author": "ljsmith970", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p66k3/gcp_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p66k3/gcp_bigquery/", "subreddit_subscribers": 129918, "created_utc": 1695376617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm wondering if anyone has ever tried setting up a dbt development environment fully locally, meaning that `dbt run` would be executed on your local machine against a local database instead of always pushing the runs to the cloud warehouse? I understand this would include massive workarounds, but the benefit would be that it would save a lot of money, especially for larger teams.", "author_fullname": "t2_ves1in2k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "True local development with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16pl3li", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695415980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m wondering if anyone has ever tried setting up a dbt development environment fully locally, meaning that &lt;code&gt;dbt run&lt;/code&gt; would be executed on your local machine against a local database instead of always pushing the runs to the cloud warehouse? I understand this would include massive workarounds, but the benefit would be that it would save a lot of money, especially for larger teams.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16pl3li", "is_robot_indexable": true, "report_reasons": null, "author": "EzPzData", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16pl3li/true_local_development_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pl3li/true_local_development_with_dbt/", "subreddit_subscribers": 129918, "created_utc": 1695415980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One of the biggest complaints I\u2019ve heard on this subreddit with the MDS tools is around issues with data quality.\n\nTo quote a comment/sentiment that I see frequently echoed:\n\n\u201cWhat's notably lacking is some way to guarantee data quality - if anything, those quick source system -&gt; SaaS connector -&gt; data warehouse pipelines just kick the bucket down the road to the analysts. \n\nSure I can bang out an MVP in a week by hooking up Fivetran to Snowflake and using a few dbt packages to quickly model and feed to Metabase or Superset. I'll get a few tables out quickly, but after a while, reality starts to look different as the MDS tools give me nothing to effectively manage data in my organization.\n\n\nEven the numerous data cataloguing and monitoring tools are just treating the symptoms IMO: we wouldn't need them if data ingestion didn't completely disregard data quality.\u201d\n\nMy perspective on this is that you\u2019re always going to have bad data getting generated from your data sources, it\u2019s just the nature of the imperfect world we live in unfortunately. \n\nSo, my question for the folks on this subreddit is:\n\nWhat does a data ingestion tool that accounts for data quality look like?\n\n\nTLDR: What would a data ingestion tool that accounts for data quality look like, in a perfect world how would you like it to work?", "author_fullname": "t2_iu7o1yoi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Perfect Data Ingestion Tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16pg9hr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695403947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of the biggest complaints I\u2019ve heard on this subreddit with the MDS tools is around issues with data quality.&lt;/p&gt;\n\n&lt;p&gt;To quote a comment/sentiment that I see frequently echoed:&lt;/p&gt;\n\n&lt;p&gt;\u201cWhat&amp;#39;s notably lacking is some way to guarantee data quality - if anything, those quick source system -&amp;gt; SaaS connector -&amp;gt; data warehouse pipelines just kick the bucket down the road to the analysts. &lt;/p&gt;\n\n&lt;p&gt;Sure I can bang out an MVP in a week by hooking up Fivetran to Snowflake and using a few dbt packages to quickly model and feed to Metabase or Superset. I&amp;#39;ll get a few tables out quickly, but after a while, reality starts to look different as the MDS tools give me nothing to effectively manage data in my organization.&lt;/p&gt;\n\n&lt;p&gt;Even the numerous data cataloguing and monitoring tools are just treating the symptoms IMO: we wouldn&amp;#39;t need them if data ingestion didn&amp;#39;t completely disregard data quality.\u201d&lt;/p&gt;\n\n&lt;p&gt;My perspective on this is that you\u2019re always going to have bad data getting generated from your data sources, it\u2019s just the nature of the imperfect world we live in unfortunately. &lt;/p&gt;\n\n&lt;p&gt;So, my question for the folks on this subreddit is:&lt;/p&gt;\n\n&lt;p&gt;What does a data ingestion tool that accounts for data quality look like?&lt;/p&gt;\n\n&lt;p&gt;TLDR: What would a data ingestion tool that accounts for data quality look like, in a perfect world how would you like it to work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16pg9hr", "is_robot_indexable": true, "report_reasons": null, "author": "YeeterSkeeter9269", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pg9hr/the_perfect_data_ingestion_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pg9hr/the_perfect_data_ingestion_tool/", "subreddit_subscribers": 129918, "created_utc": 1695403947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nin my current org, we are looking to revamp our data products. We're looking at new ELT tools as well as doing a bit of a \"reset\" for the standard approach to building data pipelines. \n\nI am struggling with pitching what I perceive as the \"modern\" solution.\n\nCurrent state:\n\nWe have an in house tool, something like Airflow, to control the workflow orchestration. For the actual code, typically my colleagues (data engineers) are writing often opaque PHP+SQL scripts. You know, scripts the create the joys of reviewing what's going wrong in 10,000+ lines of code that one colleague wrote (who happens to be on holiday when it breaks)\n\nProposed state:\n\nEssentially I am pitching that we use a [mage.ai](https://mage.ai) like solution, and build our pipelines with python+dbt+sql.\n\nI foresee [Mage.ai](https://Mage.ai) (or something similar like Airbyte) giving me the visibility over the pipeline (assuming we use granular code blocks; \\~1 for each transformation step); compared to the currently opaque PHP scripts my colleagues are producing.\n\nThen, I expect workflow orchestration to be vastly improved, thanks to dbt run. Which lets us slowly abandon the in house tool we've built up over the years.\n\nCaveat:\n\nI have come into my current \"junior manager\" role, from an analyst background, so I am quite happy to listen to the more experienced data engineering colleagues, regarding best practice.\n\nThat being said, even with less experience, I can't understand the reasoning for continuing our current path. So many of our php pipelines contain arrays being tortured to death;.....where I would just drop the data into the database and transform it in SQL. It sometimes feels like little(big) protected islands for the developer are being made, every time one of these opaque behemoth scripts is released. \n\nIs this something you have encountered in your teams? Am I on the right path? Am I fighting the good fight? If not, why?\n\n\\*We generally don't have any \\*special\\* requirements for our pipelines (streaming, big data, real time....etc)....it's often a \"regular\" ingest (from prod API) -&gt; transform -&gt; (BI) datamart type pipeline.", "author_fullname": "t2_20h89ytj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modernizing data pipelines......organizational blockers or am I just a noob?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p4ztz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695372162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;in my current org, we are looking to revamp our data products. We&amp;#39;re looking at new ELT tools as well as doing a bit of a &amp;quot;reset&amp;quot; for the standard approach to building data pipelines. &lt;/p&gt;\n\n&lt;p&gt;I am struggling with pitching what I perceive as the &amp;quot;modern&amp;quot; solution.&lt;/p&gt;\n\n&lt;p&gt;Current state:&lt;/p&gt;\n\n&lt;p&gt;We have an in house tool, something like Airflow, to control the workflow orchestration. For the actual code, typically my colleagues (data engineers) are writing often opaque PHP+SQL scripts. You know, scripts the create the joys of reviewing what&amp;#39;s going wrong in 10,000+ lines of code that one colleague wrote (who happens to be on holiday when it breaks)&lt;/p&gt;\n\n&lt;p&gt;Proposed state:&lt;/p&gt;\n\n&lt;p&gt;Essentially I am pitching that we use a &lt;a href=\"https://mage.ai\"&gt;mage.ai&lt;/a&gt; like solution, and build our pipelines with python+dbt+sql.&lt;/p&gt;\n\n&lt;p&gt;I foresee &lt;a href=\"https://Mage.ai\"&gt;Mage.ai&lt;/a&gt; (or something similar like Airbyte) giving me the visibility over the pipeline (assuming we use granular code blocks; ~1 for each transformation step); compared to the currently opaque PHP scripts my colleagues are producing.&lt;/p&gt;\n\n&lt;p&gt;Then, I expect workflow orchestration to be vastly improved, thanks to dbt run. Which lets us slowly abandon the in house tool we&amp;#39;ve built up over the years.&lt;/p&gt;\n\n&lt;p&gt;Caveat:&lt;/p&gt;\n\n&lt;p&gt;I have come into my current &amp;quot;junior manager&amp;quot; role, from an analyst background, so I am quite happy to listen to the more experienced data engineering colleagues, regarding best practice.&lt;/p&gt;\n\n&lt;p&gt;That being said, even with less experience, I can&amp;#39;t understand the reasoning for continuing our current path. So many of our php pipelines contain arrays being tortured to death;.....where I would just drop the data into the database and transform it in SQL. It sometimes feels like little(big) protected islands for the developer are being made, every time one of these opaque behemoth scripts is released. &lt;/p&gt;\n\n&lt;p&gt;Is this something you have encountered in your teams? Am I on the right path? Am I fighting the good fight? If not, why?&lt;/p&gt;\n\n&lt;p&gt;*We generally don&amp;#39;t have any *special* requirements for our pipelines (streaming, big data, real time....etc)....it&amp;#39;s often a &amp;quot;regular&amp;quot; ingest (from prod API) -&amp;gt; transform -&amp;gt; (BI) datamart type pipeline.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?auto=webp&amp;s=c6069afb05bd5a0610454eff205b0de0f4c7cef2", "width": 1524, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa478f4ea779b8ac72061216cb8a8cda7e655638", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1f4194503a2dbbc777d91bf043f284a7ce85b6d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db595a6de5fa4afca5771571c913b2f01f4119b7", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2a94dbbee373405a6a78a29452b9490eba644bb", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7f0b65ba6c15688f3916734aae188f2390a6042", "width": 960, "height": 503}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d7425076e002cc0e60bfaa4c1a94925f12856bd8", "width": 1080, "height": 566}], "variants": {}, "id": "9-9fQOf4CbozRQb-4LsE4XBYETAYFLqieboBGJfHCFI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p4ztz", "is_robot_indexable": true, "report_reasons": null, "author": "Lalagabor", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p4ztz/modernizing_data_pipelinesorganizational_blockers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p4ztz/modernizing_data_pipelinesorganizational_blockers/", "subreddit_subscribers": 129918, "created_utc": 1695372162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, i was hired as a director of data engineering 6 months ago.  The issue that I am having is that my company does not give performance reviews or allow for negative feedback.  No call, no shows are constant. Some people on my team will no call, no show for multiple days in a row.  One guy admitted to calling in via his cellphone and not being by his computer...for the entire MONTH of august\n\nI have no mechanism to stop this.  This is proven by the fact that one of my employees has reputation of this and has been passed to 3 teams and is known as the departments \"problem employee\". she has been here 5 years and my manager says he hasnt seen her do a thing.  She is also my database admin.  New hire needs access to the database? too bad lol.\n\n&amp;#x200B;\n\nBasically, I have the job title and I have the pay. Its a big company, but it has a reputation for this.  Its in the advertising industry and when i posted in the r/advertising sub. I got 100 replies with 8 people wanting to quit the exact same company due to the same thing.\n\n&amp;#x200B;\n\nI have a contract job lined up.  At a MASSIVE american company.  I did the math and i will net out 5k ahead in the contract job after taxes, insurances, and loss of 401k match.\n\n&amp;#x200B;\n\nI am debating leaving the flashy job title at a big company for just a contract data engineer job though. It will break up my resume. however, it does open the door of me being able to go contract route and be independent.\n\nWhat are your thoughts?   I feel like im babysitting with no call, no shows.  I dont care that they arent doing work to be honest, i only really care at this point that im always having to argue with people about attending meetings lol.\n\nthe main downside is that if i roughed this job out for like a year or two, it would look great on my resume.\n\nthoughts?\n\n&amp;#x200B;", "author_fullname": "t2_vnvmwnbl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Leaving my corporate director job for a contract?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16pifom", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695410057.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695409341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, i was hired as a director of data engineering 6 months ago.  The issue that I am having is that my company does not give performance reviews or allow for negative feedback.  No call, no shows are constant. Some people on my team will no call, no show for multiple days in a row.  One guy admitted to calling in via his cellphone and not being by his computer...for the entire MONTH of august&lt;/p&gt;\n\n&lt;p&gt;I have no mechanism to stop this.  This is proven by the fact that one of my employees has reputation of this and has been passed to 3 teams and is known as the departments &amp;quot;problem employee&amp;quot;. she has been here 5 years and my manager says he hasnt seen her do a thing.  She is also my database admin.  New hire needs access to the database? too bad lol.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Basically, I have the job title and I have the pay. Its a big company, but it has a reputation for this.  Its in the advertising industry and when i posted in the &lt;a href=\"/r/advertising\"&gt;r/advertising&lt;/a&gt; sub. I got 100 replies with 8 people wanting to quit the exact same company due to the same thing.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have a contract job lined up.  At a MASSIVE american company.  I did the math and i will net out 5k ahead in the contract job after taxes, insurances, and loss of 401k match.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am debating leaving the flashy job title at a big company for just a contract data engineer job though. It will break up my resume. however, it does open the door of me being able to go contract route and be independent.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts?   I feel like im babysitting with no call, no shows.  I dont care that they arent doing work to be honest, i only really care at this point that im always having to argue with people about attending meetings lol.&lt;/p&gt;\n\n&lt;p&gt;the main downside is that if i roughed this job out for like a year or two, it would look great on my resume.&lt;/p&gt;\n\n&lt;p&gt;thoughts?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16pifom", "is_robot_indexable": true, "report_reasons": null, "author": "Inevitable-Quality15", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pifom/leaving_my_corporate_director_job_for_a_contract/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pifom/leaving_my_corporate_director_job_for_a_contract/", "subreddit_subscribers": 129918, "created_utc": 1695409341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The title says it all, but I will provide more detail.\u00a0 I work for a smaller size company, but we are growing.\u00a0 Sometime last year I noticed it became harder to work with the amount of data I had collected in my two years in this role.\u00a0 Previously, I had been working with PowerBI, Excel (power pivot), &amp; OneDrive.I would put all my data in OneDrive folders &amp; structure all reports based on a star schema.\u00a0 I used Power Query to access some of the larger data sets by connecting to the file path in my file explorer.\u00a0 On top of things starting to slow down (queries take a while to refresh in Excel/Power BI), some of my collogues expressed that they wanted to have access to some of the tools I built myself, as well as a backup to our data should the company lose access to everything I have saved in OneDrive.  \nOriginally, I was hired as an analyst.\u00a0 The structure &amp; storage of data, as well as the creation of assets to help organize, update, transform, &amp; connect data seems outside of the scope of an anaylist, but I am also eager to learn.\u00a0 From my research it seems that the analyst role typically tops out at just under six figures, so if I want to move up in my career, I need to expand my skillset.\u00a0  \nThis has led me to start learning some of the basics in Azure.\u00a0 I have set up a gen2 datalake storage account &amp; I have moved all of my data there.\u00a0 I connect directly to that using the endpoint &amp; do ETL in power query still.\u00a0 I have started learning some ETL processes in Azure Data Factory.\u00a0 I have a few tables that I have also moved from my datalake into an Azure SQL server, &amp; I can use SQL to do basic queries.\u00a0 It seems like the depth &amp; complexity of Azure, &amp; all of its components are vast.\u00a0 I have spent what feels like considerable time learning about the key vault, Access Control (AIM), my subscription &amp; storage costs, types of storage, redundancy &amp; backups etc... &amp; I feel like I am just scratching the surface.\u00a0 Each table I try to create in my SQL server presents new challenges.\u00a0 I am a solo, self-taught, data engineering hackjob &amp; barley treading water.  \nAround the time I started working in Azure I also had advertisements for Fabric.\u00a0 I haven't done as much research on Fabric as I have Azure, but the marketing presents Fabric as a packaged solution that uses more interfaces that a PowerBI analyist might be able to jump into &amp; start creating right away.\u00a0\u00a0  \nSo, after that brief history &amp; introduction, here are some questions I have:  \n\n\n* Do I stay the course in Azure or jump ship to Fabric if I want to quickly backup data, make connections in a data model, &amp; give collogues access to data?\n* Do I continue with Azure simply because I will learn skills that I will be happy I have down the road?\n* Is Fabric actually an easy-to-use end-to-end solution that someone with a PowerBI background would find easier that Azure?\u00a0 Or is that just marketing.\n* At what point, in either solution I decide on, do I need to make sure my compensation is appropriate for my acquired skillsets?\u00a0 Are there milestones?\u00a0", "author_fullname": "t2_9iisakkrk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure or Fabric? - Small Company, single Analyist, very unskilled", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16pgbpu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695404108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The title says it all, but I will provide more detail.\u00a0 I work for a smaller size company, but we are growing.\u00a0 Sometime last year I noticed it became harder to work with the amount of data I had collected in my two years in this role.\u00a0 Previously, I had been working with PowerBI, Excel (power pivot), &amp;amp; OneDrive.I would put all my data in OneDrive folders &amp;amp; structure all reports based on a star schema.\u00a0 I used Power Query to access some of the larger data sets by connecting to the file path in my file explorer.\u00a0 On top of things starting to slow down (queries take a while to refresh in Excel/Power BI), some of my collogues expressed that they wanted to have access to some of the tools I built myself, as well as a backup to our data should the company lose access to everything I have saved in OneDrive.&lt;br/&gt;\nOriginally, I was hired as an analyst.\u00a0 The structure &amp;amp; storage of data, as well as the creation of assets to help organize, update, transform, &amp;amp; connect data seems outside of the scope of an anaylist, but I am also eager to learn.\u00a0 From my research it seems that the analyst role typically tops out at just under six figures, so if I want to move up in my career, I need to expand my skillset.\u00a0&lt;br/&gt;\nThis has led me to start learning some of the basics in Azure.\u00a0 I have set up a gen2 datalake storage account &amp;amp; I have moved all of my data there.\u00a0 I connect directly to that using the endpoint &amp;amp; do ETL in power query still.\u00a0 I have started learning some ETL processes in Azure Data Factory.\u00a0 I have a few tables that I have also moved from my datalake into an Azure SQL server, &amp;amp; I can use SQL to do basic queries.\u00a0 It seems like the depth &amp;amp; complexity of Azure, &amp;amp; all of its components are vast.\u00a0 I have spent what feels like considerable time learning about the key vault, Access Control (AIM), my subscription &amp;amp; storage costs, types of storage, redundancy &amp;amp; backups etc... &amp;amp; I feel like I am just scratching the surface.\u00a0 Each table I try to create in my SQL server presents new challenges.\u00a0 I am a solo, self-taught, data engineering hackjob &amp;amp; barley treading water.&lt;br/&gt;\nAround the time I started working in Azure I also had advertisements for Fabric.\u00a0 I haven&amp;#39;t done as much research on Fabric as I have Azure, but the marketing presents Fabric as a packaged solution that uses more interfaces that a PowerBI analyist might be able to jump into &amp;amp; start creating right away.\u00a0\u00a0&lt;br/&gt;\nSo, after that brief history &amp;amp; introduction, here are some questions I have:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Do I stay the course in Azure or jump ship to Fabric if I want to quickly backup data, make connections in a data model, &amp;amp; give collogues access to data?&lt;/li&gt;\n&lt;li&gt;Do I continue with Azure simply because I will learn skills that I will be happy I have down the road?&lt;/li&gt;\n&lt;li&gt;Is Fabric actually an easy-to-use end-to-end solution that someone with a PowerBI background would find easier that Azure?\u00a0 Or is that just marketing.&lt;/li&gt;\n&lt;li&gt;At what point, in either solution I decide on, do I need to make sure my compensation is appropriate for my acquired skillsets?\u00a0 Are there milestones?\u00a0&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16pgbpu", "is_robot_indexable": true, "report_reasons": null, "author": "Y2KLMNOP", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pgbpu/azure_or_fabric_small_company_single_analyist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pgbpu/azure_or_fabric_small_company_single_analyist/", "subreddit_subscribers": 129918, "created_utc": 1695404108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,   \n\n\nI was recently tasked with setting up real-time collection of data from our Veeva Platform, to help power our new recommendation engine POC.  \nI came up with the following architecture. I set up a simple Lambda function that pings the Veeva API, queries the logs, and transforms them to provide a list of users and their interactions. This list is then sent as a payload directly to my API Gateway, which integrates with an SQS queue. Next, I have another Lambda function triggered by SQS, which forwards this data to my Kafka servers. Currently, everything is working as expected; I receive the users' list in JSON format in real-time on my consumer server. My next step is to perform an insert-only merge of this data into our existing interactions table using Databricks. This will then be utilized by our Data Scientists to generate recommendations.  \n\n\nhttps://preview.redd.it/r7nbceyfotpb1.png?width=1288&amp;format=png&amp;auto=webp&amp;s=a79e4ffa3ba3f206860a705669f3a6713c6dd2a9\n\nHere's the issue I can't connect my databricks cluster to ingest my data in my Kafka cluster. \n\n1.I checked the docs, and the direct connection between Databricks and MSK using IAM roles and Instance Profiles is still in Public Preview. I spent 2 days trying to configure it and failed. I checked both the network aspects and the installation of the JAR files.\n\n2. If anyone knows how to connect them or point me in the right direction, I'd be so grateful!\n\n3. I spoke to my Product lead about this. He said that an acceptable time gap between querying the Veeva API and data landing in our bronze table is around 2-3 seconds. However, they want for this to be as close to real-time as possible, because the higher-ups also want to use Databricks live dashboards for business metrics, so they can take \"*real time*\" decisions.   \n\n\nTaking this into account I decided this would be the way to go, \n\n![img](pw34e4r7ttpb1 \"Have a consumer lambda that has an MSK trigger, that reads the interactions.json, writes it to an S3 bucket, and have autoloader ingest the data and run an Insert-only Merge on my existing interactions table. \n\")\n\nWould this work? keeping the time-constraint in mind\n\n&amp;#x200B;", "author_fullname": "t2_ic83gko1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Managed Service Kakfa to Databricks - Ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "media_metadata": {"r7nbceyfotpb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b07a705af6b3ea265256238dbcc7afc3690f14a9"}, {"y": 111, "x": 216, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a5bd7da6c115a4f5925c88088311d07d9046980"}, {"y": 165, "x": 320, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5396bcd011cb36754c0498a14eaf5e849c6b6d75"}, {"y": 330, "x": 640, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=34aa52d6365592a4d2cda12c0cc477fb796325aa"}, {"y": 496, "x": 960, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8ed0809ed8748fefd86ebeec16290c58e620492"}, {"y": 558, "x": 1080, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=08f0eb1efa60652477329e73455e119c84ebc15d"}], "s": {"y": 666, "x": 1288, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=1288&amp;format=png&amp;auto=webp&amp;s=a79e4ffa3ba3f206860a705669f3a6713c6dd2a9"}, "id": "r7nbceyfotpb1"}, "pw34e4r7ttpb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 36, "x": 108, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3efe704a959f05c157a6eff7d8358e10d0bcf158"}, {"y": 73, "x": 216, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9c0438afb4cbcd3d565897919e9b389dbccdd2b0"}, {"y": 109, "x": 320, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6e06ad469d76e2ab96fdada899b637357f40976"}, {"y": 218, "x": 640, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d453a2ebfee29a48bc710dffd76516bb91b5a4d6"}, {"y": 327, "x": 960, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=018974285eb67a015dca5492709ee9ba2bb7a0cd"}, {"y": 368, "x": 1080, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3c82623f457fb9ff3a834986deb42da9f0520815"}], "s": {"y": 584, "x": 1712, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=1712&amp;format=png&amp;auto=webp&amp;s=97b19b97a4368ba0bb88cdd5a0bfc812bf3a1c3f"}, "id": "pw34e4r7ttpb1"}}, "name": "t3_16pdpw0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/grQ8vi_-WT26eqUU7Dxncg-D8R6QjYHiu64wkK0s2og.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695397639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,   &lt;/p&gt;\n\n&lt;p&gt;I was recently tasked with setting up real-time collection of data from our Veeva Platform, to help power our new recommendation engine POC.&lt;br/&gt;\nI came up with the following architecture. I set up a simple Lambda function that pings the Veeva API, queries the logs, and transforms them to provide a list of users and their interactions. This list is then sent as a payload directly to my API Gateway, which integrates with an SQS queue. Next, I have another Lambda function triggered by SQS, which forwards this data to my Kafka servers. Currently, everything is working as expected; I receive the users&amp;#39; list in JSON format in real-time on my consumer server. My next step is to perform an insert-only merge of this data into our existing interactions table using Databricks. This will then be utilized by our Data Scientists to generate recommendations.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r7nbceyfotpb1.png?width=1288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a79e4ffa3ba3f206860a705669f3a6713c6dd2a9\"&gt;https://preview.redd.it/r7nbceyfotpb1.png?width=1288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a79e4ffa3ba3f206860a705669f3a6713c6dd2a9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the issue I can&amp;#39;t connect my databricks cluster to ingest my data in my Kafka cluster. &lt;/p&gt;\n\n&lt;p&gt;1.I checked the docs, and the direct connection between Databricks and MSK using IAM roles and Instance Profiles is still in Public Preview. I spent 2 days trying to configure it and failed. I checked both the network aspects and the installation of the JAR files.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;If anyone knows how to connect them or point me in the right direction, I&amp;#39;d be so grateful!&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I spoke to my Product lead about this. He said that an acceptable time gap between querying the Veeva API and data landing in our bronze table is around 2-3 seconds. However, they want for this to be as close to real-time as possible, because the higher-ups also want to use Databricks live dashboards for business metrics, so they can take &amp;quot;&lt;em&gt;real time&lt;/em&gt;&amp;quot; decisions.   &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Taking this into account I decided this would be the way to go, &lt;/p&gt;\n\n&lt;p&gt;![img](pw34e4r7ttpb1 &amp;quot;Have a consumer lambda that has an MSK trigger, that reads the interactions.json, writes it to an S3 bucket, and have autoloader ingest the data and run an Insert-only Merge on my existing interactions table. \n&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;Would this work? keeping the time-constraint in mind&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16pdpw0", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Debate_94", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pdpw0/aws_managed_service_kakfa_to_databricks_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pdpw0/aws_managed_service_kakfa_to_databricks_ingestion/", "subreddit_subscribers": 129918, "created_utc": 1695397639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fwerb2uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DataOps vs DevOps - A Practitioner\u2019s View", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_16pkc8d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/rjsGibpB0sKr6-VB-3qgo3rqtGkT-gq7ja3aOnF4xi4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695414090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "y42.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.y42.com/blog/dataops-vs-devops/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_XqxCxeZyWc9QjBnC8aX2Romh1dVb0nmuYPP_JGxu5k.jpg?auto=webp&amp;s=5ac3ea5b54222addf5768ed0d38ca7af8e9755d8", "width": 2160, "height": 1440}, "resolutions": [{"url": "https://external-preview.redd.it/_XqxCxeZyWc9QjBnC8aX2Romh1dVb0nmuYPP_JGxu5k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95c885383f7e1b27214386dc090887012e3d5461", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/_XqxCxeZyWc9QjBnC8aX2Romh1dVb0nmuYPP_JGxu5k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=88839f5e8242f9ac13b5fe1c63d38a229eb8678c", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/_XqxCxeZyWc9QjBnC8aX2Romh1dVb0nmuYPP_JGxu5k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=92ac7dfca78e007647046d11484c0adf93ce7b27", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/_XqxCxeZyWc9QjBnC8aX2Romh1dVb0nmuYPP_JGxu5k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=59b6ab58965c4411a1ede11f17fadc7c528eb00a", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/_XqxCxeZyWc9QjBnC8aX2Romh1dVb0nmuYPP_JGxu5k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=73f44b64b76aa1c1a7d0ff925f9f607f067d31a5", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/_XqxCxeZyWc9QjBnC8aX2Romh1dVb0nmuYPP_JGxu5k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2c1b3c21ad2184bcd049ee652373d78f15274a8e", "width": 1080, "height": 720}], "variants": {}, "id": "eZy1xbYLyRYTRJCnhYnv4Cq1YCZspkh7RYPVBOKVKJ0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16pkc8d", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Guidance599", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pkc8d/dataops_vs_devops_a_practitioners_view/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.y42.com/blog/dataops-vs-devops/", "subreddit_subscribers": 129918, "created_utc": 1695414090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My reddit consultants, I need your help for this one.\n\nI have an unofficial job offer to basically revamp the entire ETL process for a hospital IT team. It's my first job out of college, I have degree in natural sciences. The extent of my programming skills reaches creating basic flask applications with sqlite databases, hosting it on the cloud (PythonAnywhere or Azure), and maybe some local psql and postgres with bash. \n\nThe current ETL process is as follows: data is entered into a third party proprietary software, extracted via built-in functions for export as xml/xlsx, transformed with vba, then a powerBI dashboard sits on top of the excel files. \n\nMy proposal is to extract the data with Python (not sure if this third party software has a read/write API, but I was told that the folders and files of this proprietary software are accessible through on premise linux machines), transform with pandas, then load it into some sql db dedicated for analytics hosted with on prem servers. From there, we can export clean spreadsheets or create a dashboard on top of it with PowerBI. Then somehow, write the clean data from this sql db back into the proprietary software.\n\nThis was the vision I proposed during the my interview. My managers are not technically savvy, they're open to any technology to revamp the current ETL process, and they seem to like my idea. They consulted with someone with a proper CS background, and asked if I knew how to connect to linux machines with Python. I exaggerated and said \"yes I know how to use linux and Python\", as in I can type in python3 into a bash terminal and play with python locally. But I think they mean if I could connect to remote linux machines, maybe with something like paramiko? I dont have a formal CS background, I have no idea how authentication, VMs, etc, work. I have no idea how I would remotely access the proprietary software files sitting on on-prem servers. I dont know if I'll be given a work computer that doesnt have inherent access to the proprietary system, or if I could even download third party python packages. I need direction please.", "author_fullname": "t2_5pddzf1hy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connecting to a linux machine with Python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16pk5vs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695413650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My reddit consultants, I need your help for this one.&lt;/p&gt;\n\n&lt;p&gt;I have an unofficial job offer to basically revamp the entire ETL process for a hospital IT team. It&amp;#39;s my first job out of college, I have degree in natural sciences. The extent of my programming skills reaches creating basic flask applications with sqlite databases, hosting it on the cloud (PythonAnywhere or Azure), and maybe some local psql and postgres with bash. &lt;/p&gt;\n\n&lt;p&gt;The current ETL process is as follows: data is entered into a third party proprietary software, extracted via built-in functions for export as xml/xlsx, transformed with vba, then a powerBI dashboard sits on top of the excel files. &lt;/p&gt;\n\n&lt;p&gt;My proposal is to extract the data with Python (not sure if this third party software has a read/write API, but I was told that the folders and files of this proprietary software are accessible through on premise linux machines), transform with pandas, then load it into some sql db dedicated for analytics hosted with on prem servers. From there, we can export clean spreadsheets or create a dashboard on top of it with PowerBI. Then somehow, write the clean data from this sql db back into the proprietary software.&lt;/p&gt;\n\n&lt;p&gt;This was the vision I proposed during the my interview. My managers are not technically savvy, they&amp;#39;re open to any technology to revamp the current ETL process, and they seem to like my idea. They consulted with someone with a proper CS background, and asked if I knew how to connect to linux machines with Python. I exaggerated and said &amp;quot;yes I know how to use linux and Python&amp;quot;, as in I can type in python3 into a bash terminal and play with python locally. But I think they mean if I could connect to remote linux machines, maybe with something like paramiko? I dont have a formal CS background, I have no idea how authentication, VMs, etc, work. I have no idea how I would remotely access the proprietary software files sitting on on-prem servers. I dont know if I&amp;#39;ll be given a work computer that doesnt have inherent access to the proprietary system, or if I could even download third party python packages. I need direction please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16pk5vs", "is_robot_indexable": true, "report_reasons": null, "author": "knewtonslol", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pk5vs/connecting_to_a_linux_machine_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pk5vs/connecting_to_a_linux_machine_with_python/", "subreddit_subscribers": 129918, "created_utc": 1695413650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Some background:\nI currently work for a small startup (30 employees) and we are starting to mature our data infrastructure (postgres -&gt; AWS DMS-&gt; Redshift -&gt;DBT-&gt;Redshift-&gt;PowerBi). \nWe store a large amount of information in JSONs which can sometimes be &gt;100k characters long, suprassing the 65k Byte limit for Amazon Redshift data limits for a single cell. \nThese JSONs are paramount to our reporting - what options are there to be able to utilise this json without it being truncated in redshift? We don't have a dedicated data engineer and so don't want to create too much infra to upkeep. \nIs the only option to create some custom python scripts to abstract certain keys from the JSON?\nCheers!", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading Large JSON into Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p8no9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695384692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some background:\nI currently work for a small startup (30 employees) and we are starting to mature our data infrastructure (postgres -&amp;gt; AWS DMS-&amp;gt; Redshift -&amp;gt;DBT-&amp;gt;Redshift-&amp;gt;PowerBi). \nWe store a large amount of information in JSONs which can sometimes be &amp;gt;100k characters long, suprassing the 65k Byte limit for Amazon Redshift data limits for a single cell. \nThese JSONs are paramount to our reporting - what options are there to be able to utilise this json without it being truncated in redshift? We don&amp;#39;t have a dedicated data engineer and so don&amp;#39;t want to create too much infra to upkeep. \nIs the only option to create some custom python scripts to abstract certain keys from the JSON?\nCheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16p8no9", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p8no9/loading_large_json_into_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p8no9/loading_large_json_into_redshift/", "subreddit_subscribers": 129918, "created_utc": 1695384692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I come from working primarily with Microsoft data tools and am now in a cloud environment supporting a dbt/Snowflake data warehouse.  In past data roles, Microsoft Master Data Services proved to be a critical tool in helping to house business definitions and hierarchies that didn't make sense to integrate into our ERP or another business tool since it was just needed for reporting.  Is there a similar tool, that won't break the bank, that I can use with our Snowflake environment to allow users to own data mappings that support reporting?  TIA for any advice ya'll can give.  We're not looking for a crazy robust MDM tool, just something that can mimic the very basic functionality that Microsoft MDS provided.", "author_fullname": "t2_i83i7un9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Basic MDM tool for Snowflake allowing business users to own/modify data mappings?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p92i1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695385885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come from working primarily with Microsoft data tools and am now in a cloud environment supporting a dbt/Snowflake data warehouse.  In past data roles, Microsoft Master Data Services proved to be a critical tool in helping to house business definitions and hierarchies that didn&amp;#39;t make sense to integrate into our ERP or another business tool since it was just needed for reporting.  Is there a similar tool, that won&amp;#39;t break the bank, that I can use with our Snowflake environment to allow users to own data mappings that support reporting?  TIA for any advice ya&amp;#39;ll can give.  We&amp;#39;re not looking for a crazy robust MDM tool, just something that can mimic the very basic functionality that Microsoft MDS provided.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16p92i1", "is_robot_indexable": true, "report_reasons": null, "author": "Dizzy_Berry3058", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p92i1/basic_mdm_tool_for_snowflake_allowing_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p92i1/basic_mdm_tool_for_snowflake_allowing_business/", "subreddit_subscribers": 129918, "created_utc": 1695385885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have data that looks like this:\n\n&amp;#x200B;\n\n|Row|Hashed IP Address|Cookie|Persistent User ID|\n|:-|:-|:-|:-|\n|1|A|1||\n|2|B|1|DD|\n|3|B|2||\n|4|C|3|DD|\n|5|E|4|EE|\n|6||5|EE|\n\nTo improve attribution, we're trying to leverage three data points to find unique users: a hashed IP address, a tracking cookie, and a persistent user ID sent from the backend when a user is logged in.\n\nThe essential theory is that, if any data point shows up more than once, we know that's the same user. \n\nSo, by looking at the data, I know that rows 1 - 4 are the same person and that 5 - 6 are another person. What I don't know if how to tell a computer how to figure that out.\n\nI know other people have tackled this challenge before, but I can't find anything on it. Anyone know of any articles or videos on this challenge?\n\n*Note: There are other challenges here, like dealing with the possibility that multiple people get assigned the same IP, (e.g: through mobile networks or VPNs) or log in to multiple accounts, but we'll deal with one problem at a time here.*", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tracking one user with three datapoints", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p6fgo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695377680.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695377497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have data that looks like this:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Row&lt;/th&gt;\n&lt;th align=\"left\"&gt;Hashed IP Address&lt;/th&gt;\n&lt;th align=\"left\"&gt;Cookie&lt;/th&gt;\n&lt;th align=\"left\"&gt;Persistent User ID&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;B&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;DD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;B&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;C&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;DD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;td align=\"left\"&gt;E&lt;/td&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;EE&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;6&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;td align=\"left\"&gt;EE&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;To improve attribution, we&amp;#39;re trying to leverage three data points to find unique users: a hashed IP address, a tracking cookie, and a persistent user ID sent from the backend when a user is logged in.&lt;/p&gt;\n\n&lt;p&gt;The essential theory is that, if any data point shows up more than once, we know that&amp;#39;s the same user. &lt;/p&gt;\n\n&lt;p&gt;So, by looking at the data, I know that rows 1 - 4 are the same person and that 5 - 6 are another person. What I don&amp;#39;t know if how to tell a computer how to figure that out.&lt;/p&gt;\n\n&lt;p&gt;I know other people have tackled this challenge before, but I can&amp;#39;t find anything on it. Anyone know of any articles or videos on this challenge?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Note: There are other challenges here, like dealing with the possibility that multiple people get assigned the same IP, (e.g: through mobile networks or VPNs) or log in to multiple accounts, but we&amp;#39;ll deal with one problem at a time here.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16p6fgo", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p6fgo/tracking_one_user_with_three_datapoints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p6fgo/tracking_one_user_with_three_datapoints/", "subreddit_subscribers": 129918, "created_utc": 1695377497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been looking into data observability tools, but I\u2019ve had a hard time finding something that properly works with our stack which is heavily based on Azure (event hubs, azure functions, some azure data factories, and data warehouse in sql server)\u2026(plus (a lot of) legacy stuff). But mostly looking for tools that can work with the azure platform. Would love to talk to people who have been investing in, working with or thinking about data observability tools - or people that think its a waste of time!\n\nTldr; Data observability in Azure, how do?", "author_fullname": "t2_9niapxhh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data observability for Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16pls24", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695417638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been looking into data observability tools, but I\u2019ve had a hard time finding something that properly works with our stack which is heavily based on Azure (event hubs, azure functions, some azure data factories, and data warehouse in sql server)\u2026(plus (a lot of) legacy stuff). But mostly looking for tools that can work with the azure platform. Would love to talk to people who have been investing in, working with or thinking about data observability tools - or people that think its a waste of time!&lt;/p&gt;\n\n&lt;p&gt;Tldr; Data observability in Azure, how do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16pls24", "is_robot_indexable": true, "report_reasons": null, "author": "Jolly-Difference5021", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pls24/data_observability_for_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pls24/data_observability_for_azure/", "subreddit_subscribers": 129918, "created_utc": 1695417638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " We will utilize Terraform for infrastructure management and Snowflake as the data warehouse. The architecture includes databases for Clients, Transactions, Products, and Suppliers, each containing relevant information. Computation warehouses will be set up for different environments, with role-based access controls and encryption policies ensuring data security. \n\n[https://medium.com/@stefentaime\\_10958/crafting-a-robust-devops-journey-terraform-snowflake-for-scalable-data-solutions-34fc24b2f877](https://medium.com/@stefentaime_10958/crafting-a-robust-devops-journey-terraform-snowflake-for-scalable-data-solutions-34fc24b2f877)", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Crafting a Robust DevOps Journey: Terraform &amp; Snowflake for Scalable Data Solutions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16pjbmw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695411558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We will utilize Terraform for infrastructure management and Snowflake as the data warehouse. The architecture includes databases for Clients, Transactions, Products, and Suppliers, each containing relevant information. Computation warehouses will be set up for different environments, with role-based access controls and encryption policies ensuring data security. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@stefentaime_10958/crafting-a-robust-devops-journey-terraform-snowflake-for-scalable-data-solutions-34fc24b2f877\"&gt;https://medium.com/@stefentaime_10958/crafting-a-robust-devops-journey-terraform-snowflake-for-scalable-data-solutions-34fc24b2f877&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FV-yi3my4LaUo_SskrHmor4WppQMqLIokbkqMlaGFDo.jpg?auto=webp&amp;s=6a61ad0ad0788f00033844e72e326f861a1cf3c8", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/FV-yi3my4LaUo_SskrHmor4WppQMqLIokbkqMlaGFDo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1666115835ebadb43f77945526ce0aef76b8b133", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/FV-yi3my4LaUo_SskrHmor4WppQMqLIokbkqMlaGFDo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=022da04d5412cc0f8964c78d3b60c1c1dfca2b06", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/FV-yi3my4LaUo_SskrHmor4WppQMqLIokbkqMlaGFDo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6f076e8477dc39f303e0b8fa043e0761b6125e4", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/FV-yi3my4LaUo_SskrHmor4WppQMqLIokbkqMlaGFDo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5376161c3a52848866d5b367527905b3d2608a3f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/FV-yi3my4LaUo_SskrHmor4WppQMqLIokbkqMlaGFDo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=25e513a4ea81dd00259a74a8043a3f56a0799caf", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/FV-yi3my4LaUo_SskrHmor4WppQMqLIokbkqMlaGFDo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=579e0f72927bb6d166004eecaede479bb1d700c3", "width": 1080, "height": 607}], "variants": {}, "id": "ygrbgk4ISaHXDV2gzH8joEFtU9NCX_NDCsQQcdCvqko"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16pjbmw", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pjbmw/crafting_a_robust_devops_journey_terraform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pjbmw/crafting_a_robust_devops_journey_terraform/", "subreddit_subscribers": 129918, "created_utc": 1695411558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am getting data from on premise sharepoint. Users put data into folder and I need to process them.  \nSo far I didnt found a solution how on premise sharepoint can notify that file was uploaded into folder, so I am getting folder content using sharepoint API and once there is a file, I process it and move it to another folder.\n\nIt is just set of python codes that handle file processing. \n\nHow would you schedule processing? Now it is cron job on a centos server. \n\nI can use openshift, or we are also running Airflow, but I dont think Airflow would be good match for running tasks this frequently.\n\nMany thanks for your ideas!", "author_fullname": "t2_46l1zqd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to deploy and run job that needs to be run every 2 minutes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p59ze", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695373246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am getting data from on premise sharepoint. Users put data into folder and I need to process them.&lt;br/&gt;\nSo far I didnt found a solution how on premise sharepoint can notify that file was uploaded into folder, so I am getting folder content using sharepoint API and once there is a file, I process it and move it to another folder.&lt;/p&gt;\n\n&lt;p&gt;It is just set of python codes that handle file processing. &lt;/p&gt;\n\n&lt;p&gt;How would you schedule processing? Now it is cron job on a centos server. &lt;/p&gt;\n\n&lt;p&gt;I can use openshift, or we are also running Airflow, but I dont think Airflow would be good match for running tasks this frequently.&lt;/p&gt;\n\n&lt;p&gt;Many thanks for your ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p59ze", "is_robot_indexable": true, "report_reasons": null, "author": "pyzo_ryzo", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p59ze/where_to_deploy_and_run_job_that_needs_to_be_run/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p59ze/where_to_deploy_and_run_job_that_needs_to_be_run/", "subreddit_subscribers": 129918, "created_utc": 1695373246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey fellow Data Engineers,\n\nI'm currently interning as a Data Engineer at a new company, and I've encountered a bit of a challenge in my first task. I'm hoping some of you with more experience can provide some guidance and resources.\n\nHere's the issue:\n\n**Task**: My assignment is to transfer data from one S3 bucket (let's call it Account A) to another S3 bucket (Account B). The source path is s3://X/Y, and the target path should be s3://X/Y/daily  \nX being bucket name.\n\n**Data Structure**: Within the 'Y' directory in Account A, there are numerous folders, each named after a date (e.g., '2022-01-01'). Inside each of these date folders, there are multiple .csv files. My objective is to move all these folders and files to the target location while converting them to the Parquet format.\n\n**Challenges**: I've attempted various methods to achieve this, but nothing seems to be working as expected. I believe setting up a Glue ETL job could be a solution(It's compulsory that it should be done through Glue ETL only), and I have the necessary permissions for both reading and writing in both S3 accounts. So, it doesn't appear to be a permissions issue.\n\nI would greatly appreciate any assistance, guidance, or resources you can provide to help me tackle this task successfully. If you've worked with Glue ETL jobs for a similar scenario or have suggestions for alternative approaches, please share your insights.\n\nThank you all in advance for your support!", "author_fullname": "t2_eq8117jmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help with S3 Data Transfer and Glue ETL Job Setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16pv0to", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695443464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently interning as a Data Engineer at a new company, and I&amp;#39;ve encountered a bit of a challenge in my first task. I&amp;#39;m hoping some of you with more experience can provide some guidance and resources.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the issue:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Task&lt;/strong&gt;: My assignment is to transfer data from one S3 bucket (let&amp;#39;s call it Account A) to another S3 bucket (Account B). The source path is s3://X/Y, and the target path should be s3://X/Y/daily&lt;br/&gt;\nX being bucket name.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data Structure&lt;/strong&gt;: Within the &amp;#39;Y&amp;#39; directory in Account A, there are numerous folders, each named after a date (e.g., &amp;#39;2022-01-01&amp;#39;). Inside each of these date folders, there are multiple .csv files. My objective is to move all these folders and files to the target location while converting them to the Parquet format.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Challenges&lt;/strong&gt;: I&amp;#39;ve attempted various methods to achieve this, but nothing seems to be working as expected. I believe setting up a Glue ETL job could be a solution(It&amp;#39;s compulsory that it should be done through Glue ETL only), and I have the necessary permissions for both reading and writing in both S3 accounts. So, it doesn&amp;#39;t appear to be a permissions issue.&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate any assistance, guidance, or resources you can provide to help me tackle this task successfully. If you&amp;#39;ve worked with Glue ETL jobs for a similar scenario or have suggestions for alternative approaches, please share your insights.&lt;/p&gt;\n\n&lt;p&gt;Thank you all in advance for your support!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16pv0to", "is_robot_indexable": true, "report_reasons": null, "author": "vishalkarur12", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pv0to/need_help_with_s3_data_transfer_and_glue_etl_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pv0to/need_help_with_s3_data_transfer_and_glue_etl_job/", "subreddit_subscribers": 129918, "created_utc": 1695443464.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in consulting and will be starting on a Data Warehousing project that will use Amazon Redshift, Glue and EMR. I\u2019ve never used them before so besides Amazons documentation are there any good YouTube channels that will get me up to speed quickly on those technologies?", "author_fullname": "t2_8wpw0e1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best resources to learn Amazon Redshift, Glue, EMR", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ppt9i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695427877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in consulting and will be starting on a Data Warehousing project that will use Amazon Redshift, Glue and EMR. I\u2019ve never used them before so besides Amazons documentation are there any good YouTube channels that will get me up to speed quickly on those technologies?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ppt9i", "is_robot_indexable": true, "report_reasons": null, "author": "khaili109", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ppt9i/best_resources_to_learn_amazon_redshift_glue_emr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ppt9i/best_resources_to_learn_amazon_redshift_glue_emr/", "subreddit_subscribers": 129918, "created_utc": 1695427877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Data engineers, I'm being asked to spin up Trino to connect to a BI platform so that we can leverage our SingleStore investment.   \n\n\nThey're asking me for a timeframe to get this stood up. I barely have any experience with Docker or Kubernetes which seems required for standing up the infrastructure.   \n\n\nAppreciate the help! ", "author_fullname": "t2_6x0ovdlr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing Trino for a MemSQL Connection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16podyi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695424041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Data engineers, I&amp;#39;m being asked to spin up Trino to connect to a BI platform so that we can leverage our SingleStore investment.   &lt;/p&gt;\n\n&lt;p&gt;They&amp;#39;re asking me for a timeframe to get this stood up. I barely have any experience with Docker or Kubernetes which seems required for standing up the infrastructure.   &lt;/p&gt;\n\n&lt;p&gt;Appreciate the help! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16podyi", "is_robot_indexable": true, "report_reasons": null, "author": "Solid_Relationship70", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16podyi/managing_trino_for_a_memsql_connection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16podyi/managing_trino_for_a_memsql_connection/", "subreddit_subscribers": 129918, "created_utc": 1695424041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently have a Cloud Composer instance running that deals mostly with data analytics tasks, and are looking to schedule some application related jobs. \n\nAll of our application related infra is on AWS. Our image registry is GHCR, and we have a k8s cluster on EKS. \n\nIn my previous experience, we generally stuck to using the KubernetesPodOperator for all jobs as a best practice, but from my understanding Cloud Composer automatically uses the GKE cluster in the environment to create the pods. \n\nSo my question is, what's the best way to run these jobs? We will be pushing the images of these jobs onto GHCR, and we'd prefer them to run on EKS. All the resources that these jobs would interact with are on AWS as well (RDS, Dynamo etc).\n\nWould it be possible to do so using certain operators in Cloud Composer? I've stumbled upon the EKSPodOperator, but it doesn't seem like Cloud Composer supports it. \n\nAny suggestions welcome, I've been stuck on this for a while.", "author_fullname": "t2_blmi7z1v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with running jobs on EKS using Cloud Composer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16pkjmg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695414593.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently have a Cloud Composer instance running that deals mostly with data analytics tasks, and are looking to schedule some application related jobs. &lt;/p&gt;\n\n&lt;p&gt;All of our application related infra is on AWS. Our image registry is GHCR, and we have a k8s cluster on EKS. &lt;/p&gt;\n\n&lt;p&gt;In my previous experience, we generally stuck to using the KubernetesPodOperator for all jobs as a best practice, but from my understanding Cloud Composer automatically uses the GKE cluster in the environment to create the pods. &lt;/p&gt;\n\n&lt;p&gt;So my question is, what&amp;#39;s the best way to run these jobs? We will be pushing the images of these jobs onto GHCR, and we&amp;#39;d prefer them to run on EKS. All the resources that these jobs would interact with are on AWS as well (RDS, Dynamo etc).&lt;/p&gt;\n\n&lt;p&gt;Would it be possible to do so using certain operators in Cloud Composer? I&amp;#39;ve stumbled upon the EKSPodOperator, but it doesn&amp;#39;t seem like Cloud Composer supports it. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions welcome, I&amp;#39;ve been stuck on this for a while.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16pkjmg", "is_robot_indexable": true, "report_reasons": null, "author": "lingorioriorio", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pkjmg/help_with_running_jobs_on_eks_using_cloud_composer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pkjmg/help_with_running_jobs_on_eks_using_cloud_composer/", "subreddit_subscribers": 129918, "created_utc": 1695414593.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview with Timothy Sehn - Founder and CEO at DoltHub", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_16p74k2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jL6LV9Dwh8e7lBY7D9sRj6_2hgi-O5lIf7OMtBIjyoM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695379900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/interview-with-timothy-sehn-founder?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pR25QgqElDsC75OMmvtK1nhHrEo37l35RzB4CisfPgw.jpg?auto=webp&amp;s=15db4847973a77499417be19c32ad8665d2c8e56", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/pR25QgqElDsC75OMmvtK1nhHrEo37l35RzB4CisfPgw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e820eeece573e962d167aec961df1cfe51f29b4", "width": 108, "height": 108}], "variants": {}, "id": "nOZdT6kbraMnqTQhVbWj-y44qnThxTM0UF0oWJQpDW0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16p74k2", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p74k2/interview_with_timothy_sehn_founder_and_ceo_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/interview-with-timothy-sehn-founder?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "subreddit_subscribers": 129918, "created_utc": 1695379900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \n\nI am looking for advise /reassurance on how to change jobs into a Data Engineering role within Europe. \n\nI have worked in marketing for the last 4 years, but have been hating it. In my roles, I have been gravitating towards data and tech more than the marketing elements. Recently, i was made redundant due to the company collapsing. So, I am taking this opportunity to switch. I looked into bootcamps, but I can't really afford the price of \u20ac7,000. I can probably pay about \u20ac1,000 for learning etc (gotta keep bread on the table). \n\n\nI have taken some courses in the past in python. I am not versed in object oriented programming, but have a very good understanding of the fundamentals and have created a few NLP projects. And I also have a very low level understanding of SQL and mongoDB.\n\nI am wondering if anyone else has also switched careers in the European zone and what your experience was in changing? If there are books, courses or certs that would help the switch?\n\nAlso, Is it in 2023 a bad time to change careers? (I keep being told by my friends that it's the wrong time ) \ud83d\ude2d\n\nAlso a bit off topic, but would a scrum cert benefit the role?\n\n\nAny help would be appreciated \ud83d\udc4d", "author_fullname": "t2_7lu47gxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is A European Role Change To DE hard?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p5t70", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695375237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I am looking for advise /reassurance on how to change jobs into a Data Engineering role within Europe. &lt;/p&gt;\n\n&lt;p&gt;I have worked in marketing for the last 4 years, but have been hating it. In my roles, I have been gravitating towards data and tech more than the marketing elements. Recently, i was made redundant due to the company collapsing. So, I am taking this opportunity to switch. I looked into bootcamps, but I can&amp;#39;t really afford the price of \u20ac7,000. I can probably pay about \u20ac1,000 for learning etc (gotta keep bread on the table). &lt;/p&gt;\n\n&lt;p&gt;I have taken some courses in the past in python. I am not versed in object oriented programming, but have a very good understanding of the fundamentals and have created a few NLP projects. And I also have a very low level understanding of SQL and mongoDB.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if anyone else has also switched careers in the European zone and what your experience was in changing? If there are books, courses or certs that would help the switch?&lt;/p&gt;\n\n&lt;p&gt;Also, Is it in 2023 a bad time to change careers? (I keep being told by my friends that it&amp;#39;s the wrong time ) \ud83d\ude2d&lt;/p&gt;\n\n&lt;p&gt;Also a bit off topic, but would a scrum cert benefit the role?&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated \ud83d\udc4d&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16p5t70", "is_robot_indexable": true, "report_reasons": null, "author": "Representative_Two37", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p5t70/is_a_european_role_change_to_de_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p5t70/is_a_european_role_change_to_de_hard/", "subreddit_subscribers": 129918, "created_utc": 1695375237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working for a company where there pipelines are hosted on Google Cloud Run in a Flask application and orchestrated via Airflow. This model already existed when I joined, but it doesn't seem ideal to me.  Some of these scripts are very complex and use a lot of selenium to collect data.  Others are simpler pipelines. What do you recommend, refactor and move everything to Airflow or is there a better solution?", "author_fullname": "t2_96jab4in", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to host my pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p6bjh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695377099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working for a company where there pipelines are hosted on Google Cloud Run in a Flask application and orchestrated via Airflow. This model already existed when I joined, but it doesn&amp;#39;t seem ideal to me.  Some of these scripts are very complex and use a lot of selenium to collect data.  Others are simpler pipelines. What do you recommend, refactor and move everything to Airflow or is there a better solution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p6bjh", "is_robot_indexable": true, "report_reasons": null, "author": "T0ny_Corleone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p6bjh/where_to_host_my_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p6bjh/where_to_host_my_pipelines/", "subreddit_subscribers": 129918, "created_utc": 1695377099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://reddit.com/r/dataengineering/s/ZigMuAZRPP\n\nIn what order do you recommend acquiring the skills?", "author_fullname": "t2_jpnf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Follow up question to the 80/20 Pareto skills question.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p5zwo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695375932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://reddit.com/r/dataengineering/s/ZigMuAZRPP\"&gt;https://reddit.com/r/dataengineering/s/ZigMuAZRPP&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In what order do you recommend acquiring the skills?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p5zwo", "is_robot_indexable": true, "report_reasons": null, "author": "kiwifruta", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p5zwo/follow_up_question_to_the_8020_pareto_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p5zwo/follow_up_question_to_the_8020_pareto_skills/", "subreddit_subscribers": 129918, "created_utc": 1695375932.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}