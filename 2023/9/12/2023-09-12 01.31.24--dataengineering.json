{"kind": "Listing", "data": {"after": "t3_16fz3v6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_s4te90xk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The state of data content on LinkedIn: you can reduce costs by just doing less! Game changing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "name": "t3_16frhic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 176, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 176, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/b05wesGJLhREGk_kcRzO6_TvWwka0_rZJAHtbY5pAW4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694427062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/cg4yomx0plnb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/cg4yomx0plnb1.jpg?auto=webp&amp;s=57018188065745b50bd77364efd06d5a59e22c62", "width": 1079, "height": 844}, "resolutions": [{"url": "https://preview.redd.it/cg4yomx0plnb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=70d104ac81fb242540debb4b8978f475c921e006", "width": 108, "height": 84}, {"url": "https://preview.redd.it/cg4yomx0plnb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4de7359dc36ec56eacf247b1a61cf7ce344091c", "width": 216, "height": 168}, {"url": "https://preview.redd.it/cg4yomx0plnb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ea27aadc46510ddb42a20b39801c07b3705283f", "width": 320, "height": 250}, {"url": "https://preview.redd.it/cg4yomx0plnb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4f0c95004fbd47faf6d469a92d4e36022250cc01", "width": 640, "height": 500}, {"url": "https://preview.redd.it/cg4yomx0plnb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f2289b4534674760fb9d2e425e56e848d323536", "width": 960, "height": 750}], "variants": {}, "id": "PDNj3pC_3dM6xzgIhUmUiJViAwv2G7rUWS8pN56jM20"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "16frhic", "is_robot_indexable": true, "report_reasons": null, "author": "dataxp-community", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16frhic/the_state_of_data_content_on_linkedin_you_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/cg4yomx0plnb1.jpg", "subreddit_subscribers": 127977, "created_utc": 1694427062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long story short I\u2019m being put in charge of building an entire database and our entire backend infrastructure. Is this within the realm of data engineering?", "author_fullname": "t2_e0rep", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are data engineers in charge of creating database infrastructure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fmgwu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 54, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 54, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694408547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I\u2019m being put in charge of building an entire database and our entire backend infrastructure. Is this within the realm of data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fmgwu", "is_robot_indexable": true, "report_reasons": null, "author": "BiggyDeeKay", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fmgwu/are_data_engineers_in_charge_of_creating_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fmgwu/are_data_engineers_in_charge_of_creating_database/", "subreddit_subscribers": 127977, "created_utc": 1694408547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone! We currently have a series of .py files with ELT logic (load and write functions). However, we find that maintaining SQL code within these files is not elegant / best solution. What's a more elegant / best way to approach handling Spark SQL inside our elt .py files? Should we keep embedding SQL logic (say select * from table where date) inside the .py files or have .sql files in our repo and execute such .sql files inside our python elt files? Any advice/comments would be helpful!", "author_fullname": "t2_56ltry44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Elegant way of writing SQL inside of ETL .py files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fkudt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694403417.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! We currently have a series of .py files with ELT logic (load and write functions). However, we find that maintaining SQL code within these files is not elegant / best solution. What&amp;#39;s a more elegant / best way to approach handling Spark SQL inside our elt .py files? Should we keep embedding SQL logic (say select * from table where date) inside the .py files or have .sql files in our repo and execute such .sql files inside our python elt files? Any advice/comments would be helpful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fkudt", "is_robot_indexable": true, "report_reasons": null, "author": "Specific-Passage", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fkudt/elegant_way_of_writing_sql_inside_of_etl_py_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fkudt/elegant_way_of_writing_sql_inside_of_etl_py_files/", "subreddit_subscribers": 127977, "created_utc": 1694403417.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,   \nI am new DE, working on Azure and got asked if its possible to establish dynamic folder path in sync using DataFlow in ADF. I was working all the time on copy activity and though it is possible, answered positively and now I struggle to establish that. Is there any possibility? I've been chat gpt'd, youtube and stackoverflow and seems its impossible or out of the box solution? I am worried that I might be invited to some unpleasnt conversation tomorrow  \n", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have I made fool of myself?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fx672", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694443580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nI am new DE, working on Azure and got asked if its possible to establish dynamic folder path in sync using DataFlow in ADF. I was working all the time on copy activity and though it is possible, answered positively and now I struggle to establish that. Is there any possibility? I&amp;#39;ve been chat gpt&amp;#39;d, youtube and stackoverflow and seems its impossible or out of the box solution? I am worried that I might be invited to some unpleasnt conversation tomorrow  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16fx672", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 10, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fx672/have_i_made_fool_of_myself/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fx672/have_i_made_fool_of_myself/", "subreddit_subscribers": 127977, "created_utc": 1694443580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey data engineers! Wanted to share a couple of tricks I use with our dbt docs to make them look not boring.\n\nHere they are:\n\n1. Always customize the homepage. I see that many devs don't bother changing the default starting page, but in my opinion it's a very useful real estate you should utilize. We usually put there some useful links to tutorials about using dbt, links to external systems (like Jira and Github) and any other useful links (like to data glossary).\n2. Use doc blocks. This is a simple trick to utilize DRY principle -- write definition once and re-use it it many places. This approach, first of all, creates a single source of truth, second it become easier to maintain when documentation grow.\n3. Hide models and packages which you think are not useful. Many times I've seen that people just generate docs with all packages and external models, and it creates a mess. So, a bit of housekeeping really makes the docs easier to read and search.\n4. Finally, we developed our own color codes for nodes in DAG (sources are green, intermediates are blue, marts are light orange, exposures are pink). This makes it a bit easier to \"read\" the DAG and overall dependencies.\n\nThese are the tips I described in the laster issue of my dbt newsletter. But since I really value his community, I decided to make a post, instead of just throwing a link at you.", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to create an awesome documentation in dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16g64na", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694463720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey data engineers! Wanted to share a couple of tricks I use with our dbt docs to make them look not boring.&lt;/p&gt;\n\n&lt;p&gt;Here they are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Always customize the homepage. I see that many devs don&amp;#39;t bother changing the default starting page, but in my opinion it&amp;#39;s a very useful real estate you should utilize. We usually put there some useful links to tutorials about using dbt, links to external systems (like Jira and Github) and any other useful links (like to data glossary).&lt;/li&gt;\n&lt;li&gt;Use doc blocks. This is a simple trick to utilize DRY principle -- write definition once and re-use it it many places. This approach, first of all, creates a single source of truth, second it become easier to maintain when documentation grow.&lt;/li&gt;\n&lt;li&gt;Hide models and packages which you think are not useful. Many times I&amp;#39;ve seen that people just generate docs with all packages and external models, and it creates a mess. So, a bit of housekeeping really makes the docs easier to read and search.&lt;/li&gt;\n&lt;li&gt;Finally, we developed our own color codes for nodes in DAG (sources are green, intermediates are blue, marts are light orange, exposures are pink). This makes it a bit easier to &amp;quot;read&amp;quot; the DAG and overall dependencies.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;These are the tips I described in the laster issue of my dbt newsletter. But since I really value his community, I decided to make a post, instead of just throwing a link at you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16g64na", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16g64na/how_to_create_an_awesome_documentation_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16g64na/how_to_create_an_awesome_documentation_in_dbt/", "subreddit_subscribers": 127977, "created_utc": 1694463720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm an Informatics and do a good amount of data engineering, but it's not in my title or the entirety of my role. I can't seem to secure a DE role and I'm finding that a lot of these jobs are paying less than what I currently make. I see DE as a way forward in my career but the possibility of taking a pay cut isn't great.", "author_fullname": "t2_98rrwspa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would you take a pay cut to get into DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16g87ou", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694468224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an Informatics and do a good amount of data engineering, but it&amp;#39;s not in my title or the entirety of my role. I can&amp;#39;t seem to secure a DE role and I&amp;#39;m finding that a lot of these jobs are paying less than what I currently make. I see DE as a way forward in my career but the possibility of taking a pay cut isn&amp;#39;t great.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16g87ou", "is_robot_indexable": true, "report_reasons": null, "author": "what_duck", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16g87ou/would_you_take_a_pay_cut_to_get_into_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16g87ou/would_you_take_a_pay_cut_to_get_into_de/", "subreddit_subscribers": 127977, "created_utc": 1694468224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nHoping to spark some healthy debate over Git workflows.\n\nDo you think experienced engineers (e.g. Senior/Lead/Principal) should be allowed to merge from `branch` --&gt; `master`/`main`** without review?\n\n**or whatever you call your production branch\n\nI'll give a couple of examples below, for context:\n- Engineer opens a hotfix merge/pull request to fix an Airflow pipeline issue which is causing a production job to fail\n- Engineer opens a maintenance/BAU change to dbt reporting model\n- Engineer opens a new feature request which is \"urgent\"\n\nWe're currently having this discussion internally, and my feeling is that every MR/PR should be reviewed and approved by another engineer before it is merged into `master`. My reasoning behind this is:\n- It's possible that mistakes have been made and the engineer who has made those changes might overlook them\n  - Often times it takes a fresh pair of eyes to see issues or give a different perspective on changes which can help to triage issues before they occur\n- Any changes that can affect production pipelines should be treated with caution either because they can break said pipelines or the data outcomes are not what was expected\n  - I see production jobs/pipelines/code as something that needs to be managed carefully given that it can cause data to be late, business-critical reports to be incorrect and data consumers to become agitated or downright pissed-off\n\nI'm curious to hear what other engineers think on this topic and get some fresh perspectives that I hadn't considered before.", "author_fullname": "t2_pctu6cby", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Git - Merging to Master", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fsf8x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1694430642.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694430317.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Hoping to spark some healthy debate over Git workflows.&lt;/p&gt;\n\n&lt;p&gt;Do you think experienced engineers (e.g. Senior/Lead/Principal) should be allowed to merge from &lt;code&gt;branch&lt;/code&gt; --&amp;gt; &lt;code&gt;master&lt;/code&gt;/&lt;code&gt;main&lt;/code&gt;** without review?&lt;/p&gt;\n\n&lt;p&gt;**or whatever you call your production branch&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll give a couple of examples below, for context:\n- Engineer opens a hotfix merge/pull request to fix an Airflow pipeline issue which is causing a production job to fail\n- Engineer opens a maintenance/BAU change to dbt reporting model\n- Engineer opens a new feature request which is &amp;quot;urgent&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re currently having this discussion internally, and my feeling is that every MR/PR should be reviewed and approved by another engineer before it is merged into &lt;code&gt;master&lt;/code&gt;. My reasoning behind this is:\n- It&amp;#39;s possible that mistakes have been made and the engineer who has made those changes might overlook them\n  - Often times it takes a fresh pair of eyes to see issues or give a different perspective on changes which can help to triage issues before they occur\n- Any changes that can affect production pipelines should be treated with caution either because they can break said pipelines or the data outcomes are not what was expected\n  - I see production jobs/pipelines/code as something that needs to be managed carefully given that it can cause data to be late, business-critical reports to be incorrect and data consumers to become agitated or downright pissed-off&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious to hear what other engineers think on this topic and get some fresh perspectives that I hadn&amp;#39;t considered before.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fsf8x", "is_robot_indexable": true, "report_reasons": null, "author": "arthur_mills", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fsf8x/git_merging_to_master/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fsf8x/git_merging_to_master/", "subreddit_subscribers": 127977, "created_utc": 1694430317.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title says, what kind of questions would everyone ask about snowflake to a data engineer?", "author_fullname": "t2_5on7s1v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview questions for snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fxtt6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694445127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says, what kind of questions would everyone ask about snowflake to a data engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16fxtt6", "is_robot_indexable": true, "report_reasons": null, "author": "xander800", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fxtt6/interview_questions_for_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fxtt6/interview_questions_for_snowflake/", "subreddit_subscribers": 127977, "created_utc": 1694445127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there \ud83d\udc4b\n\nWhat questions you're usually asked in interviews about Airflow or what do you ask candidates?\n\nThank you for your help!  \n", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions during DE interviews about Apache Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fw7t8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694441186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there \ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;What questions you&amp;#39;re usually asked in interviews about Airflow or what do you ask candidates?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your help!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16fw7t8", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fw7t8/questions_during_de_interviews_about_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fw7t8/questions_during_de_interviews_about_apache/", "subreddit_subscribers": 127977, "created_utc": 1694441186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hey, I have a quick question, please.\n\nI'm into the type of jobs that have more engineering side than charts and analysis for business cases\n\n So just need to know if I can get a junior and first job as a machine learning engineer or data engineer if I learn what it takes for that.\n\n I see people who say you can't work in these roles unless you come from previous data-related roles such as data analyst.", "author_fullname": "t2_8sxs7l2s1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE and ML junior roles without DA road", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fp08c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694417639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I have a quick question, please.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m into the type of jobs that have more engineering side than charts and analysis for business cases&lt;/p&gt;\n\n&lt;p&gt;So just need to know if I can get a junior and first job as a machine learning engineer or data engineer if I learn what it takes for that.&lt;/p&gt;\n\n&lt;p&gt;I see people who say you can&amp;#39;t work in these roles unless you come from previous data-related roles such as data analyst.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16fp08c", "is_robot_indexable": true, "report_reasons": null, "author": "Guru_202399", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fp08c/de_and_ml_junior_roles_without_da_road/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fp08c/de_and_ml_junior_roles_without_da_road/", "subreddit_subscribers": 127977, "created_utc": 1694417639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Which services on AWS that you use the most in your daily work?\n\nMy company doesn't use any cloud solutions at all, I don't have much chance to work on it. When I look around, a lot of companies require exp in AWS/GCP/Azure. I'm studying AWS solutions and found that there are so many solutions on AWS that can be served in data engineering major. Which services that I should pay more attention on than others?", "author_fullname": "t2_9j8dvbm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS solutions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fyat4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694446256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which services on AWS that you use the most in your daily work?&lt;/p&gt;\n\n&lt;p&gt;My company doesn&amp;#39;t use any cloud solutions at all, I don&amp;#39;t have much chance to work on it. When I look around, a lot of companies require exp in AWS/GCP/Azure. I&amp;#39;m studying AWS solutions and found that there are so many solutions on AWS that can be served in data engineering major. Which services that I should pay more attention on than others?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16fyat4", "is_robot_indexable": true, "report_reasons": null, "author": "nalsman", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fyat4/aws_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fyat4/aws_solutions/", "subreddit_subscribers": 127977, "created_utc": 1694446256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In a behavioral health data warehouse, we're looking to house demographic data which is specific to an encounter (aka episode of care).\n\nSummary of existing tables:\n\n* FactEncounter table which is technically at a client-program grain (one row for each program attended during an encounter). Note that some demographics link to dimensions (i.e. EducationFacility) while others are calculated and would be stored here (i.e. veteran status Y/N) if that approach is taken.\n* Encounter (dimension) table (which is largely just used for bridging between facts based on the EncounterID)\n* Various other facts, of course, which do not currently have foreign key IDs to link to demographic data\n\nI'd like to add demographics properties (age at admission, veteran status, etc.) which are specific to to each encounter. We'll often be referencing these demographics from other fact tables within their various models (i.e. revenue summaries grouped or filtered by age groups, veteran status, educational status).\n\nMy options, as I see them, along with perceived cons of each:\n\n1. Add all encounter-specific demographics to FactEncounter and use FactEncounter-based summaries to reference those demographics when reporting on other dimensions  \n*\\*I have to use summaries (i.e. Min/Max) to force data to retrieve into a visual that's using another fact and including these demographics from FactEncounter (otherwise we get many-many type behavior), and performance is therefore bad.*\n2. Add all encounter-specific demographics to FactEncounter and also add those demographics to all other dimensions whose reporting may need to reference them  \n*\\*Repeated use of similar code makes model maintenance cumbersome and could lead to mismatched summaries if code is updated in some but not all facts later.*\n3. Add all encounter-specific demographics to Encounter dimension  \n*\\*Relationship from FactEncounter (and other facts) to the dimension(s) referenced by the demographics changes from star to snowflake*\n\nWhat is the best practice to follow in this case? These properties feel like dimension attributes to me, but I've always seen strong guidance away from snowflaking, so I'm unsure. TIA for any advice you all can offer.", "author_fullname": "t2_cknqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Demographics - Fact vs. Dimension", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fxc7r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694443984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In a behavioral health data warehouse, we&amp;#39;re looking to house demographic data which is specific to an encounter (aka episode of care).&lt;/p&gt;\n\n&lt;p&gt;Summary of existing tables:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;FactEncounter table which is technically at a client-program grain (one row for each program attended during an encounter). Note that some demographics link to dimensions (i.e. EducationFacility) while others are calculated and would be stored here (i.e. veteran status Y/N) if that approach is taken.&lt;/li&gt;\n&lt;li&gt;Encounter (dimension) table (which is largely just used for bridging between facts based on the EncounterID)&lt;/li&gt;\n&lt;li&gt;Various other facts, of course, which do not currently have foreign key IDs to link to demographic data&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d like to add demographics properties (age at admission, veteran status, etc.) which are specific to to each encounter. We&amp;#39;ll often be referencing these demographics from other fact tables within their various models (i.e. revenue summaries grouped or filtered by age groups, veteran status, educational status).&lt;/p&gt;\n\n&lt;p&gt;My options, as I see them, along with perceived cons of each:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Add all encounter-specific demographics to FactEncounter and use FactEncounter-based summaries to reference those demographics when reporting on other dimensions&lt;br/&gt;\n&lt;em&gt;\\&lt;/em&gt;I have to use summaries (i.e. Min/Max) to force data to retrieve into a visual that&amp;#39;s using another fact and including these demographics from FactEncounter (otherwise we get many-many type behavior), and performance is therefore bad.*&lt;/li&gt;\n&lt;li&gt;Add all encounter-specific demographics to FactEncounter and also add those demographics to all other dimensions whose reporting may need to reference them&lt;br/&gt;\n&lt;em&gt;\\&lt;/em&gt;Repeated use of similar code makes model maintenance cumbersome and could lead to mismatched summaries if code is updated in some but not all facts later.*&lt;/li&gt;\n&lt;li&gt;Add all encounter-specific demographics to Encounter dimension&lt;br/&gt;\n&lt;em&gt;\\&lt;/em&gt;Relationship from FactEncounter (and other facts) to the dimension(s) referenced by the demographics changes from star to snowflake*&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What is the best practice to follow in this case? These properties feel like dimension attributes to me, but I&amp;#39;ve always seen strong guidance away from snowflaking, so I&amp;#39;m unsure. TIA for any advice you all can offer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fxc7r", "is_robot_indexable": true, "report_reasons": null, "author": "Sub1ime14", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fxc7r/demographics_fact_vs_dimension/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fxc7r/demographics_fact_vs_dimension/", "subreddit_subscribers": 127977, "created_utc": 1694443984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Build Data Products? - Learn about Metric-Targeting, Semantic Engineering, Model Validation, and More.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fql3e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1694423786.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/how-to-build-data-products-design", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16fql3e", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fql3e/how_to_build_data_products_learn_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/how-to-build-data-products-design", "subreddit_subscribers": 127977, "created_utc": 1694423786.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "* donate*\nAs I\u2019m getting older I actually want to start to TRY and reach out to help other people as much as I can.  I was trying to think about anything that makes me \u2018unique\u2019 or something high value i could give\u2026 but I really have nothing other that my mad data skillz!  Is there a way to find people trying to do good who need data work done?\n\nI know it\u2019s probably a weird question\u2026 but I think we\u2019re all good at weird questions at this point! Any suggestions appreciated.", "author_fullname": "t2_sxt44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to \u2018dont ate\u2019 our type of work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16g3wub", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694458844.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;donate*\nAs I\u2019m getting older I actually want to start to TRY and reach out to help other people as much as I can.  I was trying to think about anything that makes me \u2018unique\u2019 or something high value i could give\u2026 but I really have nothing other that my mad data skillz!  Is there a way to find people trying to do good who need data work done?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I know it\u2019s probably a weird question\u2026 but I think we\u2019re all good at weird questions at this point! Any suggestions appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16g3wub", "is_robot_indexable": true, "report_reasons": null, "author": "Headybouffant", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16g3wub/is_there_a_way_to_dont_ate_our_type_of_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16g3wub/is_there_a_way_to_dont_ate_our_type_of_work/", "subreddit_subscribers": 127977, "created_utc": 1694458844.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nSo, I'm a data analyst, and for some reason, the task of re-implementing our data warehouse structure was assigned to me. For context, in the current structure, we have a 15k+ line PHP script that handles the ETL process into an on-premise Data Warehouse (PostgreSQL), and the business teams are unhappy with the current state of the data (it's not reliable; we encounter accuracy problems with the data almost every day).\n\nI love data engineering (DE) and want to transition from being a data analyst (DA) to a data engineer (DE). However, this situation is unusual because we don't have anyone with experience in data engineering to take on this role. The volume of data we're dealing with is 249GB, which spans 5 years of data. While this volume might seem relatively low, we're experiencing performance issues with our queries, most likely due to data modeling problems.\n\nOne of my constraints is that this project needs to be on-premise because we have a dedicated server for our current Data Warehouse. I've been considering the following tech stack: Minio + Dremio. However, I'm wondering if this stack might be overkill for our data volume. Additionally, since all our data comes from transactional systems, a Data Warehouse might be more suitable than a data lake like Minio. My concern with Minio is that I have limited experience with Kubernetes, which is needed to implement it.\n\nDoes anyone have any suggestions for a good and reliable tech stack to kickstart my project?", "author_fullname": "t2_8jc0mwfh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to implement a DW or Data Lake from a data Analyst perspective", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16g2hd6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694455667.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I&amp;#39;m a data analyst, and for some reason, the task of re-implementing our data warehouse structure was assigned to me. For context, in the current structure, we have a 15k+ line PHP script that handles the ETL process into an on-premise Data Warehouse (PostgreSQL), and the business teams are unhappy with the current state of the data (it&amp;#39;s not reliable; we encounter accuracy problems with the data almost every day).&lt;/p&gt;\n\n&lt;p&gt;I love data engineering (DE) and want to transition from being a data analyst (DA) to a data engineer (DE). However, this situation is unusual because we don&amp;#39;t have anyone with experience in data engineering to take on this role. The volume of data we&amp;#39;re dealing with is 249GB, which spans 5 years of data. While this volume might seem relatively low, we&amp;#39;re experiencing performance issues with our queries, most likely due to data modeling problems.&lt;/p&gt;\n\n&lt;p&gt;One of my constraints is that this project needs to be on-premise because we have a dedicated server for our current Data Warehouse. I&amp;#39;ve been considering the following tech stack: Minio + Dremio. However, I&amp;#39;m wondering if this stack might be overkill for our data volume. Additionally, since all our data comes from transactional systems, a Data Warehouse might be more suitable than a data lake like Minio. My concern with Minio is that I have limited experience with Kubernetes, which is needed to implement it.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any suggestions for a good and reliable tech stack to kickstart my project?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16g2hd6", "is_robot_indexable": true, "report_reasons": null, "author": "Andremallmann", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16g2hd6/how_to_implement_a_dw_or_data_lake_from_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16g2hd6/how_to_implement_a_dw_or_data_lake_from_a_data/", "subreddit_subscribers": 127977, "created_utc": 1694455667.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For those who have dealt with the pain of SAP ERP (r/3 or s/4), how have you gone about data extraction? The scope for me being SAP ERP -&gt; delta extraction -&gt; Azure Data Lake Landing Zone.\n\nWe currently use SAP Business Warehouse (SAP BW) for data warehousing, but are looking to migrate as we deal with a lot of data from external sources (cameras, API's, IoT, and various local Manufacturing Systems). I am not a part of the BW team, but they are a group within my direct team (same manager). I deal with all of the other sources, working in Azure.\n\nI like CDC methods, so something like Debezium or even Azure Data Factory has an SAP CDC connector. This way, we could capture each mutation on the individual tables. However, my manager mentioned the use SAP standard extractors, since that is what we use for our current SAP Business Warehouse.\n\nTo me, the standard extractors only seem relevant if we wanted to handle business rules and logic at the point of extraction (more ETL). However for real CDC (more ELT), the standard extractors don't make since as the rules and logic would be handled downstream. Therefore, I'd rather just have a 1:1 mapping between source and target tables.\n\nJust curious about anyone's experience dealing with SAP extraction and what approach works or didn't work for you.", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SAP ERP Data Extraction Methods?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fzp8w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694449420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those who have dealt with the pain of SAP ERP (r/3 or s/4), how have you gone about data extraction? The scope for me being SAP ERP -&amp;gt; delta extraction -&amp;gt; Azure Data Lake Landing Zone.&lt;/p&gt;\n\n&lt;p&gt;We currently use SAP Business Warehouse (SAP BW) for data warehousing, but are looking to migrate as we deal with a lot of data from external sources (cameras, API&amp;#39;s, IoT, and various local Manufacturing Systems). I am not a part of the BW team, but they are a group within my direct team (same manager). I deal with all of the other sources, working in Azure.&lt;/p&gt;\n\n&lt;p&gt;I like CDC methods, so something like Debezium or even Azure Data Factory has an SAP CDC connector. This way, we could capture each mutation on the individual tables. However, my manager mentioned the use SAP standard extractors, since that is what we use for our current SAP Business Warehouse.&lt;/p&gt;\n\n&lt;p&gt;To me, the standard extractors only seem relevant if we wanted to handle business rules and logic at the point of extraction (more ETL). However for real CDC (more ELT), the standard extractors don&amp;#39;t make since as the rules and logic would be handled downstream. Therefore, I&amp;#39;d rather just have a 1:1 mapping between source and target tables.&lt;/p&gt;\n\n&lt;p&gt;Just curious about anyone&amp;#39;s experience dealing with SAP extraction and what approach works or didn&amp;#39;t work for you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fzp8w", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fzp8w/sap_erp_data_extraction_methods/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fzp8w/sap_erp_data_extraction_methods/", "subreddit_subscribers": 127977, "created_utc": 1694449420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Was reading up on data management in Fundamentals of Data Engineering. \n\nFoDE is praised for exactly that, the fundamentals. So I was worried some references might me outdated. \n\nIn the book DAMA-DMBOK *(Data Management Assosciation International - Data Management Body of Knowledge)* is mentioned, Is the book outdated or are there other resources for data management? Or is it as relevant as ever?", "author_fullname": "t2_n9pcs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is DAMA-DMBOK still relevant? (Data Management)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fxmzc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694444683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was reading up on data management in Fundamentals of Data Engineering. &lt;/p&gt;\n\n&lt;p&gt;FoDE is praised for exactly that, the fundamentals. So I was worried some references might me outdated. &lt;/p&gt;\n\n&lt;p&gt;In the book DAMA-DMBOK &lt;em&gt;(Data Management Assosciation International - Data Management Body of Knowledge)&lt;/em&gt; is mentioned, Is the book outdated or are there other resources for data management? Or is it as relevant as ever?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fxmzc", "is_robot_indexable": true, "report_reasons": null, "author": "Zer0designs", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fxmzc/is_damadmbok_still_relevant_data_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fxmzc/is_damadmbok_still_relevant_data_management/", "subreddit_subscribers": 127977, "created_utc": 1694444683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a student trying to create a program that will be able to perform real-time data analysis and visualisation using live IoT data for educational purposes/for my portfolio. For this presentation, I'm looking for a live data source that can provide small amounts of this data for a short period. Is there anywhere I can find an API or site that offers this data whilst still being free/inexpensive? I would not require anything over 5 GB of data in total. I'd prefer this to be some form of automotive, geographic data, or anything related to logistics/transport but really, any live data from IoT devices would be good.", "author_fullname": "t2_4gzqmpdv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Live, Publicly Accessible Cheap/Inexpensive IoT Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16g3qxx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694458486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a student trying to create a program that will be able to perform real-time data analysis and visualisation using live IoT data for educational purposes/for my portfolio. For this presentation, I&amp;#39;m looking for a live data source that can provide small amounts of this data for a short period. Is there anywhere I can find an API or site that offers this data whilst still being free/inexpensive? I would not require anything over 5 GB of data in total. I&amp;#39;d prefer this to be some form of automotive, geographic data, or anything related to logistics/transport but really, any live data from IoT devices would be good.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16g3qxx", "is_robot_indexable": true, "report_reasons": null, "author": "Drubby19", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16g3qxx/live_publicly_accessible_cheapinexpensive_iot_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16g3qxx/live_publicly_accessible_cheapinexpensive_iot_data/", "subreddit_subscribers": 127977, "created_utc": 1694458486.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We run big data mart queries in dbt that usually take ~1hr for the 20-30 upstream builds then another 30mins-1hr for the model itself. Repeat this for another 2 models and they all run sequentially so end to end, it takes really long.\n\nI want to shorten the whole thing as much as I can, but its also hard now to review the model queries because there are select statements within the Join statements, and lots of business logic/case-when/NVLs scattered throughout as patch fixes for messy data in the company. Im sure thats one area we have to clean up anyway.\n\nOverall how do you optimize your data mart queries? Whenever I search on Google , they really just say the same \"select specific columns, avoid distinct, use when instead of having for filters\". But curious for how you've done it in your work.", "author_fullname": "t2_7ki1otgv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to optimize data mart queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16g0kfb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694451370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We run big data mart queries in dbt that usually take ~1hr for the 20-30 upstream builds then another 30mins-1hr for the model itself. Repeat this for another 2 models and they all run sequentially so end to end, it takes really long.&lt;/p&gt;\n\n&lt;p&gt;I want to shorten the whole thing as much as I can, but its also hard now to review the model queries because there are select statements within the Join statements, and lots of business logic/case-when/NVLs scattered throughout as patch fixes for messy data in the company. Im sure thats one area we have to clean up anyway.&lt;/p&gt;\n\n&lt;p&gt;Overall how do you optimize your data mart queries? Whenever I search on Google , they really just say the same &amp;quot;select specific columns, avoid distinct, use when instead of having for filters&amp;quot;. But curious for how you&amp;#39;ve done it in your work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16g0kfb", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Soup4733", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16g0kfb/how_to_optimize_data_mart_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16g0kfb/how_to_optimize_data_mart_queries/", "subreddit_subscribers": 127977, "created_utc": 1694451370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, as the title says I am the only one at my company that is responsible, well I would not say responsible but the one who should bring some strong solutions for the data lineage of the company. The deal is that everyone at the company uses google sheets and they like that because they can manage whatever they want to manage there, most of our data sources come from third parties like GIS web apps, ARP softwares and some other project management apps. And putting this into spreadsheet and cross data in a pain. The process right now is a pull everything I need put it in s3 bucket it, read it in a lambda an return the url for the report (ya they want report, don\u2019t like visuals ). And this is not the way for many reasons I have been learning through this data journey. So I am planning implementing something that empowers teams with data and do their own reports. Because we already have google env. Was thinking keeping the lambdas and s3 and create a data warehouse in google big query which connect with google spreadsheets and looker (try to encourage to use it). But problem comes with stack tech I am the only responsible for this to come up with a solution and don\u2019t want to make a mess and generate a costly solution that no one wanted to use. What are your recommendations and thoughts? I have seen databricks to do pre-processing then DBT to help to create data structures in big query? Or just use all google environment to all this? Yeah basically I little lost here and, alone lol", "author_fullname": "t2_42yrzhea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Overcome spreadsheet company for a one man dataeng team", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fl51g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694404364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, as the title says I am the only one at my company that is responsible, well I would not say responsible but the one who should bring some strong solutions for the data lineage of the company. The deal is that everyone at the company uses google sheets and they like that because they can manage whatever they want to manage there, most of our data sources come from third parties like GIS web apps, ARP softwares and some other project management apps. And putting this into spreadsheet and cross data in a pain. The process right now is a pull everything I need put it in s3 bucket it, read it in a lambda an return the url for the report (ya they want report, don\u2019t like visuals ). And this is not the way for many reasons I have been learning through this data journey. So I am planning implementing something that empowers teams with data and do their own reports. Because we already have google env. Was thinking keeping the lambdas and s3 and create a data warehouse in google big query which connect with google spreadsheets and looker (try to encourage to use it). But problem comes with stack tech I am the only responsible for this to come up with a solution and don\u2019t want to make a mess and generate a costly solution that no one wanted to use. What are your recommendations and thoughts? I have seen databricks to do pre-processing then DBT to help to create data structures in big query? Or just use all google environment to all this? Yeah basically I little lost here and, alone lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16fl51g", "is_robot_indexable": true, "report_reasons": null, "author": "josejo9423", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16fl51g/overcome_spreadsheet_company_for_a_one_man/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fl51g/overcome_spreadsheet_company_for_a_one_man/", "subreddit_subscribers": 127977, "created_utc": 1694404364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone done this before? I was planning on using AWS DMS to be able to do this, however Timescale does not allow super users which I needed for the CDC changes. Any ideas?", "author_fullname": "t2_t3mak0w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming TimescaleDB CDC to S3?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16gd7rb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694480460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone done this before? I was planning on using AWS DMS to be able to do this, however Timescale does not allow super users which I needed for the CDC changes. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16gd7rb", "is_robot_indexable": true, "report_reasons": null, "author": "801Fluidity", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16gd7rb/streaming_timescaledb_cdc_to_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16gd7rb/streaming_timescaledb_cdc_to_s3/", "subreddit_subscribers": 127977, "created_utc": 1694480460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey :)  \nI hope you can help a newbie who really strive to learn and be better.\n\nI'm trying to make a project for learning (and for my portfolio..): Pneumonia Detection.  \nThe data consists of DICOM images, and I was wondering how to approach such format.\n\nFrom the research I did:\n\n1. DICOM consists more than a simple image, there's metadata which I think I don't need for the detection problem\n2. I can work with DICOM format directly using only the pixel\\_array in np array (or tensor)\n3. I   can convert the format to PNG, although the image quality will be   worse, and if I don't keep the metadata before the conversion, it'll be   lost.\n\nIs it generally   better for image quality to work directly with DICOM  files, or are the   benefits of PNG conversion worth the trade-offs\n\nI'm   planning to start with Jupyter and do some ETL, EDA and trying some   models, mostly ones that I'll train, I'm not sure if I want to deal with   pre-trained models and fine tune them just yet, although I assume  it'll  be a valuable knowledge to acquire.\n\nFinally, I guess I'll pick the best model and move to an IDE like Pycharm and make a more robust design for the project.\n\nI'm planning to use Pytorch if it's important to know.\n\nThanks in advance. any help will be much appreciated!!!", "author_fullname": "t2_5vngcus6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle DICOM X-ray images and some tips for a project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16g73hb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694465820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey :)&lt;br/&gt;\nI hope you can help a newbie who really strive to learn and be better.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to make a project for learning (and for my portfolio..): Pneumonia Detection.&lt;br/&gt;\nThe data consists of DICOM images, and I was wondering how to approach such format.&lt;/p&gt;\n\n&lt;p&gt;From the research I did:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;DICOM consists more than a simple image, there&amp;#39;s metadata which I think I don&amp;#39;t need for the detection problem&lt;/li&gt;\n&lt;li&gt;I can work with DICOM format directly using only the pixel_array in np array (or tensor)&lt;/li&gt;\n&lt;li&gt;I   can convert the format to PNG, although the image quality will be   worse, and if I don&amp;#39;t keep the metadata before the conversion, it&amp;#39;ll be   lost.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Is it generally   better for image quality to work directly with DICOM  files, or are the   benefits of PNG conversion worth the trade-offs&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m   planning to start with Jupyter and do some ETL, EDA and trying some   models, mostly ones that I&amp;#39;ll train, I&amp;#39;m not sure if I want to deal with   pre-trained models and fine tune them just yet, although I assume  it&amp;#39;ll  be a valuable knowledge to acquire.&lt;/p&gt;\n\n&lt;p&gt;Finally, I guess I&amp;#39;ll pick the best model and move to an IDE like Pycharm and make a more robust design for the project.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to use Pytorch if it&amp;#39;s important to know.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance. any help will be much appreciated!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16g73hb", "is_robot_indexable": true, "report_reasons": null, "author": "01jasper", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16g73hb/how_to_handle_dicom_xray_images_and_some_tips_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16g73hb/how_to_handle_dicom_xray_images_and_some_tips_for/", "subreddit_subscribers": 127977, "created_utc": 1694465820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone using ArcticDB? I am wondering about your use cases and why/when to (not) use it. Comparison with some other tools would be more than welcome :)", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ArcticDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16g50cs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694461268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone using ArcticDB? I am wondering about your use cases and why/when to (not) use it. Comparison with some other tools would be more than welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16g50cs", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16g50cs/arcticdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16g50cs/arcticdb/", "subreddit_subscribers": 127977, "created_utc": 1694461268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All,\n\nI was wondering if handling multiple fact tables, which includes the below:\n\n1. Modeling the data warehouse, integration layer and data mart tables.\n2. Handling these fact tables (more than one) inside a reporting tool, power bi in my case\n\nwould be good enough to answer the typical question \"what is the toughest challenge you faced technically in your career\" for a data engineer II/III interview or is it very basic.", "author_fullname": "t2_uqu7iar6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Question - Toughest Challenge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16g0i23", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694451222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\n\n&lt;p&gt;I was wondering if handling multiple fact tables, which includes the below:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Modeling the data warehouse, integration layer and data mart tables.&lt;/li&gt;\n&lt;li&gt;Handling these fact tables (more than one) inside a reporting tool, power bi in my case&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;would be good enough to answer the typical question &amp;quot;what is the toughest challenge you faced technically in your career&amp;quot; for a data engineer II/III interview or is it very basic.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16g0i23", "is_robot_indexable": true, "report_reasons": null, "author": "InvestigatorMuted622", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16g0i23/interview_question_toughest_challenge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16g0i23/interview_question_toughest_challenge/", "subreddit_subscribers": 127977, "created_utc": 1694451222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a team that transforms data to fit into our data warehouse. We use Databricks notebooks to perform the transformations and Airflow DAG for orchestration.\n\nTypically the notebooks have a set of parameters (source schema, source table, target schema, target table) that are fed to the notebook from Airflow. (When we create the tasks, we add the notebook path and parameters)\n\nA new team has proposed storing all the parameters in a table, instead of in the Airflow script. So Airflow would just have the notebook name and would look to a table for all the specific parameters related to the notebook. I'm wondering if this is a good idea or not.\n\nPersonally, I'm leaning towards not. The Databricks notebooks and Airflow scripts are all stored in GitHub, so all changes are tracked and easily monitored. Tracking changes to a table is not as easy. In addition to this, adding a new table that needs to be maintained adds a new layer of complexity to the data flow that doesn't really seem necessary.\n\nMaybe I'm missing something but it seems like it will complicate things more than simplify them. Please let me know what you all think.", "author_fullname": "t2_arc3347", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should parameters be stored in tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16fz3v6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694448080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a team that transforms data to fit into our data warehouse. We use Databricks notebooks to perform the transformations and Airflow DAG for orchestration.&lt;/p&gt;\n\n&lt;p&gt;Typically the notebooks have a set of parameters (source schema, source table, target schema, target table) that are fed to the notebook from Airflow. (When we create the tasks, we add the notebook path and parameters)&lt;/p&gt;\n\n&lt;p&gt;A new team has proposed storing all the parameters in a table, instead of in the Airflow script. So Airflow would just have the notebook name and would look to a table for all the specific parameters related to the notebook. I&amp;#39;m wondering if this is a good idea or not.&lt;/p&gt;\n\n&lt;p&gt;Personally, I&amp;#39;m leaning towards not. The Databricks notebooks and Airflow scripts are all stored in GitHub, so all changes are tracked and easily monitored. Tracking changes to a table is not as easy. In addition to this, adding a new table that needs to be maintained adds a new layer of complexity to the data flow that doesn&amp;#39;t really seem necessary.&lt;/p&gt;\n\n&lt;p&gt;Maybe I&amp;#39;m missing something but it seems like it will complicate things more than simplify them. Please let me know what you all think.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16fz3v6", "is_robot_indexable": true, "report_reasons": null, "author": "odzihodo", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16fz3v6/should_parameters_be_stored_in_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16fz3v6/should_parameters_be_stored_in_tables/", "subreddit_subscribers": 127977, "created_utc": 1694448080.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}