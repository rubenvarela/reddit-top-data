{"kind": "Listing", "data": {"after": "t3_16hq7rk", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "(Disclosure: founder of GlareDB)\n\nHi everyone,\n\nWe've recently released GlareDB 0.5.0, and our biggest feature for this release is Hybrid Execution.\n\nFirst, what is GlareDB? GlareDB is SQL database that can read data from a variety of sources, including Postgres, Snowflake, S3, and more. Our goal is enabling running analytics across data sources without having to move data around.\n\nAnd with Hybrid Execute, we're taking that a step further. Hybrid Execution lets you connect to a GlareDB Cloud deployment from the GlareDB CLI or Python library, which  allows queries to use the combined resources of both the local and remote machines. GlareDB splits up the query during planning, and executes parts of the query locally or remotely depending on the data being referenced.\n\nThis also enables being able to join local data (parquet, csv, and json files) onto tables that exist in your cloud deployment. For example, if we have a `user_metrics` table in our cloud deployment, and a `company_users.csv` file that's just sitting on our laptop, we're able to write a single query that joins data from both:\n\n    SELECT\n      m.user_id,\n      max(m.output_rows),\n      avg(m.output_rows)::int\n    FROM\n      user_metrics m\n    INNER JOIN './company_users.csv' u on m.user_id = u.id\n    GROUP BY m.user_id\n    LIMIT 5;\n\nWe don't have to manually upload csv files, we just write a single sql query and glaredb takes care of the rest. This also works for dataframes when using our Python library.\n\nWe can go even crazier. Since GlareDB has integrations for connecting to databases like Postgres, Snowflake, and more, we're able to join local data onto remote data sources all in a single query. Here's what that could look like in Python:\n\n    import glaredb\n    import pandas as pd\n    \n    con = glaredb.connect(\"glaredb://&lt;user&gt;:&lt;pass&gt;@glaredb-better.remote.glaredb.com:6443/dogfood\")\n    \n    users = pd.DataFrame({\"email\": [\"sean@glaredb.com\"]})\n    \n    con.sql(\"\"\"\n    select count(*) as query_count,\n           max(e.elapsed_compute_ns) as max_elapsed\n      from snowflake_segment.glaredb_prod.execution_metric e\n      inner join pg_prod.public.users u on e.user_id = u.id\n      inner join users u2 on u.email = u2.email\n      where e.timestamp::timestamp &gt; now() - interval '3 day'\n    \"\"\").show()\n\nAnd as before, we're not having to do anything special with the `users` dataframe, we just reference it in the query, and glaredb will work its magic.\n\nIf you want to learn more about Hybrid Execution, check out our blog post: [https://glaredb.com/blog/hybrid-execution](https://glaredb.com/blog/hybrid-execution)\n\nAnd take a peek at our open source repo: [https://github.com/GlareDB/glaredb](https://github.com/GlareDB/glaredb)\n\nLet us know what you think!", "author_fullname": "t2_butu3hfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hybrid Execution in GlareDB: Scale your workflow with GlareDB Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16gyw84", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1694543082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Disclosure: founder of GlareDB)&lt;/p&gt;\n\n&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve recently released GlareDB 0.5.0, and our biggest feature for this release is Hybrid Execution.&lt;/p&gt;\n\n&lt;p&gt;First, what is GlareDB? GlareDB is SQL database that can read data from a variety of sources, including Postgres, Snowflake, S3, and more. Our goal is enabling running analytics across data sources without having to move data around.&lt;/p&gt;\n\n&lt;p&gt;And with Hybrid Execute, we&amp;#39;re taking that a step further. Hybrid Execution lets you connect to a GlareDB Cloud deployment from the GlareDB CLI or Python library, which  allows queries to use the combined resources of both the local and remote machines. GlareDB splits up the query during planning, and executes parts of the query locally or remotely depending on the data being referenced.&lt;/p&gt;\n\n&lt;p&gt;This also enables being able to join local data (parquet, csv, and json files) onto tables that exist in your cloud deployment. For example, if we have a &lt;code&gt;user_metrics&lt;/code&gt; table in our cloud deployment, and a &lt;code&gt;company_users.csv&lt;/code&gt; file that&amp;#39;s just sitting on our laptop, we&amp;#39;re able to write a single query that joins data from both:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT\n  m.user_id,\n  max(m.output_rows),\n  avg(m.output_rows)::int\nFROM\n  user_metrics m\nINNER JOIN &amp;#39;./company_users.csv&amp;#39; u on m.user_id = u.id\nGROUP BY m.user_id\nLIMIT 5;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;We don&amp;#39;t have to manually upload csv files, we just write a single sql query and glaredb takes care of the rest. This also works for dataframes when using our Python library.&lt;/p&gt;\n\n&lt;p&gt;We can go even crazier. Since GlareDB has integrations for connecting to databases like Postgres, Snowflake, and more, we&amp;#39;re able to join local data onto remote data sources all in a single query. Here&amp;#39;s what that could look like in Python:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import glaredb\nimport pandas as pd\n\ncon = glaredb.connect(&amp;quot;glaredb://&amp;lt;user&amp;gt;:&amp;lt;pass&amp;gt;@glaredb-better.remote.glaredb.com:6443/dogfood&amp;quot;)\n\nusers = pd.DataFrame({&amp;quot;email&amp;quot;: [&amp;quot;sean@glaredb.com&amp;quot;]})\n\ncon.sql(&amp;quot;&amp;quot;&amp;quot;\nselect count(*) as query_count,\n       max(e.elapsed_compute_ns) as max_elapsed\n  from snowflake_segment.glaredb_prod.execution_metric e\n  inner join pg_prod.public.users u on e.user_id = u.id\n  inner join users u2 on u.email = u2.email\n  where e.timestamp::timestamp &amp;gt; now() - interval &amp;#39;3 day&amp;#39;\n&amp;quot;&amp;quot;&amp;quot;).show()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And as before, we&amp;#39;re not having to do anything special with the &lt;code&gt;users&lt;/code&gt; dataframe, we just reference it in the query, and glaredb will work its magic.&lt;/p&gt;\n\n&lt;p&gt;If you want to learn more about Hybrid Execution, check out our blog post: &lt;a href=\"https://glaredb.com/blog/hybrid-execution\"&gt;https://glaredb.com/blog/hybrid-execution&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And take a peek at our open source repo: &lt;a href=\"https://github.com/GlareDB/glaredb\"&gt;https://github.com/GlareDB/glaredb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let us know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q3PgN1-VoY2bx-1qIUOAVkJT5CD-KAHlUmNTVWbi5aI.jpg?auto=webp&amp;s=4dee1b5daaf4f28142bb37367338cbcc49861006", "width": 3840, "height": 2160}, "resolutions": [{"url": "https://external-preview.redd.it/q3PgN1-VoY2bx-1qIUOAVkJT5CD-KAHlUmNTVWbi5aI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c9eed218c2708d33a7c12e9cf1aa2afdcdc3bedf", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/q3PgN1-VoY2bx-1qIUOAVkJT5CD-KAHlUmNTVWbi5aI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=37ee0ccd27b516133613a5739ef600c91d4cb166", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/q3PgN1-VoY2bx-1qIUOAVkJT5CD-KAHlUmNTVWbi5aI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a83a204550161c8771f17321a21101814cc3ba86", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/q3PgN1-VoY2bx-1qIUOAVkJT5CD-KAHlUmNTVWbi5aI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=469a3f369526e393f5425a0a928ee3eb6384e46e", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/q3PgN1-VoY2bx-1qIUOAVkJT5CD-KAHlUmNTVWbi5aI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b13e482322f01561ac1ef9212d52947d466d6b5a", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/q3PgN1-VoY2bx-1qIUOAVkJT5CD-KAHlUmNTVWbi5aI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fccb73db62b1310d319ec542101a7e956b593ea6", "width": 1080, "height": 607}], "variants": {}, "id": "fGYMbTG5G_tyhn5Hv-oYbSHPqwimRoEYC-UzkmsKqYM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16gyw84", "is_robot_indexable": true, "report_reasons": null, "author": "sean-glaredb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16gyw84/hybrid_execution_in_glaredb_scale_your_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16gyw84/hybrid_execution_in_glaredb_scale_your_workflow/", "subreddit_subscribers": 128245, "created_utc": 1694543082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you had to start over, what would be your plan to go from zero to hero in 6 months", "author_fullname": "t2_etp40tkx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you had to start over..", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16gxz8v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694540935.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you had to start over, what would be your plan to go from zero to hero in 6 months&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16gxz8v", "is_robot_indexable": true, "report_reasons": null, "author": "Physical_Flatworm689", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16gxz8v/if_you_had_to_start_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16gxz8v/if_you_had_to_start_over/", "subreddit_subscribers": 128245, "created_utc": 1694540935.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I used to work with BigQuery as my org's data warehouse solution and I miss it...\n\nBelow are some of my qualms with Redshift:\n\n* It isnt magically scaleable like BigQuery\n* Doesn't have sharding\n* Doesn't have DB replication\n* Cannot query data catalogs in glue if with IAM authentication (even with the expensive nodes + extremely poor documentation around this)\n* Cannot run cross-DB queries on external schemas (extremely poor documentation around this) and to get around it you need to make an external schema in every DB if you want the functionality\n* Pay per node vs data scanned (like in BigQuery, so you are pissing money away during downtime)\n* Spectrum is slower than the resources it is querying from (slower than it would be to just use athena)\n\nAre there any other downsides besides those? Or even better am I missing any plus sides?", "author_fullname": "t2_4ffbvgzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Does Anybody Make Good Use of Redshift?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16gzu8p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694545290.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to work with BigQuery as my org&amp;#39;s data warehouse solution and I miss it...&lt;/p&gt;\n\n&lt;p&gt;Below are some of my qualms with Redshift:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It isnt magically scaleable like BigQuery&lt;/li&gt;\n&lt;li&gt;Doesn&amp;#39;t have sharding&lt;/li&gt;\n&lt;li&gt;Doesn&amp;#39;t have DB replication&lt;/li&gt;\n&lt;li&gt;Cannot query data catalogs in glue if with IAM authentication (even with the expensive nodes + extremely poor documentation around this)&lt;/li&gt;\n&lt;li&gt;Cannot run cross-DB queries on external schemas (extremely poor documentation around this) and to get around it you need to make an external schema in every DB if you want the functionality&lt;/li&gt;\n&lt;li&gt;Pay per node vs data scanned (like in BigQuery, so you are pissing money away during downtime)&lt;/li&gt;\n&lt;li&gt;Spectrum is slower than the resources it is querying from (slower than it would be to just use athena)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Are there any other downsides besides those? Or even better am I missing any plus sides?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16gzu8p", "is_robot_indexable": true, "report_reasons": null, "author": "ReporterNervous6822", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16gzu8p/how_does_anybody_make_good_use_of_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16gzu8p/how_does_anybody_make_good_use_of_redshift/", "subreddit_subscribers": 128245, "created_utc": 1694545290.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I guess I never fully understood the love for being a contractor versus fulltime when it comes to data engineering careers. I, like many of you, are getting hit with a lot of LinkedIn recruiters right now and some of these roles they're trying to fill are \"long term contract\". I guess I see contract work as great when you want something that might pay more but is probably shorter term so you can move onto the next job and get a variety of experiences over the long term. I have a family I support so I've always prioritized fulltime roles so that I don't end up stranded at the end of a contract that may/may not get renewed. A recruiter reached out to me recently with a really great paying \"long term contract\" role but since it is contract I'm very hesitant to seriously consider it. For those of you who prefer contract roles, why exactly?", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ok, who here actually prefers working in a contract position versus being fulltime?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16h0kpv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694546958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I guess I never fully understood the love for being a contractor versus fulltime when it comes to data engineering careers. I, like many of you, are getting hit with a lot of LinkedIn recruiters right now and some of these roles they&amp;#39;re trying to fill are &amp;quot;long term contract&amp;quot;. I guess I see contract work as great when you want something that might pay more but is probably shorter term so you can move onto the next job and get a variety of experiences over the long term. I have a family I support so I&amp;#39;ve always prioritized fulltime roles so that I don&amp;#39;t end up stranded at the end of a contract that may/may not get renewed. A recruiter reached out to me recently with a really great paying &amp;quot;long term contract&amp;quot; role but since it is contract I&amp;#39;m very hesitant to seriously consider it. For those of you who prefer contract roles, why exactly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16h0kpv", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16h0kpv/ok_who_here_actually_prefers_working_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16h0kpv/ok_who_here_actually_prefers_working_in_a/", "subreddit_subscribers": 128245, "created_utc": 1694546958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "DA with 5+ years experience. Mainly SQL, Tableau, and Python for some data exploration, analysis, regression models, and some web scraping.\n\nI want to break into DE, so I have been doing very basic projects. I feel like I am missing a huge piece of what to do, and what a DE really is. I need some help, please. \n\nSo far I have done three projects that have involved Pandas:\n\n1.) Taken a dataset from an API (one requiring a key, two open source)\n\n2.) Formatting that from JSON into a data frame / cleaned the data up.\n\n3.) Importing it into a SQL Database\n\n4.) Building some visualizations out of it in Tableau and PBI.\n\nI'm a bit lost. Where does the modeling, data scheduling, and all of that come in. I hear things about BigQuery, Azure, Airflow, etc. I can't imagine telling a future employer that this is what I have done, and have them believe I am capable of being a Data Engineer.\n\nWhat would you consider the next step to be?", "author_fullname": "t2_6iptp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I feel like I am on the edge of doing cool DE things, but I need some help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16h0nvc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1694568850.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694547160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;DA with 5+ years experience. Mainly SQL, Tableau, and Python for some data exploration, analysis, regression models, and some web scraping.&lt;/p&gt;\n\n&lt;p&gt;I want to break into DE, so I have been doing very basic projects. I feel like I am missing a huge piece of what to do, and what a DE really is. I need some help, please. &lt;/p&gt;\n\n&lt;p&gt;So far I have done three projects that have involved Pandas:&lt;/p&gt;\n\n&lt;p&gt;1.) Taken a dataset from an API (one requiring a key, two open source)&lt;/p&gt;\n\n&lt;p&gt;2.) Formatting that from JSON into a data frame / cleaned the data up.&lt;/p&gt;\n\n&lt;p&gt;3.) Importing it into a SQL Database&lt;/p&gt;\n\n&lt;p&gt;4.) Building some visualizations out of it in Tableau and PBI.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a bit lost. Where does the modeling, data scheduling, and all of that come in. I hear things about BigQuery, Azure, Airflow, etc. I can&amp;#39;t imagine telling a future employer that this is what I have done, and have them believe I am capable of being a Data Engineer.&lt;/p&gt;\n\n&lt;p&gt;What would you consider the next step to be?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16h0nvc", "is_robot_indexable": true, "report_reasons": null, "author": "tits_mcgee_92", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16h0nvc/i_feel_like_i_am_on_the_edge_of_doing_cool_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16h0nvc/i_feel_like_i_am_on_the_edge_of_doing_cool_de/", "subreddit_subscribers": 128245, "created_utc": 1694547160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6lg1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte made huge progress on Postgres replication performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_16hqwpl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/kglnFOYWzScHGih_8jeM5cATAPWNE1KPTslylXHtl70.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694620926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/postgres-replication-performance-benchmark-airbyte-vs-fivetran", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?auto=webp&amp;s=056537f6f2992c8852f6cb95bbc4c00c1e229c51", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e03d950d72419b87f9d5aae966a4c768510ad3d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=51611433beec1d0c28d0a3b19f660558679b5c93", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ae364d8a384ecdcaa7b33b103f56ac454efde0e", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea1404c8fa73aa64041ee85644b9820989de6df0", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f13c773843bfa750b7b0898f0d5469e9d1693f65", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e18e53e06677f7d693f436caaa78f11e99b2664b", "width": 1080, "height": 565}], "variants": {}, "id": "WV3P-nqqnUiqFMV44_ujfp4HcBKISrl5GlfKY6P3V70"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16hqwpl", "is_robot_indexable": true, "report_reasons": null, "author": "evantahler", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hqwpl/airbyte_made_huge_progress_on_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/postgres-replication-performance-benchmark-airbyte-vs-fivetran", "subreddit_subscribers": 128245, "created_utc": 1694620926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We've been working on a simple API for running Python functions in the cloud. In this guide, we use three methods to process the NYC taxi dataset and compare runtimes:  \n\\- locally, 5 hr  \n\\- on a big VM in the cloud, 30 min  \n\\- parallel processing on a big VM in the cloud, 5 min\n\nThis post walks through how you can adapt your Python functions with minimal code changes using Coiled functions [https://medium.com/coiled-hq/parallel-serverless-functions-at-scale-cd6ee4a7def5](https://medium.com/coiled-hq/parallel-serverless-functions-at-scale-cd6ee4a7def5).\n\nIt'd be great to hear your thoughts!\n\nIn the interest of transparency, I work for Coiled, and a colleague of mine wrote this post.", "author_fullname": "t2_w7crvjmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with many parquet files on S3 in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16gzn4d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1694544841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve been working on a simple API for running Python functions in the cloud. In this guide, we use three methods to process the NYC taxi dataset and compare runtimes:&lt;br/&gt;\n- locally, 5 hr&lt;br/&gt;\n- on a big VM in the cloud, 30 min&lt;br/&gt;\n- parallel processing on a big VM in the cloud, 5 min&lt;/p&gt;\n\n&lt;p&gt;This post walks through how you can adapt your Python functions with minimal code changes using Coiled functions &lt;a href=\"https://medium.com/coiled-hq/parallel-serverless-functions-at-scale-cd6ee4a7def5\"&gt;https://medium.com/coiled-hq/parallel-serverless-functions-at-scale-cd6ee4a7def5&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;d be great to hear your thoughts!&lt;/p&gt;\n\n&lt;p&gt;In the interest of transparency, I work for Coiled, and a colleague of mine wrote this post.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SkQn1xF2PKPFMF5RrjYn0BaQ_AK_EKCfbwjtLmunb4Q.jpg?auto=webp&amp;s=048892709cccbd1b2c68896a31e04218a992e888", "width": 1200, "height": 746}, "resolutions": [{"url": "https://external-preview.redd.it/SkQn1xF2PKPFMF5RrjYn0BaQ_AK_EKCfbwjtLmunb4Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=968dca02548dce0e1f7fc3652d5017b006779069", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/SkQn1xF2PKPFMF5RrjYn0BaQ_AK_EKCfbwjtLmunb4Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b72d5e9b0e68c4a6c19a8570d858026c7f6e7470", "width": 216, "height": 134}, {"url": "https://external-preview.redd.it/SkQn1xF2PKPFMF5RrjYn0BaQ_AK_EKCfbwjtLmunb4Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=544bc6977f20756c3bce657c46e6a8829f59fb20", "width": 320, "height": 198}, {"url": "https://external-preview.redd.it/SkQn1xF2PKPFMF5RrjYn0BaQ_AK_EKCfbwjtLmunb4Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=17000a825a6a4920790368c7472dac300f1727e2", "width": 640, "height": 397}, {"url": "https://external-preview.redd.it/SkQn1xF2PKPFMF5RrjYn0BaQ_AK_EKCfbwjtLmunb4Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b319f4edf6d70992e730d4fad3b19416f2db26fd", "width": 960, "height": 596}, {"url": "https://external-preview.redd.it/SkQn1xF2PKPFMF5RrjYn0BaQ_AK_EKCfbwjtLmunb4Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=05791914c7082496c2c4442f73acf0e1643154d1", "width": 1080, "height": 671}], "variants": {}, "id": "A5iyBO5ZP8sgdDc1RsvBp2CEzZYJPo5gZlneKlSl-6Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16gzn4d", "is_robot_indexable": true, "report_reasons": null, "author": "dask-jeeves", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16gzn4d/working_with_many_parquet_files_on_s3_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16gzn4d/working_with_many_parquet_files_on_s3_in_python/", "subreddit_subscribers": 128245, "created_utc": 1694544841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know a lot of people have said that DDIA book is the best way to learn system design, but I am personally not a good reader and I\u2019m a visual learner. I finished fundamental of data engineering but honestly have not learned much knowledge from the book lol. What are some resources that will help to learn system design? \n\nAlso when everyone talks about system design, is there any system design specifically for software engineers or data engineers?", "author_fullname": "t2_1xrjwd6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to learn more about system design?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16gynnq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694542541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know a lot of people have said that DDIA book is the best way to learn system design, but I am personally not a good reader and I\u2019m a visual learner. I finished fundamental of data engineering but honestly have not learned much knowledge from the book lol. What are some resources that will help to learn system design? &lt;/p&gt;\n\n&lt;p&gt;Also when everyone talks about system design, is there any system design specifically for software engineers or data engineers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16gynnq", "is_robot_indexable": true, "report_reasons": null, "author": "Fasthandman", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16gynnq/how_to_learn_more_about_system_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16gynnq/how_to_learn_more_about_system_design/", "subreddit_subscribers": 128245, "created_utc": 1694542541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TL;DR: need to ETL from db but primary keys get shuffled around after each migration. Wat do?\n\n\\---\n\nHey folks,\n\nI have a tricky situation and I'm not sure what's the best way to address it.\n\nBasically we have a backend in postgres managed by Django ORM. For some goddamn reason, after each migration, the PKs of (most) records get shuffled around randomly, even if there's no change in data at all.\n\nFor example, if before migration I have this:\n\n|ID (PK)|Name|Surname|\n|:-|:-|:-|\n|1|John|Wayne|\n|234|Julia|Roberts|\n\n&amp;#x200B;\n\nafter migration I can end up with:\n\n|ID (PK)|Name|Surname|\n|:-|:-|:-|\n|234|John|Wayne|\n|890|Julia|Roberts|\n\n&amp;#x200B;\n\nOn top that, for a bunch of tables, natural keys are not 100% reliable due to how they get inserted (it's a human CRUD process).\n\nGiven that I have absolutely zero control on the backend, what strategies would you recommend I apply for E(T)L and data warehousing activities?\n\nThanks in advance for your input!", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL from postgres, but Django shuffles Primary Keys after each migration.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16gxw34", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694540728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: need to ETL from db but primary keys get shuffled around after each migration. Wat do?&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I have a tricky situation and I&amp;#39;m not sure what&amp;#39;s the best way to address it.&lt;/p&gt;\n\n&lt;p&gt;Basically we have a backend in postgres managed by Django ORM. For some goddamn reason, after each migration, the PKs of (most) records get shuffled around randomly, even if there&amp;#39;s no change in data at all.&lt;/p&gt;\n\n&lt;p&gt;For example, if before migration I have this:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;ID (PK)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Surname&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;John&lt;/td&gt;\n&lt;td align=\"left\"&gt;Wayne&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;234&lt;/td&gt;\n&lt;td align=\"left\"&gt;Julia&lt;/td&gt;\n&lt;td align=\"left\"&gt;Roberts&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;after migration I can end up with:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;ID (PK)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Surname&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;234&lt;/td&gt;\n&lt;td align=\"left\"&gt;John&lt;/td&gt;\n&lt;td align=\"left\"&gt;Wayne&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;890&lt;/td&gt;\n&lt;td align=\"left\"&gt;Julia&lt;/td&gt;\n&lt;td align=\"left\"&gt;Roberts&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;On top that, for a bunch of tables, natural keys are not 100% reliable due to how they get inserted (it&amp;#39;s a human CRUD process).&lt;/p&gt;\n\n&lt;p&gt;Given that I have absolutely zero control on the backend, what strategies would you recommend I apply for E(T)L and data warehousing activities?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your input!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16gxw34", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16gxw34/etl_from_postgres_but_django_shuffles_primary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16gxw34/etl_from_postgres_but_django_shuffles_primary/", "subreddit_subscribers": 128245, "created_utc": 1694540728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What techniques do you use to create large numbers of simple ETL jobs? For example, we have thousands of tables that we want to copy from point A to B with minimum modifications (adding standardized metadata). Each job is independent so there\u2019s no dependencies to worry about. Previous developers created a job via a GUI for each one which meant months of tedious point and click hell. \n\nHow do you handle bulk job creation and management in the tool of your choice?", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL job overload", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16h8bi8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694565277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What techniques do you use to create large numbers of simple ETL jobs? For example, we have thousands of tables that we want to copy from point A to B with minimum modifications (adding standardized metadata). Each job is independent so there\u2019s no dependencies to worry about. Previous developers created a job via a GUI for each one which meant months of tedious point and click hell. &lt;/p&gt;\n\n&lt;p&gt;How do you handle bulk job creation and management in the tool of your choice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16h8bi8", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16h8bi8/etl_job_overload/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16h8bi8/etl_job_overload/", "subreddit_subscribers": 128245, "created_utc": 1694565277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \n\nI have come across an opportunity that has significant bigger salary but it is not traditional DE job. It\u2019s with a service based org which requires pre-sales work for data team. Be the first point of contact for data related projects and products. \n\nPlease note this is India. \n\nIs a right career move? \nPros: salary and learning. I might learn a lot with a spectrum of technologies. \nCompletely remote. \n\nCons: cannot work deep into integration and delivery. Except compensation, benefits are. Jon existent.  No people attachment. \n\nLooking forward to your questions and opinions.", "author_fullname": "t2_42i9lwic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is pre-sales data engineering good?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16gya6e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694541665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I have come across an opportunity that has significant bigger salary but it is not traditional DE job. It\u2019s with a service based org which requires pre-sales work for data team. Be the first point of contact for data related projects and products. &lt;/p&gt;\n\n&lt;p&gt;Please note this is India. &lt;/p&gt;\n\n&lt;p&gt;Is a right career move? \nPros: salary and learning. I might learn a lot with a spectrum of technologies. \nCompletely remote. &lt;/p&gt;\n\n&lt;p&gt;Cons: cannot work deep into integration and delivery. Except compensation, benefits are. Jon existent.  No people attachment. &lt;/p&gt;\n\n&lt;p&gt;Looking forward to your questions and opinions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16gya6e", "is_robot_indexable": true, "report_reasons": null, "author": "snapperPanda", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16gya6e/is_presales_data_engineering_good/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16gya6e/is_presales_data_engineering_good/", "subreddit_subscribers": 128245, "created_utc": 1694541665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm wondering if a good portfolio (I don't know yet how good) and certificates can cover the 4 years gap I have. Knowing that I didn't choose to chill for 4 years (and it was far away from that anyway). Employers don't seem to care about why I was hit hard by life to the point of discontinuing my career. Anyway, I'm not asking for their pitty but I'm determined to make my profile succeed in getting me good jobs despite of the gap. I've been autolearning for a year now and I built some projects from scratch. How can I show my motivation by exposing these projects ?", "author_fullname": "t2_um2qwii8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to do with a 4 years gap in an IT resume", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hpyf2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694618742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering if a good portfolio (I don&amp;#39;t know yet how good) and certificates can cover the 4 years gap I have. Knowing that I didn&amp;#39;t choose to chill for 4 years (and it was far away from that anyway). Employers don&amp;#39;t seem to care about why I was hit hard by life to the point of discontinuing my career. Anyway, I&amp;#39;m not asking for their pitty but I&amp;#39;m determined to make my profile succeed in getting me good jobs despite of the gap. I&amp;#39;ve been autolearning for a year now and I built some projects from scratch. How can I show my motivation by exposing these projects ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16hpyf2", "is_robot_indexable": true, "report_reasons": null, "author": "NoChemical1223", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hpyf2/what_to_do_with_a_4_years_gap_in_an_it_resume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hpyf2/what_to_do_with_a_4_years_gap_in_an_it_resume/", "subreddit_subscribers": 128245, "created_utc": 1694618742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as a data engineer in data science, assisting scientists in building predicting models. I've been tasked with creating an inventory of features, along with their sources and derivations, used across our models and determine any overlap.\n\nOur typical process flow is as simple as this:\n\n    df = read_sql(open('query_from_warehouse).read()`)\n    df.columns = ['ID', 'Column1', 'Column2', ... )\n    # create features\n    df['feature1'] = np.where(df['Column1]=='some_value'],1,0)\n\nI'm hoping to find a library that can come at the very least, tell me how `feature1`  was derived. In a perfect world, it could tell me the feature came from the original `df` where I could associate it with a sql query. I know some sql parsing libraries exist I've yet to check out.\n\nFellow data engineers, Do you know of any tools that can accomplish this? If there's a word for this, can you tell me what I'm searching for?\n\nMany thanks in advance.", "author_fullname": "t2_16he60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python &amp; SQL Variable Definition / Inventory / Data Lineage / Derivations / Taxonomy Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hn402", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694611855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a data engineer in data science, assisting scientists in building predicting models. I&amp;#39;ve been tasked with creating an inventory of features, along with their sources and derivations, used across our models and determine any overlap.&lt;/p&gt;\n\n&lt;p&gt;Our typical process flow is as simple as this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df = read_sql(open(&amp;#39;query_from_warehouse).read()`)\ndf.columns = [&amp;#39;ID&amp;#39;, &amp;#39;Column1&amp;#39;, &amp;#39;Column2&amp;#39;, ... )\n# create features\ndf[&amp;#39;feature1&amp;#39;] = np.where(df[&amp;#39;Column1]==&amp;#39;some_value&amp;#39;],1,0)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;m hoping to find a library that can come at the very least, tell me how &lt;code&gt;feature1&lt;/code&gt;  was derived. In a perfect world, it could tell me the feature came from the original &lt;code&gt;df&lt;/code&gt; where I could associate it with a sql query. I know some sql parsing libraries exist I&amp;#39;ve yet to check out.&lt;/p&gt;\n\n&lt;p&gt;Fellow data engineers, Do you know of any tools that can accomplish this? If there&amp;#39;s a word for this, can you tell me what I&amp;#39;m searching for?&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hn402", "is_robot_indexable": true, "report_reasons": null, "author": "thefreakypeople", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hn402/python_sql_variable_definition_inventory_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hn402/python_sql_variable_definition_inventory_data/", "subreddit_subscribers": 128245, "created_utc": 1694611855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a Data Engineer located in Hong Kong with 4 years experience. Before this position I was an analyst but it's more a \"full-stacked\" position since I was involved in developing data pipeline and data modeling beside the analytic part.\n\nMy current team is small, only me and a DevOps Engineer design, develop and maintain a data platform. Although our platform are built based on AWS machines, most of components are picked from different open source projects instead of using cloud services (e.g self-managed Spark Application instead of EMR). The DevOps Engineer help setup the environment and infrastructure management. I'm responsible for pipeline development and optimization, pipeline orchestration, data modeling, data visualization. I also handle some basic DevOps jobs in case the DevOps Engineer is not available and it's really challenging. We also have a BA and a PM in the team, I need to work with team to understand the business requirement sometime.\n\nRecently I started discussion with different people and did some research in the data field in Hong Kong. It looks like my skillset is not match to the market since most of companies (around 7\\~8/10) use cloud service fully or enterprise data platform like Cloudera or Databricks. More importantly, it seems like you're not experienced if you didn't use any cloud service like AWS EMR before, even though I've a project in GitHub and profile in SO.\n\nThat makes me feel frustrated. I enjoy pipeline development and data modeling, also I wish my work can impact and contribute to the business, but i'm not sure the boundary between a Data Engineer and DevOps Engineer, especially I'm not familiar with the DevOps side like networking and IAM. On the other hand, it seems my skill set is not transferrable to my next job.\n\nI would like to ask:\n\n1. What do you think a Data Engineer should focus on? I prevent being jack of all trades but master of none.\n2. Is it worthing to spend time getting the cloud / Databricks certificates? Is it helpful when you applying the job? Should I spending more time on studying cloud service instead of the open source project?\n\nThanks for spending time on this post. I would love to know your advice and also your story on how do you grow in the role of Data Engineer.\n\nThank you.", "author_fullname": "t2_a7y0xzcr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ask for advice on the next move on Data Engineer role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hgvlq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694591596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a Data Engineer located in Hong Kong with 4 years experience. Before this position I was an analyst but it&amp;#39;s more a &amp;quot;full-stacked&amp;quot; position since I was involved in developing data pipeline and data modeling beside the analytic part.&lt;/p&gt;\n\n&lt;p&gt;My current team is small, only me and a DevOps Engineer design, develop and maintain a data platform. Although our platform are built based on AWS machines, most of components are picked from different open source projects instead of using cloud services (e.g self-managed Spark Application instead of EMR). The DevOps Engineer help setup the environment and infrastructure management. I&amp;#39;m responsible for pipeline development and optimization, pipeline orchestration, data modeling, data visualization. I also handle some basic DevOps jobs in case the DevOps Engineer is not available and it&amp;#39;s really challenging. We also have a BA and a PM in the team, I need to work with team to understand the business requirement sometime.&lt;/p&gt;\n\n&lt;p&gt;Recently I started discussion with different people and did some research in the data field in Hong Kong. It looks like my skillset is not match to the market since most of companies (around 7~8/10) use cloud service fully or enterprise data platform like Cloudera or Databricks. More importantly, it seems like you&amp;#39;re not experienced if you didn&amp;#39;t use any cloud service like AWS EMR before, even though I&amp;#39;ve a project in GitHub and profile in SO.&lt;/p&gt;\n\n&lt;p&gt;That makes me feel frustrated. I enjoy pipeline development and data modeling, also I wish my work can impact and contribute to the business, but i&amp;#39;m not sure the boundary between a Data Engineer and DevOps Engineer, especially I&amp;#39;m not familiar with the DevOps side like networking and IAM. On the other hand, it seems my skill set is not transferrable to my next job.&lt;/p&gt;\n\n&lt;p&gt;I would like to ask:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What do you think a Data Engineer should focus on? I prevent being jack of all trades but master of none.&lt;/li&gt;\n&lt;li&gt;Is it worthing to spend time getting the cloud / Databricks certificates? Is it helpful when you applying the job? Should I spending more time on studying cloud service instead of the open source project?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks for spending time on this post. I would love to know your advice and also your story on how do you grow in the role of Data Engineer.&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16hgvlq", "is_robot_indexable": true, "report_reasons": null, "author": "masamibb", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hgvlq/ask_for_advice_on_the_next_move_on_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hgvlq/ask_for_advice_on_the_next_move_on_data_engineer/", "subreddit_subscribers": 128245, "created_utc": 1694591596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a DA whose in the early stages of my own consulting company and need to increase my DE skillset. I am interested in having someone walk through a basic data engineering project that includes gathering data in python through an API, ingesting it into a cloud instance of Snowflake, and setting it to automate the ingestion process. \n\nThis would happen via Google Meets at an agreed upon time. I am, of course, willing to pay for your time. Please DM me if interested.", "author_fullname": "t2_f5lvn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone willing to walk through a data engineering project from gathering data in python through an API to using that data to build a Snowflake database in the cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16h9imr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694568478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a DA whose in the early stages of my own consulting company and need to increase my DE skillset. I am interested in having someone walk through a basic data engineering project that includes gathering data in python through an API, ingesting it into a cloud instance of Snowflake, and setting it to automate the ingestion process. &lt;/p&gt;\n\n&lt;p&gt;This would happen via Google Meets at an agreed upon time. I am, of course, willing to pay for your time. Please DM me if interested.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16h9imr", "is_robot_indexable": true, "report_reasons": null, "author": "takemyderivative", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16h9imr/anyone_willing_to_walk_through_a_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16h9imr/anyone_willing_to_walk_through_a_data_engineering/", "subreddit_subscribers": 128245, "created_utc": 1694568478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am tasked with designing the data infrastructure for my company from scratch in Azure. They have data from various ERP systems (mostly Microsoft Dynamics). They want to create reports using Power BI. \n\nPreviously I worked as a BI developer / data analyst, so I'm in a bit over my head. I'm currently going through the DP-203 certification material. Any other advice or interesting learning resources I can look at to better handle this project?", "author_fullname": "t2_7a8qzc12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data infrastructure from scratch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hlca1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694606997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am tasked with designing the data infrastructure for my company from scratch in Azure. They have data from various ERP systems (mostly Microsoft Dynamics). They want to create reports using Power BI. &lt;/p&gt;\n\n&lt;p&gt;Previously I worked as a BI developer / data analyst, so I&amp;#39;m in a bit over my head. I&amp;#39;m currently going through the DP-203 certification material. Any other advice or interesting learning resources I can look at to better handle this project?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hlca1", "is_robot_indexable": true, "report_reasons": null, "author": "vroemboem", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hlca1/data_infrastructure_from_scratch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hlca1/data_infrastructure_from_scratch/", "subreddit_subscribers": 128245, "created_utc": 1694606997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to extract the API from Oracle Netsuite. Can someone please guide me? ", "author_fullname": "t2_dfjkwfqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Oracle Netsuite API to extract the data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hkdig", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694604076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to extract the API from Oracle Netsuite. Can someone please guide me? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hkdig", "is_robot_indexable": true, "report_reasons": null, "author": "Boss2508", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hkdig/oracle_netsuite_api_to_extract_the_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hkdig/oracle_netsuite_api_to_extract_the_data/", "subreddit_subscribers": 128245, "created_utc": 1694604076.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently in a role where I manage the data engineering team and of course the warehouse. We are going through some org and policy changes and one of the suggested policies is NO access to data, in OLTP or DWH. \nMy question is\u2026 is this normal? I\u2019ve been in tech (and data) for 10+ years and I\u2019ve never heard this edict before. RBAC, the concept of least access for role, data masking/obfuscation, yes\u2026 but just flat out not giving users access\u2026 I don\u2019t understand?", "author_fullname": "t2_9rbxc91v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should users have access", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hbaj7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694573365.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently in a role where I manage the data engineering team and of course the warehouse. We are going through some org and policy changes and one of the suggested policies is NO access to data, in OLTP or DWH. \nMy question is\u2026 is this normal? I\u2019ve been in tech (and data) for 10+ years and I\u2019ve never heard this edict before. RBAC, the concept of least access for role, data masking/obfuscation, yes\u2026 but just flat out not giving users access\u2026 I don\u2019t understand?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hbaj7", "is_robot_indexable": true, "report_reasons": null, "author": "AnnualDepth8843", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hbaj7/should_users_have_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hbaj7/should_users_have_access/", "subreddit_subscribers": 128245, "created_utc": 1694573365.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\nI\u2019ve been working on a case study where I\u2019ve to ingest text data into MongoDB(data comes in csv format). Later i have to create some triggers and another collection by utilizing the initial collection.\n\nInitially I involved cleaned this data using pandas(not too big of a dataset), and then imported the cleaned data into MongoDB. Once in MongoDB, I\u2019ve set up some triggers for updating new data with another field.\n\nHowever, I\u2019m pondering the scalability of this system. Specifically, for future data additions, would it be more efficient to handle all data cleaning and transformation directly in MongoDB using aggregations and triggers?\n\nFor those with experience in both python(pandas/spark/polars) and MongoDB, how do you decide where to place your data wrangling logic? What are the trade-offs, and what best practices do you recommend for a system that needs to be scalable and handle new data efficiently? It\u2019s not real time but i just feel adding the extra step of first cleaning in python and then importing is an extra step?\n\nAny insights or advice would be greatly appreciated!\nI\u2019m really new and never worked with MongoDB before, but here it\u2019s a compulsion to use it..\n\nTIA!", "author_fullname": "t2_51y0ecgg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices for Data Wrangling for MongoDB: python vs. MongoDB Aggregations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16gxyl5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694540889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been working on a case study where I\u2019ve to ingest text data into MongoDB(data comes in csv format). Later i have to create some triggers and another collection by utilizing the initial collection.&lt;/p&gt;\n\n&lt;p&gt;Initially I involved cleaned this data using pandas(not too big of a dataset), and then imported the cleaned data into MongoDB. Once in MongoDB, I\u2019ve set up some triggers for updating new data with another field.&lt;/p&gt;\n\n&lt;p&gt;However, I\u2019m pondering the scalability of this system. Specifically, for future data additions, would it be more efficient to handle all data cleaning and transformation directly in MongoDB using aggregations and triggers?&lt;/p&gt;\n\n&lt;p&gt;For those with experience in both python(pandas/spark/polars) and MongoDB, how do you decide where to place your data wrangling logic? What are the trade-offs, and what best practices do you recommend for a system that needs to be scalable and handle new data efficiently? It\u2019s not real time but i just feel adding the extra step of first cleaning in python and then importing is an extra step?&lt;/p&gt;\n\n&lt;p&gt;Any insights or advice would be greatly appreciated!\nI\u2019m really new and never worked with MongoDB before, but here it\u2019s a compulsion to use it..&lt;/p&gt;\n\n&lt;p&gt;TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16gxyl5", "is_robot_indexable": true, "report_reasons": null, "author": "ikhan0007", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16gxyl5/best_practices_for_data_wrangling_for_mongodb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16gxyl5/best_practices_for_data_wrangling_for_mongodb/", "subreddit_subscribers": 128245, "created_utc": 1694540889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello\n\nI am using redshift for a while now and i' got used to it not beeing very fast. I also only use one dc2.large node.\nHowever i have one Table with 1.3 Mio rows and around 20 columns. It is created by an INSERT INTO Clause so all Table configurations are Automatic. \nHowever when i try to group it by 2 columns and sum another column redshift's CPU is at 100% Performance and The query does Not complete (i Stopped after 30 mins).\nI think 1.3 Mio rows are Not very many and redshift should get the Job done but apparently i have to optimize Something. I tried vacuuming The Table already but it didnt lead to any improvements. Do you Guys have any Idea what i could do? Or is it that Just one node ist enough?", "author_fullname": "t2_as1vw8gf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift Times Out die to a simple group by?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hov7p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694616206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I am using redshift for a while now and i&amp;#39; got used to it not beeing very fast. I also only use one dc2.large node.\nHowever i have one Table with 1.3 Mio rows and around 20 columns. It is created by an INSERT INTO Clause so all Table configurations are Automatic. \nHowever when i try to group it by 2 columns and sum another column redshift&amp;#39;s CPU is at 100% Performance and The query does Not complete (i Stopped after 30 mins).\nI think 1.3 Mio rows are Not very many and redshift should get the Job done but apparently i have to optimize Something. I tried vacuuming The Table already but it didnt lead to any improvements. Do you Guys have any Idea what i could do? Or is it that Just one node ist enough?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hov7p", "is_robot_indexable": true, "report_reasons": null, "author": "One_Indication_6921", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hov7p/redshift_times_out_die_to_a_simple_group_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hov7p/redshift_times_out_die_to_a_simple_group_by/", "subreddit_subscribers": 128245, "created_utc": 1694616206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am using ADF for an ETL job and I use one generic pipelines per database type (PostgreSQL, DB2, SqlServer). The data is copied from tables in respective database and saved in a data lake (which I call bronze). In a table called MetaBronze I have specified servernames, connectionsstrings and what tables to be copied as well as in which container and under what filepath in the data lake the copy should be stored. I also mark which date this was done so I can reload by updating the date to an earlier one.\n\nIt is a bit like the following approach: [https://techcommunity.microsoft.com/t5/fasttrack-for-azure/metadata-driven-pipelines-for-microsoft-fabric/ba-p/3891651](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/metadata-driven-pipelines-for-microsoft-fabric/ba-p/3891651)\n\nSo far so good since I have a one-to-one relationship between source and destination. However, between my bronze layer and silver layer there is a many-to-many relationship. A specific table (table here is a parquet file copied from the source) from bronze can go to many different tables in the silver layer and multiple tables from bronze can go to one table in the silver layer.\n\nHow should I design this?\n\nBetween bronze and silver we need to keep track of the max date of the files that we used to build the tables in silver since some of the tables are updated incrementally.\n\nLastly between Silver and Gold there is a many-to-one relationship.\n\nCurrently I plan on making one table for each step: MetaBronze, MetaSilver, MetaGold and then an Mapping table that maps Id's between the steps. However I am not sure this is a good Idea.\n\nThanks In advance! Any tip on reading material would be much appreciated as well as suggestions on how this should be designed.\n\nEdit:\n\nFollowing is a rough draft on how I imagine the design should be (obviously there will be much more columns in each table, but in this draft I wanted to address the many-to-many issue). Hope this sketch makes it easier to understand what I want to achieve.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/u414cv9pr0ob1.png?width=1505&amp;format=png&amp;auto=webp&amp;s=6c2fcb893d374b23cc41546417a1ba8b3a71ddad\n\n&amp;#x200B;", "author_fullname": "t2_7ckv97pys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to design a metadata table (or tables) to handle many-to-many relationship between source and destination", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 60, "top_awarded_type": null, "hide_score": false, "media_metadata": {"u414cv9pr0ob1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 46, "x": 108, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aad7b6f4a0ee92e60d53513589e3e2918f9467c5"}, {"y": 93, "x": 216, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=48582d554781d4421c97d18359afa95a5d9467c5"}, {"y": 138, "x": 320, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e58d7f1a13e46fb2830a22fe0581779cefa0bbd"}, {"y": 277, "x": 640, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f597040652b7cdbe8e1e461a7a1a99d2399344c9"}, {"y": 416, "x": 960, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2fe524ed95dd878197548aaf4bdafed89adb9b50"}, {"y": 468, "x": 1080, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ca4bb7e7c16043d1b3417aba60e90dc2a734b9a8"}], "s": {"y": 653, "x": 1505, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=1505&amp;format=png&amp;auto=webp&amp;s=6c2fcb893d374b23cc41546417a1ba8b3a71ddad"}, "id": "u414cv9pr0ob1"}}, "name": "t3_16hjoc6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6yNDHhukBxPTebG3Gr50l4nN9dR28VzyHUVaJ9Gbd2g.jpg", "edited": 1694609372.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694601755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am using ADF for an ETL job and I use one generic pipelines per database type (PostgreSQL, DB2, SqlServer). The data is copied from tables in respective database and saved in a data lake (which I call bronze). In a table called MetaBronze I have specified servernames, connectionsstrings and what tables to be copied as well as in which container and under what filepath in the data lake the copy should be stored. I also mark which date this was done so I can reload by updating the date to an earlier one.&lt;/p&gt;\n\n&lt;p&gt;It is a bit like the following approach: &lt;a href=\"https://techcommunity.microsoft.com/t5/fasttrack-for-azure/metadata-driven-pipelines-for-microsoft-fabric/ba-p/3891651\"&gt;https://techcommunity.microsoft.com/t5/fasttrack-for-azure/metadata-driven-pipelines-for-microsoft-fabric/ba-p/3891651&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So far so good since I have a one-to-one relationship between source and destination. However, between my bronze layer and silver layer there is a many-to-many relationship. A specific table (table here is a parquet file copied from the source) from bronze can go to many different tables in the silver layer and multiple tables from bronze can go to one table in the silver layer.&lt;/p&gt;\n\n&lt;p&gt;How should I design this?&lt;/p&gt;\n\n&lt;p&gt;Between bronze and silver we need to keep track of the max date of the files that we used to build the tables in silver since some of the tables are updated incrementally.&lt;/p&gt;\n\n&lt;p&gt;Lastly between Silver and Gold there is a many-to-one relationship.&lt;/p&gt;\n\n&lt;p&gt;Currently I plan on making one table for each step: MetaBronze, MetaSilver, MetaGold and then an Mapping table that maps Id&amp;#39;s between the steps. However I am not sure this is a good Idea.&lt;/p&gt;\n\n&lt;p&gt;Thanks In advance! Any tip on reading material would be much appreciated as well as suggestions on how this should be designed.&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;Following is a rough draft on how I imagine the design should be (obviously there will be much more columns in each table, but in this draft I wanted to address the many-to-many issue). Hope this sketch makes it easier to understand what I want to achieve.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u414cv9pr0ob1.png?width=1505&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6c2fcb893d374b23cc41546417a1ba8b3a71ddad\"&gt;https://preview.redd.it/u414cv9pr0ob1.png?width=1505&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6c2fcb893d374b23cc41546417a1ba8b3a71ddad&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hjoc6", "is_robot_indexable": true, "report_reasons": null, "author": "Ygolopot272", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hjoc6/how_to_design_a_metadata_table_or_tables_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hjoc6/how_to_design_a_metadata_table_or_tables_to/", "subreddit_subscribers": 128245, "created_utc": 1694601755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_49cfbl1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Productionizing Jupyter Notebooks with Versatile Data Kit - Community Meeting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_16hrrvy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/U6M6UzsoiqY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Productionizing Jupyter Notebooks with Versatile Data Kit\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Productionizing Jupyter Notebooks with Versatile Data Kit", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/U6M6UzsoiqY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Productionizing Jupyter Notebooks with Versatile Data Kit\"&gt;&lt;/iframe&gt;", "author_name": "Versatile Data Kit", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/U6M6UzsoiqY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@versatiledatakit8891"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/U6M6UzsoiqY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Productionizing Jupyter Notebooks with Versatile Data Kit\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/16hrrvy", "height": 200}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wwy8STXGYk3eFYtkKVCCiwO1LuhhPYm_i6GoqP4yLHU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694622955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/U6M6UzsoiqY?si=mKrqwMmWEuqusVOM", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VsejW1FKtamHdvUVTIpW-gTRqIkBQcuZi0OxYehZ6-U.jpg?auto=webp&amp;s=5d7226c1dedff591489ca5fc151a0983ba9ce944", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/VsejW1FKtamHdvUVTIpW-gTRqIkBQcuZi0OxYehZ6-U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9936eaac16fea2f670d88b4c5ca6e8315e5cf665", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/VsejW1FKtamHdvUVTIpW-gTRqIkBQcuZi0OxYehZ6-U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e903a678622b2cd7f42295037b8558b142ffbf7", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/VsejW1FKtamHdvUVTIpW-gTRqIkBQcuZi0OxYehZ6-U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b912542a5f22be369282022a5bd02586c30a0a6", "width": 320, "height": 240}], "variants": {}, "id": "W_DojcQEFVtCorjVwTlUBo5COWFMPEZ1rd2CU-Yy9bM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16hrrvy", "is_robot_indexable": true, "report_reasons": null, "author": "zverulacis", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hrrvy/productionizing_jupyter_notebooks_with_versatile/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/U6M6UzsoiqY?si=mKrqwMmWEuqusVOM", "subreddit_subscribers": 128245, "created_utc": 1694622955.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Productionizing Jupyter Notebooks with Versatile Data Kit", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/U6M6UzsoiqY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Productionizing Jupyter Notebooks with Versatile Data Kit\"&gt;&lt;/iframe&gt;", "author_name": "Versatile Data Kit", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/U6M6UzsoiqY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@versatiledatakit8891"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I joined a data engineering team that the previous team of 5 had all left in the previous year with 4 walking out on the same day due to the work from home policy change.\n\nThey had replaced the team with me as  a lead.  1 sr software engineer who was a problem employee and 3 contract employees from south america that were all very young ... 22-24 ish.\n\nThe issue started with the problem employee refusing to answer the teams request for 2 months. She was the teams databricks admin and stalled the teams plans for 2 months as i was unable to bring data into databricks myself.\n\nI got pretty frustrated that this continued on for 2 months despite daily scrums.  like she would make up her own work that was all administrative. like submit support tickets for the team and just sit there and not do anything.  \n\ni have never seen anyone stonewall a group of people like that for 2 months and literally do nothing in return.\n\nThen we had one of the contract workers admit to admitting to not attending work after scrum to another employee for a month and do no work.  they took no action other than to require attendance during scrum.  the other two contractors were pretty sub par due to their age for a data engineering job and needed a ton of hand holding.\n\nI was a director of engineering paid 140k in a city with a high cost of living.   In my opinion the company was taking advantage by putting me into that situation and felt like they were just using me to maintain a dysfunctional product for existing clients before it was sunset.\n\nIt was a company with 50k employees. but i did not feel like this situation would improve due to the fact that i have never seen a company allow for someone to just do nothing for two months like that and abandon her role. \n\n i thought they had planned to sunset the product and were running out a replacement squad for the time being and i was just going to work my ass off to get beat up\n\nthoughts on this situation?", "author_fullname": "t2_grbdwzn7k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Leaving a role soon after hire, thoughts on this Data Engineering situaton?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16hrg3c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694622163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I joined a data engineering team that the previous team of 5 had all left in the previous year with 4 walking out on the same day due to the work from home policy change.&lt;/p&gt;\n\n&lt;p&gt;They had replaced the team with me as  a lead.  1 sr software engineer who was a problem employee and 3 contract employees from south america that were all very young ... 22-24 ish.&lt;/p&gt;\n\n&lt;p&gt;The issue started with the problem employee refusing to answer the teams request for 2 months. She was the teams databricks admin and stalled the teams plans for 2 months as i was unable to bring data into databricks myself.&lt;/p&gt;\n\n&lt;p&gt;I got pretty frustrated that this continued on for 2 months despite daily scrums.  like she would make up her own work that was all administrative. like submit support tickets for the team and just sit there and not do anything.  &lt;/p&gt;\n\n&lt;p&gt;i have never seen anyone stonewall a group of people like that for 2 months and literally do nothing in return.&lt;/p&gt;\n\n&lt;p&gt;Then we had one of the contract workers admit to admitting to not attending work after scrum to another employee for a month and do no work.  they took no action other than to require attendance during scrum.  the other two contractors were pretty sub par due to their age for a data engineering job and needed a ton of hand holding.&lt;/p&gt;\n\n&lt;p&gt;I was a director of engineering paid 140k in a city with a high cost of living.   In my opinion the company was taking advantage by putting me into that situation and felt like they were just using me to maintain a dysfunctional product for existing clients before it was sunset.&lt;/p&gt;\n\n&lt;p&gt;It was a company with 50k employees. but i did not feel like this situation would improve due to the fact that i have never seen a company allow for someone to just do nothing for two months like that and abandon her role. &lt;/p&gt;\n\n&lt;p&gt;i thought they had planned to sunset the product and were running out a replacement squad for the time being and i was just going to work my ass off to get beat up&lt;/p&gt;\n\n&lt;p&gt;thoughts on this situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16hrg3c", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical-End-1524", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hrg3c/leaving_a_role_soon_after_hire_thoughts_on_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hrg3c/leaving_a_role_soon_after_hire_thoughts_on_this/", "subreddit_subscribers": 128245, "created_utc": 1694622163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\n[Interpolating bathymetry point dataset using python](https://preview.redd.it/r0kar94bl1ob1.png?width=579&amp;format=png&amp;auto=webp&amp;s=5c8318def79ec710e66262a8798ed6a44f58b287)\n\n[Interpolating bathymetry point dataset using python](https://spatial-dev.guru/2023/07/31/interpolating-bathymetry-point-dataset/)", "author_fullname": "t2_avt84u4i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interpolating bathymetry point dataset using python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "media_metadata": {"r0kar94bl1ob1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 84, "x": 108, "u": "https://preview.redd.it/r0kar94bl1ob1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cdaa0e7a97e4305853bdc423ebed05e4dacd4e7f"}, {"y": 168, "x": 216, "u": "https://preview.redd.it/r0kar94bl1ob1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c34ff1006fba71eb8faf790439f55e79b7cd06d0"}, {"y": 250, "x": 320, "u": "https://preview.redd.it/r0kar94bl1ob1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7bc02bf1a5afd1dae0160d1b7b854f67bc0514a"}], "s": {"y": 453, "x": 579, "u": "https://preview.redd.it/r0kar94bl1ob1.png?width=579&amp;format=png&amp;auto=webp&amp;s=5c8318def79ec710e66262a8798ed6a44f58b287"}, "id": "r0kar94bl1ob1"}}, "name": "t3_16hq7w5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jClx3X7WaOtwOX6nI2hW7emqQsKDfLLMP_PckjmEhno.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1694619352.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r0kar94bl1ob1.png?width=579&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c8318def79ec710e66262a8798ed6a44f58b287\"&gt;Interpolating bathymetry point dataset using python&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://spatial-dev.guru/2023/07/31/interpolating-bathymetry-point-dataset/\"&gt;Interpolating bathymetry point dataset using python&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/S2275Z_UMEPbiKImFbLlXX9h39FlXNTlcb6989F63yA.jpg?auto=webp&amp;s=f39e97e0b90fcbaefbae3ec238427e62993e320c", "width": 579, "height": 453}, "resolutions": [{"url": "https://external-preview.redd.it/S2275Z_UMEPbiKImFbLlXX9h39FlXNTlcb6989F63yA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f830d5d1374f9dbc3935d6b442c42f9a792edea", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/S2275Z_UMEPbiKImFbLlXX9h39FlXNTlcb6989F63yA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=631694a70e28d6f7bb3c6f4089fd377236c36d39", "width": 216, "height": 168}, {"url": "https://external-preview.redd.it/S2275Z_UMEPbiKImFbLlXX9h39FlXNTlcb6989F63yA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7bcf0bebba2340e734f5f201184344b41e475bc", "width": 320, "height": 250}], "variants": {}, "id": "gXs2eFzpnnyKOsdl5OI3jff2o2ddJeEQgbGsCoKon8A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16hq7w5", "is_robot_indexable": true, "report_reasons": null, "author": "iamgeoknight", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hq7w5/interpolating_bathymetry_point_dataset_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hq7w5/interpolating_bathymetry_point_dataset_using/", "subreddit_subscribers": 128245, "created_utc": 1694619352.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in an analyst role but I need to build data engineering skills. I have a project in front of me but no idea where to start. I need to build a platform that runs daily comparisons and real-time monitoring across different systems (Kafka, Graph, SQL, NoSQL). The main goals are to make sure systems are in sync (daily comparisons between system A &amp; B), and make sure contracts/rules are not broken (real time monitoring of events from system C). The daily comparisons can land in a database to be used later or generate a daily report. The real-time monitoring would result in an alert sent through slack if something broke a rule. But ultimately these comparisons will be very different across systems.\n\nMy DE knowledge is very limited so I was planning to run custom python scripts on a server but I'm sure there are better ways to orchestrate this. I don't have any problem integrating with the various data sources via Python, setting up a database to read/write, and writing alerts to slack. I just don't think this is the best way. What tools should I look into?", "author_fullname": "t2_b3bhj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need suggestions on tech to look into", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hq7rk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694619344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in an analyst role but I need to build data engineering skills. I have a project in front of me but no idea where to start. I need to build a platform that runs daily comparisons and real-time monitoring across different systems (Kafka, Graph, SQL, NoSQL). The main goals are to make sure systems are in sync (daily comparisons between system A &amp;amp; B), and make sure contracts/rules are not broken (real time monitoring of events from system C). The daily comparisons can land in a database to be used later or generate a daily report. The real-time monitoring would result in an alert sent through slack if something broke a rule. But ultimately these comparisons will be very different across systems.&lt;/p&gt;\n\n&lt;p&gt;My DE knowledge is very limited so I was planning to run custom python scripts on a server but I&amp;#39;m sure there are better ways to orchestrate this. I don&amp;#39;t have any problem integrating with the various data sources via Python, setting up a database to read/write, and writing alerts to slack. I just don&amp;#39;t think this is the best way. What tools should I look into?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hq7rk", "is_robot_indexable": true, "report_reasons": null, "author": "FeltZ85", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hq7rk/i_need_suggestions_on_tech_to_look_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hq7rk/i_need_suggestions_on_tech_to_look_into/", "subreddit_subscribers": 128245, "created_utc": 1694619344.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}