{"kind": "Listing", "data": {"after": "t3_16bcx87", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work for a Fortune 500 company as a data engineer. I was promised when hired, that I would be a data scientist and get plenty of predictive modeling work. I have done some modeling work, but feel like there is no purpose to the work I am doing. We don't have much vision or leadership on the DS side and have a huge focus on DE. I have been in the position for about 1-1/2 year. It seems like we are doing a bunch of busy work to move the data into AWS, pay huge sums of money to Amazon, and get no business value out of the effort. There is a lot of talk about how great AWS is and how much of a game-changer this effort will be. Everyone on the team seems to think it's great and cool what we are doing, but I just cannot grasp the value and am extremely uninterested in the DE work. ", "author_fullname": "t2_9bjl9255", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hate my job - Waste of time?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16aop0z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 145, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 145, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693928982.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693921498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a Fortune 500 company as a data engineer. I was promised when hired, that I would be a data scientist and get plenty of predictive modeling work. I have done some modeling work, but feel like there is no purpose to the work I am doing. We don&amp;#39;t have much vision or leadership on the DS side and have a huge focus on DE. I have been in the position for about 1-1/2 year. It seems like we are doing a bunch of busy work to move the data into AWS, pay huge sums of money to Amazon, and get no business value out of the effort. There is a lot of talk about how great AWS is and how much of a game-changer this effort will be. Everyone on the team seems to think it&amp;#39;s great and cool what we are doing, but I just cannot grasp the value and am extremely uninterested in the DE work. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16aop0z", "is_robot_indexable": true, "report_reasons": null, "author": "Substantial-Lime7107", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16aop0z/hate_my_job_waste_of_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16aop0z/hate_my_job_waste_of_time/", "subreddit_subscribers": 1033273, "created_utc": 1693921498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There's always so much criticism of hiring processes in the tech world, from hating take home tests or the recent post complaining about what looks like a \\~5 minute task if you know SQL.\n\nI'm curious how everyone would realistically redesign / create their own application process since we're so critical of the existing ones. \n\nLet's say you're the hiring manager for a Data science role that you've benchmarked as needing someone with \\~1 to 2 years experience. The job role automatically closes after it's got 1000 applicants... which you get in about a day.\n\nHow do you handle those 1000 applicants? \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_4qacn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would YOU handle Data Science recruitment ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16aox1s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 121, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 121, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693922029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s always so much criticism of hiring processes in the tech world, from hating take home tests or the recent post complaining about what looks like a ~5 minute task if you know SQL.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious how everyone would realistically redesign / create their own application process since we&amp;#39;re so critical of the existing ones. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say you&amp;#39;re the hiring manager for a Data science role that you&amp;#39;ve benchmarked as needing someone with ~1 to 2 years experience. The job role automatically closes after it&amp;#39;s got 1000 applicants... which you get in about a day.&lt;/p&gt;\n\n&lt;p&gt;How do you handle those 1000 applicants? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16aox1s", "is_robot_indexable": true, "report_reasons": null, "author": "Littleish", "discussion_type": null, "num_comments": 91, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16aox1s/how_would_you_handle_data_science_recruitment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16aox1s/how_would_you_handle_data_science_recruitment/", "subreddit_subscribers": 1033273, "created_utc": 1693922029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_igz592h4w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Data matters more than the model\". Do you agree with that?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16baj8u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693974559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16baj8u", "is_robot_indexable": true, "report_reasons": null, "author": "quynhonaicenter", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16baj8u/data_matters_more_than_the_model_do_you_agree/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16baj8u/data_matters_more_than_the_model_do_you_agree/", "subreddit_subscribers": 1033273, "created_utc": 1693974559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "We all know about the big tech layoffs this past year.. but has things gotten better?", "author_fullname": "t2_tm0ugms", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has tech started hiring again?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b7i0p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693965661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We all know about the big tech layoffs this past year.. but has things gotten better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b7i0p", "is_robot_indexable": true, "report_reasons": null, "author": "SuchExplanation", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b7i0p/has_tech_started_hiring_again/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b7i0p/has_tech_started_hiring_again/", "subreddit_subscribers": 1033273, "created_utc": 1693965661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello! I am 29, working at a Fortune 50 company in their modeling department, making decent money for a MCOL area. Recently I have realized, big job title, really high compensation doesn\u2019t really motivate me anymore. I still want to switch jobs and try to work on different kind of things but work life balance and healthy work environment has started to become number 1 priority for me. So much so that when I look for job after the current one, I am not willing to compromise my wlb despite the money. One of the reasons, it\u2019s getting difficult for me to think about switching jobs since the current one is the ideal job for me. \n\nI am sharing this to understand if I ma doing something really wrong and could be a career suicide. What do you guys think? Can I keep increasing my TC while having WLB?\n\nThanks!", "author_fullname": "t2_bv171ji2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I setting myself for an unsuccessful career if I have no motivation to climb the corporate ladder?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b194c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693950256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I am 29, working at a Fortune 50 company in their modeling department, making decent money for a MCOL area. Recently I have realized, big job title, really high compensation doesn\u2019t really motivate me anymore. I still want to switch jobs and try to work on different kind of things but work life balance and healthy work environment has started to become number 1 priority for me. So much so that when I look for job after the current one, I am not willing to compromise my wlb despite the money. One of the reasons, it\u2019s getting difficult for me to think about switching jobs since the current one is the ideal job for me. &lt;/p&gt;\n\n&lt;p&gt;I am sharing this to understand if I ma doing something really wrong and could be a career suicide. What do you guys think? Can I keep increasing my TC while having WLB?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b194c", "is_robot_indexable": true, "report_reasons": null, "author": "quite--average", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b194c/am_i_setting_myself_for_an_unsuccessful_career_if/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b194c/am_i_setting_myself_for_an_unsuccessful_career_if/", "subreddit_subscribers": 1033273, "created_utc": 1693950256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Every time I've tested a SQL query and realized that some null somewhere broke my CONCAT statement, or run an R script and realized that I forgot to add \"na.rm = TRUE\" to a function somewhere, I wonder what was going through the development team's minds when they decided to build these tools these ways.\n\nWhy isn't ignoring nulls the default behavior?\n\nGenuinely curious if there's a logic to this that I haven't considered.", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELI5: Why are nulls not ignored by default?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16al6h4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693911891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every time I&amp;#39;ve tested a SQL query and realized that some null somewhere broke my CONCAT statement, or run an R script and realized that I forgot to add &amp;quot;na.rm = TRUE&amp;quot; to a function somewhere, I wonder what was going through the development team&amp;#39;s minds when they decided to build these tools these ways.&lt;/p&gt;\n\n&lt;p&gt;Why isn&amp;#39;t ignoring nulls the default behavior?&lt;/p&gt;\n\n&lt;p&gt;Genuinely curious if there&amp;#39;s a logic to this that I haven&amp;#39;t considered.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16al6h4", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16al6h4/eli5_why_are_nulls_not_ignored_by_default/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16al6h4/eli5_why_are_nulls_not_ignored_by_default/", "subreddit_subscribers": 1033273, "created_utc": 1693911891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My job responsibilities are way below my skill level so I\u2019m losing motivation to keep learning. Job hoping doesn\u2019t fix this as every job is underwhelming.", "author_fullname": "t2_kcl3tfwe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle burnout?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16azemj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693946296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My job responsibilities are way below my skill level so I\u2019m losing motivation to keep learning. Job hoping doesn\u2019t fix this as every job is underwhelming.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16azemj", "is_robot_indexable": true, "report_reasons": null, "author": "NewEcho2940", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16azemj/how_do_you_handle_burnout/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16azemj/how_do_you_handle_burnout/", "subreddit_subscribers": 1033273, "created_utc": 1693946296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am working with a music genre classifier problem where the dependent variable has 11 classes and the data is imbalanced(Added picture).\n\nThe problem I am facing is, as far as I know, the best way to deal with these types of problems is to oversample or undersample. But, the issue is this is not a binary classification problem ,rather multi-class classification problem. \n\nThe second approach that I found out was adjusting weights(either by using the argument class\\_weight = 'balanced' in the algorithm like randomforestclassifier or  by computing weight manually like this and using it while fitting any model:\n\n`from sklearn.utils.class_weight import compute_sample_weight`  \n`class_weights = compute_sample_weight(class_weight='balanced', y=y_train)`\n\nMy dilemma is what can be the correct approach here? \n\nThe reason I am asking is because for the normal case(where i split the dataset and directly train a model), I am getting higher macro f1 scores. But, when I am going with the weight approach, I am getting comparatively lower macro f1 scores. \n\n[This is how the dataset looks\\(without any preprocessing\\)](https://preview.redd.it/hdktlckc5kmb1.png?width=1732&amp;format=png&amp;auto=webp&amp;s=ac5149f420ac32690aabf82c658950c66cd97bf8)\n\n[The dependent variable](https://preview.redd.it/4l1qgf0m5kmb1.png?width=580&amp;format=png&amp;auto=webp&amp;s=f5aa138d85431a01dd27271cad76538ad3fc1639)", "author_fullname": "t2_4n24sb1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to understand the right approach for a multi-class classification problem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 16, "top_awarded_type": null, "hide_score": false, "media_metadata": {"hdktlckc5kmb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 13, "x": 108, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b56892572425b4ca3486f65853cdcceec81946ef"}, {"y": 26, "x": 216, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f02d87817c8285b62370680e482f6ebc03b1a34f"}, {"y": 38, "x": 320, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f165cca041290b6e39b86e4b89cb57026477a5db"}, {"y": 77, "x": 640, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3554331a28cb03a2b9d3667e35a80eff14a625c4"}, {"y": 115, "x": 960, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e1197c936ed4ebacba38f76d51bbf45a303c666"}, {"y": 130, "x": 1080, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b765dd16748b9473e87a13ab1c5a1afc5b63fdf4"}], "s": {"y": 209, "x": 1732, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=1732&amp;format=png&amp;auto=webp&amp;s=ac5149f420ac32690aabf82c658950c66cd97bf8"}, "id": "hdktlckc5kmb1"}, "4l1qgf0m5kmb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 80, "x": 108, "u": "https://preview.redd.it/4l1qgf0m5kmb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f28afa8aee093cc2bfa3188635fea8636f56617c"}, {"y": 160, "x": 216, "u": "https://preview.redd.it/4l1qgf0m5kmb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=25cea1fcfcaa2c2e77a68424354d3f2292f3adcd"}, {"y": 238, "x": 320, "u": "https://preview.redd.it/4l1qgf0m5kmb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8639f89d1d39fe8740541af283837e6e6101d20e"}], "s": {"y": 432, "x": 580, "u": "https://preview.redd.it/4l1qgf0m5kmb1.png?width=580&amp;format=png&amp;auto=webp&amp;s=f5aa138d85431a01dd27271cad76538ad3fc1639"}, "id": "4l1qgf0m5kmb1"}}, "name": "t3_16b9v11", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/EHgERDPQB2tgcNWYKMcstfu9--yCFZYW9DI_PXRAPV8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693972477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with a music genre classifier problem where the dependent variable has 11 classes and the data is imbalanced(Added picture).&lt;/p&gt;\n\n&lt;p&gt;The problem I am facing is, as far as I know, the best way to deal with these types of problems is to oversample or undersample. But, the issue is this is not a binary classification problem ,rather multi-class classification problem. &lt;/p&gt;\n\n&lt;p&gt;The second approach that I found out was adjusting weights(either by using the argument class_weight = &amp;#39;balanced&amp;#39; in the algorithm like randomforestclassifier or  by computing weight manually like this and using it while fitting any model:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;from sklearn.utils.class_weight import compute_sample_weight&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;class_weights = compute_sample_weight(class_weight=&amp;#39;balanced&amp;#39;, y=y_train)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;My dilemma is what can be the correct approach here? &lt;/p&gt;\n\n&lt;p&gt;The reason I am asking is because for the normal case(where i split the dataset and directly train a model), I am getting higher macro f1 scores. But, when I am going with the weight approach, I am getting comparatively lower macro f1 scores. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hdktlckc5kmb1.png?width=1732&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac5149f420ac32690aabf82c658950c66cd97bf8\"&gt;This is how the dataset looks(without any preprocessing)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4l1qgf0m5kmb1.png?width=580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5aa138d85431a01dd27271cad76538ad3fc1639\"&gt;The dependent variable&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b9v11", "is_robot_indexable": true, "report_reasons": null, "author": "ProfessorS11", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b9v11/trying_to_understand_the_right_approach_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b9v11/trying_to_understand_the_right_approach_for_a/", "subreddit_subscribers": 1033273, "created_utc": 1693972477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nI recently created a CLI that lets you compile Jupyter notebooks into FastAPI apps. \n\nIt let\u2019s you write comments like \u201c\u201d\u201d@HTTP\u201d\u201d\u201d, \u201c\u201d\u201d@WS\u201d\u201d\u201d, and \u201c\u201d\u201d@SCHEDULE\u201d\u201d\u201d\u201d to expose your cells as http or websockets endpoints, or programmatically run your cells as scheduled time intervals for simple data pipelines.\n\nIt creates a /build folder with your dockerized code for easy running and deployment.\n\nhttps://github.com/neutrino-ai/neutrino-notebooks\n\nHope you find this helpful! I would appreciate any feedback", "author_fullname": "t2_2rm8fild", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A CLI that turns notebooks into FastAPI apps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16arf00", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693928015.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I recently created a CLI that lets you compile Jupyter notebooks into FastAPI apps. &lt;/p&gt;\n\n&lt;p&gt;It let\u2019s you write comments like \u201c\u201d\u201d@HTTP\u201d\u201d\u201d, \u201c\u201d\u201d@WS\u201d\u201d\u201d, and \u201c\u201d\u201d@SCHEDULE\u201d\u201d\u201d\u201d to expose your cells as http or websockets endpoints, or programmatically run your cells as scheduled time intervals for simple data pipelines.&lt;/p&gt;\n\n&lt;p&gt;It creates a /build folder with your dockerized code for easy running and deployment.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/neutrino-ai/neutrino-notebooks\"&gt;https://github.com/neutrino-ai/neutrino-notebooks&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hope you find this helpful! I would appreciate any feedback&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?auto=webp&amp;s=5394e103b0086b4eff4d06568f294956507f0ecb", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=43271c21e8a34542bc37693cfaf1dbacc436d23f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9701d58c16fd8ce19979b7db2dc7bdd8c459dd59", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=711a7236b9019520b595a3090b2f212b668efe75", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=03ab1ab20ef97b7eff2703ac8562037562678684", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=97db8e91a7af642bc87a0d9454c87ad4c0286dac", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ec562818e6bf8236d636bae3338627ea6dfd4584", "width": 1080, "height": 540}], "variants": {}, "id": "h0H4QH_NNA8eb-artCZYZeYRXgCLX9TCOO7CamwnaCk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16arf00", "is_robot_indexable": true, "report_reasons": null, "author": "gibbybutwithrandck", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16arf00/a_cli_that_turns_notebooks_into_fastapi_apps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16arf00/a_cli_that_turns_notebooks_into_fastapi_apps/", "subreddit_subscribers": 1033273, "created_utc": 1693928015.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "A recent survey by GitLab reveals a growing trend among organizations implementing AI in their software development processes, deeming it essential to stay competitive.\n\nTo stay on top of the latest advancements in AI, [look here first.](https://supercharged-ai.beehiiv.com/subscribe?utm_source=reddit&amp;utm_medium=gitlab-ai-coding&amp;utm_campaign=campaign)\n\nhttps://preview.redd.it/cgdc3wpxnkmb1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=6de802d14cba72c972c4b0bcf389308fabbed784\n\n**AI becomes crucial for software development**\n\n* GitLab's report reveals that most respondents (83%) consider AI essential for their software development, regardless of their position, job level, or years of experience.\n* Most organizations have deemed AI adoption successful, with 90% stating confidence in using AI tools daily.\n\n**Areas of AI application and concerns about its integration**\n\n* AI's application in software development extends beyond simply generating codes, focusing more on natural language chatbots, automated test generation, and tracking machine learning model experiments.\n* However, despite the growing adoption, concerns about AI-generated codes lacking copyright protection (48%) and potentially introducing vulnerabilities (39%) are rising.\n* The rising fear of AI replacing existing roles is evident, with 57% predicting that their jobs might be threatened within five years.\n\n**The need for training and the real-world implications of AI integration**\n\n* As AI permeates workplaces, nearly 81% believe they require more training.\n* Interestingly, those with more AI experience were less likely to link it with productivity gains and faster cycle times, highlighting the importance of human verification in AI-generated codes for ensuring error-free, secure, and copyright-compliant production.\n\n[(source)](https://about.gitlab.com/developer-survey/)\n\n**P.S. If you like this kind of analysis,** you\u2019ll love my [free newsletter](https://supercharged-ai.beehiiv.com/subscribe?utm_source=reddit&amp;utm_medium=gitlab-ai-coding&amp;utm_campaign=campaign), which covers the latest advancements in AI. Professionals from Google, Meta, and OpenAI are already on board.", "author_fullname": "t2_h4jb4maul", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitLab survey reveals increasing reliance on AI in software development", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"cgdc3wpxnkmb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8402631e1a2bc281b67be61ca6f3070899f276f1"}, {"y": 113, "x": 216, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=161943aaec81d2624305af5f4bcecf18ed7c47ff"}, {"y": 168, "x": 320, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f7cc9ebd675629554f256b2e2040950bf5f00cb"}, {"y": 337, "x": 640, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9556b2104281cb23b4d4cca6d60840a6af462507"}, {"y": 506, "x": 960, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a6ef7703827dbc3e23e9d4e4391092301c0fc69"}, {"y": 569, "x": 1080, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a46bb64395b772199afe151f6399d3cf9e26c57e"}], "s": {"y": 647, "x": 1226, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=6de802d14cba72c972c4b0bcf389308fabbed784"}, "id": "cgdc3wpxnkmb1"}}, "name": "t3_16bbs07", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/rPQz06Dwfph2KnPWxOJOGKgRmPdZbNB9p1w88oxG6fA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1693978548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A recent survey by GitLab reveals a growing trend among organizations implementing AI in their software development processes, deeming it essential to stay competitive.&lt;/p&gt;\n\n&lt;p&gt;To stay on top of the latest advancements in AI, &lt;a href=\"https://supercharged-ai.beehiiv.com/subscribe?utm_source=reddit&amp;amp;utm_medium=gitlab-ai-coding&amp;amp;utm_campaign=campaign\"&gt;look here first.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cgdc3wpxnkmb1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6de802d14cba72c972c4b0bcf389308fabbed784\"&gt;https://preview.redd.it/cgdc3wpxnkmb1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6de802d14cba72c972c4b0bcf389308fabbed784&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;AI becomes crucial for software development&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;GitLab&amp;#39;s report reveals that most respondents (83%) consider AI essential for their software development, regardless of their position, job level, or years of experience.&lt;/li&gt;\n&lt;li&gt;Most organizations have deemed AI adoption successful, with 90% stating confidence in using AI tools daily.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Areas of AI application and concerns about its integration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AI&amp;#39;s application in software development extends beyond simply generating codes, focusing more on natural language chatbots, automated test generation, and tracking machine learning model experiments.&lt;/li&gt;\n&lt;li&gt;However, despite the growing adoption, concerns about AI-generated codes lacking copyright protection (48%) and potentially introducing vulnerabilities (39%) are rising.&lt;/li&gt;\n&lt;li&gt;The rising fear of AI replacing existing roles is evident, with 57% predicting that their jobs might be threatened within five years.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The need for training and the real-world implications of AI integration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;As AI permeates workplaces, nearly 81% believe they require more training.&lt;/li&gt;\n&lt;li&gt;Interestingly, those with more AI experience were less likely to link it with productivity gains and faster cycle times, highlighting the importance of human verification in AI-generated codes for ensuring error-free, secure, and copyright-compliant production.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://about.gitlab.com/developer-survey/\"&gt;(source)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;P.S. If you like this kind of analysis,&lt;/strong&gt; you\u2019ll love my &lt;a href=\"https://supercharged-ai.beehiiv.com/subscribe?utm_source=reddit&amp;amp;utm_medium=gitlab-ai-coding&amp;amp;utm_campaign=campaign\"&gt;free newsletter&lt;/a&gt;, which covers the latest advancements in AI. Professionals from Google, Meta, and OpenAI are already on board.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?auto=webp&amp;s=8972442682f23755fe3d4d7aea312e1cff5c8512", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ce15b9c726f05a168b1db503df7ab26f60f62501", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=177bdfd4af3db3eb7dfe0b92698bbd1d97974ee8", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3630ec555e6a1910b62043cf2097eb88a6f14ef5", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=312e7a99fa2a2080e445758016e4648398339990", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=87aa77b180bceb48e3af8ae0450b505860d95694", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=78860fb6488536b343b979bab9c14c0815d49eb5", "width": 1080, "height": 567}], "variants": {}, "id": "NPZM0p8FtC5HwSLNn0lZ-Kh6AiQlvJ78GtZ_8REUGxc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bbs07", "is_robot_indexable": true, "report_reasons": null, "author": "AIsupercharged", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bbs07/gitlab_survey_reveals_increasing_reliance_on_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bbs07/gitlab_survey_reveals_increasing_reliance_on_ai/", "subreddit_subscribers": 1033273, "created_utc": 1693978548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Not sure if many people felt this way, but \u201cdomain knowledge\u201d in practice is mostly just an arbitrary set of rules and constructs your organization came up with. I\u2019m sure there are many translatable elements across companies in the same industry, but I do believe a huge part of these knowledge will become obsolete once you leave the organization.\n\nThe DS field is so vast and expanding so fast, and we have a finite capacity for how much we can learn in a period of time. Domain knowledge just doesn\u2019t seem to be the most optimal thing to invest your attention in, as compared to more generalizable ML techniques or technologies. \n\nI would assume if I invested too much attention into one organization, it would mean starting over should I apply to a company in a different industry. \n\nFor people in the hiring position, how much does previous experience in the same industry matter? And is there a disadvantage for someone who has only been in one but applied to a completely different domain?", "author_fullname": "t2_2kh4l8ej", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Those with hiring experience, does having too much \u201cdomain knowledge\u201d makes it harder to enter another industry?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ar0lb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693927097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if many people felt this way, but \u201cdomain knowledge\u201d in practice is mostly just an arbitrary set of rules and constructs your organization came up with. I\u2019m sure there are many translatable elements across companies in the same industry, but I do believe a huge part of these knowledge will become obsolete once you leave the organization.&lt;/p&gt;\n\n&lt;p&gt;The DS field is so vast and expanding so fast, and we have a finite capacity for how much we can learn in a period of time. Domain knowledge just doesn\u2019t seem to be the most optimal thing to invest your attention in, as compared to more generalizable ML techniques or technologies. &lt;/p&gt;\n\n&lt;p&gt;I would assume if I invested too much attention into one organization, it would mean starting over should I apply to a company in a different industry. &lt;/p&gt;\n\n&lt;p&gt;For people in the hiring position, how much does previous experience in the same industry matter? And is there a disadvantage for someone who has only been in one but applied to a completely different domain?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ar0lb", "is_robot_indexable": true, "report_reasons": null, "author": "supper_ham", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ar0lb/those_with_hiring_experience_does_having_too_much/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ar0lb/those_with_hiring_experience_does_having_too_much/", "subreddit_subscribers": 1033273, "created_utc": 1693927097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to scrape UK house prices with Rightmove.co.uk. I can access the property search well enough, and collect a list of URLs for individual properties returned by the search.\n\nI then want to request one of these links but on my first attemptI get an error 429 which means I've requested too much. \n\nI'm still able to request data from the search URL. As much as I want. Just never the URL for an individual property. \n\nI know the URL works because I've searched it manually as well. \n\nI thought if the server didn't want me to access these pages at all, they would return error 403, as I found with Zoopla.co.uk\n\nIs there a way around this or some other issue I maybe haven't thought of?", "author_fullname": "t2_yp4sh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Error 429 when scraping data with pyhton", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16bgbxo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693995140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to scrape UK house prices with Rightmove.co.uk. I can access the property search well enough, and collect a list of URLs for individual properties returned by the search.&lt;/p&gt;\n\n&lt;p&gt;I then want to request one of these links but on my first attemptI get an error 429 which means I&amp;#39;ve requested too much. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m still able to request data from the search URL. As much as I want. Just never the URL for an individual property. &lt;/p&gt;\n\n&lt;p&gt;I know the URL works because I&amp;#39;ve searched it manually as well. &lt;/p&gt;\n\n&lt;p&gt;I thought if the server didn&amp;#39;t want me to access these pages at all, they would return error 403, as I found with Zoopla.co.uk&lt;/p&gt;\n\n&lt;p&gt;Is there a way around this or some other issue I maybe haven&amp;#39;t thought of?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bgbxo", "is_robot_indexable": true, "report_reasons": null, "author": "callumbous", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bgbxo/error_429_when_scraping_data_with_pyhton/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bgbxo/error_429_when_scraping_data_with_pyhton/", "subreddit_subscribers": 1033273, "created_utc": 1693995140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Can you recommend any practical course/book/tutorial, for ML engineering, that's also relevant in the industry?  There are plenty of resources out there on ML theory and how to train/build models, but online content often lacks ML engineering part (deploying/productionalizing) and learning the useful tools that's used widely in the industry. So, for all practitioners out there, is there something that you could recommend?", "author_fullname": "t2_697359ug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practical MLE course/MOOC/book?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bdzbq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693986510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can you recommend any practical course/book/tutorial, for ML engineering, that&amp;#39;s also relevant in the industry?  There are plenty of resources out there on ML theory and how to train/build models, but online content often lacks ML engineering part (deploying/productionalizing) and learning the useful tools that&amp;#39;s used widely in the industry. So, for all practitioners out there, is there something that you could recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bdzbq", "is_robot_indexable": true, "report_reasons": null, "author": "GRDDT", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bdzbq/best_practical_mle_coursemoocbook/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bdzbq/best_practical_mle_coursemoocbook/", "subreddit_subscribers": 1033273, "created_utc": 1693986510.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello Everyone,\n\nI've put together an [**Ultimate dbt-utils Cheat Sheet**](https://datacoves.com/post/dbt-utils-cheatsheet) that I believe can aid those utilizing dbt in their data science endeavors. \n\nThe dbt-utils package has:\n\n* SQL generators for effective data manipulation.\n* Data validation strategies.\n* Introspective macros for better data comprehension.\n\nI regularly share helpful content over at Datacoves. \n\nWould love to know if you find this resource helpful or if there are any other dbt areas you'd like a deep dive into!", "author_fullname": "t2_fosm1pwyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From me to you: Handy dbt-utils Cheat Sheet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b4y7a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693958979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve put together an &lt;a href=\"https://datacoves.com/post/dbt-utils-cheatsheet\"&gt;&lt;strong&gt;Ultimate dbt-utils Cheat Sheet&lt;/strong&gt;&lt;/a&gt; that I believe can aid those utilizing dbt in their data science endeavors. &lt;/p&gt;\n\n&lt;p&gt;The dbt-utils package has:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SQL generators for effective data manipulation.&lt;/li&gt;\n&lt;li&gt;Data validation strategies.&lt;/li&gt;\n&lt;li&gt;Introspective macros for better data comprehension.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I regularly share helpful content over at Datacoves. &lt;/p&gt;\n\n&lt;p&gt;Would love to know if you find this resource helpful or if there are any other dbt areas you&amp;#39;d like a deep dive into!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?auto=webp&amp;s=c72294a445c689d26088117fc5fff4bb905f488d", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7997828bba9f07448ccf1bc5fc04e76033b71944", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7195abbc2a5d6cdc443796e0ed5f9086f845e24d", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33d24d3f4487eaf5c747d62b56db72b2c8b84ebf", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=545aeb79a24b1e25d483c5cd733b6ca413f25389", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7207869fedc316bad07e0b593dcb7995e5baae33", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=42828b93ff9d50484b7639ebe782f4de5c04e40c", "width": 1080, "height": 564}], "variants": {}, "id": "fRjQkbaAYceSqHYVE-NBwTYR2ZoPUS8MUARvVnek75k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b4y7a", "is_robot_indexable": true, "report_reasons": null, "author": "Data-Queen-Mayra", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b4y7a/from_me_to_you_handy_dbtutils_cheat_sheet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b4y7a/from_me_to_you_handy_dbtutils_cheat_sheet/", "subreddit_subscribers": 1033273, "created_utc": 1693958979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Ever wondered about the potential of R for diverse applications beyond statistics? Rafael Camargo, a Spatial Data Scientist at Quantis, deeply delves into Berlin's flourishing R User Group (RUG).\n\nThe blog also features Rafael's personal journey with R, from automating tasks to exploring machine learning, offering a rich perspective on the tool's versatility. \n\nThe Berlin RUG is actively looking for venue sponsors for their in-person events. It's a unique chance to align your brand with innovation and thought leadership in Data Science.\n\n\ud83d\udd17Read more: [https://www.r-consortium.org/blog/2023/09/05/spatial-data-science-using-r-in-berlin-germany](https://www.r-consortium.org/blog/2023/09/05/spatial-data-science-using-r-in-berlin-germany) \n\nLet's keep the spirit of collaboration and learning alive!", "author_fullname": "t2_6ow4kclla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unlocking R's Potential Beyond Stats: Inside Berlin's R User Group with Rafael Camargo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b0jp9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693948747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever wondered about the potential of R for diverse applications beyond statistics? Rafael Camargo, a Spatial Data Scientist at Quantis, deeply delves into Berlin&amp;#39;s flourishing R User Group (RUG).&lt;/p&gt;\n\n&lt;p&gt;The blog also features Rafael&amp;#39;s personal journey with R, from automating tasks to exploring machine learning, offering a rich perspective on the tool&amp;#39;s versatility. &lt;/p&gt;\n\n&lt;p&gt;The Berlin RUG is actively looking for venue sponsors for their in-person events. It&amp;#39;s a unique chance to align your brand with innovation and thought leadership in Data Science.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd17Read more: &lt;a href=\"https://www.r-consortium.org/blog/2023/09/05/spatial-data-science-using-r-in-berlin-germany\"&gt;https://www.r-consortium.org/blog/2023/09/05/spatial-data-science-using-r-in-berlin-germany&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s keep the spirit of collaboration and learning alive!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/M_7602wFXzkSJXA2lWBADY35BbVuEEi8w3O3r_rhxk0.jpg?auto=webp&amp;s=5f66f0f109e1ec1b9675818171d3cf3c3abb011a", "width": 536, "height": 322}, "resolutions": [{"url": "https://external-preview.redd.it/M_7602wFXzkSJXA2lWBADY35BbVuEEi8w3O3r_rhxk0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a1bc172172be63c21a412692c45873e62f18cabb", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/M_7602wFXzkSJXA2lWBADY35BbVuEEi8w3O3r_rhxk0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4001305e6c5c236efea8f24aa143856146258ff8", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/M_7602wFXzkSJXA2lWBADY35BbVuEEi8w3O3r_rhxk0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=80699aaf841b63637c9795be345d1c370f266f84", "width": 320, "height": 192}], "variants": {}, "id": "PbMwLAkOMtIFWEg3WA69xWXz5OQ8EkpX9n3Lh7jEQ9I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b0jp9", "is_robot_indexable": true, "report_reasons": null, "author": "Interesting_Chance31", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b0jp9/unlocking_rs_potential_beyond_stats_inside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b0jp9/unlocking_rs_potential_beyond_stats_inside/", "subreddit_subscribers": 1033273, "created_utc": 1693948747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi All! I am currently a Data Analyst at an Investment Bank and I really want to get my masters in Data Science. I just graduated with my BS in Business Administration but concentrated in Data Analytics and currently work with tools like R and Alteryx. I absolutely love data science! \n\nHowever, money is tight for my family and I and I would like to pay down my undergrad loans rather than accumulate more during my pursuit of a masters degree.\n\nMy company will help pay for the degree as long as I continue working for them. So part time is really my only option at this point if I don\u2019t want to take out tens of thousands of more dollars in loans. \n\nThere are some professional programs out there (Northwestern, Johns Hopkins)  that allow MS in data science part time. However, I notice that these are offered through alternative schools (Northwestern SPS vs Northwestern McCormick school of engineering and JHU Engineering for professionals vs JHU Whiting School of Engineering) \n\nTo those familiar - Is the difference extreme? Like, will my MS degree be from JHU Whiting School of Engineering for professionals or will it be from JHU Whiting School of Engineering? Is the coursework as comprehensive / rigorous and are the professors experienced and accessible? \n\nHoping for some clarity as I haven\u2019t seen much discussion on these types of schools in this sub. TYIA", "author_fullname": "t2_ct0r9u9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MS in Data Science Professional Programs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16aylh9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693944489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All! I am currently a Data Analyst at an Investment Bank and I really want to get my masters in Data Science. I just graduated with my BS in Business Administration but concentrated in Data Analytics and currently work with tools like R and Alteryx. I absolutely love data science! &lt;/p&gt;\n\n&lt;p&gt;However, money is tight for my family and I and I would like to pay down my undergrad loans rather than accumulate more during my pursuit of a masters degree.&lt;/p&gt;\n\n&lt;p&gt;My company will help pay for the degree as long as I continue working for them. So part time is really my only option at this point if I don\u2019t want to take out tens of thousands of more dollars in loans. &lt;/p&gt;\n\n&lt;p&gt;There are some professional programs out there (Northwestern, Johns Hopkins)  that allow MS in data science part time. However, I notice that these are offered through alternative schools (Northwestern SPS vs Northwestern McCormick school of engineering and JHU Engineering for professionals vs JHU Whiting School of Engineering) &lt;/p&gt;\n\n&lt;p&gt;To those familiar - Is the difference extreme? Like, will my MS degree be from JHU Whiting School of Engineering for professionals or will it be from JHU Whiting School of Engineering? Is the coursework as comprehensive / rigorous and are the professors experienced and accessible? &lt;/p&gt;\n\n&lt;p&gt;Hoping for some clarity as I haven\u2019t seen much discussion on these types of schools in this sub. TYIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16aylh9", "is_robot_indexable": true, "report_reasons": null, "author": "NewManufacturer3888", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16aylh9/ms_in_data_science_professional_programs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16aylh9/ms_in_data_science_professional_programs/", "subreddit_subscribers": 1033273, "created_utc": 1693944489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " *Hi guys, i'm starting an email newsletter on practical uses with AI/ChatGPT. The following will be my content for my first email. Please let me know what you think! Any thoughts, refinements, etc.*\n\n[https://superpineapple.beehiiv.com/subscribe](https://superpineapple.beehiiv.com/subscribe)\n\n# Step-By-Step\n\nIt\u2019s not as simple as asking ChatGPT - \u201ctell me whether I should invest in Company A\u201d. You need a plan, and you need choose the right tools.\n\n### Getting Ready\n\n**Prerequisites**: ChatGPT Plus\n\n**Overall Objective:** You will want answers to the key questions below (\u201cObjectives\u201d), as well as anything you think are relevant to help your investment decision\n\n**Installation:** Install the following plugins with the GPT4 Model - \u201cCompany Transcript\u201d, \u201cPolygon\u201d, \u201cAI PDF\u201d, \u201cBrowser\u201d, \u201cBrowserOp\u201d\n\n### Phase 1 - First Hand Research\n\nPlugins: \u201cCompany Transcript\u201d, \u201cPolygon\u201d, \u201cAI PDF\u201d\n\n**Step 1 (Learn from Earning Transcripts):** Use \u201cCompany Transcript\u201d to get answers from recent earning calls. You can literally use prompts like *\u201cHow has X performed\u201d*, and *\u201cwhat is A\u2019s strategy for growth and profitability\u201d*\n\n**Step 2 (Learn from Annual / Quarterly Reports)**: Use \u201cPolygon\u201d pull data from recent annual/quarterly reports. Alternatively, you can use \u201cAI PDF\u201d. Get the pdf links on these company's investor relations site (e.g. [***AMD***](https://ir.amd.com/sec-filings/filter/annual-filings?utm_source=superpineapple.beehiiv.com&amp;utm_medium=newsletter&amp;utm_campaign=chatgpt-investment-research)), and query the annual/quarterly report more directly. Similar questions apply\n\nNote: You will probably need to ask follow-up and clarifying questions as you go along, but the idea is to have ChatGPT read these lengthy docs for you.\n\n### Phase 2 - Analyst Reports\n\nPlugins: \u201cBrowser and \u201cBrowserOp\u201d\n\n**Step 3 (Get your list of articles):** Use \u201cBrowser\u201d to get a list of articles from the internet that contain analyst reports and recommendations. An example - *\u201cFind me analyst articles on AMD that give an opinion on their current performance and future prospects. Summarize them\u201d*\n\n**Step 4 (Dig into the articles)**: Use \u201cBrowserOp\u201d to actually access the relevant articles and dig into details. My favorite prompt is as follows - *\u201cCan you use BrowserOp to access these articles and collate the information across all these articles. I would like the depth and reasons behind the recommendations. Present everything in a thesis / anti-thesis format as to whether I should* buy AMD stock\u201d\n\nNote: Follow up questions are definitely needed here as well. Try to have a conversation with ChatGPT to sense check the recommendations.\n\n# Objectives\n\nI like to be able to get a robust and detailed answer to these questions, to help with my investment decisions. Feel free to steal them as prompts!\n\n* **Business Model:** How does the company make money?\n* **Demand Dynamics:** Are its products or services in demand, and why?\n* **Historical Performance:** How has the company performed in the past?\n* **Leadership:** Are talented, experienced managers in charge?\n* **Growth Prospects:** Is the company positioned for growth and profitability?\n* **Financial Health:** How much debt does the company have?\n* **Industry Analysis:** How is the company\u2019s industry doing as a whole?\n* **Challenges:** What are the obstacles and challenges the company faces?\n* **Risks:** Does the company face any economic, political, or cultural risks?\n\nYou also want to know these metrics. Compare the company\u2019s metrics to the broader market, and their specific industry.\n\n* **EPS (Earnings Per Share)**\n* **P/E Ratio (Price to Earnings Ratio)**\n* **Price to Sales Ratio**\n* **Debt to Equity Ratio**", "author_fullname": "t2_8xwepztbe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ChatGPT + Investment Research", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ay5p6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693943515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Hi guys, i&amp;#39;m starting an email newsletter on practical uses with AI/ChatGPT. The following will be my content for my first email. Please let me know what you think! Any thoughts, refinements, etc.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://superpineapple.beehiiv.com/subscribe\"&gt;https://superpineapple.beehiiv.com/subscribe&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Step-By-Step&lt;/h1&gt;\n\n&lt;p&gt;It\u2019s not as simple as asking ChatGPT - \u201ctell me whether I should invest in Company A\u201d. You need a plan, and you need choose the right tools.&lt;/p&gt;\n\n&lt;h3&gt;Getting Ready&lt;/h3&gt;\n\n&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: ChatGPT Plus&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Overall Objective:&lt;/strong&gt; You will want answers to the key questions below (\u201cObjectives\u201d), as well as anything you think are relevant to help your investment decision&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Installation:&lt;/strong&gt; Install the following plugins with the GPT4 Model - \u201cCompany Transcript\u201d, \u201cPolygon\u201d, \u201cAI PDF\u201d, \u201cBrowser\u201d, \u201cBrowserOp\u201d&lt;/p&gt;\n\n&lt;h3&gt;Phase 1 - First Hand Research&lt;/h3&gt;\n\n&lt;p&gt;Plugins: \u201cCompany Transcript\u201d, \u201cPolygon\u201d, \u201cAI PDF\u201d&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 1 (Learn from Earning Transcripts):&lt;/strong&gt; Use \u201cCompany Transcript\u201d to get answers from recent earning calls. You can literally use prompts like &lt;em&gt;\u201cHow has X performed\u201d&lt;/em&gt;, and &lt;em&gt;\u201cwhat is A\u2019s strategy for growth and profitability\u201d&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 2 (Learn from Annual / Quarterly Reports)&lt;/strong&gt;: Use \u201cPolygon\u201d pull data from recent annual/quarterly reports. Alternatively, you can use \u201cAI PDF\u201d. Get the pdf links on these company&amp;#39;s investor relations site (e.g. &lt;a href=\"https://ir.amd.com/sec-filings/filter/annual-filings?utm_source=superpineapple.beehiiv.com&amp;amp;utm_medium=newsletter&amp;amp;utm_campaign=chatgpt-investment-research\"&gt;&lt;strong&gt;&lt;em&gt;AMD&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;), and query the annual/quarterly report more directly. Similar questions apply&lt;/p&gt;\n\n&lt;p&gt;Note: You will probably need to ask follow-up and clarifying questions as you go along, but the idea is to have ChatGPT read these lengthy docs for you.&lt;/p&gt;\n\n&lt;h3&gt;Phase 2 - Analyst Reports&lt;/h3&gt;\n\n&lt;p&gt;Plugins: \u201cBrowser and \u201cBrowserOp\u201d&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 3 (Get your list of articles):&lt;/strong&gt; Use \u201cBrowser\u201d to get a list of articles from the internet that contain analyst reports and recommendations. An example - &lt;em&gt;\u201cFind me analyst articles on AMD that give an opinion on their current performance and future prospects. Summarize them\u201d&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 4 (Dig into the articles)&lt;/strong&gt;: Use \u201cBrowserOp\u201d to actually access the relevant articles and dig into details. My favorite prompt is as follows - &lt;em&gt;\u201cCan you use BrowserOp to access these articles and collate the information across all these articles. I would like the depth and reasons behind the recommendations. Present everything in a thesis / anti-thesis format as to whether I should&lt;/em&gt; buy AMD stock\u201d&lt;/p&gt;\n\n&lt;p&gt;Note: Follow up questions are definitely needed here as well. Try to have a conversation with ChatGPT to sense check the recommendations.&lt;/p&gt;\n\n&lt;h1&gt;Objectives&lt;/h1&gt;\n\n&lt;p&gt;I like to be able to get a robust and detailed answer to these questions, to help with my investment decisions. Feel free to steal them as prompts!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Business Model:&lt;/strong&gt; How does the company make money?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Demand Dynamics:&lt;/strong&gt; Are its products or services in demand, and why?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Historical Performance:&lt;/strong&gt; How has the company performed in the past?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Leadership:&lt;/strong&gt; Are talented, experienced managers in charge?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Growth Prospects:&lt;/strong&gt; Is the company positioned for growth and profitability?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Financial Health:&lt;/strong&gt; How much debt does the company have?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Industry Analysis:&lt;/strong&gt; How is the company\u2019s industry doing as a whole?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Challenges:&lt;/strong&gt; What are the obstacles and challenges the company faces?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Risks:&lt;/strong&gt; Does the company face any economic, political, or cultural risks?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You also want to know these metrics. Compare the company\u2019s metrics to the broader market, and their specific industry.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;EPS (Earnings Per Share)&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;P/E Ratio (Price to Earnings Ratio)&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Price to Sales Ratio&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Debt to Equity Ratio&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?auto=webp&amp;s=88c7167e5ad8150d2f429e247a76802e8a1a6148", "width": 1199, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e1e1bd23c89f7ad647b1de6fde23ca337ffeacca", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400a1a2a825ef770ed8f56f3088f813cd2de9462", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e035677ebc7d242bebdf9e1b9fb8d252c9461f0", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=421af7e1068f8578afffb95812da82b11ae641fc", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5894c1be66680f613b52a21831ac258135a3159d", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f50e7e5a490938cb5bca5b92f55b1a2ce2614267", "width": 1080, "height": 1080}], "variants": {}, "id": "VImqdnKEmjE76MlFz5BQ0zc2dSL9Ya9-VTTUuw96kgE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ay5p6", "is_robot_indexable": true, "report_reasons": null, "author": "saasthom", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ay5p6/chatgpt_investment_research/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ay5p6/chatgpt_investment_research/", "subreddit_subscribers": 1033273, "created_utc": 1693943515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello.\n\nI study DA by myself, and with overwhelming amount of different courses, books and youtube playlists i wonder - is the staff that was made year (two, three or even more) is still actual? Maybe the stuff i try to code aren't necessary anymore and i'm just wasting my time?\n\nWhat i use(d) to study (if you can say is that good or not or recommend something better i'll be glad):\n\nZero to Hero couse by Andrej Karpathy  - in process, there are a lof of low-level things like manual gradients calculating - i'm not sure that i'll need to write it in practice (although course is still good for me).\n\nPyTorch course by Daniel Bourke - completed, quite long, but overall i liked it.\n\nDeep Learning with PyTorch Step-by-Step. A Beginner\u2019s Guide by Godoy D.V. - good book, in process.\n\nCourses by FastAI - in process, their framework seems quite good, although i'm not sure about it.\n\nUsually i just try to copy the notebook and code the same things (maybe changing a little bit).\n\nWhat can you tell me about them, are they good, or is the something better?\n\nThanks for help and have a good day anyway.\n\n&amp;#x200B;", "author_fullname": "t2_5uu6codx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are data science courses from past years are still actual? And which ones can you recommend?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ay0u6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693943211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.&lt;/p&gt;\n\n&lt;p&gt;I study DA by myself, and with overwhelming amount of different courses, books and youtube playlists i wonder - is the staff that was made year (two, three or even more) is still actual? Maybe the stuff i try to code aren&amp;#39;t necessary anymore and i&amp;#39;m just wasting my time?&lt;/p&gt;\n\n&lt;p&gt;What i use(d) to study (if you can say is that good or not or recommend something better i&amp;#39;ll be glad):&lt;/p&gt;\n\n&lt;p&gt;Zero to Hero couse by Andrej Karpathy  - in process, there are a lof of low-level things like manual gradients calculating - i&amp;#39;m not sure that i&amp;#39;ll need to write it in practice (although course is still good for me).&lt;/p&gt;\n\n&lt;p&gt;PyTorch course by Daniel Bourke - completed, quite long, but overall i liked it.&lt;/p&gt;\n\n&lt;p&gt;Deep Learning with PyTorch Step-by-Step. A Beginner\u2019s Guide by Godoy D.V. - good book, in process.&lt;/p&gt;\n\n&lt;p&gt;Courses by FastAI - in process, their framework seems quite good, although i&amp;#39;m not sure about it.&lt;/p&gt;\n\n&lt;p&gt;Usually i just try to copy the notebook and code the same things (maybe changing a little bit).&lt;/p&gt;\n\n&lt;p&gt;What can you tell me about them, are they good, or is the something better?&lt;/p&gt;\n\n&lt;p&gt;Thanks for help and have a good day anyway.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ay0u6", "is_robot_indexable": true, "report_reasons": null, "author": "Lotaristo", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ay0u6/are_data_science_courses_from_past_years_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ay0u6/are_data_science_courses_from_past_years_are/", "subreddit_subscribers": 1033273, "created_utc": 1693943211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\u201cMCMC vs VI\u201d is no longer a discussion about your favourite Roman numeral. If you share my trepidation for model performance in the face of data sparsity, or you simply suffer from anxiety uncertainty, you might be tempted into the Bayesian world. Years later at the precipice of your career (and mental health degeneracy) you over-engineer probabilistic models so intractable that would stress Lord Bayes himself into stomach ulcers. The solution? Approximate inference, the true antihero to model simplification. I wrote a brief primer for those who enjoy maths and those who disdain it, in both cases it's impossible to avoid using maths while discussing Bayesian statistics so I kept it as light as I could.\n\nPS - This is a Reddit-friendly copypasta from my medium article, so if you're a visual person then head over there to get the visuals.\n\n**Bayesian Modelling**\n\nA closed-form solution to a machine learning model is one that can be written down on a sheet of paper using a finite number of standard mathematical operations. For example, linear models have closed-form solutions IF the design covariance matrix is invertible, otherwise we obtain a solution using iterative optimisation.\n\nBayesian models do not typically have exact closed-form solutions for their posterior distributions. One thing that typically helps is choosing simple models, Gaussian likelihood functions and conjugate priors. A prior distribution is said to be conjugate to a likelihood function if the resulting posterior belongs to the same distribution family as the prior.\n\nBayesian linear regression is a model that typically assumes Gaussian priors over both the regression coefficients and the likelihood function. When we update the prior with the observed data (using Bayes\u2019 theorem), the resulting posterior distribution for the regression coefficients will also follow a normal distribution. This can be written down analytically and sampled using standard methods in Python.\n\nConjugacy, however, does not always guarantee tractability. High-dimensional parameter spaces, hierarchical structures, non-Gaussian likelihoods with non-linear prior interactions can give rise to intractable integrals for the normalisation constant (which involve over the entire parameter space). This actually becomes prohibitive when want to build, say, a Multivariate Gaussian Linear Regression model with many predictors, or when we want to model count data using a Poisson likelihood and control for overfitting using on using a Laplace (non-conjugate) prior. Thankfully, a solution as old as the first computers comes to the rescue: Markov Chain Monte Carlo (MCMC).\n\n**Markov Chain Monte Carlo**\n\nMarkov Chain Monte Carlo can be described with enough mathematical jargon to send one fleeing back to first-derivative optimisers, so I\u2019ll skip the stomach ulcers and give an intuitive overview instead.\n\nGiven a probabilistic model parameterised by latent continuous random variables z, and observed values x, we can write down the known form for its probability density function P(z | x). If P(z | x) is intractable, we want to to generate an empirical distribution of samples based on a Markov chain that approximates the probability distribution.\n\nThis empirical distribution can then be used in place of the analytical solution to estimate posterior means, variances, quantiles, and other probabilistic summaries of the model parameters. The most important question: who is Markov and why are we talking about his chain?\n\nIn MCMC, a Markov chain is simply a sequence of samples where each sample is \u201cmemoryless\u201d, i.e. the probability of transitioning to the next sample depends only on the current sample and not on the previous history. This helps us reach a \u201cstationary\u201d distribution over samples, i.e. when we run the chain long enough, the probability P(0 &lt; z &lt; 1 | x)\\_{n} at iteration n and P(0 &lt; z &lt; 1 | x)\\_{m} at iteration m should be equal.\n\nWhat\u2019s amazing is that the distribution over samples from our Markov chain provides asymptotic exactness; MCMC converges to the true posterior distribution in the limit of infinite samples. How is this implemented in practice?\n\n**The Metropolis-Hastings (MH) Algorithm**\n\nMetropolis-Hastings (MH) is a specific type of (MCMC) algorithm ubiquitously used in approximate inference. The idea is to build a chain of samples with a proposal distribution that selects \u201cthe next\u201d sample based only on the \u201cthe current\u201d sample (remember the Markov principle).\n\nProposed samples with higher probabilities in our posterior are accepted into the chain more frequently and those with lower probabilities are rejected more often (don\u2019t make it into the chain). How do we capture the \u201ctails\u201d of our posterior if we\u2019re busy focusing on high probability regions?\n\nThis is where Metropolis-Hastings acceptance/rejection mechanism really shines:\n\nFor any given proposed sample, we define acceptance probability = min(1, \u03b1), where \u03b1 as the ratio of target and proposal distributions at proposed and current samples.\n\nNext comes the heart of the algorithm\u2019s exploration-exploitation mechanism: We generate a random number n in the domain \\[0,1\\]. If n \u2264 \u03b1, accept the new sample in the chain; if n &gt; \u03b1, keep the current sample and don\u2019t extend the chain.\n\nThe best bit? Samples with acceptance probabilities close to 1 are more likely to move the chain towards higher probability regions. Those with low acceptance probabilities can be accepted when 0 \u2264 n \u2264 \u03b1, exploring lower probability areas and avoiding local modes.\n\nWhat diagnostics do we run to check that MH successfully converged to a stationary empirical approximation to the posterior?\n\n**Trace Plots and Chain Mixing**\n\nA \u201ctrace plot\u201d allows us to inspect the chain by plotting accepted sample values for each iteration.\n\nWhat we\u2019re looking for is low autocorrelation between successive samples, and full exploration of the sample space characterised by high variance across moving windows of the trace. Chain 1 is an example of an ideal trace. Chain 2 initially has high autocorrelation and low variance but converges to stationarity after iteration t \\~1500. We discard the head segment t &lt; 1500 (so-called burn-in samples) since they\u2019re unlikely to be part of our target distribution.\n\nWhat about Chain 3? This trace demonstrates poor chain mixing, moving slowly across the parameter space between different regions of the distribution. One problem could be that we just haven\u2019t let the algorithm run long enough; but Model complexity increases with multimodality, high dimensionality and correlated parameters the asymptotic exactness guarantee of MCMC doesn\u2019t come with a tqdm, you could be waiting for quite a while. In these cases, we present the next best thing: variational inference.\n\n**Variatonal Inference**\n\nWhile MCMC offers asymptotic exactness around high dimensional distributions, it can be computationally intensive and impractical for complex distributions. Often we\u2019re just interested in a rough approximation to the posterior that scales well for deployment.\n\nVariational Inference (VI) frames the problem of approximating the posterior as an optimisation problem. Starting with a synthetic posterior Q(z | \u03bb) built from families of simpler distributions (known as the variational family), we optimise over parameters \u03bb that minimise the distance between the variational family and the true posterior P(z | x). This sounds cool but how do we choose the variational family? What even is a distance between distributions?\n\n**KL-Divergence**\n\nThe choice of Q(z | \u03bb) depends on the degree of flexibility required (increasing with the complexity of P(z | x)), but common choices are exponential or Gaussian distributions. To compare the \u201ccloseness\u201d of P and Q, we employ a similarity measure such as the Kullback-Leibler (KL) divergence:\n\nAlthough the KL divergence is asymmetric (DKL(Q||P) =/= DKL(P||Q)), it helps to quantify the difference between Q and P.\n\n\u201cBut we don\u2019t have a closed form solution for P(z|x)!\u201d I hear you exclaiming correctly. That\u2019s why we compute something called the Evidence Lower Bound (ELBO) instead: ELBO(\u03bb) = log( P(x) ) \u2212DKL(Q(z | \u03bb)||P(z | x)). This can be rearranged as ELBO(\u03bb) = E\u200b\\[log P(x, z)\\]\u2212E\u200b\\[log Q(z\u2223\u03bb)\\] helps us avoid that pesky intractable marginal integral.\n\nThus, maximizing ELBO is equivalent to minimizing the KL divergence, serves as an objective function we optimise using standard methods like co-ordinate ascent. Once \u03bb are optimised, the approximating distribution Q(z | \u03bb) serves as a surrogate for the true posterior. This approximation can then be used for downstream tasks like prediction, data imputation, or model interpretation.", "author_fullname": "t2_7h1mi6us", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tl;dr Approximate Inference methods made easy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16awjx5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693939860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\u201cMCMC vs VI\u201d is no longer a discussion about your favourite Roman numeral. If you share my trepidation for model performance in the face of data sparsity, or you simply suffer from anxiety uncertainty, you might be tempted into the Bayesian world. Years later at the precipice of your career (and mental health degeneracy) you over-engineer probabilistic models so intractable that would stress Lord Bayes himself into stomach ulcers. The solution? Approximate inference, the true antihero to model simplification. I wrote a brief primer for those who enjoy maths and those who disdain it, in both cases it&amp;#39;s impossible to avoid using maths while discussing Bayesian statistics so I kept it as light as I could.&lt;/p&gt;\n\n&lt;p&gt;PS - This is a Reddit-friendly copypasta from my medium article, so if you&amp;#39;re a visual person then head over there to get the visuals.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Bayesian Modelling&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A closed-form solution to a machine learning model is one that can be written down on a sheet of paper using a finite number of standard mathematical operations. For example, linear models have closed-form solutions IF the design covariance matrix is invertible, otherwise we obtain a solution using iterative optimisation.&lt;/p&gt;\n\n&lt;p&gt;Bayesian models do not typically have exact closed-form solutions for their posterior distributions. One thing that typically helps is choosing simple models, Gaussian likelihood functions and conjugate priors. A prior distribution is said to be conjugate to a likelihood function if the resulting posterior belongs to the same distribution family as the prior.&lt;/p&gt;\n\n&lt;p&gt;Bayesian linear regression is a model that typically assumes Gaussian priors over both the regression coefficients and the likelihood function. When we update the prior with the observed data (using Bayes\u2019 theorem), the resulting posterior distribution for the regression coefficients will also follow a normal distribution. This can be written down analytically and sampled using standard methods in Python.&lt;/p&gt;\n\n&lt;p&gt;Conjugacy, however, does not always guarantee tractability. High-dimensional parameter spaces, hierarchical structures, non-Gaussian likelihoods with non-linear prior interactions can give rise to intractable integrals for the normalisation constant (which involve over the entire parameter space). This actually becomes prohibitive when want to build, say, a Multivariate Gaussian Linear Regression model with many predictors, or when we want to model count data using a Poisson likelihood and control for overfitting using on using a Laplace (non-conjugate) prior. Thankfully, a solution as old as the first computers comes to the rescue: Markov Chain Monte Carlo (MCMC).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Markov Chain Monte Carlo&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Markov Chain Monte Carlo can be described with enough mathematical jargon to send one fleeing back to first-derivative optimisers, so I\u2019ll skip the stomach ulcers and give an intuitive overview instead.&lt;/p&gt;\n\n&lt;p&gt;Given a probabilistic model parameterised by latent continuous random variables z, and observed values x, we can write down the known form for its probability density function P(z | x). If P(z | x) is intractable, we want to to generate an empirical distribution of samples based on a Markov chain that approximates the probability distribution.&lt;/p&gt;\n\n&lt;p&gt;This empirical distribution can then be used in place of the analytical solution to estimate posterior means, variances, quantiles, and other probabilistic summaries of the model parameters. The most important question: who is Markov and why are we talking about his chain?&lt;/p&gt;\n\n&lt;p&gt;In MCMC, a Markov chain is simply a sequence of samples where each sample is \u201cmemoryless\u201d, i.e. the probability of transitioning to the next sample depends only on the current sample and not on the previous history. This helps us reach a \u201cstationary\u201d distribution over samples, i.e. when we run the chain long enough, the probability P(0 &amp;lt; z &amp;lt; 1 | x)_{n} at iteration n and P(0 &amp;lt; z &amp;lt; 1 | x)_{m} at iteration m should be equal.&lt;/p&gt;\n\n&lt;p&gt;What\u2019s amazing is that the distribution over samples from our Markov chain provides asymptotic exactness; MCMC converges to the true posterior distribution in the limit of infinite samples. How is this implemented in practice?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Metropolis-Hastings (MH) Algorithm&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Metropolis-Hastings (MH) is a specific type of (MCMC) algorithm ubiquitously used in approximate inference. The idea is to build a chain of samples with a proposal distribution that selects \u201cthe next\u201d sample based only on the \u201cthe current\u201d sample (remember the Markov principle).&lt;/p&gt;\n\n&lt;p&gt;Proposed samples with higher probabilities in our posterior are accepted into the chain more frequently and those with lower probabilities are rejected more often (don\u2019t make it into the chain). How do we capture the \u201ctails\u201d of our posterior if we\u2019re busy focusing on high probability regions?&lt;/p&gt;\n\n&lt;p&gt;This is where Metropolis-Hastings acceptance/rejection mechanism really shines:&lt;/p&gt;\n\n&lt;p&gt;For any given proposed sample, we define acceptance probability = min(1, \u03b1), where \u03b1 as the ratio of target and proposal distributions at proposed and current samples.&lt;/p&gt;\n\n&lt;p&gt;Next comes the heart of the algorithm\u2019s exploration-exploitation mechanism: We generate a random number n in the domain [0,1]. If n \u2264 \u03b1, accept the new sample in the chain; if n &amp;gt; \u03b1, keep the current sample and don\u2019t extend the chain.&lt;/p&gt;\n\n&lt;p&gt;The best bit? Samples with acceptance probabilities close to 1 are more likely to move the chain towards higher probability regions. Those with low acceptance probabilities can be accepted when 0 \u2264 n \u2264 \u03b1, exploring lower probability areas and avoiding local modes.&lt;/p&gt;\n\n&lt;p&gt;What diagnostics do we run to check that MH successfully converged to a stationary empirical approximation to the posterior?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Trace Plots and Chain Mixing&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A \u201ctrace plot\u201d allows us to inspect the chain by plotting accepted sample values for each iteration.&lt;/p&gt;\n\n&lt;p&gt;What we\u2019re looking for is low autocorrelation between successive samples, and full exploration of the sample space characterised by high variance across moving windows of the trace. Chain 1 is an example of an ideal trace. Chain 2 initially has high autocorrelation and low variance but converges to stationarity after iteration t ~1500. We discard the head segment t &amp;lt; 1500 (so-called burn-in samples) since they\u2019re unlikely to be part of our target distribution.&lt;/p&gt;\n\n&lt;p&gt;What about Chain 3? This trace demonstrates poor chain mixing, moving slowly across the parameter space between different regions of the distribution. One problem could be that we just haven\u2019t let the algorithm run long enough; but Model complexity increases with multimodality, high dimensionality and correlated parameters the asymptotic exactness guarantee of MCMC doesn\u2019t come with a tqdm, you could be waiting for quite a while. In these cases, we present the next best thing: variational inference.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Variatonal Inference&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;While MCMC offers asymptotic exactness around high dimensional distributions, it can be computationally intensive and impractical for complex distributions. Often we\u2019re just interested in a rough approximation to the posterior that scales well for deployment.&lt;/p&gt;\n\n&lt;p&gt;Variational Inference (VI) frames the problem of approximating the posterior as an optimisation problem. Starting with a synthetic posterior Q(z | \u03bb) built from families of simpler distributions (known as the variational family), we optimise over parameters \u03bb that minimise the distance between the variational family and the true posterior P(z | x). This sounds cool but how do we choose the variational family? What even is a distance between distributions?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;KL-Divergence&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The choice of Q(z | \u03bb) depends on the degree of flexibility required (increasing with the complexity of P(z | x)), but common choices are exponential or Gaussian distributions. To compare the \u201ccloseness\u201d of P and Q, we employ a similarity measure such as the Kullback-Leibler (KL) divergence:&lt;/p&gt;\n\n&lt;p&gt;Although the KL divergence is asymmetric (DKL(Q||P) =/= DKL(P||Q)), it helps to quantify the difference between Q and P.&lt;/p&gt;\n\n&lt;p&gt;\u201cBut we don\u2019t have a closed form solution for P(z|x)!\u201d I hear you exclaiming correctly. That\u2019s why we compute something called the Evidence Lower Bound (ELBO) instead: ELBO(\u03bb) = log( P(x) ) \u2212DKL(Q(z | \u03bb)||P(z | x)). This can be rearranged as ELBO(\u03bb) = E\u200b[log P(x, z)]\u2212E\u200b[log Q(z\u2223\u03bb)] helps us avoid that pesky intractable marginal integral.&lt;/p&gt;\n\n&lt;p&gt;Thus, maximizing ELBO is equivalent to minimizing the KL divergence, serves as an objective function we optimise using standard methods like co-ordinate ascent. Once \u03bb are optimised, the approximating distribution Q(z | \u03bb) serves as a surrogate for the true posterior. This approximation can then be used for downstream tasks like prediction, data imputation, or model interpretation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16awjx5", "is_robot_indexable": true, "report_reasons": null, "author": "SnooPaintings5866", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16awjx5/tldr_approximate_inference_methods_made_easy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16awjx5/tldr_approximate_inference_methods_made_easy/", "subreddit_subscribers": 1033273, "created_utc": 1693939860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My new company has given me a $5k learning budget. For the last 9 years, I always have been heads down working and never thought of taking courses. I searched this channel, as well as Coursera, EdX and Udemy for courses, but their are either very basic or are $10k+.  \n\nDoes anybody have a good idea what are good advance courses that are worth taking?", "author_fullname": "t2_3w5z8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advanced DS/DA courses after 9 years in area", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16bgbsq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693995127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My new company has given me a $5k learning budget. For the last 9 years, I always have been heads down working and never thought of taking courses. I searched this channel, as well as Coursera, EdX and Udemy for courses, but their are either very basic or are $10k+.  &lt;/p&gt;\n\n&lt;p&gt;Does anybody have a good idea what are good advance courses that are worth taking?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bgbsq", "is_robot_indexable": true, "report_reasons": null, "author": "kingrandow", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bgbsq/looking_for_advanced_dsda_courses_after_9_years/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bgbsq/looking_for_advanced_dsda_courses_after_9_years/", "subreddit_subscribers": 1033273, "created_utc": 1693995127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Energy prices forecasting questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bfbst", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_mhg1n3dx", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "Hello,\n\nthank you for your time\n\n&amp;#x200B;\n\n# Objective\n\nI need help with creating a model to forecast day-ahead energy market prices. The results have to be as accurate as possible as this is for a business project. My goal is to:\n\n* Get accurate predictions for at least one week forward\n* Be able to easily update/retrain the model with new data when it loses its accuracy\n* Avoid unnecessary complication and complexity\n* Keep resource usage at a minimum as long as I'm not compromising accuracy (I have a strong PC locally but I plan on deploying to the cloud)\n\n&amp;#x200B;\n\n# Dataset\n\nI have a very large dataset with hourly records from approximately 5 years back consisting of:\n\n* weather data (temperature, wind speed, cloudiness etc.)\n* grid data (generation per type, demand, total generation etc.)\n* balancing market prices\n* day ahead prices\n\nThe data is stored in a PostgreSQL database. It can be exported as csv files.\n\nIf needed I can get my hands on historical commodities prices (coal, fossil oil etc.), however I had trouble finding useful sources for these parameters.\n\nI can also find data about historical weather forecasts. however I would prefer not to go that way.\n\n&amp;#x200B;\n\n# Questions\n\nI have a few questions:\n\n&amp;#x200B;\n\n1. Which algorithm should I use?\n2. Which python ML framework should I use?\n3. Should the model be trained on real weather data or on historical forecasts?\n4. Could you share any good resources about creating similar models? Tutorials, articles etc.\n\n&amp;#x200B;\n\nAny advice would be very useful!\n\n&amp;#x200B;\n\n# My background\n\nAs a web developer and market analytics person I am not experienced when it comes to ML. I know python very well. I messed around making models in Google Colab before, nothing special. The results of the model will be used in a startup that I am currently founding.\n\n&amp;#x200B;\n\nThank you very much for your advice!\n\nHave a nice day.\n\n&amp;#x200B;", "author_fullname": "t2_mhg1n3dx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[P] Energy prices forecasting questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "four", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bf9yu", "quarantine": false, "link_flair_text_color": null, "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Project", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693991340.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;thank you for your time&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;Objective&lt;/h1&gt;\n\n&lt;p&gt;I need help with creating a model to forecast day-ahead energy market prices. The results have to be as accurate as possible as this is for a business project. My goal is to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Get accurate predictions for at least one week forward&lt;/li&gt;\n&lt;li&gt;Be able to easily update/retrain the model with new data when it loses its accuracy&lt;/li&gt;\n&lt;li&gt;Avoid unnecessary complication and complexity&lt;/li&gt;\n&lt;li&gt;Keep resource usage at a minimum as long as I&amp;#39;m not compromising accuracy (I have a strong PC locally but I plan on deploying to the cloud)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;Dataset&lt;/h1&gt;\n\n&lt;p&gt;I have a very large dataset with hourly records from approximately 5 years back consisting of:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;weather data (temperature, wind speed, cloudiness etc.)&lt;/li&gt;\n&lt;li&gt;grid data (generation per type, demand, total generation etc.)&lt;/li&gt;\n&lt;li&gt;balancing market prices&lt;/li&gt;\n&lt;li&gt;day ahead prices&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The data is stored in a PostgreSQL database. It can be exported as csv files.&lt;/p&gt;\n\n&lt;p&gt;If needed I can get my hands on historical commodities prices (coal, fossil oil etc.), however I had trouble finding useful sources for these parameters.&lt;/p&gt;\n\n&lt;p&gt;I can also find data about historical weather forecasts. however I would prefer not to go that way.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;Questions&lt;/h1&gt;\n\n&lt;p&gt;I have a few questions:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which algorithm should I use?&lt;/li&gt;\n&lt;li&gt;Which python ML framework should I use?&lt;/li&gt;\n&lt;li&gt;Should the model be trained on real weather data or on historical forecasts?&lt;/li&gt;\n&lt;li&gt;Could you share any good resources about creating similar models? Tutorials, articles etc.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any advice would be very useful!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;My background&lt;/h1&gt;\n\n&lt;p&gt;As a web developer and market analytics person I am not experienced when it comes to ML. I know python very well. I messed around making models in Google Colab before, nothing special. The results of the model will be used in a startup that I am currently founding.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you very much for your advice!&lt;/p&gt;\n\n&lt;p&gt;Have a nice day.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "16bf9yu", "is_robot_indexable": true, "report_reasons": null, "author": "Jena700", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/16bf9yu/p_energy_prices_forecasting_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/MachineLearning/comments/16bf9yu/p_energy_prices_forecasting_questions/", "subreddit_subscribers": 2767373, "created_utc": 1693991340.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1693991541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/MachineLearning/comments/16bf9yu/p_energy_prices_forecasting_questions/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bfbst", "is_robot_indexable": true, "report_reasons": null, "author": "Jena700", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_16bf9yu", "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bfbst/energy_prices_forecasting_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/MachineLearning/comments/16bf9yu/p_energy_prices_forecasting_questions/", "subreddit_subscribers": 1033273, "created_utc": 1693991541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[News] AI-Based Physics Predictions in Your Web-Browser!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 97, "top_awarded_type": null, "hide_score": false, "name": "t3_16beetx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_m4gixi5h", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jMMoiEoB_eThzhpfG721WO3-mFiY1x12cTteqbNMVHY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "If you are interested in Engineering simualtion and ML, check out [this webinar from SimScale](https://www.simscale.com/webinars-workshops/ai-based-physics-predictions/) on the 4th of October!\n\n[Join the webinar to find out more. https:\\/\\/www.simscale.com\\/webinars-workshops\\/ai-based-physics-predictions\\/](https://preview.redd.it/46zgauqhclmb1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=c9a730983be497a9e465dba34409244c254fb326)\n\n[https://www.reddit.com/r/simscale/comments/16bdq3x/aibased\\_physics\\_predictions\\_in\\_your\\_webbrowser/?utm\\_source=share&amp;utm\\_medium=web2x&amp;context=3](https://www.reddit.com/r/simscale/comments/16bdq3x/aibased_physics_predictions_in_your_webbrowser/?utm_source=share&amp;utm_medium=web2x&amp;context=3)", "author_fullname": "t2_m4gixi5h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[News] AI-Based Physics Predictions in Your Web-Browser!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "two", "downs": 0, "thumbnail_height": 97, "top_awarded_type": null, "hide_score": false, "media_metadata": {"46zgauqhclmb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/46zgauqhclmb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=726364c57ca767141752f2e92647bee2cf0360f3"}, {"y": 113, "x": 216, "u": "https://preview.redd.it/46zgauqhclmb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=465a6633a043e43550769c387f57da87881fa9c2"}, {"y": 168, "x": 320, "u": "https://preview.redd.it/46zgauqhclmb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e826c24b1886995de01cb644a3c81c13e30c80dc"}, {"y": 336, "x": 640, "u": "https://preview.redd.it/46zgauqhclmb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8ab00edb343848c2c17a5f0f72889b6f6361048"}, {"y": 504, "x": 960, "u": "https://preview.redd.it/46zgauqhclmb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b12dcdf448c2ddf14165f3d0b00848e18c40842d"}, {"y": 567, "x": 1080, "u": "https://preview.redd.it/46zgauqhclmb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2eac0f191cf162542585238cf0a8d4f72e99da32"}], "s": {"y": 630, "x": 1200, "u": "https://preview.redd.it/46zgauqhclmb1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=c9a730983be497a9e465dba34409244c254fb326"}, "id": "46zgauqhclmb1"}}, "name": "t3_16be1v1", "quarantine": false, "link_flair_text_color": null, "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jMMoiEoB_eThzhpfG721WO3-mFiY1x12cTteqbNMVHY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693986760.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are interested in Engineering simualtion and ML, check out &lt;a href=\"https://www.simscale.com/webinars-workshops/ai-based-physics-predictions/\"&gt;this webinar from SimScale&lt;/a&gt; on the 4th of October!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/46zgauqhclmb1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c9a730983be497a9e465dba34409244c254fb326\"&gt;Join the webinar to find out more. https://www.simscale.com/webinars-workshops/ai-based-physics-predictions/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/simscale/comments/16bdq3x/aibased_physics_predictions_in_your_webbrowser/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;https://www.reddit.com/r/simscale/comments/16bdq3x/aibased_physics_predictions_in_your_webbrowser/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?auto=webp&amp;s=b148412fc830eb683516018426ee68898846bb8b", "width": 1107, "height": 768}, "resolutions": [{"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c51d4fe5e8744505add5d4720104c107bdc91ec9", "width": 108, "height": 74}, {"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2f5968a33d327458cba23a969954d575666bf91", "width": 216, "height": 149}, {"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=413f825300b51ea243d378a24beb14cda1793108", "width": 320, "height": 222}, {"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3d09036dc02582cd3fca17f0e7e83659dcbf1df", "width": 640, "height": 444}, {"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eb8465a1c90442a9b3ddd58695390a20a71865fa", "width": 960, "height": 666}, {"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7bf68c341fabfe0e55d0cb90cbf77b7b4cd89a1b", "width": 1080, "height": 749}], "variants": {}, "id": "WbUCM7bmF4N_bt1cztifuHfPGszPmOAvhhH76a2CPTU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "16be1v1", "is_robot_indexable": true, "report_reasons": null, "author": "s_laine", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/16be1v1/news_aibased_physics_predictions_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/MachineLearning/comments/16be1v1/news_aibased_physics_predictions_in_your/", "subreddit_subscribers": 2767373, "created_utc": 1693986760.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1693988044.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/MachineLearning/comments/16be1v1/news_aibased_physics_predictions_in_your/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?auto=webp&amp;s=b148412fc830eb683516018426ee68898846bb8b", "width": 1107, "height": 768}, "resolutions": [{"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c51d4fe5e8744505add5d4720104c107bdc91ec9", "width": 108, "height": 74}, {"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2f5968a33d327458cba23a969954d575666bf91", "width": 216, "height": 149}, {"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=413f825300b51ea243d378a24beb14cda1793108", "width": 320, "height": 222}, {"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3d09036dc02582cd3fca17f0e7e83659dcbf1df", "width": 640, "height": 444}, {"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eb8465a1c90442a9b3ddd58695390a20a71865fa", "width": 960, "height": 666}, {"url": "https://external-preview.redd.it/JLYbnbcFssZnSpd9NLXeVaTyulOoByfklTkEPbKzwk4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7bf68c341fabfe0e55d0cb90cbf77b7b4cd89a1b", "width": 1080, "height": 749}], "variants": {}, "id": "WbUCM7bmF4N_bt1cztifuHfPGszPmOAvhhH76a2CPTU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16beetx", "is_robot_indexable": true, "report_reasons": null, "author": "s_laine", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_16be1v1", "author_flair_text_color": null, "permalink": "/r/datascience/comments/16beetx/news_aibased_physics_predictions_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/MachineLearning/comments/16be1v1/news_aibased_physics_predictions_in_your/", "subreddit_subscribers": 1033273, "created_utc": 1693988044.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone, I have a project and need help considering which variables are irrelevant before even constructing a lm and testing it against test data. The dataset consists of +-20k observations\nMy response variable is (total)\n\nMy variables are: \n\n\u2022 id: record index.\n\n\u2022 dte: date of rental.\n\n\u2022 season: 1: springer, 2: summer, 3: fall, 4: winter.\n\n\u2022 year: Year of date. 0: 2011. 1: 2012.\n\n\u2022 month: Month of date, 1 to 12.\n\n\u2022 hour: Hour of date, 0 to 23.\n\n\u2022 holiday: 1: If day is a holiday. 0: Otherwise.\n\n\u2022 weekday: Day of the week. 0: Sunday, 1: Monday, . . ., 6: Saturday.\n\n\u2022 workingday: 1: If a day is neither in the weekend nor a holiday is 1. 0: Otherwise.\n\n\u2022 weatherforcast: 1: Clear, Partly cloudy. 2: Mist and Few clouds. 3: Light rain/snow 4: Heavy rain,\nhail, thunderstorm, snow and fog.\n\n\u2022 temp: Normalised temperature in Celsius (observations were divided by 41).\n\n\u2022 tempf: Normalised feeling temperature in Celsius (observations were divided by 50).\n\n\u2022 humid: Normalised humidity (observations were divided by 100).\n\n\u2022 windspeed: Normalised wind speed (observations were divided by 67).\n\n\u2022 casual: Total number of casual users.\n\u2022 registered: Total number of registered users.\n\u2022 total: Total number of rentals.\n\nWhat would be the best approach to model variables, should I convert my categoricals to factors. Note that if I create a linear model such that lm1 &lt;- lm(total ~., data = data) I get r squared and r adjust values of 1 so my model indicates over fitting. Any tips ? Thanks", "author_fullname": "t2_645b9mvb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "R studio: Stats Regression Model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bdxmb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693986330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I have a project and need help considering which variables are irrelevant before even constructing a lm and testing it against test data. The dataset consists of +-20k observations\nMy response variable is (total)&lt;/p&gt;\n\n&lt;p&gt;My variables are: &lt;/p&gt;\n\n&lt;p&gt;\u2022 id: record index.&lt;/p&gt;\n\n&lt;p&gt;\u2022 dte: date of rental.&lt;/p&gt;\n\n&lt;p&gt;\u2022 season: 1: springer, 2: summer, 3: fall, 4: winter.&lt;/p&gt;\n\n&lt;p&gt;\u2022 year: Year of date. 0: 2011. 1: 2012.&lt;/p&gt;\n\n&lt;p&gt;\u2022 month: Month of date, 1 to 12.&lt;/p&gt;\n\n&lt;p&gt;\u2022 hour: Hour of date, 0 to 23.&lt;/p&gt;\n\n&lt;p&gt;\u2022 holiday: 1: If day is a holiday. 0: Otherwise.&lt;/p&gt;\n\n&lt;p&gt;\u2022 weekday: Day of the week. 0: Sunday, 1: Monday, . . ., 6: Saturday.&lt;/p&gt;\n\n&lt;p&gt;\u2022 workingday: 1: If a day is neither in the weekend nor a holiday is 1. 0: Otherwise.&lt;/p&gt;\n\n&lt;p&gt;\u2022 weatherforcast: 1: Clear, Partly cloudy. 2: Mist and Few clouds. 3: Light rain/snow 4: Heavy rain,\nhail, thunderstorm, snow and fog.&lt;/p&gt;\n\n&lt;p&gt;\u2022 temp: Normalised temperature in Celsius (observations were divided by 41).&lt;/p&gt;\n\n&lt;p&gt;\u2022 tempf: Normalised feeling temperature in Celsius (observations were divided by 50).&lt;/p&gt;\n\n&lt;p&gt;\u2022 humid: Normalised humidity (observations were divided by 100).&lt;/p&gt;\n\n&lt;p&gt;\u2022 windspeed: Normalised wind speed (observations were divided by 67).&lt;/p&gt;\n\n&lt;p&gt;\u2022 casual: Total number of casual users.\n\u2022 registered: Total number of registered users.\n\u2022 total: Total number of rentals.&lt;/p&gt;\n\n&lt;p&gt;What would be the best approach to model variables, should I convert my categoricals to factors. Note that if I create a linear model such that lm1 &amp;lt;- lm(total ~., data = data) I get r squared and r adjust values of 1 so my model indicates over fitting. Any tips ? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bdxmb", "is_robot_indexable": true, "report_reasons": null, "author": "jjnaude219", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bdxmb/r_studio_stats_regression_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bdxmb/r_studio_stats_regression_model/", "subreddit_subscribers": 1033273, "created_utc": 1693986330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_bkc0rduts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "M\u00e1ster in Data Science: Universidad de Alcal\u00e1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"5xhj8anb3lmb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 130, "x": 108, "u": "https://preview.redd.it/5xhj8anb3lmb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0758bd2df6f664284bd852fce1cf45423685a90c"}, {"y": 261, "x": 216, "u": "https://preview.redd.it/5xhj8anb3lmb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0be0eb5bafc144f215e4d189760d7701bf19e44b"}, {"y": 387, "x": 320, "u": "https://preview.redd.it/5xhj8anb3lmb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=59561970447cbebc29ad0aba9870a5cad39594ed"}], "s": {"y": 567, "x": 468, "u": "https://preview.redd.it/5xhj8anb3lmb1.png?width=468&amp;format=png&amp;auto=webp&amp;s=7380dcdc0dfa4185e1a2bbfe7377ed86154feef5"}, "id": "5xhj8anb3lmb1"}, "bwfiv6ob3lmb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 125, "x": 108, "u": "https://preview.redd.it/bwfiv6ob3lmb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8536ab8a4fa0c5bde10aa34dc0df9639f00a9e2e"}, {"y": 250, "x": 216, "u": "https://preview.redd.it/bwfiv6ob3lmb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d7b4f8cfb3b3dfae9fe2b785b4328cadb6cd20a4"}, {"y": 371, "x": 320, "u": "https://preview.redd.it/bwfiv6ob3lmb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a905f0dd060a2cf1ec6ece319880006fed93302"}], "s": {"y": 547, "x": 471, "u": "https://preview.redd.it/bwfiv6ob3lmb1.png?width=471&amp;format=png&amp;auto=webp&amp;s=da8129a6bc19ae3adbea925f8cc73bef1a17f250"}, "id": "bwfiv6ob3lmb1"}}, "name": "t3_16bd9mg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "Good morning, guys! I'm considering a Data Science Master's and need experienced tech professionals' opinions. Your help is much appreciated! (Is it worth it?)", "media_id": "5xhj8anb3lmb1", "id": 325616350}, {"media_id": "bwfiv6ob3lmb1", "id": 325616351}]}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ssawGCarDkaYZVB1_PWKnDo-CsXBr9zLjvzXarp4L-E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693983859.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/16bd9mg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": null, "id": "16bd9mg", "is_robot_indexable": true, "report_reasons": null, "author": "carl0sd4", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bd9mg/m\u00e1ster_in_data_science_universidad_de_alcal\u00e1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/16bd9mg", "subreddit_subscribers": 1033273, "created_utc": 1693983859.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys, I have a graduation in hospitality and worked few years there. I completed a Data Science certification recently but finding it hard to get internships or jobs in any sector hence I am planning to do some kind of financial course to add to my education and make portfolio in fintech related projects. Can I get some suggestions on what kind of course to look for particularly to get into fintech?", "author_fullname": "t2_gy9kw9sv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Financial course suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bcx87", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693982618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I have a graduation in hospitality and worked few years there. I completed a Data Science certification recently but finding it hard to get internships or jobs in any sector hence I am planning to do some kind of financial course to add to my education and make portfolio in fintech related projects. Can I get some suggestions on what kind of course to look for particularly to get into fintech?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bcx87", "is_robot_indexable": true, "report_reasons": null, "author": "fuhrers_shadow", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bcx87/financial_course_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bcx87/financial_course_suggestions/", "subreddit_subscribers": 1033273, "created_utc": 1693982618.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}