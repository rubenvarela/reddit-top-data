{"kind": "Listing", "data": {"after": "t3_16bnpho", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_igz592h4w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Data matters more than the model\". Do you agree with that?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16baj8u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 102, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 102, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693974559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16baj8u", "is_robot_indexable": true, "report_reasons": null, "author": "quynhonaicenter", "discussion_type": null, "num_comments": 69, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16baj8u/data_matters_more_than_the_model_do_you_agree/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16baj8u/data_matters_more_than_the_model_do_you_agree/", "subreddit_subscribers": 1033798, "created_utc": 1693974559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "We all know about the big tech layoffs this past year.. but has things gotten better?", "author_fullname": "t2_tm0ugms", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has tech started hiring again?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b7i0p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 76, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 76, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693965661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We all know about the big tech layoffs this past year.. but has things gotten better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b7i0p", "is_robot_indexable": true, "report_reasons": null, "author": "SuchExplanation", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b7i0p/has_tech_started_hiring_again/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b7i0p/has_tech_started_hiring_again/", "subreddit_subscribers": 1033798, "created_utc": 1693965661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello! I am 29, working at a Fortune 50 company in their modeling department, making decent money for a MCOL area. Recently I have realized, big job title, really high compensation doesn\u2019t really motivate me anymore. I still want to switch jobs and try to work on different kind of things but work life balance and healthy work environment has started to become number 1 priority for me. So much so that when I look for job after the current one, I am not willing to compromise my wlb despite the money. One of the reasons, it\u2019s getting difficult for me to think about switching jobs since the current one is the ideal job for me. \n\nI am sharing this to understand if I ma doing something really wrong and could be a career suicide. What do you guys think? Can I keep increasing my TC while having WLB?\n\nThanks!", "author_fullname": "t2_bv171ji2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I setting myself for an unsuccessful career if I have no motivation to climb the corporate ladder?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b194c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693950256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I am 29, working at a Fortune 50 company in their modeling department, making decent money for a MCOL area. Recently I have realized, big job title, really high compensation doesn\u2019t really motivate me anymore. I still want to switch jobs and try to work on different kind of things but work life balance and healthy work environment has started to become number 1 priority for me. So much so that when I look for job after the current one, I am not willing to compromise my wlb despite the money. One of the reasons, it\u2019s getting difficult for me to think about switching jobs since the current one is the ideal job for me. &lt;/p&gt;\n\n&lt;p&gt;I am sharing this to understand if I ma doing something really wrong and could be a career suicide. What do you guys think? Can I keep increasing my TC while having WLB?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b194c", "is_robot_indexable": true, "report_reasons": null, "author": "quite--average", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b194c/am_i_setting_myself_for_an_unsuccessful_career_if/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b194c/am_i_setting_myself_for_an_unsuccessful_career_if/", "subreddit_subscribers": 1033798, "created_utc": 1693950256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My job responsibilities are way below my skill level so I\u2019m losing motivation to keep learning. Job hoping doesn\u2019t fix this as every job is underwhelming.", "author_fullname": "t2_kcl3tfwe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle burnout?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16azemj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693946296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My job responsibilities are way below my skill level so I\u2019m losing motivation to keep learning. Job hoping doesn\u2019t fix this as every job is underwhelming.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16azemj", "is_robot_indexable": true, "report_reasons": null, "author": "NewEcho2940", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16azemj/how_do_you_handle_burnout/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16azemj/how_do_you_handle_burnout/", "subreddit_subscribers": 1033798, "created_utc": 1693946296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m relatively new to the world of large languages models  and I\u2019m currently hiking up the learning curve.\n\nRAG is a seemingly cheap way of customising LLMs to query and generate from specified document bases. Essentially, semantically-relevant documents are retrieved via vector similarity and then injected into an LLM prompt (in-context learning). You can basically talk to your own documents without fine tuning models. See here: https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html\n\nThis is exactly what many businesses want. Frameworks for RAG do exist on both Azure and AWS (+open source) but anecdotally the adoption doesn\u2019t seem that mature. Hardly anyone seems to know about it.\n\nWhat am I missing? Will RAG soon become commonplace and I\u2019m just a bit ahead of the curve? Or are there practical considerations that I\u2019m overlooking? What\u2019s the catch?", "author_fullname": "t2_mqso8rf2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is Retrieval Augmented Generation (RAG) not everywhere?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bja0s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694004129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m relatively new to the world of large languages models  and I\u2019m currently hiking up the learning curve.&lt;/p&gt;\n\n&lt;p&gt;RAG is a seemingly cheap way of customising LLMs to query and generate from specified document bases. Essentially, semantically-relevant documents are retrieved via vector similarity and then injected into an LLM prompt (in-context learning). You can basically talk to your own documents without fine tuning models. See here: &lt;a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html\"&gt;https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is exactly what many businesses want. Frameworks for RAG do exist on both Azure and AWS (+open source) but anecdotally the adoption doesn\u2019t seem that mature. Hardly anyone seems to know about it.&lt;/p&gt;\n\n&lt;p&gt;What am I missing? Will RAG soon become commonplace and I\u2019m just a bit ahead of the curve? Or are there practical considerations that I\u2019m overlooking? What\u2019s the catch?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bja0s", "is_robot_indexable": true, "report_reasons": null, "author": "Prize-Flow-3197", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bja0s/why_is_retrieval_augmented_generation_rag_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bja0s/why_is_retrieval_augmented_generation_rag_not/", "subreddit_subscribers": 1033798, "created_utc": 1694004129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a fraud data analyst working on a fraud operations team for a fintech, and starting to become pretty dissatisfied with my job. I can feel my technical skills fading the longer I stay in this position, and there is so much governance/red tape that I get to spend very little time on work that is meaningful to me.\n\nI was hoping to hear from some data scientists working in a dedicated fraud data science or fraud analytics team as to what your day to day looks like? What kind of things do you spend your time on? How is your work/life balance?\n\nFor context, some of the things I spend the majority of my time on:\n\n\u2022 Compiling model documentation for our vendor models and answering questions from Model Risk Management (this makes me want to pull my hair our, most of their questions are extremely trivial and feel like a waste of time)\n\n\u2022 Creating dashboards/reporting in Looker Studio\n\n\u2022 Basic data analysis using jupyter notebooks and Python (this is the most technical work I get to do, and could easily be done by any junior data analyst)\n\n\u2022 Reporting on metrics for senior leadership\n\n\u2022 Managing changes to our fraud rulesets\n\n\u2022 Supporting our operations team with SQL queries and ad-hoc requests\n\nI have a technical background, with a graduate degree in statistics, and have always been drawn more towards technical work (model development, graph analytics, more advanced programming). I'm not getting any of that in my current role.\n\nI'm considering trying to make the jump to a role with the title \"fraud data scientist\" or the like, but want to get a feel of those types of positions, I don't want to make the jump and be back in the same place within a year.\n\nAny insight from folks working on a fraud data science team for fintech/banking/e-commerce would be much appreciated.", "author_fullname": "t2_e57zd1e2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fraud data scientists, what do your daily tasks look like?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16bpamg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1694019712.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694018973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a fraud data analyst working on a fraud operations team for a fintech, and starting to become pretty dissatisfied with my job. I can feel my technical skills fading the longer I stay in this position, and there is so much governance/red tape that I get to spend very little time on work that is meaningful to me.&lt;/p&gt;\n\n&lt;p&gt;I was hoping to hear from some data scientists working in a dedicated fraud data science or fraud analytics team as to what your day to day looks like? What kind of things do you spend your time on? How is your work/life balance?&lt;/p&gt;\n\n&lt;p&gt;For context, some of the things I spend the majority of my time on:&lt;/p&gt;\n\n&lt;p&gt;\u2022 Compiling model documentation for our vendor models and answering questions from Model Risk Management (this makes me want to pull my hair our, most of their questions are extremely trivial and feel like a waste of time)&lt;/p&gt;\n\n&lt;p&gt;\u2022 Creating dashboards/reporting in Looker Studio&lt;/p&gt;\n\n&lt;p&gt;\u2022 Basic data analysis using jupyter notebooks and Python (this is the most technical work I get to do, and could easily be done by any junior data analyst)&lt;/p&gt;\n\n&lt;p&gt;\u2022 Reporting on metrics for senior leadership&lt;/p&gt;\n\n&lt;p&gt;\u2022 Managing changes to our fraud rulesets&lt;/p&gt;\n\n&lt;p&gt;\u2022 Supporting our operations team with SQL queries and ad-hoc requests&lt;/p&gt;\n\n&lt;p&gt;I have a technical background, with a graduate degree in statistics, and have always been drawn more towards technical work (model development, graph analytics, more advanced programming). I&amp;#39;m not getting any of that in my current role.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering trying to make the jump to a role with the title &amp;quot;fraud data scientist&amp;quot; or the like, but want to get a feel of those types of positions, I don&amp;#39;t want to make the jump and be back in the same place within a year.&lt;/p&gt;\n\n&lt;p&gt;Any insight from folks working on a fraud data science team for fintech/banking/e-commerce would be much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bpamg", "is_robot_indexable": true, "report_reasons": null, "author": "ShrimpUnforgivenCow", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bpamg/fraud_data_scientists_what_do_your_daily_tasks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bpamg/fraud_data_scientists_what_do_your_daily_tasks/", "subreddit_subscribers": 1033798, "created_utc": 1694018973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am working with a music genre classifier problem where the dependent variable has 11 classes and the data is imbalanced(Added picture).\n\nThe problem I am facing is, as far as I know, the best way to deal with these types of problems is to oversample or undersample. But, the issue is this is not a binary classification problem ,rather multi-class classification problem. \n\nThe second approach that I found out was adjusting weights(either by using the argument class\\_weight = 'balanced' in the algorithm like randomforestclassifier or  by computing weight manually like this and using it while fitting any model:\n\n`from sklearn.utils.class_weight import compute_sample_weight`  \n`class_weights = compute_sample_weight(class_weight='balanced', y=y_train)`\n\nMy dilemma is what can be the correct approach here? \n\nThe reason I am asking is because for the normal case(where i split the dataset and directly train a model), I am getting higher macro f1 scores. But, when I am going with the weight approach, I am getting comparatively lower macro f1 scores. \n\n[This is how the dataset looks\\(without any preprocessing\\)](https://preview.redd.it/hdktlckc5kmb1.png?width=1732&amp;format=png&amp;auto=webp&amp;s=ac5149f420ac32690aabf82c658950c66cd97bf8)\n\n[The dependent variable](https://preview.redd.it/4l1qgf0m5kmb1.png?width=580&amp;format=png&amp;auto=webp&amp;s=f5aa138d85431a01dd27271cad76538ad3fc1639)", "author_fullname": "t2_4n24sb1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to understand the right approach for a multi-class classification problem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 16, "top_awarded_type": null, "hide_score": false, "media_metadata": {"hdktlckc5kmb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 13, "x": 108, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b56892572425b4ca3486f65853cdcceec81946ef"}, {"y": 26, "x": 216, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f02d87817c8285b62370680e482f6ebc03b1a34f"}, {"y": 38, "x": 320, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f165cca041290b6e39b86e4b89cb57026477a5db"}, {"y": 77, "x": 640, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3554331a28cb03a2b9d3667e35a80eff14a625c4"}, {"y": 115, "x": 960, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e1197c936ed4ebacba38f76d51bbf45a303c666"}, {"y": 130, "x": 1080, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b765dd16748b9473e87a13ab1c5a1afc5b63fdf4"}], "s": {"y": 209, "x": 1732, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=1732&amp;format=png&amp;auto=webp&amp;s=ac5149f420ac32690aabf82c658950c66cd97bf8"}, "id": "hdktlckc5kmb1"}, "4l1qgf0m5kmb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 80, "x": 108, "u": "https://preview.redd.it/4l1qgf0m5kmb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f28afa8aee093cc2bfa3188635fea8636f56617c"}, {"y": 160, "x": 216, "u": "https://preview.redd.it/4l1qgf0m5kmb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=25cea1fcfcaa2c2e77a68424354d3f2292f3adcd"}, {"y": 238, "x": 320, "u": "https://preview.redd.it/4l1qgf0m5kmb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8639f89d1d39fe8740541af283837e6e6101d20e"}], "s": {"y": 432, "x": 580, "u": "https://preview.redd.it/4l1qgf0m5kmb1.png?width=580&amp;format=png&amp;auto=webp&amp;s=f5aa138d85431a01dd27271cad76538ad3fc1639"}, "id": "4l1qgf0m5kmb1"}}, "name": "t3_16b9v11", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/EHgERDPQB2tgcNWYKMcstfu9--yCFZYW9DI_PXRAPV8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693972477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with a music genre classifier problem where the dependent variable has 11 classes and the data is imbalanced(Added picture).&lt;/p&gt;\n\n&lt;p&gt;The problem I am facing is, as far as I know, the best way to deal with these types of problems is to oversample or undersample. But, the issue is this is not a binary classification problem ,rather multi-class classification problem. &lt;/p&gt;\n\n&lt;p&gt;The second approach that I found out was adjusting weights(either by using the argument class_weight = &amp;#39;balanced&amp;#39; in the algorithm like randomforestclassifier or  by computing weight manually like this and using it while fitting any model:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;from sklearn.utils.class_weight import compute_sample_weight&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;class_weights = compute_sample_weight(class_weight=&amp;#39;balanced&amp;#39;, y=y_train)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;My dilemma is what can be the correct approach here? &lt;/p&gt;\n\n&lt;p&gt;The reason I am asking is because for the normal case(where i split the dataset and directly train a model), I am getting higher macro f1 scores. But, when I am going with the weight approach, I am getting comparatively lower macro f1 scores. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hdktlckc5kmb1.png?width=1732&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac5149f420ac32690aabf82c658950c66cd97bf8\"&gt;This is how the dataset looks(without any preprocessing)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4l1qgf0m5kmb1.png?width=580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5aa138d85431a01dd27271cad76538ad3fc1639\"&gt;The dependent variable&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b9v11", "is_robot_indexable": true, "report_reasons": null, "author": "ProfessorS11", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b9v11/trying_to_understand_the_right_approach_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b9v11/trying_to_understand_the_right_approach_for_a/", "subreddit_subscribers": 1033798, "created_utc": 1693972477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I was assigned a very high impact roadmap type of project to lead few months ago. I am in a data science lead role but was mostly a technical person until recently. So this was new and challenging and I didn't really hit the ground running. Still I invested lots of time and energy into countless meetings, presentations and roadmap drafts.\n\nI struggled to get use-cases out of my main stakeholders and my boss was getting frustrated. My boss decided to get another data scientist to help who is a role that's one level below me (senior data sceintist). This guy was immediately quite aggressive and clearly wanting to take the lead. I am pretty naive and welcomed him, spent about a month sharing what I have learned and immediately invited him to all important meetings. I was also managing 2 junior data scientists who worked on this project with me.\n\nInitially he was as stuck with the stakeholders as me and still no progress with use-cases. We talked multiple times a day and I found it helpful to have someone to debrief meetings with. Then, in a sudden turn of events, there was an incident that prompted stakeholders to ask us for a use-case. Senior data scientist has technical expertise in a very niche area that coincided with this use-case (I don't). My boss immediately told stakeholders \"this is great, senior DS can help you with this!\". \n\nEver since this moment, I am suddenly dropped out of emails and meetings. Stakeholders only invite senior DS to meetings and he doesn't forward meetings to me. I mean, fine, he has the expertise, however one use-case doesn't make this project successful and is only maybe 1% of the progress. There is still a complex roadmap to deliver (that I have put a lot of effort into planning). His desire to automatically drop me out of everything is disappointing. \n\nTo make things worse, my boss told me that he is also re-assigning both of my junior DS to work on that first use-case. So senior DS is now meeting with my team completely excluding me and there is noone to work on the rest of roadmap with.\n\nI am not sure how to handle this. So far I have reminded senior DS to include me in the meetings but I am not getting anywhere. He pretends to \"forget\" and eventually forwards me the meetings after reminding him few times. At meetings, when I offer my opinion or advice, he is dismissive and snappy. Both of junior DS just look stressed.\n\nI am still the lead in the name only, having lost my team and being excluded from everything. I suspect that my boss doesn't actually care who does what but just wants the project delivered. And it looks good for senior DS to have landed the first use-case after months of stagnation. It doesn't look good for me at all.\n\nShould I try to keep pushing my way into the meetings? I just don't see this working longer-term. I worry that speaking to my boss openly will only highlight the issues. He hates complaining and likes action. \n\nI have started applying for other jobs but no luck so far :(\n\n&amp;#x200B;", "author_fullname": "t2_ekbfo234c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Being sidelined on my own project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16biy27", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694003271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I was assigned a very high impact roadmap type of project to lead few months ago. I am in a data science lead role but was mostly a technical person until recently. So this was new and challenging and I didn&amp;#39;t really hit the ground running. Still I invested lots of time and energy into countless meetings, presentations and roadmap drafts.&lt;/p&gt;\n\n&lt;p&gt;I struggled to get use-cases out of my main stakeholders and my boss was getting frustrated. My boss decided to get another data scientist to help who is a role that&amp;#39;s one level below me (senior data sceintist). This guy was immediately quite aggressive and clearly wanting to take the lead. I am pretty naive and welcomed him, spent about a month sharing what I have learned and immediately invited him to all important meetings. I was also managing 2 junior data scientists who worked on this project with me.&lt;/p&gt;\n\n&lt;p&gt;Initially he was as stuck with the stakeholders as me and still no progress with use-cases. We talked multiple times a day and I found it helpful to have someone to debrief meetings with. Then, in a sudden turn of events, there was an incident that prompted stakeholders to ask us for a use-case. Senior data scientist has technical expertise in a very niche area that coincided with this use-case (I don&amp;#39;t). My boss immediately told stakeholders &amp;quot;this is great, senior DS can help you with this!&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;Ever since this moment, I am suddenly dropped out of emails and meetings. Stakeholders only invite senior DS to meetings and he doesn&amp;#39;t forward meetings to me. I mean, fine, he has the expertise, however one use-case doesn&amp;#39;t make this project successful and is only maybe 1% of the progress. There is still a complex roadmap to deliver (that I have put a lot of effort into planning). His desire to automatically drop me out of everything is disappointing. &lt;/p&gt;\n\n&lt;p&gt;To make things worse, my boss told me that he is also re-assigning both of my junior DS to work on that first use-case. So senior DS is now meeting with my team completely excluding me and there is noone to work on the rest of roadmap with.&lt;/p&gt;\n\n&lt;p&gt;I am not sure how to handle this. So far I have reminded senior DS to include me in the meetings but I am not getting anywhere. He pretends to &amp;quot;forget&amp;quot; and eventually forwards me the meetings after reminding him few times. At meetings, when I offer my opinion or advice, he is dismissive and snappy. Both of junior DS just look stressed.&lt;/p&gt;\n\n&lt;p&gt;I am still the lead in the name only, having lost my team and being excluded from everything. I suspect that my boss doesn&amp;#39;t actually care who does what but just wants the project delivered. And it looks good for senior DS to have landed the first use-case after months of stagnation. It doesn&amp;#39;t look good for me at all.&lt;/p&gt;\n\n&lt;p&gt;Should I try to keep pushing my way into the meetings? I just don&amp;#39;t see this working longer-term. I worry that speaking to my boss openly will only highlight the issues. He hates complaining and likes action. &lt;/p&gt;\n\n&lt;p&gt;I have started applying for other jobs but no luck so far :(&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16biy27", "is_robot_indexable": true, "report_reasons": null, "author": "Ashamed-Tie6059", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16biy27/being_sidelined_on_my_own_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16biy27/being_sidelined_on_my_own_project/", "subreddit_subscribers": 1033798, "created_utc": 1694003271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have no prior experience. I\u2019m not degree educated at the minute. I am just a postman right now. I\u2019m 34 and just started thee Google Data Analytics certificate (is it worth it?). I have read somewhere I should look into do a SQL (sp?) course once I\u2019m done but it\u2019s still very early days. I\u2019ve also been told to do my own portfolio which I hope I will know enough to be able to do that soon. Has anyone got any advice on if there\u2019s anything else I could do or should be doing? What will doing a portfolio involve? I\u2019ve never had to do anything like that in the past so I hope I can nail it", "author_fullname": "t2_4n0j9oeho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking into a career as a data analyst in the U.K.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bns9m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694015451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have no prior experience. I\u2019m not degree educated at the minute. I am just a postman right now. I\u2019m 34 and just started thee Google Data Analytics certificate (is it worth it?). I have read somewhere I should look into do a SQL (sp?) course once I\u2019m done but it\u2019s still very early days. I\u2019ve also been told to do my own portfolio which I hope I will know enough to be able to do that soon. Has anyone got any advice on if there\u2019s anything else I could do or should be doing? What will doing a portfolio involve? I\u2019ve never had to do anything like that in the past so I hope I can nail it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bns9m", "is_robot_indexable": true, "report_reasons": null, "author": "TrueSolid611", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bns9m/looking_into_a_career_as_a_data_analyst_in_the_uk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bns9m/looking_into_a_career_as_a_data_analyst_in_the_uk/", "subreddit_subscribers": 1033798, "created_utc": 1694015451.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "A recent survey by GitLab reveals a growing trend among organizations implementing AI in their software development processes, deeming it essential to stay competitive.\n\nTo stay on top of the latest advancements in AI, [look here first.](https://supercharged-ai.beehiiv.com/subscribe?utm_source=reddit&amp;utm_medium=gitlab-ai-coding&amp;utm_campaign=campaign)\n\nhttps://preview.redd.it/cgdc3wpxnkmb1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=6de802d14cba72c972c4b0bcf389308fabbed784\n\n**AI becomes crucial for software development**\n\n* GitLab's report reveals that most respondents (83%) consider AI essential for their software development, regardless of their position, job level, or years of experience.\n* Most organizations have deemed AI adoption successful, with 90% stating confidence in using AI tools daily.\n\n**Areas of AI application and concerns about its integration**\n\n* AI's application in software development extends beyond simply generating codes, focusing more on natural language chatbots, automated test generation, and tracking machine learning model experiments.\n* However, despite the growing adoption, concerns about AI-generated codes lacking copyright protection (48%) and potentially introducing vulnerabilities (39%) are rising.\n* The rising fear of AI replacing existing roles is evident, with 57% predicting that their jobs might be threatened within five years.\n\n**The need for training and the real-world implications of AI integration**\n\n* As AI permeates workplaces, nearly 81% believe they require more training.\n* Interestingly, those with more AI experience were less likely to link it with productivity gains and faster cycle times, highlighting the importance of human verification in AI-generated codes for ensuring error-free, secure, and copyright-compliant production.\n\n[(source)](https://about.gitlab.com/developer-survey/)\n\n**P.S. If you like this kind of analysis,** you\u2019ll love my [free newsletter](https://supercharged-ai.beehiiv.com/subscribe?utm_source=reddit&amp;utm_medium=gitlab-ai-coding&amp;utm_campaign=campaign), which covers the latest advancements in AI. Professionals from Google, Meta, and OpenAI are already on board.", "author_fullname": "t2_h4jb4maul", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitLab survey reveals increasing reliance on AI in software development", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"cgdc3wpxnkmb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8402631e1a2bc281b67be61ca6f3070899f276f1"}, {"y": 113, "x": 216, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=161943aaec81d2624305af5f4bcecf18ed7c47ff"}, {"y": 168, "x": 320, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f7cc9ebd675629554f256b2e2040950bf5f00cb"}, {"y": 337, "x": 640, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9556b2104281cb23b4d4cca6d60840a6af462507"}, {"y": 506, "x": 960, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a6ef7703827dbc3e23e9d4e4391092301c0fc69"}, {"y": 569, "x": 1080, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a46bb64395b772199afe151f6399d3cf9e26c57e"}], "s": {"y": 647, "x": 1226, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=6de802d14cba72c972c4b0bcf389308fabbed784"}, "id": "cgdc3wpxnkmb1"}}, "name": "t3_16bbs07", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/rPQz06Dwfph2KnPWxOJOGKgRmPdZbNB9p1w88oxG6fA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1693978548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A recent survey by GitLab reveals a growing trend among organizations implementing AI in their software development processes, deeming it essential to stay competitive.&lt;/p&gt;\n\n&lt;p&gt;To stay on top of the latest advancements in AI, &lt;a href=\"https://supercharged-ai.beehiiv.com/subscribe?utm_source=reddit&amp;amp;utm_medium=gitlab-ai-coding&amp;amp;utm_campaign=campaign\"&gt;look here first.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cgdc3wpxnkmb1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6de802d14cba72c972c4b0bcf389308fabbed784\"&gt;https://preview.redd.it/cgdc3wpxnkmb1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6de802d14cba72c972c4b0bcf389308fabbed784&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;AI becomes crucial for software development&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;GitLab&amp;#39;s report reveals that most respondents (83%) consider AI essential for their software development, regardless of their position, job level, or years of experience.&lt;/li&gt;\n&lt;li&gt;Most organizations have deemed AI adoption successful, with 90% stating confidence in using AI tools daily.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Areas of AI application and concerns about its integration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AI&amp;#39;s application in software development extends beyond simply generating codes, focusing more on natural language chatbots, automated test generation, and tracking machine learning model experiments.&lt;/li&gt;\n&lt;li&gt;However, despite the growing adoption, concerns about AI-generated codes lacking copyright protection (48%) and potentially introducing vulnerabilities (39%) are rising.&lt;/li&gt;\n&lt;li&gt;The rising fear of AI replacing existing roles is evident, with 57% predicting that their jobs might be threatened within five years.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The need for training and the real-world implications of AI integration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;As AI permeates workplaces, nearly 81% believe they require more training.&lt;/li&gt;\n&lt;li&gt;Interestingly, those with more AI experience were less likely to link it with productivity gains and faster cycle times, highlighting the importance of human verification in AI-generated codes for ensuring error-free, secure, and copyright-compliant production.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://about.gitlab.com/developer-survey/\"&gt;(source)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;P.S. If you like this kind of analysis,&lt;/strong&gt; you\u2019ll love my &lt;a href=\"https://supercharged-ai.beehiiv.com/subscribe?utm_source=reddit&amp;amp;utm_medium=gitlab-ai-coding&amp;amp;utm_campaign=campaign\"&gt;free newsletter&lt;/a&gt;, which covers the latest advancements in AI. Professionals from Google, Meta, and OpenAI are already on board.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?auto=webp&amp;s=8972442682f23755fe3d4d7aea312e1cff5c8512", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ce15b9c726f05a168b1db503df7ab26f60f62501", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=177bdfd4af3db3eb7dfe0b92698bbd1d97974ee8", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3630ec555e6a1910b62043cf2097eb88a6f14ef5", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=312e7a99fa2a2080e445758016e4648398339990", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=87aa77b180bceb48e3af8ae0450b505860d95694", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=78860fb6488536b343b979bab9c14c0815d49eb5", "width": 1080, "height": 567}], "variants": {}, "id": "NPZM0p8FtC5HwSLNn0lZ-Kh6AiQlvJ78GtZ_8REUGxc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bbs07", "is_robot_indexable": true, "report_reasons": null, "author": "AIsupercharged", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bbs07/gitlab_survey_reveals_increasing_reliance_on_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bbs07/gitlab_survey_reveals_increasing_reliance_on_ai/", "subreddit_subscribers": 1033798, "created_utc": 1693978548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a final project in my master\u2019s program that I see as an opportunity to put in my CV and to apply for jobs in a chosen field. However, I don\u2019t have a chosen field. My first idea was the customer churn, as it applies everywhere, but maybe credit card fraud can also be a lucrative choice?\n\nIn terms of interest, I\u2019m into music so i thought maybe i should go into recommendation systems, but it seems really hard.", "author_fullname": "t2_m826ekr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Project suggestion for future career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bku3r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694008187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a final project in my master\u2019s program that I see as an opportunity to put in my CV and to apply for jobs in a chosen field. However, I don\u2019t have a chosen field. My first idea was the customer churn, as it applies everywhere, but maybe credit card fraud can also be a lucrative choice?&lt;/p&gt;\n\n&lt;p&gt;In terms of interest, I\u2019m into music so i thought maybe i should go into recommendation systems, but it seems really hard.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bku3r", "is_robot_indexable": true, "report_reasons": null, "author": "Utterizi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bku3r/project_suggestion_for_future_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bku3r/project_suggestion_for_future_career/", "subreddit_subscribers": 1033798, "created_utc": 1694008187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to scrape UK house prices with Rightmove.co.uk. I can access the property search well enough, and collect a list of URLs for individual properties returned by the search.\n\nI then want to request one of these links but on my first attemptI get an error 429 which means I've requested too much. \n\nI'm still able to request data from the search URL. As much as I want. Just never the URL for an individual property. \n\nI know the URL works because I've searched it manually as well. \n\nI thought if the server didn't want me to access these pages at all, they would return error 403, as I found with Zoopla.co.uk\n\nIs there a way around this or some other issue I maybe haven't thought of?", "author_fullname": "t2_yp4sh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Error 429 when scraping data with pyhton", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bgbxo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693995140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to scrape UK house prices with Rightmove.co.uk. I can access the property search well enough, and collect a list of URLs for individual properties returned by the search.&lt;/p&gt;\n\n&lt;p&gt;I then want to request one of these links but on my first attemptI get an error 429 which means I&amp;#39;ve requested too much. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m still able to request data from the search URL. As much as I want. Just never the URL for an individual property. &lt;/p&gt;\n\n&lt;p&gt;I know the URL works because I&amp;#39;ve searched it manually as well. &lt;/p&gt;\n\n&lt;p&gt;I thought if the server didn&amp;#39;t want me to access these pages at all, they would return error 403, as I found with Zoopla.co.uk&lt;/p&gt;\n\n&lt;p&gt;Is there a way around this or some other issue I maybe haven&amp;#39;t thought of?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bgbxo", "is_robot_indexable": true, "report_reasons": null, "author": "callumbous", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bgbxo/error_429_when_scraping_data_with_pyhton/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bgbxo/error_429_when_scraping_data_with_pyhton/", "subreddit_subscribers": 1033798, "created_utc": 1693995140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My new company has given me a $5k learning budget. For the last 9 years, I always have been heads down working and never thought of taking courses. I searched this channel, as well as Coursera, EdX and Udemy for courses, but their are either very basic or are $10k+.  \n\nDoes anybody have a good idea what are good advance courses that are worth taking?", "author_fullname": "t2_3w5z8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advanced DS/DA courses after 9 years in area", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bgbsq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693995127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My new company has given me a $5k learning budget. For the last 9 years, I always have been heads down working and never thought of taking courses. I searched this channel, as well as Coursera, EdX and Udemy for courses, but their are either very basic or are $10k+.  &lt;/p&gt;\n\n&lt;p&gt;Does anybody have a good idea what are good advance courses that are worth taking?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bgbsq", "is_robot_indexable": true, "report_reasons": null, "author": "kingrandow", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bgbsq/looking_for_advanced_dsda_courses_after_9_years/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bgbsq/looking_for_advanced_dsda_courses_after_9_years/", "subreddit_subscribers": 1033798, "created_utc": 1693995127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello Everyone,\n\nI've put together an [**Ultimate dbt-utils Cheat Sheet**](https://datacoves.com/post/dbt-utils-cheatsheet) that I believe can aid those utilizing dbt in their data science endeavors. \n\nThe dbt-utils package has:\n\n* SQL generators for effective data manipulation.\n* Data validation strategies.\n* Introspective macros for better data comprehension.\n\nI regularly share helpful content over at Datacoves. \n\nWould love to know if you find this resource helpful or if there are any other dbt areas you'd like a deep dive into!", "author_fullname": "t2_fosm1pwyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From me to you: Handy dbt-utils Cheat Sheet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b4y7a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693958979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve put together an &lt;a href=\"https://datacoves.com/post/dbt-utils-cheatsheet\"&gt;&lt;strong&gt;Ultimate dbt-utils Cheat Sheet&lt;/strong&gt;&lt;/a&gt; that I believe can aid those utilizing dbt in their data science endeavors. &lt;/p&gt;\n\n&lt;p&gt;The dbt-utils package has:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SQL generators for effective data manipulation.&lt;/li&gt;\n&lt;li&gt;Data validation strategies.&lt;/li&gt;\n&lt;li&gt;Introspective macros for better data comprehension.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I regularly share helpful content over at Datacoves. &lt;/p&gt;\n\n&lt;p&gt;Would love to know if you find this resource helpful or if there are any other dbt areas you&amp;#39;d like a deep dive into!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?auto=webp&amp;s=c72294a445c689d26088117fc5fff4bb905f488d", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7997828bba9f07448ccf1bc5fc04e76033b71944", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7195abbc2a5d6cdc443796e0ed5f9086f845e24d", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33d24d3f4487eaf5c747d62b56db72b2c8b84ebf", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=545aeb79a24b1e25d483c5cd733b6ca413f25389", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7207869fedc316bad07e0b593dcb7995e5baae33", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=42828b93ff9d50484b7639ebe782f4de5c04e40c", "width": 1080, "height": 564}], "variants": {}, "id": "fRjQkbaAYceSqHYVE-NBwTYR2ZoPUS8MUARvVnek75k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b4y7a", "is_robot_indexable": true, "report_reasons": null, "author": "Data-Queen-Mayra", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b4y7a/from_me_to_you_handy_dbtutils_cheat_sheet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b4y7a/from_me_to_you_handy_dbtutils_cheat_sheet/", "subreddit_subscribers": 1033798, "created_utc": 1693958979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi folks I'm studying data science for a while and I've done some real projects and now I'm looking for getting my first data science job so I'm searching for a mentor who can guide me to get that.\n\nso if you know anyone would you give me suggestions plz?\n\n&amp;#x200B;\n\nthat's my Linkedin: [https://www.linkedin.com/in/ahmed-mayalou-043885267/](https://www.linkedin.com/in/ahmed-mayalou-043885267/)\n\nand that's my Kaggle: [https://www.kaggle.com/ahmedmyalo/code](https://www.kaggle.com/ahmedmyalo/code)", "author_fullname": "t2_j40ui2c69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data science mentor for getting my first job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b3lr3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693955617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks I&amp;#39;m studying data science for a while and I&amp;#39;ve done some real projects and now I&amp;#39;m looking for getting my first data science job so I&amp;#39;m searching for a mentor who can guide me to get that.&lt;/p&gt;\n\n&lt;p&gt;so if you know anyone would you give me suggestions plz?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;that&amp;#39;s my Linkedin: &lt;a href=\"https://www.linkedin.com/in/ahmed-mayalou-043885267/\"&gt;https://www.linkedin.com/in/ahmed-mayalou-043885267/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;and that&amp;#39;s my Kaggle: &lt;a href=\"https://www.kaggle.com/ahmedmyalo/code\"&gt;https://www.kaggle.com/ahmedmyalo/code&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b3lr3", "is_robot_indexable": true, "report_reasons": null, "author": "AhmedMyalo11", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b3lr3/data_science_mentor_for_getting_my_first_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b3lr3/data_science_mentor_for_getting_my_first_job/", "subreddit_subscribers": 1033798, "created_utc": 1693955617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Ever wondered about the potential of R for diverse applications beyond statistics? Rafael Camargo, a Spatial Data Scientist at Quantis, deeply delves into Berlin's flourishing R User Group (RUG).\n\nThe blog also features Rafael's personal journey with R, from automating tasks to exploring machine learning, offering a rich perspective on the tool's versatility. \n\nThe Berlin RUG is actively looking for venue sponsors for their in-person events. It's a unique chance to align your brand with innovation and thought leadership in Data Science.\n\n\ud83d\udd17Read more: [https://www.r-consortium.org/blog/2023/09/05/spatial-data-science-using-r-in-berlin-germany](https://www.r-consortium.org/blog/2023/09/05/spatial-data-science-using-r-in-berlin-germany) \n\nLet's keep the spirit of collaboration and learning alive!", "author_fullname": "t2_6ow4kclla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unlocking R's Potential Beyond Stats: Inside Berlin's R User Group with Rafael Camargo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b0jp9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693948747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever wondered about the potential of R for diverse applications beyond statistics? Rafael Camargo, a Spatial Data Scientist at Quantis, deeply delves into Berlin&amp;#39;s flourishing R User Group (RUG).&lt;/p&gt;\n\n&lt;p&gt;The blog also features Rafael&amp;#39;s personal journey with R, from automating tasks to exploring machine learning, offering a rich perspective on the tool&amp;#39;s versatility. &lt;/p&gt;\n\n&lt;p&gt;The Berlin RUG is actively looking for venue sponsors for their in-person events. It&amp;#39;s a unique chance to align your brand with innovation and thought leadership in Data Science.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd17Read more: &lt;a href=\"https://www.r-consortium.org/blog/2023/09/05/spatial-data-science-using-r-in-berlin-germany\"&gt;https://www.r-consortium.org/blog/2023/09/05/spatial-data-science-using-r-in-berlin-germany&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s keep the spirit of collaboration and learning alive!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/M_7602wFXzkSJXA2lWBADY35BbVuEEi8w3O3r_rhxk0.jpg?auto=webp&amp;s=5f66f0f109e1ec1b9675818171d3cf3c3abb011a", "width": 536, "height": 322}, "resolutions": [{"url": "https://external-preview.redd.it/M_7602wFXzkSJXA2lWBADY35BbVuEEi8w3O3r_rhxk0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a1bc172172be63c21a412692c45873e62f18cabb", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/M_7602wFXzkSJXA2lWBADY35BbVuEEi8w3O3r_rhxk0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4001305e6c5c236efea8f24aa143856146258ff8", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/M_7602wFXzkSJXA2lWBADY35BbVuEEi8w3O3r_rhxk0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=80699aaf841b63637c9795be345d1c370f266f84", "width": 320, "height": 192}], "variants": {}, "id": "PbMwLAkOMtIFWEg3WA69xWXz5OQ8EkpX9n3Lh7jEQ9I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b0jp9", "is_robot_indexable": true, "report_reasons": null, "author": "Interesting_Chance31", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b0jp9/unlocking_rs_potential_beyond_stats_inside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b0jp9/unlocking_rs_potential_beyond_stats_inside/", "subreddit_subscribers": 1033798, "created_utc": 1693948747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently embarking on my first experience as a Data Scientist during my master's degree internship in a big and well known company, and I am also working on my thesis at the same time. As a newcomer to this field, I am facing several challenges. I persuaded my manager to let me tackle an ambitious deep learning project, but I have found myself struggling on my own to overcome various obstacles. Handling a complex project independently and dealing with structuring and managing code optimization has been challenging. I encountered difficulties collaborating with my team members and regret not seeking assistance from others.\n\nI do not have a background in computer science, and this is my very first experience in a corporate environment. This situation has been quite demotivating for me.\n\nHow can I bounce back, address my weaknesses, and position myself for success in a real Data Scientist role once I graduate?", "author_fullname": "t2_8xgr0r83", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bad first experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b08p5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693948081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently embarking on my first experience as a Data Scientist during my master&amp;#39;s degree internship in a big and well known company, and I am also working on my thesis at the same time. As a newcomer to this field, I am facing several challenges. I persuaded my manager to let me tackle an ambitious deep learning project, but I have found myself struggling on my own to overcome various obstacles. Handling a complex project independently and dealing with structuring and managing code optimization has been challenging. I encountered difficulties collaborating with my team members and regret not seeking assistance from others.&lt;/p&gt;\n\n&lt;p&gt;I do not have a background in computer science, and this is my very first experience in a corporate environment. This situation has been quite demotivating for me.&lt;/p&gt;\n\n&lt;p&gt;How can I bounce back, address my weaknesses, and position myself for success in a real Data Scientist role once I graduate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b08p5", "is_robot_indexable": true, "report_reasons": null, "author": "Weak_Two_6732", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b08p5/bad_first_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b08p5/bad_first_experience/", "subreddit_subscribers": 1033798, "created_utc": 1693948081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi All! I am currently a Data Analyst at an Investment Bank and I really want to get my masters in Data Science. I just graduated with my BS in Business Administration but concentrated in Data Analytics and currently work with tools like R and Alteryx. I absolutely love data science! \n\nHowever, money is tight for my family and I and I would like to pay down my undergrad loans rather than accumulate more during my pursuit of a masters degree.\n\nMy company will help pay for the degree as long as I continue working for them. So part time is really my only option at this point if I don\u2019t want to take out tens of thousands of more dollars in loans. \n\nThere are some professional programs out there (Northwestern, Johns Hopkins)  that allow MS in data science part time. However, I notice that these are offered through alternative schools (Northwestern SPS vs Northwestern McCormick school of engineering and JHU Engineering for professionals vs JHU Whiting School of Engineering) \n\nTo those familiar - Is the difference extreme? Like, will my MS degree be from JHU Whiting School of Engineering for professionals or will it be from JHU Whiting School of Engineering? Is the coursework as comprehensive / rigorous and are the professors experienced and accessible? \n\nHoping for some clarity as I haven\u2019t seen much discussion on these types of schools in this sub. TYIA", "author_fullname": "t2_ct0r9u9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MS in Data Science Professional Programs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16aylh9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693944489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All! I am currently a Data Analyst at an Investment Bank and I really want to get my masters in Data Science. I just graduated with my BS in Business Administration but concentrated in Data Analytics and currently work with tools like R and Alteryx. I absolutely love data science! &lt;/p&gt;\n\n&lt;p&gt;However, money is tight for my family and I and I would like to pay down my undergrad loans rather than accumulate more during my pursuit of a masters degree.&lt;/p&gt;\n\n&lt;p&gt;My company will help pay for the degree as long as I continue working for them. So part time is really my only option at this point if I don\u2019t want to take out tens of thousands of more dollars in loans. &lt;/p&gt;\n\n&lt;p&gt;There are some professional programs out there (Northwestern, Johns Hopkins)  that allow MS in data science part time. However, I notice that these are offered through alternative schools (Northwestern SPS vs Northwestern McCormick school of engineering and JHU Engineering for professionals vs JHU Whiting School of Engineering) &lt;/p&gt;\n\n&lt;p&gt;To those familiar - Is the difference extreme? Like, will my MS degree be from JHU Whiting School of Engineering for professionals or will it be from JHU Whiting School of Engineering? Is the coursework as comprehensive / rigorous and are the professors experienced and accessible? &lt;/p&gt;\n\n&lt;p&gt;Hoping for some clarity as I haven\u2019t seen much discussion on these types of schools in this sub. TYIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16aylh9", "is_robot_indexable": true, "report_reasons": null, "author": "NewManufacturer3888", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16aylh9/ms_in_data_science_professional_programs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16aylh9/ms_in_data_science_professional_programs/", "subreddit_subscribers": 1033798, "created_utc": 1693944489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " *Hi guys, i'm starting an email newsletter on practical uses with AI/ChatGPT. The following will be my content for my first email. Please let me know what you think! Any thoughts, refinements, etc.*\n\n[https://superpineapple.beehiiv.com/subscribe](https://superpineapple.beehiiv.com/subscribe)\n\n# Step-By-Step\n\nIt\u2019s not as simple as asking ChatGPT - \u201ctell me whether I should invest in Company A\u201d. You need a plan, and you need choose the right tools.\n\n### Getting Ready\n\n**Prerequisites**: ChatGPT Plus\n\n**Overall Objective:** You will want answers to the key questions below (\u201cObjectives\u201d), as well as anything you think are relevant to help your investment decision\n\n**Installation:** Install the following plugins with the GPT4 Model - \u201cCompany Transcript\u201d, \u201cPolygon\u201d, \u201cAI PDF\u201d, \u201cBrowser\u201d, \u201cBrowserOp\u201d\n\n### Phase 1 - First Hand Research\n\nPlugins: \u201cCompany Transcript\u201d, \u201cPolygon\u201d, \u201cAI PDF\u201d\n\n**Step 1 (Learn from Earning Transcripts):** Use \u201cCompany Transcript\u201d to get answers from recent earning calls. You can literally use prompts like *\u201cHow has X performed\u201d*, and *\u201cwhat is A\u2019s strategy for growth and profitability\u201d*\n\n**Step 2 (Learn from Annual / Quarterly Reports)**: Use \u201cPolygon\u201d pull data from recent annual/quarterly reports. Alternatively, you can use \u201cAI PDF\u201d. Get the pdf links on these company's investor relations site (e.g. [***AMD***](https://ir.amd.com/sec-filings/filter/annual-filings?utm_source=superpineapple.beehiiv.com&amp;utm_medium=newsletter&amp;utm_campaign=chatgpt-investment-research)), and query the annual/quarterly report more directly. Similar questions apply\n\nNote: You will probably need to ask follow-up and clarifying questions as you go along, but the idea is to have ChatGPT read these lengthy docs for you.\n\n### Phase 2 - Analyst Reports\n\nPlugins: \u201cBrowser and \u201cBrowserOp\u201d\n\n**Step 3 (Get your list of articles):** Use \u201cBrowser\u201d to get a list of articles from the internet that contain analyst reports and recommendations. An example - *\u201cFind me analyst articles on AMD that give an opinion on their current performance and future prospects. Summarize them\u201d*\n\n**Step 4 (Dig into the articles)**: Use \u201cBrowserOp\u201d to actually access the relevant articles and dig into details. My favorite prompt is as follows - *\u201cCan you use BrowserOp to access these articles and collate the information across all these articles. I would like the depth and reasons behind the recommendations. Present everything in a thesis / anti-thesis format as to whether I should* buy AMD stock\u201d\n\nNote: Follow up questions are definitely needed here as well. Try to have a conversation with ChatGPT to sense check the recommendations.\n\n# Objectives\n\nI like to be able to get a robust and detailed answer to these questions, to help with my investment decisions. Feel free to steal them as prompts!\n\n* **Business Model:** How does the company make money?\n* **Demand Dynamics:** Are its products or services in demand, and why?\n* **Historical Performance:** How has the company performed in the past?\n* **Leadership:** Are talented, experienced managers in charge?\n* **Growth Prospects:** Is the company positioned for growth and profitability?\n* **Financial Health:** How much debt does the company have?\n* **Industry Analysis:** How is the company\u2019s industry doing as a whole?\n* **Challenges:** What are the obstacles and challenges the company faces?\n* **Risks:** Does the company face any economic, political, or cultural risks?\n\nYou also want to know these metrics. Compare the company\u2019s metrics to the broader market, and their specific industry.\n\n* **EPS (Earnings Per Share)**\n* **P/E Ratio (Price to Earnings Ratio)**\n* **Price to Sales Ratio**\n* **Debt to Equity Ratio**", "author_fullname": "t2_8xwepztbe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ChatGPT + Investment Research", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ay5p6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693943515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Hi guys, i&amp;#39;m starting an email newsletter on practical uses with AI/ChatGPT. The following will be my content for my first email. Please let me know what you think! Any thoughts, refinements, etc.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://superpineapple.beehiiv.com/subscribe\"&gt;https://superpineapple.beehiiv.com/subscribe&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Step-By-Step&lt;/h1&gt;\n\n&lt;p&gt;It\u2019s not as simple as asking ChatGPT - \u201ctell me whether I should invest in Company A\u201d. You need a plan, and you need choose the right tools.&lt;/p&gt;\n\n&lt;h3&gt;Getting Ready&lt;/h3&gt;\n\n&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: ChatGPT Plus&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Overall Objective:&lt;/strong&gt; You will want answers to the key questions below (\u201cObjectives\u201d), as well as anything you think are relevant to help your investment decision&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Installation:&lt;/strong&gt; Install the following plugins with the GPT4 Model - \u201cCompany Transcript\u201d, \u201cPolygon\u201d, \u201cAI PDF\u201d, \u201cBrowser\u201d, \u201cBrowserOp\u201d&lt;/p&gt;\n\n&lt;h3&gt;Phase 1 - First Hand Research&lt;/h3&gt;\n\n&lt;p&gt;Plugins: \u201cCompany Transcript\u201d, \u201cPolygon\u201d, \u201cAI PDF\u201d&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 1 (Learn from Earning Transcripts):&lt;/strong&gt; Use \u201cCompany Transcript\u201d to get answers from recent earning calls. You can literally use prompts like &lt;em&gt;\u201cHow has X performed\u201d&lt;/em&gt;, and &lt;em&gt;\u201cwhat is A\u2019s strategy for growth and profitability\u201d&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 2 (Learn from Annual / Quarterly Reports)&lt;/strong&gt;: Use \u201cPolygon\u201d pull data from recent annual/quarterly reports. Alternatively, you can use \u201cAI PDF\u201d. Get the pdf links on these company&amp;#39;s investor relations site (e.g. &lt;a href=\"https://ir.amd.com/sec-filings/filter/annual-filings?utm_source=superpineapple.beehiiv.com&amp;amp;utm_medium=newsletter&amp;amp;utm_campaign=chatgpt-investment-research\"&gt;&lt;strong&gt;&lt;em&gt;AMD&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;), and query the annual/quarterly report more directly. Similar questions apply&lt;/p&gt;\n\n&lt;p&gt;Note: You will probably need to ask follow-up and clarifying questions as you go along, but the idea is to have ChatGPT read these lengthy docs for you.&lt;/p&gt;\n\n&lt;h3&gt;Phase 2 - Analyst Reports&lt;/h3&gt;\n\n&lt;p&gt;Plugins: \u201cBrowser and \u201cBrowserOp\u201d&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 3 (Get your list of articles):&lt;/strong&gt; Use \u201cBrowser\u201d to get a list of articles from the internet that contain analyst reports and recommendations. An example - &lt;em&gt;\u201cFind me analyst articles on AMD that give an opinion on their current performance and future prospects. Summarize them\u201d&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 4 (Dig into the articles)&lt;/strong&gt;: Use \u201cBrowserOp\u201d to actually access the relevant articles and dig into details. My favorite prompt is as follows - &lt;em&gt;\u201cCan you use BrowserOp to access these articles and collate the information across all these articles. I would like the depth and reasons behind the recommendations. Present everything in a thesis / anti-thesis format as to whether I should&lt;/em&gt; buy AMD stock\u201d&lt;/p&gt;\n\n&lt;p&gt;Note: Follow up questions are definitely needed here as well. Try to have a conversation with ChatGPT to sense check the recommendations.&lt;/p&gt;\n\n&lt;h1&gt;Objectives&lt;/h1&gt;\n\n&lt;p&gt;I like to be able to get a robust and detailed answer to these questions, to help with my investment decisions. Feel free to steal them as prompts!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Business Model:&lt;/strong&gt; How does the company make money?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Demand Dynamics:&lt;/strong&gt; Are its products or services in demand, and why?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Historical Performance:&lt;/strong&gt; How has the company performed in the past?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Leadership:&lt;/strong&gt; Are talented, experienced managers in charge?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Growth Prospects:&lt;/strong&gt; Is the company positioned for growth and profitability?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Financial Health:&lt;/strong&gt; How much debt does the company have?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Industry Analysis:&lt;/strong&gt; How is the company\u2019s industry doing as a whole?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Challenges:&lt;/strong&gt; What are the obstacles and challenges the company faces?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Risks:&lt;/strong&gt; Does the company face any economic, political, or cultural risks?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You also want to know these metrics. Compare the company\u2019s metrics to the broader market, and their specific industry.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;EPS (Earnings Per Share)&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;P/E Ratio (Price to Earnings Ratio)&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Price to Sales Ratio&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Debt to Equity Ratio&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?auto=webp&amp;s=88c7167e5ad8150d2f429e247a76802e8a1a6148", "width": 1199, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e1e1bd23c89f7ad647b1de6fde23ca337ffeacca", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400a1a2a825ef770ed8f56f3088f813cd2de9462", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e035677ebc7d242bebdf9e1b9fb8d252c9461f0", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=421af7e1068f8578afffb95812da82b11ae641fc", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5894c1be66680f613b52a21831ac258135a3159d", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f50e7e5a490938cb5bca5b92f55b1a2ce2614267", "width": 1080, "height": 1080}], "variants": {}, "id": "VImqdnKEmjE76MlFz5BQ0zc2dSL9Ya9-VTTUuw96kgE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ay5p6", "is_robot_indexable": true, "report_reasons": null, "author": "saasthom", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ay5p6/chatgpt_investment_research/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ay5p6/chatgpt_investment_research/", "subreddit_subscribers": 1033798, "created_utc": 1693943515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\u201cMCMC vs VI\u201d is no longer a discussion about your favourite Roman numeral. If you share my trepidation for model performance in the face of data sparsity, or you simply suffer from anxiety uncertainty, you might be tempted into the Bayesian world. Years later at the precipice of your career (and mental health degeneracy) you over-engineer probabilistic models so intractable that would stress Lord Bayes himself into stomach ulcers. The solution? Approximate inference, the true antihero to model simplification. I wrote a brief primer for those who enjoy maths and those who disdain it, in both cases it's impossible to avoid using maths while discussing Bayesian statistics so I kept it as light as I could.\n\nPS - This is a Reddit-friendly copypasta from my medium article, so if you're a visual person then head over there to get the visuals.\n\n**Bayesian Modelling**\n\nA closed-form solution to a machine learning model is one that can be written down on a sheet of paper using a finite number of standard mathematical operations. For example, linear models have closed-form solutions IF the design covariance matrix is invertible, otherwise we obtain a solution using iterative optimisation.\n\nBayesian models do not typically have exact closed-form solutions for their posterior distributions. One thing that typically helps is choosing simple models, Gaussian likelihood functions and conjugate priors. A prior distribution is said to be conjugate to a likelihood function if the resulting posterior belongs to the same distribution family as the prior.\n\nBayesian linear regression is a model that typically assumes Gaussian priors over both the regression coefficients and the likelihood function. When we update the prior with the observed data (using Bayes\u2019 theorem), the resulting posterior distribution for the regression coefficients will also follow a normal distribution. This can be written down analytically and sampled using standard methods in Python.\n\nConjugacy, however, does not always guarantee tractability. High-dimensional parameter spaces, hierarchical structures, non-Gaussian likelihoods with non-linear prior interactions can give rise to intractable integrals for the normalisation constant (which involve over the entire parameter space). This actually becomes prohibitive when want to build, say, a Multivariate Gaussian Linear Regression model with many predictors, or when we want to model count data using a Poisson likelihood and control for overfitting using on using a Laplace (non-conjugate) prior. Thankfully, a solution as old as the first computers comes to the rescue: Markov Chain Monte Carlo (MCMC).\n\n**Markov Chain Monte Carlo**\n\nMarkov Chain Monte Carlo can be described with enough mathematical jargon to send one fleeing back to first-derivative optimisers, so I\u2019ll skip the stomach ulcers and give an intuitive overview instead.\n\nGiven a probabilistic model parameterised by latent continuous random variables z, and observed values x, we can write down the known form for its probability density function P(z | x). If P(z | x) is intractable, we want to to generate an empirical distribution of samples based on a Markov chain that approximates the probability distribution.\n\nThis empirical distribution can then be used in place of the analytical solution to estimate posterior means, variances, quantiles, and other probabilistic summaries of the model parameters. The most important question: who is Markov and why are we talking about his chain?\n\nIn MCMC, a Markov chain is simply a sequence of samples where each sample is \u201cmemoryless\u201d, i.e. the probability of transitioning to the next sample depends only on the current sample and not on the previous history. This helps us reach a \u201cstationary\u201d distribution over samples, i.e. when we run the chain long enough, the probability P(0 &lt; z &lt; 1 | x)\\_{n} at iteration n and P(0 &lt; z &lt; 1 | x)\\_{m} at iteration m should be equal.\n\nWhat\u2019s amazing is that the distribution over samples from our Markov chain provides asymptotic exactness; MCMC converges to the true posterior distribution in the limit of infinite samples. How is this implemented in practice?\n\n**The Metropolis-Hastings (MH) Algorithm**\n\nMetropolis-Hastings (MH) is a specific type of (MCMC) algorithm ubiquitously used in approximate inference. The idea is to build a chain of samples with a proposal distribution that selects \u201cthe next\u201d sample based only on the \u201cthe current\u201d sample (remember the Markov principle).\n\nProposed samples with higher probabilities in our posterior are accepted into the chain more frequently and those with lower probabilities are rejected more often (don\u2019t make it into the chain). How do we capture the \u201ctails\u201d of our posterior if we\u2019re busy focusing on high probability regions?\n\nThis is where Metropolis-Hastings acceptance/rejection mechanism really shines:\n\nFor any given proposed sample, we define acceptance probability = min(1, \u03b1), where \u03b1 as the ratio of target and proposal distributions at proposed and current samples.\n\nNext comes the heart of the algorithm\u2019s exploration-exploitation mechanism: We generate a random number n in the domain \\[0,1\\]. If n \u2264 \u03b1, accept the new sample in the chain; if n &gt; \u03b1, keep the current sample and don\u2019t extend the chain.\n\nThe best bit? Samples with acceptance probabilities close to 1 are more likely to move the chain towards higher probability regions. Those with low acceptance probabilities can be accepted when 0 \u2264 n \u2264 \u03b1, exploring lower probability areas and avoiding local modes.\n\nWhat diagnostics do we run to check that MH successfully converged to a stationary empirical approximation to the posterior?\n\n**Trace Plots and Chain Mixing**\n\nA \u201ctrace plot\u201d allows us to inspect the chain by plotting accepted sample values for each iteration.\n\nWhat we\u2019re looking for is low autocorrelation between successive samples, and full exploration of the sample space characterised by high variance across moving windows of the trace. Chain 1 is an example of an ideal trace. Chain 2 initially has high autocorrelation and low variance but converges to stationarity after iteration t \\~1500. We discard the head segment t &lt; 1500 (so-called burn-in samples) since they\u2019re unlikely to be part of our target distribution.\n\nWhat about Chain 3? This trace demonstrates poor chain mixing, moving slowly across the parameter space between different regions of the distribution. One problem could be that we just haven\u2019t let the algorithm run long enough; but Model complexity increases with multimodality, high dimensionality and correlated parameters the asymptotic exactness guarantee of MCMC doesn\u2019t come with a tqdm, you could be waiting for quite a while. In these cases, we present the next best thing: variational inference.\n\n**Variatonal Inference**\n\nWhile MCMC offers asymptotic exactness around high dimensional distributions, it can be computationally intensive and impractical for complex distributions. Often we\u2019re just interested in a rough approximation to the posterior that scales well for deployment.\n\nVariational Inference (VI) frames the problem of approximating the posterior as an optimisation problem. Starting with a synthetic posterior Q(z | \u03bb) built from families of simpler distributions (known as the variational family), we optimise over parameters \u03bb that minimise the distance between the variational family and the true posterior P(z | x). This sounds cool but how do we choose the variational family? What even is a distance between distributions?\n\n**KL-Divergence**\n\nThe choice of Q(z | \u03bb) depends on the degree of flexibility required (increasing with the complexity of P(z | x)), but common choices are exponential or Gaussian distributions. To compare the \u201ccloseness\u201d of P and Q, we employ a similarity measure such as the Kullback-Leibler (KL) divergence:\n\nAlthough the KL divergence is asymmetric (DKL(Q||P) =/= DKL(P||Q)), it helps to quantify the difference between Q and P.\n\n\u201cBut we don\u2019t have a closed form solution for P(z|x)!\u201d I hear you exclaiming correctly. That\u2019s why we compute something called the Evidence Lower Bound (ELBO) instead: ELBO(\u03bb) = log( P(x) ) \u2212DKL(Q(z | \u03bb)||P(z | x)). This can be rearranged as ELBO(\u03bb) = E\u200b\\[log P(x, z)\\]\u2212E\u200b\\[log Q(z\u2223\u03bb)\\] helps us avoid that pesky intractable marginal integral.\n\nThus, maximizing ELBO is equivalent to minimizing the KL divergence, serves as an objective function we optimise using standard methods like co-ordinate ascent. Once \u03bb are optimised, the approximating distribution Q(z | \u03bb) serves as a surrogate for the true posterior. This approximation can then be used for downstream tasks like prediction, data imputation, or model interpretation.", "author_fullname": "t2_7h1mi6us", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tl;dr Approximate Inference methods made easy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16awjx5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693939860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\u201cMCMC vs VI\u201d is no longer a discussion about your favourite Roman numeral. If you share my trepidation for model performance in the face of data sparsity, or you simply suffer from anxiety uncertainty, you might be tempted into the Bayesian world. Years later at the precipice of your career (and mental health degeneracy) you over-engineer probabilistic models so intractable that would stress Lord Bayes himself into stomach ulcers. The solution? Approximate inference, the true antihero to model simplification. I wrote a brief primer for those who enjoy maths and those who disdain it, in both cases it&amp;#39;s impossible to avoid using maths while discussing Bayesian statistics so I kept it as light as I could.&lt;/p&gt;\n\n&lt;p&gt;PS - This is a Reddit-friendly copypasta from my medium article, so if you&amp;#39;re a visual person then head over there to get the visuals.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Bayesian Modelling&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A closed-form solution to a machine learning model is one that can be written down on a sheet of paper using a finite number of standard mathematical operations. For example, linear models have closed-form solutions IF the design covariance matrix is invertible, otherwise we obtain a solution using iterative optimisation.&lt;/p&gt;\n\n&lt;p&gt;Bayesian models do not typically have exact closed-form solutions for their posterior distributions. One thing that typically helps is choosing simple models, Gaussian likelihood functions and conjugate priors. A prior distribution is said to be conjugate to a likelihood function if the resulting posterior belongs to the same distribution family as the prior.&lt;/p&gt;\n\n&lt;p&gt;Bayesian linear regression is a model that typically assumes Gaussian priors over both the regression coefficients and the likelihood function. When we update the prior with the observed data (using Bayes\u2019 theorem), the resulting posterior distribution for the regression coefficients will also follow a normal distribution. This can be written down analytically and sampled using standard methods in Python.&lt;/p&gt;\n\n&lt;p&gt;Conjugacy, however, does not always guarantee tractability. High-dimensional parameter spaces, hierarchical structures, non-Gaussian likelihoods with non-linear prior interactions can give rise to intractable integrals for the normalisation constant (which involve over the entire parameter space). This actually becomes prohibitive when want to build, say, a Multivariate Gaussian Linear Regression model with many predictors, or when we want to model count data using a Poisson likelihood and control for overfitting using on using a Laplace (non-conjugate) prior. Thankfully, a solution as old as the first computers comes to the rescue: Markov Chain Monte Carlo (MCMC).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Markov Chain Monte Carlo&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Markov Chain Monte Carlo can be described with enough mathematical jargon to send one fleeing back to first-derivative optimisers, so I\u2019ll skip the stomach ulcers and give an intuitive overview instead.&lt;/p&gt;\n\n&lt;p&gt;Given a probabilistic model parameterised by latent continuous random variables z, and observed values x, we can write down the known form for its probability density function P(z | x). If P(z | x) is intractable, we want to to generate an empirical distribution of samples based on a Markov chain that approximates the probability distribution.&lt;/p&gt;\n\n&lt;p&gt;This empirical distribution can then be used in place of the analytical solution to estimate posterior means, variances, quantiles, and other probabilistic summaries of the model parameters. The most important question: who is Markov and why are we talking about his chain?&lt;/p&gt;\n\n&lt;p&gt;In MCMC, a Markov chain is simply a sequence of samples where each sample is \u201cmemoryless\u201d, i.e. the probability of transitioning to the next sample depends only on the current sample and not on the previous history. This helps us reach a \u201cstationary\u201d distribution over samples, i.e. when we run the chain long enough, the probability P(0 &amp;lt; z &amp;lt; 1 | x)_{n} at iteration n and P(0 &amp;lt; z &amp;lt; 1 | x)_{m} at iteration m should be equal.&lt;/p&gt;\n\n&lt;p&gt;What\u2019s amazing is that the distribution over samples from our Markov chain provides asymptotic exactness; MCMC converges to the true posterior distribution in the limit of infinite samples. How is this implemented in practice?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Metropolis-Hastings (MH) Algorithm&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Metropolis-Hastings (MH) is a specific type of (MCMC) algorithm ubiquitously used in approximate inference. The idea is to build a chain of samples with a proposal distribution that selects \u201cthe next\u201d sample based only on the \u201cthe current\u201d sample (remember the Markov principle).&lt;/p&gt;\n\n&lt;p&gt;Proposed samples with higher probabilities in our posterior are accepted into the chain more frequently and those with lower probabilities are rejected more often (don\u2019t make it into the chain). How do we capture the \u201ctails\u201d of our posterior if we\u2019re busy focusing on high probability regions?&lt;/p&gt;\n\n&lt;p&gt;This is where Metropolis-Hastings acceptance/rejection mechanism really shines:&lt;/p&gt;\n\n&lt;p&gt;For any given proposed sample, we define acceptance probability = min(1, \u03b1), where \u03b1 as the ratio of target and proposal distributions at proposed and current samples.&lt;/p&gt;\n\n&lt;p&gt;Next comes the heart of the algorithm\u2019s exploration-exploitation mechanism: We generate a random number n in the domain [0,1]. If n \u2264 \u03b1, accept the new sample in the chain; if n &amp;gt; \u03b1, keep the current sample and don\u2019t extend the chain.&lt;/p&gt;\n\n&lt;p&gt;The best bit? Samples with acceptance probabilities close to 1 are more likely to move the chain towards higher probability regions. Those with low acceptance probabilities can be accepted when 0 \u2264 n \u2264 \u03b1, exploring lower probability areas and avoiding local modes.&lt;/p&gt;\n\n&lt;p&gt;What diagnostics do we run to check that MH successfully converged to a stationary empirical approximation to the posterior?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Trace Plots and Chain Mixing&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A \u201ctrace plot\u201d allows us to inspect the chain by plotting accepted sample values for each iteration.&lt;/p&gt;\n\n&lt;p&gt;What we\u2019re looking for is low autocorrelation between successive samples, and full exploration of the sample space characterised by high variance across moving windows of the trace. Chain 1 is an example of an ideal trace. Chain 2 initially has high autocorrelation and low variance but converges to stationarity after iteration t ~1500. We discard the head segment t &amp;lt; 1500 (so-called burn-in samples) since they\u2019re unlikely to be part of our target distribution.&lt;/p&gt;\n\n&lt;p&gt;What about Chain 3? This trace demonstrates poor chain mixing, moving slowly across the parameter space between different regions of the distribution. One problem could be that we just haven\u2019t let the algorithm run long enough; but Model complexity increases with multimodality, high dimensionality and correlated parameters the asymptotic exactness guarantee of MCMC doesn\u2019t come with a tqdm, you could be waiting for quite a while. In these cases, we present the next best thing: variational inference.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Variatonal Inference&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;While MCMC offers asymptotic exactness around high dimensional distributions, it can be computationally intensive and impractical for complex distributions. Often we\u2019re just interested in a rough approximation to the posterior that scales well for deployment.&lt;/p&gt;\n\n&lt;p&gt;Variational Inference (VI) frames the problem of approximating the posterior as an optimisation problem. Starting with a synthetic posterior Q(z | \u03bb) built from families of simpler distributions (known as the variational family), we optimise over parameters \u03bb that minimise the distance between the variational family and the true posterior P(z | x). This sounds cool but how do we choose the variational family? What even is a distance between distributions?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;KL-Divergence&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The choice of Q(z | \u03bb) depends on the degree of flexibility required (increasing with the complexity of P(z | x)), but common choices are exponential or Gaussian distributions. To compare the \u201ccloseness\u201d of P and Q, we employ a similarity measure such as the Kullback-Leibler (KL) divergence:&lt;/p&gt;\n\n&lt;p&gt;Although the KL divergence is asymmetric (DKL(Q||P) =/= DKL(P||Q)), it helps to quantify the difference between Q and P.&lt;/p&gt;\n\n&lt;p&gt;\u201cBut we don\u2019t have a closed form solution for P(z|x)!\u201d I hear you exclaiming correctly. That\u2019s why we compute something called the Evidence Lower Bound (ELBO) instead: ELBO(\u03bb) = log( P(x) ) \u2212DKL(Q(z | \u03bb)||P(z | x)). This can be rearranged as ELBO(\u03bb) = E\u200b[log P(x, z)]\u2212E\u200b[log Q(z\u2223\u03bb)] helps us avoid that pesky intractable marginal integral.&lt;/p&gt;\n\n&lt;p&gt;Thus, maximizing ELBO is equivalent to minimizing the KL divergence, serves as an objective function we optimise using standard methods like co-ordinate ascent. Once \u03bb are optimised, the approximating distribution Q(z | \u03bb) serves as a surrogate for the true posterior. This approximation can then be used for downstream tasks like prediction, data imputation, or model interpretation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16awjx5", "is_robot_indexable": true, "report_reasons": null, "author": "SnooPaintings5866", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16awjx5/tldr_approximate_inference_methods_made_easy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16awjx5/tldr_approximate_inference_methods_made_easy/", "subreddit_subscribers": 1033798, "created_utc": 1693939860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all -\n\nI've been working as a Data Scientist for almost five years. I've hopped around through a couple industries at this point, namely tech, insurance and a non-profit. Over the last two years, I've started to gain an interest in cities and how they're planned and how they can be used. I'm interested in potentially switching into this line of work, but I don't know where to start.\n\n1. Cities are obviously governed by the public sector, but which part of the local government should I be looking at. In Washington DC we have the Office of Planning, but that doesn't really have data jobs from what I've seen so far.\n2. Has anyone had success at different levels of government beyond local?\n3. Are there private sector companies that engage in this line of work?\n4. Are there any specific skills or techniques I should pick up.\n5. Is there any literature I should start/continue reading. The stuff I've read thus far are from policy folks. \n\n**Apologies if this belongs in the weekly entering/transitioning thread.** I looked at the examples, and that thread seems very geared towards new folks or folks who are transitioning from another career. I also didn't find great threads when I searched :/\n\n&amp;#x200B;", "author_fullname": "t2_a071r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Breaking into Urban Informatics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16br1c9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694023022.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all -&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working as a Data Scientist for almost five years. I&amp;#39;ve hopped around through a couple industries at this point, namely tech, insurance and a non-profit. Over the last two years, I&amp;#39;ve started to gain an interest in cities and how they&amp;#39;re planned and how they can be used. I&amp;#39;m interested in potentially switching into this line of work, but I don&amp;#39;t know where to start.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Cities are obviously governed by the public sector, but which part of the local government should I be looking at. In Washington DC we have the Office of Planning, but that doesn&amp;#39;t really have data jobs from what I&amp;#39;ve seen so far.&lt;/li&gt;\n&lt;li&gt;Has anyone had success at different levels of government beyond local?&lt;/li&gt;\n&lt;li&gt;Are there private sector companies that engage in this line of work?&lt;/li&gt;\n&lt;li&gt;Are there any specific skills or techniques I should pick up.&lt;/li&gt;\n&lt;li&gt;Is there any literature I should start/continue reading. The stuff I&amp;#39;ve read thus far are from policy folks. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Apologies if this belongs in the weekly entering/transitioning thread.&lt;/strong&gt; I looked at the examples, and that thread seems very geared towards new folks or folks who are transitioning from another career. I also didn&amp;#39;t find great threads when I searched :/&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16br1c9", "is_robot_indexable": true, "report_reasons": null, "author": "IAteQuarters", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16br1c9/breaking_into_urban_informatics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16br1c9/breaking_into_urban_informatics/", "subreddit_subscribers": 1033798, "created_utc": 1694023022.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys, I\u2019m looking to do some projects to build my portfolio. \nI have some hands on experience with python and sql.\nI\u2019ve also worked in projects involving k means, linear regression. \nI really do want to up-skill by doing more projects, but I\u2019m unable to find any good ones\nCould you guys suggest some projects that I can work on to help me develop key skills for data science?\nThank you. :)", "author_fullname": "t2_8g65jgwz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Projects to build portfolio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16bqtfx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694022507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I\u2019m looking to do some projects to build my portfolio. \nI have some hands on experience with python and sql.\nI\u2019ve also worked in projects involving k means, linear regression. \nI really do want to up-skill by doing more projects, but I\u2019m unable to find any good ones\nCould you guys suggest some projects that I can work on to help me develop key skills for data science?\nThank you. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bqtfx", "is_robot_indexable": true, "report_reasons": null, "author": "yanco03", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bqtfx/projects_to_build_portfolio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bqtfx/projects_to_build_portfolio/", "subreddit_subscribers": 1033798, "created_utc": 1694022507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_74100cyzx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vertex AI and the ML Workflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_16bpbuf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QuvPUscW6_M?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Vertex AI and the ML Workflow\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Vertex AI and the ML Workflow", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QuvPUscW6_M?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Vertex AI and the ML Workflow\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/QuvPUscW6_M/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QuvPUscW6_M?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Vertex AI and the ML Workflow\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/16bpbuf", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dzztQkzDOtHXq3NCF9_qFlvQZZ6yvIAOQR_BYiPeZnU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694019051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/QuvPUscW6_M?si=AetGfpb8gsDRD0nE", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-YPD8BySFZSz8yCslVReRumZZTiTnWCEvnnwqOSmJ3M.jpg?auto=webp&amp;s=01aa8a866dcc18a2b5878ce3622a97acae2d0869", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/-YPD8BySFZSz8yCslVReRumZZTiTnWCEvnnwqOSmJ3M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d7354e3deae5ea21f7cf6721804ae08307f4551", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/-YPD8BySFZSz8yCslVReRumZZTiTnWCEvnnwqOSmJ3M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=97e4d8cc27f6d817f4cbf129a918152f5ea79fe4", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/-YPD8BySFZSz8yCslVReRumZZTiTnWCEvnnwqOSmJ3M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e0d7e07ad8e61f18fab28ca2a7806d814b978609", "width": 320, "height": 240}], "variants": {}, "id": "mtzGT5vBpn8t8bEIjErreC4c4LdRcc_7IfAM3NGHsmQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bpbuf", "is_robot_indexable": true, "report_reasons": null, "author": "derekplates", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bpbuf/vertex_ai_and_the_ml_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/QuvPUscW6_M?si=AetGfpb8gsDRD0nE", "subreddit_subscribers": 1033798, "created_utc": 1694019051.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Vertex AI and the ML Workflow", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QuvPUscW6_M?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Vertex AI and the ML Workflow\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/QuvPUscW6_M/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How do you deal with careless/insufficient effort cases in your data? I have been removing anything that is 2 SD or more from the mean on intra-individual variability and long strings of the same response for each measure. Is there good literature out there on this that I\u2019m missing?", "author_fullname": "t2_okwnq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dealing with careless cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16boky1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694017299.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you deal with careless/insufficient effort cases in your data? I have been removing anything that is 2 SD or more from the mean on intra-individual variability and long strings of the same response for each measure. Is there good literature out there on this that I\u2019m missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16boky1", "is_robot_indexable": true, "report_reasons": null, "author": "jrdubbleu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16boky1/dealing_with_careless_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16boky1/dealing_with_careless_cases/", "subreddit_subscribers": 1033798, "created_utc": 1694017299.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am looking for a remote data scientist to work on a part time basis. Freshers or students are welcome, provided they possess a solid foundation in Python, data visualization, and modeling. If you're interested, please send your resume along with your salary expectations to endlabava@gmail.com", "author_fullname": "t2_ffdv5pbv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hiring - Data Scientist (Part Time). Freshers or students are welcome to apply", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bnpho", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1694018582.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694015268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a remote data scientist to work on a part time basis. Freshers or students are welcome, provided they possess a solid foundation in Python, data visualization, and modeling. If you&amp;#39;re interested, please send your resume along with your salary expectations to &lt;a href=\"mailto:endlabava@gmail.com\"&gt;endlabava@gmail.com&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bnpho", "is_robot_indexable": true, "report_reasons": null, "author": "Several-Donut4969", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bnpho/hiring_data_scientist_part_time_freshers_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bnpho/hiring_data_scientist_part_time_freshers_or/", "subreddit_subscribers": 1033798, "created_utc": 1694015268.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}