{"kind": "Listing", "data": {"after": "t3_16b4pvq", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work for a Fortune 500 company as a data engineer. I was promised when hired, that I would be a data scientist and get plenty of predictive modeling work. I have done some modeling work, but feel like there is no purpose to the work I am doing. We don't have much vision or leadership on the DS side and have a huge focus on DE. I have been in the position for about 1-1/2 year. It seems like we are doing a bunch of busy work to move the data into AWS, pay huge sums of money to Amazon, and get no business value out of the effort. There is a lot of talk about how great AWS is and how much of a game-changer this effort will be. Everyone on the team seems to think it's great and cool what we are doing, but I just cannot grasp the value and am extremely uninterested in the DE work. ", "author_fullname": "t2_9bjl9255", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hate my job - Waste of time?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16aop0z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 129, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 129, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693928982.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693921498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a Fortune 500 company as a data engineer. I was promised when hired, that I would be a data scientist and get plenty of predictive modeling work. I have done some modeling work, but feel like there is no purpose to the work I am doing. We don&amp;#39;t have much vision or leadership on the DS side and have a huge focus on DE. I have been in the position for about 1-1/2 year. It seems like we are doing a bunch of busy work to move the data into AWS, pay huge sums of money to Amazon, and get no business value out of the effort. There is a lot of talk about how great AWS is and how much of a game-changer this effort will be. Everyone on the team seems to think it&amp;#39;s great and cool what we are doing, but I just cannot grasp the value and am extremely uninterested in the DE work. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16aop0z", "is_robot_indexable": true, "report_reasons": null, "author": "Substantial-Lime7107", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16aop0z/hate_my_job_waste_of_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16aop0z/hate_my_job_waste_of_time/", "subreddit_subscribers": 1033087, "created_utc": 1693921498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There's always so much criticism of hiring processes in the tech world, from hating take home tests or the recent post complaining about what looks like a \\~5 minute task if you know SQL.\n\nI'm curious how everyone would realistically redesign / create their own application process since we're so critical of the existing ones. \n\nLet's say you're the hiring manager for a Data science role that you've benchmarked as needing someone with \\~1 to 2 years experience. The job role automatically closes after it's got 1000 applicants... which you get in about a day.\n\nHow do you handle those 1000 applicants? \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_4qacn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would YOU handle Data Science recruitment ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16aox1s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 119, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 119, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693922029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s always so much criticism of hiring processes in the tech world, from hating take home tests or the recent post complaining about what looks like a ~5 minute task if you know SQL.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious how everyone would realistically redesign / create their own application process since we&amp;#39;re so critical of the existing ones. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say you&amp;#39;re the hiring manager for a Data science role that you&amp;#39;ve benchmarked as needing someone with ~1 to 2 years experience. The job role automatically closes after it&amp;#39;s got 1000 applicants... which you get in about a day.&lt;/p&gt;\n\n&lt;p&gt;How do you handle those 1000 applicants? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16aox1s", "is_robot_indexable": true, "report_reasons": null, "author": "Littleish", "discussion_type": null, "num_comments": 89, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16aox1s/how_would_you_handle_data_science_recruitment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16aox1s/how_would_you_handle_data_science_recruitment/", "subreddit_subscribers": 1033087, "created_utc": 1693922029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "We all know about the big tech layoffs this past year.. but has things gotten better?", "author_fullname": "t2_tm0ugms", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has tech started hiring again?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b7i0p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693965661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We all know about the big tech layoffs this past year.. but has things gotten better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b7i0p", "is_robot_indexable": true, "report_reasons": null, "author": "SuchExplanation", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b7i0p/has_tech_started_hiring_again/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b7i0p/has_tech_started_hiring_again/", "subreddit_subscribers": 1033087, "created_utc": 1693965661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello! I am 29, working at a Fortune 50 company in their modeling department, making decent money for a MCOL area. Recently I have realized, big job title, really high compensation doesn\u2019t really motivate me anymore. I still want to switch jobs and try to work on different kind of things but work life balance and healthy work environment has started to become number 1 priority for me. So much so that when I look for job after the current one, I am not willing to compromise my wlb despite the money. One of the reasons, it\u2019s getting difficult for me to think about switching jobs since the current one is the ideal job for me. \n\nI am sharing this to understand if I ma doing something really wrong and could be a career suicide. What do you guys think? Can I keep increasing my TC while having WLB?\n\nThanks!", "author_fullname": "t2_bv171ji2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I setting myself for an unsuccessful career if I have no motivation to climb the corporate ladder?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b194c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693950256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I am 29, working at a Fortune 50 company in their modeling department, making decent money for a MCOL area. Recently I have realized, big job title, really high compensation doesn\u2019t really motivate me anymore. I still want to switch jobs and try to work on different kind of things but work life balance and healthy work environment has started to become number 1 priority for me. So much so that when I look for job after the current one, I am not willing to compromise my wlb despite the money. One of the reasons, it\u2019s getting difficult for me to think about switching jobs since the current one is the ideal job for me. &lt;/p&gt;\n\n&lt;p&gt;I am sharing this to understand if I ma doing something really wrong and could be a career suicide. What do you guys think? Can I keep increasing my TC while having WLB?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b194c", "is_robot_indexable": true, "report_reasons": null, "author": "quite--average", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b194c/am_i_setting_myself_for_an_unsuccessful_career_if/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b194c/am_i_setting_myself_for_an_unsuccessful_career_if/", "subreddit_subscribers": 1033087, "created_utc": 1693950256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Every time I've tested a SQL query and realized that some null somewhere broke my CONCAT statement, or run an R script and realized that I forgot to add \"na.rm = TRUE\" to a function somewhere, I wonder what was going through the development team's minds when they decided to build these tools these ways.\n\nWhy isn't ignoring nulls the default behavior?\n\nGenuinely curious if there's a logic to this that I haven't considered.", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELI5: Why are nulls not ignored by default?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16al6h4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693911891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every time I&amp;#39;ve tested a SQL query and realized that some null somewhere broke my CONCAT statement, or run an R script and realized that I forgot to add &amp;quot;na.rm = TRUE&amp;quot; to a function somewhere, I wonder what was going through the development team&amp;#39;s minds when they decided to build these tools these ways.&lt;/p&gt;\n\n&lt;p&gt;Why isn&amp;#39;t ignoring nulls the default behavior?&lt;/p&gt;\n\n&lt;p&gt;Genuinely curious if there&amp;#39;s a logic to this that I haven&amp;#39;t considered.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16al6h4", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16al6h4/eli5_why_are_nulls_not_ignored_by_default/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16al6h4/eli5_why_are_nulls_not_ignored_by_default/", "subreddit_subscribers": 1033087, "created_utc": 1693911891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_igz592h4w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Data matters more than the model\". Do you agree with that?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16baj8u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693974559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16baj8u", "is_robot_indexable": true, "report_reasons": null, "author": "quynhonaicenter", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16baj8u/data_matters_more_than_the_model_do_you_agree/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16baj8u/data_matters_more_than_the_model_do_you_agree/", "subreddit_subscribers": 1033087, "created_utc": 1693974559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Data science hobby projects I've built so far:\n\n\ud83c\udfda 2012 - \"Rent Scraper\" \u2014 A tool to find me the cheapest apartment to rent by scraping several websites every minute. (Bash, Python)\n\n\ud83d\uddde 2013 - \"Prionews\" \u2014 A system to summarize daily news in an unbiased manner. (Bash, SQL, Python)\n\n\ud83e\udd16 2014 - \"Jarvic\" \u2014 A self-learning chatbot. (Python)\n\n\ud83c\udfa8 2015 - \"Write-Here-Anything\" \u2014 A hard-to-describe website and art project. (JavaScript)\n\n\ud83c\udf10 2016 - \"Learn-Languages\" \u2014 A simple script that prints the top 1,000 most important words to learn in any language, based on the script of the Friends TV show. (Python)\n\n\ud83d\udda5 2017 - \"User-Generator\" \u2014 A Python script that simulates log/data creation for a mobile app, intended for educational purposes. (Python)\n\n\ud83d\udc6c 2018 - \"A/B Testing Redirect\" \u2014 Code to implement an A/B test without using third-party tools. (JavaScript + Python)\n\n\ud83d\udcc8 2019 - \"Simple User Log\" \u2014 A basic analytics tool designed to replace Google Analytics. (JavaScript + Python + Flask)\n\n\ud83d\udc68\u200d\ud83c\udfeb 2020 - \"Best Bet\" \u2014 A game that educates people about the concept of \"expected value.\" (Python + Flask + HTML)\n\n\u2618 2021 - \"Automated Gardener\" \u2014 A hardware project that automatically takes care of my plants. (Python + Bash + Raspberry Pi)\n\n\ud83d\udcb0 2022 - \"BitPanda\\_DCA\" \u2014 A simple automation tool that performs dollar-cost averaging on the Bitpanda platform for me. (Python + Bash)\n\n\ud83e\udd43 2023 - \"WhiskyReturns\" \u2014 A platform that collects data on whisky investments and displays them in a simple chart. (Python, Bash, SQL, HTML, APIs, etc.)\n\nMost of these projects are retired and offline, but they've been invaluable in teaching me about data science and coding. Building a hobby project is never a waste of time. You should start yours!", "author_fullname": "t2_11i3en", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My Last Decade of Data Science Hobby Projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16aid9m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693902224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data science hobby projects I&amp;#39;ve built so far:&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udfda 2012 - &amp;quot;Rent Scraper&amp;quot; \u2014 A tool to find me the cheapest apartment to rent by scraping several websites every minute. (Bash, Python)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\uddde 2013 - &amp;quot;Prionews&amp;quot; \u2014 A system to summarize daily news in an unbiased manner. (Bash, SQL, Python)&lt;/p&gt;\n\n&lt;p&gt;\ud83e\udd16 2014 - &amp;quot;Jarvic&amp;quot; \u2014 A self-learning chatbot. (Python)&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udfa8 2015 - &amp;quot;Write-Here-Anything&amp;quot; \u2014 A hard-to-describe website and art project. (JavaScript)&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udf10 2016 - &amp;quot;Learn-Languages&amp;quot; \u2014 A simple script that prints the top 1,000 most important words to learn in any language, based on the script of the Friends TV show. (Python)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udda5 2017 - &amp;quot;User-Generator&amp;quot; \u2014 A Python script that simulates log/data creation for a mobile app, intended for educational purposes. (Python)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udc6c 2018 - &amp;quot;A/B Testing Redirect&amp;quot; \u2014 Code to implement an A/B test without using third-party tools. (JavaScript + Python)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcc8 2019 - &amp;quot;Simple User Log&amp;quot; \u2014 A basic analytics tool designed to replace Google Analytics. (JavaScript + Python + Flask)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udc68\u200d\ud83c\udfeb 2020 - &amp;quot;Best Bet&amp;quot; \u2014 A game that educates people about the concept of &amp;quot;expected value.&amp;quot; (Python + Flask + HTML)&lt;/p&gt;\n\n&lt;p&gt;\u2618 2021 - &amp;quot;Automated Gardener&amp;quot; \u2014 A hardware project that automatically takes care of my plants. (Python + Bash + Raspberry Pi)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcb0 2022 - &amp;quot;BitPanda_DCA&amp;quot; \u2014 A simple automation tool that performs dollar-cost averaging on the Bitpanda platform for me. (Python + Bash)&lt;/p&gt;\n\n&lt;p&gt;\ud83e\udd43 2023 - &amp;quot;WhiskyReturns&amp;quot; \u2014 A platform that collects data on whisky investments and displays them in a simple chart. (Python, Bash, SQL, HTML, APIs, etc.)&lt;/p&gt;\n\n&lt;p&gt;Most of these projects are retired and offline, but they&amp;#39;ve been invaluable in teaching me about data science and coding. Building a hobby project is never a waste of time. You should start yours!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16aid9m", "is_robot_indexable": true, "report_reasons": null, "author": "mestitomi", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16aid9m/my_last_decade_of_data_science_hobby_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16aid9m/my_last_decade_of_data_science_hobby_projects/", "subreddit_subscribers": 1033087, "created_utc": 1693902224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My job responsibilities are way below my skill level so I\u2019m losing motivation to keep learning. Job hoping doesn\u2019t fix this as every job is underwhelming.", "author_fullname": "t2_kcl3tfwe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle burnout?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16azemj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693946296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My job responsibilities are way below my skill level so I\u2019m losing motivation to keep learning. Job hoping doesn\u2019t fix this as every job is underwhelming.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16azemj", "is_robot_indexable": true, "report_reasons": null, "author": "NewEcho2940", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16azemj/how_do_you_handle_burnout/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16azemj/how_do_you_handle_burnout/", "subreddit_subscribers": 1033087, "created_utc": 1693946296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nI recently created a CLI that lets you compile Jupyter notebooks into FastAPI apps. \n\nIt let\u2019s you write comments like \u201c\u201d\u201d@HTTP\u201d\u201d\u201d, \u201c\u201d\u201d@WS\u201d\u201d\u201d, and \u201c\u201d\u201d@SCHEDULE\u201d\u201d\u201d\u201d to expose your cells as http or websockets endpoints, or programmatically run your cells as scheduled time intervals for simple data pipelines.\n\nIt creates a /build folder with your dockerized code for easy running and deployment.\n\nhttps://github.com/neutrino-ai/neutrino-notebooks\n\nHope you find this helpful! I would appreciate any feedback", "author_fullname": "t2_2rm8fild", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A CLI that turns notebooks into FastAPI apps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16arf00", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693928015.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I recently created a CLI that lets you compile Jupyter notebooks into FastAPI apps. &lt;/p&gt;\n\n&lt;p&gt;It let\u2019s you write comments like \u201c\u201d\u201d@HTTP\u201d\u201d\u201d, \u201c\u201d\u201d@WS\u201d\u201d\u201d, and \u201c\u201d\u201d@SCHEDULE\u201d\u201d\u201d\u201d to expose your cells as http or websockets endpoints, or programmatically run your cells as scheduled time intervals for simple data pipelines.&lt;/p&gt;\n\n&lt;p&gt;It creates a /build folder with your dockerized code for easy running and deployment.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/neutrino-ai/neutrino-notebooks\"&gt;https://github.com/neutrino-ai/neutrino-notebooks&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hope you find this helpful! I would appreciate any feedback&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?auto=webp&amp;s=5394e103b0086b4eff4d06568f294956507f0ecb", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=43271c21e8a34542bc37693cfaf1dbacc436d23f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9701d58c16fd8ce19979b7db2dc7bdd8c459dd59", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=711a7236b9019520b595a3090b2f212b668efe75", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=03ab1ab20ef97b7eff2703ac8562037562678684", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=97db8e91a7af642bc87a0d9454c87ad4c0286dac", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ec562818e6bf8236d636bae3338627ea6dfd4584", "width": 1080, "height": 540}], "variants": {}, "id": "h0H4QH_NNA8eb-artCZYZeYRXgCLX9TCOO7CamwnaCk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16arf00", "is_robot_indexable": true, "report_reasons": null, "author": "gibbybutwithrandck", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16arf00/a_cli_that_turns_notebooks_into_fastapi_apps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16arf00/a_cli_that_turns_notebooks_into_fastapi_apps/", "subreddit_subscribers": 1033087, "created_utc": 1693928015.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am working with a music genre classifier problem where the dependent variable has 11 classes and the data is imbalanced(Added picture).\n\nThe problem I am facing is, as far as I know, the best way to deal with these types of problems is to oversample or undersample. But, the issue is this is not a binary classification problem ,rather multi-class classification problem. \n\nThe second approach that I found out was adjusting weights(either by using the argument class\\_weight = 'balanced' in the algorithm like randomforestclassifier or  by computing weight manually like this and using it while fitting any model:\n\n`from sklearn.utils.class_weight import compute_sample_weight`  \n`class_weights = compute_sample_weight(class_weight='balanced', y=y_train)`\n\nMy dilemma is what can be the correct approach here? \n\nThe reason I am asking is because for the normal case(where i split the dataset and directly train a model), I am getting higher macro f1 scores. But, when I am going with the weight approach, I am getting comparatively lower macro f1 scores. \n\n[This is how the dataset looks\\(without any preprocessing\\)](https://preview.redd.it/hdktlckc5kmb1.png?width=1732&amp;format=png&amp;auto=webp&amp;s=ac5149f420ac32690aabf82c658950c66cd97bf8)\n\n[The dependent variable](https://preview.redd.it/4l1qgf0m5kmb1.png?width=580&amp;format=png&amp;auto=webp&amp;s=f5aa138d85431a01dd27271cad76538ad3fc1639)", "author_fullname": "t2_4n24sb1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to understand the right approach for a multi-class classification problem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 16, "top_awarded_type": null, "hide_score": false, "media_metadata": {"hdktlckc5kmb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 13, "x": 108, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b56892572425b4ca3486f65853cdcceec81946ef"}, {"y": 26, "x": 216, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f02d87817c8285b62370680e482f6ebc03b1a34f"}, {"y": 38, "x": 320, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f165cca041290b6e39b86e4b89cb57026477a5db"}, {"y": 77, "x": 640, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3554331a28cb03a2b9d3667e35a80eff14a625c4"}, {"y": 115, "x": 960, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e1197c936ed4ebacba38f76d51bbf45a303c666"}, {"y": 130, "x": 1080, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b765dd16748b9473e87a13ab1c5a1afc5b63fdf4"}], "s": {"y": 209, "x": 1732, "u": "https://preview.redd.it/hdktlckc5kmb1.png?width=1732&amp;format=png&amp;auto=webp&amp;s=ac5149f420ac32690aabf82c658950c66cd97bf8"}, "id": "hdktlckc5kmb1"}, "4l1qgf0m5kmb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 80, "x": 108, "u": "https://preview.redd.it/4l1qgf0m5kmb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f28afa8aee093cc2bfa3188635fea8636f56617c"}, {"y": 160, "x": 216, "u": "https://preview.redd.it/4l1qgf0m5kmb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=25cea1fcfcaa2c2e77a68424354d3f2292f3adcd"}, {"y": 238, "x": 320, "u": "https://preview.redd.it/4l1qgf0m5kmb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8639f89d1d39fe8740541af283837e6e6101d20e"}], "s": {"y": 432, "x": 580, "u": "https://preview.redd.it/4l1qgf0m5kmb1.png?width=580&amp;format=png&amp;auto=webp&amp;s=f5aa138d85431a01dd27271cad76538ad3fc1639"}, "id": "4l1qgf0m5kmb1"}}, "name": "t3_16b9v11", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/EHgERDPQB2tgcNWYKMcstfu9--yCFZYW9DI_PXRAPV8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693972477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with a music genre classifier problem where the dependent variable has 11 classes and the data is imbalanced(Added picture).&lt;/p&gt;\n\n&lt;p&gt;The problem I am facing is, as far as I know, the best way to deal with these types of problems is to oversample or undersample. But, the issue is this is not a binary classification problem ,rather multi-class classification problem. &lt;/p&gt;\n\n&lt;p&gt;The second approach that I found out was adjusting weights(either by using the argument class_weight = &amp;#39;balanced&amp;#39; in the algorithm like randomforestclassifier or  by computing weight manually like this and using it while fitting any model:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;from sklearn.utils.class_weight import compute_sample_weight&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;class_weights = compute_sample_weight(class_weight=&amp;#39;balanced&amp;#39;, y=y_train)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;My dilemma is what can be the correct approach here? &lt;/p&gt;\n\n&lt;p&gt;The reason I am asking is because for the normal case(where i split the dataset and directly train a model), I am getting higher macro f1 scores. But, when I am going with the weight approach, I am getting comparatively lower macro f1 scores. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hdktlckc5kmb1.png?width=1732&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac5149f420ac32690aabf82c658950c66cd97bf8\"&gt;This is how the dataset looks(without any preprocessing)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4l1qgf0m5kmb1.png?width=580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5aa138d85431a01dd27271cad76538ad3fc1639\"&gt;The dependent variable&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b9v11", "is_robot_indexable": true, "report_reasons": null, "author": "ProfessorS11", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b9v11/trying_to_understand_the_right_approach_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b9v11/trying_to_understand_the_right_approach_for_a/", "subreddit_subscribers": 1033087, "created_utc": 1693972477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "A recent survey by GitLab reveals a growing trend among organizations implementing AI in their software development processes, deeming it essential to stay competitive.\n\nTo stay on top of the latest advancements in AI, [look here first.](https://supercharged-ai.beehiiv.com/subscribe?utm_source=reddit&amp;utm_medium=gitlab-ai-coding&amp;utm_campaign=campaign)\n\nhttps://preview.redd.it/cgdc3wpxnkmb1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=6de802d14cba72c972c4b0bcf389308fabbed784\n\n**AI becomes crucial for software development**\n\n* GitLab's report reveals that most respondents (83%) consider AI essential for their software development, regardless of their position, job level, or years of experience.\n* Most organizations have deemed AI adoption successful, with 90% stating confidence in using AI tools daily.\n\n**Areas of AI application and concerns about its integration**\n\n* AI's application in software development extends beyond simply generating codes, focusing more on natural language chatbots, automated test generation, and tracking machine learning model experiments.\n* However, despite the growing adoption, concerns about AI-generated codes lacking copyright protection (48%) and potentially introducing vulnerabilities (39%) are rising.\n* The rising fear of AI replacing existing roles is evident, with 57% predicting that their jobs might be threatened within five years.\n\n**The need for training and the real-world implications of AI integration**\n\n* As AI permeates workplaces, nearly 81% believe they require more training.\n* Interestingly, those with more AI experience were less likely to link it with productivity gains and faster cycle times, highlighting the importance of human verification in AI-generated codes for ensuring error-free, secure, and copyright-compliant production.\n\n[(source)](https://about.gitlab.com/developer-survey/)\n\n**P.S. If you like this kind of analysis,** you\u2019ll love my [free newsletter](https://supercharged-ai.beehiiv.com/subscribe?utm_source=reddit&amp;utm_medium=gitlab-ai-coding&amp;utm_campaign=campaign), which covers the latest advancements in AI. Professionals from Google, Meta, and OpenAI are already on board.", "author_fullname": "t2_h4jb4maul", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitLab survey reveals increasing reliance on AI in software development", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"cgdc3wpxnkmb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8402631e1a2bc281b67be61ca6f3070899f276f1"}, {"y": 113, "x": 216, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=161943aaec81d2624305af5f4bcecf18ed7c47ff"}, {"y": 168, "x": 320, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f7cc9ebd675629554f256b2e2040950bf5f00cb"}, {"y": 337, "x": 640, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9556b2104281cb23b4d4cca6d60840a6af462507"}, {"y": 506, "x": 960, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a6ef7703827dbc3e23e9d4e4391092301c0fc69"}, {"y": 569, "x": 1080, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a46bb64395b772199afe151f6399d3cf9e26c57e"}], "s": {"y": 647, "x": 1226, "u": "https://preview.redd.it/cgdc3wpxnkmb1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=6de802d14cba72c972c4b0bcf389308fabbed784"}, "id": "cgdc3wpxnkmb1"}}, "name": "t3_16bbs07", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/rPQz06Dwfph2KnPWxOJOGKgRmPdZbNB9p1w88oxG6fA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1693978548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A recent survey by GitLab reveals a growing trend among organizations implementing AI in their software development processes, deeming it essential to stay competitive.&lt;/p&gt;\n\n&lt;p&gt;To stay on top of the latest advancements in AI, &lt;a href=\"https://supercharged-ai.beehiiv.com/subscribe?utm_source=reddit&amp;amp;utm_medium=gitlab-ai-coding&amp;amp;utm_campaign=campaign\"&gt;look here first.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cgdc3wpxnkmb1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6de802d14cba72c972c4b0bcf389308fabbed784\"&gt;https://preview.redd.it/cgdc3wpxnkmb1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6de802d14cba72c972c4b0bcf389308fabbed784&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;AI becomes crucial for software development&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;GitLab&amp;#39;s report reveals that most respondents (83%) consider AI essential for their software development, regardless of their position, job level, or years of experience.&lt;/li&gt;\n&lt;li&gt;Most organizations have deemed AI adoption successful, with 90% stating confidence in using AI tools daily.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Areas of AI application and concerns about its integration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AI&amp;#39;s application in software development extends beyond simply generating codes, focusing more on natural language chatbots, automated test generation, and tracking machine learning model experiments.&lt;/li&gt;\n&lt;li&gt;However, despite the growing adoption, concerns about AI-generated codes lacking copyright protection (48%) and potentially introducing vulnerabilities (39%) are rising.&lt;/li&gt;\n&lt;li&gt;The rising fear of AI replacing existing roles is evident, with 57% predicting that their jobs might be threatened within five years.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The need for training and the real-world implications of AI integration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;As AI permeates workplaces, nearly 81% believe they require more training.&lt;/li&gt;\n&lt;li&gt;Interestingly, those with more AI experience were less likely to link it with productivity gains and faster cycle times, highlighting the importance of human verification in AI-generated codes for ensuring error-free, secure, and copyright-compliant production.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://about.gitlab.com/developer-survey/\"&gt;(source)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;P.S. If you like this kind of analysis,&lt;/strong&gt; you\u2019ll love my &lt;a href=\"https://supercharged-ai.beehiiv.com/subscribe?utm_source=reddit&amp;amp;utm_medium=gitlab-ai-coding&amp;amp;utm_campaign=campaign\"&gt;free newsletter&lt;/a&gt;, which covers the latest advancements in AI. Professionals from Google, Meta, and OpenAI are already on board.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?auto=webp&amp;s=8972442682f23755fe3d4d7aea312e1cff5c8512", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ce15b9c726f05a168b1db503df7ab26f60f62501", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=177bdfd4af3db3eb7dfe0b92698bbd1d97974ee8", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3630ec555e6a1910b62043cf2097eb88a6f14ef5", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=312e7a99fa2a2080e445758016e4648398339990", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=87aa77b180bceb48e3af8ae0450b505860d95694", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/m25T0KLl0EPq2gxIYaASkncODJHc8QB_82behpL6X_o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=78860fb6488536b343b979bab9c14c0815d49eb5", "width": 1080, "height": 567}], "variants": {}, "id": "NPZM0p8FtC5HwSLNn0lZ-Kh6AiQlvJ78GtZ_8REUGxc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bbs07", "is_robot_indexable": true, "report_reasons": null, "author": "AIsupercharged", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bbs07/gitlab_survey_reveals_increasing_reliance_on_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bbs07/gitlab_survey_reveals_increasing_reliance_on_ai/", "subreddit_subscribers": 1033087, "created_utc": 1693978548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello Everyone,\n\nI've put together an [**Ultimate dbt-utils Cheat Sheet**](https://datacoves.com/post/dbt-utils-cheatsheet) that I believe can aid those utilizing dbt in their data science endeavors. \n\nThe dbt-utils package has:\n\n* SQL generators for effective data manipulation.\n* Data validation strategies.\n* Introspective macros for better data comprehension.\n\nI regularly share helpful content over at Datacoves. \n\nWould love to know if you find this resource helpful or if there are any other dbt areas you'd like a deep dive into!", "author_fullname": "t2_fosm1pwyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From me to you: Handy dbt-utils Cheat Sheet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b4y7a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693958979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve put together an &lt;a href=\"https://datacoves.com/post/dbt-utils-cheatsheet\"&gt;&lt;strong&gt;Ultimate dbt-utils Cheat Sheet&lt;/strong&gt;&lt;/a&gt; that I believe can aid those utilizing dbt in their data science endeavors. &lt;/p&gt;\n\n&lt;p&gt;The dbt-utils package has:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SQL generators for effective data manipulation.&lt;/li&gt;\n&lt;li&gt;Data validation strategies.&lt;/li&gt;\n&lt;li&gt;Introspective macros for better data comprehension.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I regularly share helpful content over at Datacoves. &lt;/p&gt;\n\n&lt;p&gt;Would love to know if you find this resource helpful or if there are any other dbt areas you&amp;#39;d like a deep dive into!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?auto=webp&amp;s=c72294a445c689d26088117fc5fff4bb905f488d", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7997828bba9f07448ccf1bc5fc04e76033b71944", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7195abbc2a5d6cdc443796e0ed5f9086f845e24d", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33d24d3f4487eaf5c747d62b56db72b2c8b84ebf", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=545aeb79a24b1e25d483c5cd733b6ca413f25389", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7207869fedc316bad07e0b593dcb7995e5baae33", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=42828b93ff9d50484b7639ebe782f4de5c04e40c", "width": 1080, "height": 564}], "variants": {}, "id": "fRjQkbaAYceSqHYVE-NBwTYR2ZoPUS8MUARvVnek75k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b4y7a", "is_robot_indexable": true, "report_reasons": null, "author": "Data-Queen-Mayra", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b4y7a/from_me_to_you_handy_dbtutils_cheat_sheet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b4y7a/from_me_to_you_handy_dbtutils_cheat_sheet/", "subreddit_subscribers": 1033087, "created_utc": 1693958979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\u201cMCMC vs VI\u201d is no longer a discussion about your favourite Roman numeral. If you share my trepidation for model performance in the face of data sparsity, or you simply suffer from anxiety uncertainty, you might be tempted into the Bayesian world. Years later at the precipice of your career (and mental health degeneracy) you over-engineer probabilistic models so intractable that would stress Lord Bayes himself into stomach ulcers. The solution? Approximate inference, the true antihero to model simplification. I wrote a brief primer for those who enjoy maths and those who disdain it, in both cases it's impossible to avoid using maths while discussing Bayesian statistics so I kept it as light as I could.\n\nPS - This is a Reddit-friendly copypasta from my medium article, so if you're a visual person then head over there to get the visuals.\n\n**Bayesian Modelling**\n\nA closed-form solution to a machine learning model is one that can be written down on a sheet of paper using a finite number of standard mathematical operations. For example, linear models have closed-form solutions IF the design covariance matrix is invertible, otherwise we obtain a solution using iterative optimisation.\n\nBayesian models do not typically have exact closed-form solutions for their posterior distributions. One thing that typically helps is choosing simple models, Gaussian likelihood functions and conjugate priors. A prior distribution is said to be conjugate to a likelihood function if the resulting posterior belongs to the same distribution family as the prior.\n\nBayesian linear regression is a model that typically assumes Gaussian priors over both the regression coefficients and the likelihood function. When we update the prior with the observed data (using Bayes\u2019 theorem), the resulting posterior distribution for the regression coefficients will also follow a normal distribution. This can be written down analytically and sampled using standard methods in Python.\n\nConjugacy, however, does not always guarantee tractability. High-dimensional parameter spaces, hierarchical structures, non-Gaussian likelihoods with non-linear prior interactions can give rise to intractable integrals for the normalisation constant (which involve over the entire parameter space). This actually becomes prohibitive when want to build, say, a Multivariate Gaussian Linear Regression model with many predictors, or when we want to model count data using a Poisson likelihood and control for overfitting using on using a Laplace (non-conjugate) prior. Thankfully, a solution as old as the first computers comes to the rescue: Markov Chain Monte Carlo (MCMC).\n\n**Markov Chain Monte Carlo**\n\nMarkov Chain Monte Carlo can be described with enough mathematical jargon to send one fleeing back to first-derivative optimisers, so I\u2019ll skip the stomach ulcers and give an intuitive overview instead.\n\nGiven a probabilistic model parameterised by latent continuous random variables z, and observed values x, we can write down the known form for its probability density function P(z | x). If P(z | x) is intractable, we want to to generate an empirical distribution of samples based on a Markov chain that approximates the probability distribution.\n\nThis empirical distribution can then be used in place of the analytical solution to estimate posterior means, variances, quantiles, and other probabilistic summaries of the model parameters. The most important question: who is Markov and why are we talking about his chain?\n\nIn MCMC, a Markov chain is simply a sequence of samples where each sample is \u201cmemoryless\u201d, i.e. the probability of transitioning to the next sample depends only on the current sample and not on the previous history. This helps us reach a \u201cstationary\u201d distribution over samples, i.e. when we run the chain long enough, the probability P(0 &lt; z &lt; 1 | x)\\_{n} at iteration n and P(0 &lt; z &lt; 1 | x)\\_{m} at iteration m should be equal.\n\nWhat\u2019s amazing is that the distribution over samples from our Markov chain provides asymptotic exactness; MCMC converges to the true posterior distribution in the limit of infinite samples. How is this implemented in practice?\n\n**The Metropolis-Hastings (MH) Algorithm**\n\nMetropolis-Hastings (MH) is a specific type of (MCMC) algorithm ubiquitously used in approximate inference. The idea is to build a chain of samples with a proposal distribution that selects \u201cthe next\u201d sample based only on the \u201cthe current\u201d sample (remember the Markov principle).\n\nProposed samples with higher probabilities in our posterior are accepted into the chain more frequently and those with lower probabilities are rejected more often (don\u2019t make it into the chain). How do we capture the \u201ctails\u201d of our posterior if we\u2019re busy focusing on high probability regions?\n\nThis is where Metropolis-Hastings acceptance/rejection mechanism really shines:\n\nFor any given proposed sample, we define acceptance probability = min(1, \u03b1), where \u03b1 as the ratio of target and proposal distributions at proposed and current samples.\n\nNext comes the heart of the algorithm\u2019s exploration-exploitation mechanism: We generate a random number n in the domain \\[0,1\\]. If n \u2264 \u03b1, accept the new sample in the chain; if n &gt; \u03b1, keep the current sample and don\u2019t extend the chain.\n\nThe best bit? Samples with acceptance probabilities close to 1 are more likely to move the chain towards higher probability regions. Those with low acceptance probabilities can be accepted when 0 \u2264 n \u2264 \u03b1, exploring lower probability areas and avoiding local modes.\n\nWhat diagnostics do we run to check that MH successfully converged to a stationary empirical approximation to the posterior?\n\n**Trace Plots and Chain Mixing**\n\nA \u201ctrace plot\u201d allows us to inspect the chain by plotting accepted sample values for each iteration.\n\nWhat we\u2019re looking for is low autocorrelation between successive samples, and full exploration of the sample space characterised by high variance across moving windows of the trace. Chain 1 is an example of an ideal trace. Chain 2 initially has high autocorrelation and low variance but converges to stationarity after iteration t \\~1500. We discard the head segment t &lt; 1500 (so-called burn-in samples) since they\u2019re unlikely to be part of our target distribution.\n\nWhat about Chain 3? This trace demonstrates poor chain mixing, moving slowly across the parameter space between different regions of the distribution. One problem could be that we just haven\u2019t let the algorithm run long enough; but Model complexity increases with multimodality, high dimensionality and correlated parameters the asymptotic exactness guarantee of MCMC doesn\u2019t come with a tqdm, you could be waiting for quite a while. In these cases, we present the next best thing: variational inference.\n\n**Variatonal Inference**\n\nWhile MCMC offers asymptotic exactness around high dimensional distributions, it can be computationally intensive and impractical for complex distributions. Often we\u2019re just interested in a rough approximation to the posterior that scales well for deployment.\n\nVariational Inference (VI) frames the problem of approximating the posterior as an optimisation problem. Starting with a synthetic posterior Q(z | \u03bb) built from families of simpler distributions (known as the variational family), we optimise over parameters \u03bb that minimise the distance between the variational family and the true posterior P(z | x). This sounds cool but how do we choose the variational family? What even is a distance between distributions?\n\n**KL-Divergence**\n\nThe choice of Q(z | \u03bb) depends on the degree of flexibility required (increasing with the complexity of P(z | x)), but common choices are exponential or Gaussian distributions. To compare the \u201ccloseness\u201d of P and Q, we employ a similarity measure such as the Kullback-Leibler (KL) divergence:\n\nAlthough the KL divergence is asymmetric (DKL(Q||P) =/= DKL(P||Q)), it helps to quantify the difference between Q and P.\n\n\u201cBut we don\u2019t have a closed form solution for P(z|x)!\u201d I hear you exclaiming correctly. That\u2019s why we compute something called the Evidence Lower Bound (ELBO) instead: ELBO(\u03bb) = log( P(x) ) \u2212DKL(Q(z | \u03bb)||P(z | x)). This can be rearranged as ELBO(\u03bb) = E\u200b\\[log P(x, z)\\]\u2212E\u200b\\[log Q(z\u2223\u03bb)\\] helps us avoid that pesky intractable marginal integral.\n\nThus, maximizing ELBO is equivalent to minimizing the KL divergence, serves as an objective function we optimise using standard methods like co-ordinate ascent. Once \u03bb are optimised, the approximating distribution Q(z | \u03bb) serves as a surrogate for the true posterior. This approximation can then be used for downstream tasks like prediction, data imputation, or model interpretation.", "author_fullname": "t2_7h1mi6us", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tl;dr Approximate Inference methods made easy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16awjx5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693939860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\u201cMCMC vs VI\u201d is no longer a discussion about your favourite Roman numeral. If you share my trepidation for model performance in the face of data sparsity, or you simply suffer from anxiety uncertainty, you might be tempted into the Bayesian world. Years later at the precipice of your career (and mental health degeneracy) you over-engineer probabilistic models so intractable that would stress Lord Bayes himself into stomach ulcers. The solution? Approximate inference, the true antihero to model simplification. I wrote a brief primer for those who enjoy maths and those who disdain it, in both cases it&amp;#39;s impossible to avoid using maths while discussing Bayesian statistics so I kept it as light as I could.&lt;/p&gt;\n\n&lt;p&gt;PS - This is a Reddit-friendly copypasta from my medium article, so if you&amp;#39;re a visual person then head over there to get the visuals.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Bayesian Modelling&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A closed-form solution to a machine learning model is one that can be written down on a sheet of paper using a finite number of standard mathematical operations. For example, linear models have closed-form solutions IF the design covariance matrix is invertible, otherwise we obtain a solution using iterative optimisation.&lt;/p&gt;\n\n&lt;p&gt;Bayesian models do not typically have exact closed-form solutions for their posterior distributions. One thing that typically helps is choosing simple models, Gaussian likelihood functions and conjugate priors. A prior distribution is said to be conjugate to a likelihood function if the resulting posterior belongs to the same distribution family as the prior.&lt;/p&gt;\n\n&lt;p&gt;Bayesian linear regression is a model that typically assumes Gaussian priors over both the regression coefficients and the likelihood function. When we update the prior with the observed data (using Bayes\u2019 theorem), the resulting posterior distribution for the regression coefficients will also follow a normal distribution. This can be written down analytically and sampled using standard methods in Python.&lt;/p&gt;\n\n&lt;p&gt;Conjugacy, however, does not always guarantee tractability. High-dimensional parameter spaces, hierarchical structures, non-Gaussian likelihoods with non-linear prior interactions can give rise to intractable integrals for the normalisation constant (which involve over the entire parameter space). This actually becomes prohibitive when want to build, say, a Multivariate Gaussian Linear Regression model with many predictors, or when we want to model count data using a Poisson likelihood and control for overfitting using on using a Laplace (non-conjugate) prior. Thankfully, a solution as old as the first computers comes to the rescue: Markov Chain Monte Carlo (MCMC).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Markov Chain Monte Carlo&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Markov Chain Monte Carlo can be described with enough mathematical jargon to send one fleeing back to first-derivative optimisers, so I\u2019ll skip the stomach ulcers and give an intuitive overview instead.&lt;/p&gt;\n\n&lt;p&gt;Given a probabilistic model parameterised by latent continuous random variables z, and observed values x, we can write down the known form for its probability density function P(z | x). If P(z | x) is intractable, we want to to generate an empirical distribution of samples based on a Markov chain that approximates the probability distribution.&lt;/p&gt;\n\n&lt;p&gt;This empirical distribution can then be used in place of the analytical solution to estimate posterior means, variances, quantiles, and other probabilistic summaries of the model parameters. The most important question: who is Markov and why are we talking about his chain?&lt;/p&gt;\n\n&lt;p&gt;In MCMC, a Markov chain is simply a sequence of samples where each sample is \u201cmemoryless\u201d, i.e. the probability of transitioning to the next sample depends only on the current sample and not on the previous history. This helps us reach a \u201cstationary\u201d distribution over samples, i.e. when we run the chain long enough, the probability P(0 &amp;lt; z &amp;lt; 1 | x)_{n} at iteration n and P(0 &amp;lt; z &amp;lt; 1 | x)_{m} at iteration m should be equal.&lt;/p&gt;\n\n&lt;p&gt;What\u2019s amazing is that the distribution over samples from our Markov chain provides asymptotic exactness; MCMC converges to the true posterior distribution in the limit of infinite samples. How is this implemented in practice?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Metropolis-Hastings (MH) Algorithm&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Metropolis-Hastings (MH) is a specific type of (MCMC) algorithm ubiquitously used in approximate inference. The idea is to build a chain of samples with a proposal distribution that selects \u201cthe next\u201d sample based only on the \u201cthe current\u201d sample (remember the Markov principle).&lt;/p&gt;\n\n&lt;p&gt;Proposed samples with higher probabilities in our posterior are accepted into the chain more frequently and those with lower probabilities are rejected more often (don\u2019t make it into the chain). How do we capture the \u201ctails\u201d of our posterior if we\u2019re busy focusing on high probability regions?&lt;/p&gt;\n\n&lt;p&gt;This is where Metropolis-Hastings acceptance/rejection mechanism really shines:&lt;/p&gt;\n\n&lt;p&gt;For any given proposed sample, we define acceptance probability = min(1, \u03b1), where \u03b1 as the ratio of target and proposal distributions at proposed and current samples.&lt;/p&gt;\n\n&lt;p&gt;Next comes the heart of the algorithm\u2019s exploration-exploitation mechanism: We generate a random number n in the domain [0,1]. If n \u2264 \u03b1, accept the new sample in the chain; if n &amp;gt; \u03b1, keep the current sample and don\u2019t extend the chain.&lt;/p&gt;\n\n&lt;p&gt;The best bit? Samples with acceptance probabilities close to 1 are more likely to move the chain towards higher probability regions. Those with low acceptance probabilities can be accepted when 0 \u2264 n \u2264 \u03b1, exploring lower probability areas and avoiding local modes.&lt;/p&gt;\n\n&lt;p&gt;What diagnostics do we run to check that MH successfully converged to a stationary empirical approximation to the posterior?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Trace Plots and Chain Mixing&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A \u201ctrace plot\u201d allows us to inspect the chain by plotting accepted sample values for each iteration.&lt;/p&gt;\n\n&lt;p&gt;What we\u2019re looking for is low autocorrelation between successive samples, and full exploration of the sample space characterised by high variance across moving windows of the trace. Chain 1 is an example of an ideal trace. Chain 2 initially has high autocorrelation and low variance but converges to stationarity after iteration t ~1500. We discard the head segment t &amp;lt; 1500 (so-called burn-in samples) since they\u2019re unlikely to be part of our target distribution.&lt;/p&gt;\n\n&lt;p&gt;What about Chain 3? This trace demonstrates poor chain mixing, moving slowly across the parameter space between different regions of the distribution. One problem could be that we just haven\u2019t let the algorithm run long enough; but Model complexity increases with multimodality, high dimensionality and correlated parameters the asymptotic exactness guarantee of MCMC doesn\u2019t come with a tqdm, you could be waiting for quite a while. In these cases, we present the next best thing: variational inference.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Variatonal Inference&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;While MCMC offers asymptotic exactness around high dimensional distributions, it can be computationally intensive and impractical for complex distributions. Often we\u2019re just interested in a rough approximation to the posterior that scales well for deployment.&lt;/p&gt;\n\n&lt;p&gt;Variational Inference (VI) frames the problem of approximating the posterior as an optimisation problem. Starting with a synthetic posterior Q(z | \u03bb) built from families of simpler distributions (known as the variational family), we optimise over parameters \u03bb that minimise the distance between the variational family and the true posterior P(z | x). This sounds cool but how do we choose the variational family? What even is a distance between distributions?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;KL-Divergence&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The choice of Q(z | \u03bb) depends on the degree of flexibility required (increasing with the complexity of P(z | x)), but common choices are exponential or Gaussian distributions. To compare the \u201ccloseness\u201d of P and Q, we employ a similarity measure such as the Kullback-Leibler (KL) divergence:&lt;/p&gt;\n\n&lt;p&gt;Although the KL divergence is asymmetric (DKL(Q||P) =/= DKL(P||Q)), it helps to quantify the difference between Q and P.&lt;/p&gt;\n\n&lt;p&gt;\u201cBut we don\u2019t have a closed form solution for P(z|x)!\u201d I hear you exclaiming correctly. That\u2019s why we compute something called the Evidence Lower Bound (ELBO) instead: ELBO(\u03bb) = log( P(x) ) \u2212DKL(Q(z | \u03bb)||P(z | x)). This can be rearranged as ELBO(\u03bb) = E\u200b[log P(x, z)]\u2212E\u200b[log Q(z\u2223\u03bb)] helps us avoid that pesky intractable marginal integral.&lt;/p&gt;\n\n&lt;p&gt;Thus, maximizing ELBO is equivalent to minimizing the KL divergence, serves as an objective function we optimise using standard methods like co-ordinate ascent. Once \u03bb are optimised, the approximating distribution Q(z | \u03bb) serves as a surrogate for the true posterior. This approximation can then be used for downstream tasks like prediction, data imputation, or model interpretation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16awjx5", "is_robot_indexable": true, "report_reasons": null, "author": "SnooPaintings5866", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16awjx5/tldr_approximate_inference_methods_made_easy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16awjx5/tldr_approximate_inference_methods_made_easy/", "subreddit_subscribers": 1033087, "created_utc": 1693939860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Not sure if many people felt this way, but \u201cdomain knowledge\u201d in practice is mostly just an arbitrary set of rules and constructs your organization came up with. I\u2019m sure there are many translatable elements across companies in the same industry, but I do believe a huge part of these knowledge will become obsolete once you leave the organization.\n\nThe DS field is so vast and expanding so fast, and we have a finite capacity for how much we can learn in a period of time. Domain knowledge just doesn\u2019t seem to be the most optimal thing to invest your attention in, as compared to more generalizable ML techniques or technologies. \n\nI would assume if I invested too much attention into one organization, it would mean starting over should I apply to a company in a different industry. \n\nFor people in the hiring position, how much does previous experience in the same industry matter? And is there a disadvantage for someone who has only been in one but applied to a completely different domain?", "author_fullname": "t2_2kh4l8ej", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Those with hiring experience, does having too much \u201cdomain knowledge\u201d makes it harder to enter another industry?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ar0lb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693927097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if many people felt this way, but \u201cdomain knowledge\u201d in practice is mostly just an arbitrary set of rules and constructs your organization came up with. I\u2019m sure there are many translatable elements across companies in the same industry, but I do believe a huge part of these knowledge will become obsolete once you leave the organization.&lt;/p&gt;\n\n&lt;p&gt;The DS field is so vast and expanding so fast, and we have a finite capacity for how much we can learn in a period of time. Domain knowledge just doesn\u2019t seem to be the most optimal thing to invest your attention in, as compared to more generalizable ML techniques or technologies. &lt;/p&gt;\n\n&lt;p&gt;I would assume if I invested too much attention into one organization, it would mean starting over should I apply to a company in a different industry. &lt;/p&gt;\n\n&lt;p&gt;For people in the hiring position, how much does previous experience in the same industry matter? And is there a disadvantage for someone who has only been in one but applied to a completely different domain?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ar0lb", "is_robot_indexable": true, "report_reasons": null, "author": "supper_ham", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ar0lb/those_with_hiring_experience_does_having_too_much/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ar0lb/those_with_hiring_experience_does_having_too_much/", "subreddit_subscribers": 1033087, "created_utc": 1693927097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This is a word cloud (See picture), and it gives an idea of what the most common words are in a string.\n\nI want to get a quantified version of this, i.e. 20% of people think knowledge is important. However, I also want to automatically make synonyms equal. i.e.  quick = fast, bad = horrible. At the end of the day, I want to get a breakdown of what a group of people are saying.\n\nhttps://preview.redd.it/bqixl7289fmb1.jpg?width=927&amp;format=pjpg&amp;auto=webp&amp;s=eee160f4f117c15a9afae83baa1bf9a092f069f5\n\nI'm sure there are existing techniques/ libraries out there to do this. Any thoughts?", "author_fullname": "t2_vmnhefbj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting insights from word clouds? (Sentiment Analysis)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"bqixl7289fmb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/bqixl7289fmb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a4f3319cbcb0979b9772f8fdea66e389402f69fe"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/bqixl7289fmb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7dbd1831d19b90ec6155b5ab9f2979226da87478"}, {"y": 178, "x": 320, "u": "https://preview.redd.it/bqixl7289fmb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e688671c20eb4678ed9ca1b66574c20b8ab22af2"}, {"y": 357, "x": 640, "u": "https://preview.redd.it/bqixl7289fmb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=606bc880bcfc4ac63ad9471f378c6c6ff486b0bc"}], "s": {"y": 518, "x": 927, "u": "https://preview.redd.it/bqixl7289fmb1.jpg?width=927&amp;format=pjpg&amp;auto=webp&amp;s=eee160f4f117c15a9afae83baa1bf9a092f069f5"}, "id": "bqixl7289fmb1"}}, "name": "t3_16akhq4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CgLVIijj4i-Qzkq-Jfex_U9OGvSnLaiqSrFCzBys2Pc.jpg", "edited": 1693913066.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693909596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a word cloud (See picture), and it gives an idea of what the most common words are in a string.&lt;/p&gt;\n\n&lt;p&gt;I want to get a quantified version of this, i.e. 20% of people think knowledge is important. However, I also want to automatically make synonyms equal. i.e.  quick = fast, bad = horrible. At the end of the day, I want to get a breakdown of what a group of people are saying.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bqixl7289fmb1.jpg?width=927&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eee160f4f117c15a9afae83baa1bf9a092f069f5\"&gt;https://preview.redd.it/bqixl7289fmb1.jpg?width=927&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eee160f4f117c15a9afae83baa1bf9a092f069f5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure there are existing techniques/ libraries out there to do this. Any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16akhq4", "is_robot_indexable": true, "report_reasons": null, "author": "tootieloolie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16akhq4/getting_insights_from_word_clouds_sentiment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16akhq4/getting_insights_from_word_clouds_sentiment/", "subreddit_subscribers": 1033087, "created_utc": 1693909596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Currently we are discussing having a data science monorepo in our company as it will insure consistency in the deployment pipelines and it will make sharing files between projects easier.  \n\n\nHowever, we are concerned about one point, each data science project needs a lot of iterations and experiments to reach its final form. If the experiments and jupyter notebooks of all projects lie in the same repo, we are afraid it may make the repo heavy for cloning and laggy when working with it.   \n\n\nDo you think these concerns are valid? what is your experience with working with data science monorepos?", "author_fullname": "t2_42ammirh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "a question about having a monorepo for data science projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ajc39", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693905717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently we are discussing having a data science monorepo in our company as it will insure consistency in the deployment pipelines and it will make sharing files between projects easier.  &lt;/p&gt;\n\n&lt;p&gt;However, we are concerned about one point, each data science project needs a lot of iterations and experiments to reach its final form. If the experiments and jupyter notebooks of all projects lie in the same repo, we are afraid it may make the repo heavy for cloning and laggy when working with it.   &lt;/p&gt;\n\n&lt;p&gt;Do you think these concerns are valid? what is your experience with working with data science monorepos?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ajc39", "is_robot_indexable": true, "report_reasons": null, "author": "GreyWardenMage", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ajc39/a_question_about_having_a_monorepo_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ajc39/a_question_about_having_a_monorepo_for_data/", "subreddit_subscribers": 1033087, "created_utc": 1693905717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Can you recommend any practical course/book/tutorial, for ML engineering, that's also relevant in the industry?  There are plenty of resources out there on ML theory and how to train/build models, but online content often lacks ML engineering part (deploying/productionalizing) and learning the useful tools that's used widely in the industry. So, for all practitioners out there, is there something that you could recommend?", "author_fullname": "t2_697359ug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practical MLE course/MOOC/book?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16bdzbq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693986510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can you recommend any practical course/book/tutorial, for ML engineering, that&amp;#39;s also relevant in the industry?  There are plenty of resources out there on ML theory and how to train/build models, but online content often lacks ML engineering part (deploying/productionalizing) and learning the useful tools that&amp;#39;s used widely in the industry. So, for all practitioners out there, is there something that you could recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bdzbq", "is_robot_indexable": true, "report_reasons": null, "author": "GRDDT", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bdzbq/best_practical_mle_coursemoocbook/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bdzbq/best_practical_mle_coursemoocbook/", "subreddit_subscribers": 1033087, "created_utc": 1693986510.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone, I have a project and need help considering which variables are irrelevant before even constructing a lm and testing it against test data. The dataset consists of +-20k observations\nMy response variable is (total)\n\nMy variables are: \n\n\u2022 id: record index.\n\n\u2022 dte: date of rental.\n\n\u2022 season: 1: springer, 2: summer, 3: fall, 4: winter.\n\n\u2022 year: Year of date. 0: 2011. 1: 2012.\n\n\u2022 month: Month of date, 1 to 12.\n\n\u2022 hour: Hour of date, 0 to 23.\n\n\u2022 holiday: 1: If day is a holiday. 0: Otherwise.\n\n\u2022 weekday: Day of the week. 0: Sunday, 1: Monday, . . ., 6: Saturday.\n\n\u2022 workingday: 1: If a day is neither in the weekend nor a holiday is 1. 0: Otherwise.\n\n\u2022 weatherforcast: 1: Clear, Partly cloudy. 2: Mist and Few clouds. 3: Light rain/snow 4: Heavy rain,\nhail, thunderstorm, snow and fog.\n\n\u2022 temp: Normalised temperature in Celsius (observations were divided by 41).\n\n\u2022 tempf: Normalised feeling temperature in Celsius (observations were divided by 50).\n\n\u2022 humid: Normalised humidity (observations were divided by 100).\n\n\u2022 windspeed: Normalised wind speed (observations were divided by 67).\n\n\u2022 casual: Total number of casual users.\n\u2022 registered: Total number of registered users.\n\u2022 total: Total number of rentals.\n\nWhat would be the best approach to model variables, should I convert my categoricals to factors. Note that if I create a linear model such that lm1 &lt;- lm(total ~., data = data) I get r squared and r adjust values of 1 so my model indicates over fitting. Any tips ? Thanks", "author_fullname": "t2_645b9mvb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "R studio: Stats Regression Model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16bdxmb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693986330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I have a project and need help considering which variables are irrelevant before even constructing a lm and testing it against test data. The dataset consists of +-20k observations\nMy response variable is (total)&lt;/p&gt;\n\n&lt;p&gt;My variables are: &lt;/p&gt;\n\n&lt;p&gt;\u2022 id: record index.&lt;/p&gt;\n\n&lt;p&gt;\u2022 dte: date of rental.&lt;/p&gt;\n\n&lt;p&gt;\u2022 season: 1: springer, 2: summer, 3: fall, 4: winter.&lt;/p&gt;\n\n&lt;p&gt;\u2022 year: Year of date. 0: 2011. 1: 2012.&lt;/p&gt;\n\n&lt;p&gt;\u2022 month: Month of date, 1 to 12.&lt;/p&gt;\n\n&lt;p&gt;\u2022 hour: Hour of date, 0 to 23.&lt;/p&gt;\n\n&lt;p&gt;\u2022 holiday: 1: If day is a holiday. 0: Otherwise.&lt;/p&gt;\n\n&lt;p&gt;\u2022 weekday: Day of the week. 0: Sunday, 1: Monday, . . ., 6: Saturday.&lt;/p&gt;\n\n&lt;p&gt;\u2022 workingday: 1: If a day is neither in the weekend nor a holiday is 1. 0: Otherwise.&lt;/p&gt;\n\n&lt;p&gt;\u2022 weatherforcast: 1: Clear, Partly cloudy. 2: Mist and Few clouds. 3: Light rain/snow 4: Heavy rain,\nhail, thunderstorm, snow and fog.&lt;/p&gt;\n\n&lt;p&gt;\u2022 temp: Normalised temperature in Celsius (observations were divided by 41).&lt;/p&gt;\n\n&lt;p&gt;\u2022 tempf: Normalised feeling temperature in Celsius (observations were divided by 50).&lt;/p&gt;\n\n&lt;p&gt;\u2022 humid: Normalised humidity (observations were divided by 100).&lt;/p&gt;\n\n&lt;p&gt;\u2022 windspeed: Normalised wind speed (observations were divided by 67).&lt;/p&gt;\n\n&lt;p&gt;\u2022 casual: Total number of casual users.\n\u2022 registered: Total number of registered users.\n\u2022 total: Total number of rentals.&lt;/p&gt;\n\n&lt;p&gt;What would be the best approach to model variables, should I convert my categoricals to factors. Note that if I create a linear model such that lm1 &amp;lt;- lm(total ~., data = data) I get r squared and r adjust values of 1 so my model indicates over fitting. Any tips ? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bdxmb", "is_robot_indexable": true, "report_reasons": null, "author": "jjnaude219", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bdxmb/r_studio_stats_regression_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bdxmb/r_studio_stats_regression_model/", "subreddit_subscribers": 1033087, "created_utc": 1693986330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_bkc0rduts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "M\u00e1ster in Data Science: Universidad de Alcal\u00e1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"5xhj8anb3lmb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 130, "x": 108, "u": "https://preview.redd.it/5xhj8anb3lmb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0758bd2df6f664284bd852fce1cf45423685a90c"}, {"y": 261, "x": 216, "u": "https://preview.redd.it/5xhj8anb3lmb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0be0eb5bafc144f215e4d189760d7701bf19e44b"}, {"y": 387, "x": 320, "u": "https://preview.redd.it/5xhj8anb3lmb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=59561970447cbebc29ad0aba9870a5cad39594ed"}], "s": {"y": 567, "x": 468, "u": "https://preview.redd.it/5xhj8anb3lmb1.png?width=468&amp;format=png&amp;auto=webp&amp;s=7380dcdc0dfa4185e1a2bbfe7377ed86154feef5"}, "id": "5xhj8anb3lmb1"}, "bwfiv6ob3lmb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 125, "x": 108, "u": "https://preview.redd.it/bwfiv6ob3lmb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8536ab8a4fa0c5bde10aa34dc0df9639f00a9e2e"}, {"y": 250, "x": 216, "u": "https://preview.redd.it/bwfiv6ob3lmb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d7b4f8cfb3b3dfae9fe2b785b4328cadb6cd20a4"}, {"y": 371, "x": 320, "u": "https://preview.redd.it/bwfiv6ob3lmb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a905f0dd060a2cf1ec6ece319880006fed93302"}], "s": {"y": 547, "x": 471, "u": "https://preview.redd.it/bwfiv6ob3lmb1.png?width=471&amp;format=png&amp;auto=webp&amp;s=da8129a6bc19ae3adbea925f8cc73bef1a17f250"}, "id": "bwfiv6ob3lmb1"}}, "name": "t3_16bd9mg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "Good morning, guys! I'm considering a Data Science Master's and need experienced tech professionals' opinions. Your help is much appreciated! (Is it worth it?)", "media_id": "5xhj8anb3lmb1", "id": 325616350}, {"media_id": "bwfiv6ob3lmb1", "id": 325616351}]}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ssawGCarDkaYZVB1_PWKnDo-CsXBr9zLjvzXarp4L-E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693983859.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/16bd9mg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": null, "id": "16bd9mg", "is_robot_indexable": true, "report_reasons": null, "author": "carl0sd4", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bd9mg/m\u00e1ster_in_data_science_universidad_de_alcal\u00e1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/16bd9mg", "subreddit_subscribers": 1033087, "created_utc": 1693983859.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys, I have a graduation in hospitality and worked few years there. I completed a Data Science certification recently but finding it hard to get internships or jobs in any sector hence I am planning to do some kind of financial course to add to my education and make portfolio in fintech related projects. Can I get some suggestions on what kind of course to look for particularly to get into fintech?", "author_fullname": "t2_gy9kw9sv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Financial course suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16bcx87", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693982618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I have a graduation in hospitality and worked few years there. I completed a Data Science certification recently but finding it hard to get internships or jobs in any sector hence I am planning to do some kind of financial course to add to my education and make portfolio in fintech related projects. Can I get some suggestions on what kind of course to look for particularly to get into fintech?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bcx87", "is_robot_indexable": true, "report_reasons": null, "author": "fuhrers_shadow", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bcx87/financial_course_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bcx87/financial_course_suggestions/", "subreddit_subscribers": 1033087, "created_utc": 1693982618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys, if you have explored using Llama-2 in doing sentiment analysis, just wanted to get your experience in how Llama-2 perform in this task?\n\nI have tried using GPT and it\u2019s pretty accurate.\n\nIf Llama-2 isn\u2019t all that good in sentiment analysis, which other open LLM would you recommend? \n\nThank heaps!", "author_fullname": "t2_hjlrj5fp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does Llama-2 perform in sentiment analysis?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bcocl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693981697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, if you have explored using Llama-2 in doing sentiment analysis, just wanted to get your experience in how Llama-2 perform in this task?&lt;/p&gt;\n\n&lt;p&gt;I have tried using GPT and it\u2019s pretty accurate.&lt;/p&gt;\n\n&lt;p&gt;If Llama-2 isn\u2019t all that good in sentiment analysis, which other open LLM would you recommend? &lt;/p&gt;\n\n&lt;p&gt;Thank heaps!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16bcocl", "is_robot_indexable": true, "report_reasons": null, "author": "--leockl--", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16bcocl/how_does_llama2_perform_in_sentiment_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16bcocl/how_does_llama2_perform_in_sentiment_analysis/", "subreddit_subscribers": 1033087, "created_utc": 1693981697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi All,\n\nTitle. I'm building predictive models for banks to forecast their deposit portfolio, but that field is relatively new to me, and given the proprietary nature of it, I'm having a hard time finding resources online regarding it. Does anyone know of some industry standard resources on this stuff?  ", "author_fullname": "t2_6l5ebj6st", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deposit Forecasting for Banks --- Any Resources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b8fk2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693968251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;Title. I&amp;#39;m building predictive models for banks to forecast their deposit portfolio, but that field is relatively new to me, and given the proprietary nature of it, I&amp;#39;m having a hard time finding resources online regarding it. Does anyone know of some industry standard resources on this stuff?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b8fk2", "is_robot_indexable": true, "report_reasons": null, "author": "PlusTelephone9598", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b8fk2/deposit_forecasting_for_banks_any_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b8fk2/deposit_forecasting_for_banks_any_resources/", "subreddit_subscribers": 1033087, "created_utc": 1693968251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nI have a set of original frequency counts from a dataset and I have the same frequency counts but they are perturbed by adding noise to prevent some counts to be reported explicitly.\n\nWhat I am trying to assess is between original and perturbed count sets, how much information is lost when reporting original v/s perturb counts. \n\nIs there any formal test that I can perform in a typical situation like this?\nAre there any traditional statistical methods that I can extend to for this use-case\n\nFor e.g, something that came to my mind is using Root Mean Squared Deviation/ Error from linear regression between original and perturbed counts to get and R^2 as an estimate of deviation in perturbed counts from original counts.\nAgain, not an expert here so I dont know if that is scientifically and theoretically true.\n\nAny guidance is appreciated!\nThanks!", "author_fullname": "t2_9po1x00s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comparing original values with perturbed values to assess information loss", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b5s7p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693961085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a set of original frequency counts from a dataset and I have the same frequency counts but they are perturbed by adding noise to prevent some counts to be reported explicitly.&lt;/p&gt;\n\n&lt;p&gt;What I am trying to assess is between original and perturbed count sets, how much information is lost when reporting original v/s perturb counts. &lt;/p&gt;\n\n&lt;p&gt;Is there any formal test that I can perform in a typical situation like this?\nAre there any traditional statistical methods that I can extend to for this use-case&lt;/p&gt;\n\n&lt;p&gt;For e.g, something that came to my mind is using Root Mean Squared Deviation/ Error from linear regression between original and perturbed counts to get and R&lt;sup&gt;2&lt;/sup&gt; as an estimate of deviation in perturbed counts from original counts.\nAgain, not an expert here so I dont know if that is scientifically and theoretically true.&lt;/p&gt;\n\n&lt;p&gt;Any guidance is appreciated!\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b5s7p", "is_robot_indexable": true, "report_reasons": null, "author": "HealthtoML", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b5s7p/comparing_original_values_with_perturbed_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b5s7p/comparing_original_values_with_perturbed_values/", "subreddit_subscribers": 1033087, "created_utc": 1693961085.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "[https://opendatascience.com/a-primer-to-scaling-pandas/](https://opendatascience.com/a-primer-to-scaling-pandas/)", "author_fullname": "t2_8glql2df", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get pandas to work at scale with one small change.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b5fqb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693960210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://opendatascience.com/a-primer-to-scaling-pandas/\"&gt;https://opendatascience.com/a-primer-to-scaling-pandas/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JJ-UsASBX_8XOT-BSED5EiZzxasdKe-XXN7-T1ZZ2rc.jpg?auto=webp&amp;s=06fea5f82deca630b813e0d8985f4faac71ed99f", "width": 640, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/JJ-UsASBX_8XOT-BSED5EiZzxasdKe-XXN7-T1ZZ2rc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3095593a81ff0543cd34ec5c15f1a6eff286e6d5", "width": 108, "height": 50}, {"url": "https://external-preview.redd.it/JJ-UsASBX_8XOT-BSED5EiZzxasdKe-XXN7-T1ZZ2rc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=642a7a04fdda820458ff4c67dbd9b51e5ec6187c", "width": 216, "height": 101}, {"url": "https://external-preview.redd.it/JJ-UsASBX_8XOT-BSED5EiZzxasdKe-XXN7-T1ZZ2rc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5cad24f571a3c2311268581d584f7d2af25b0ec1", "width": 320, "height": 150}, {"url": "https://external-preview.redd.it/JJ-UsASBX_8XOT-BSED5EiZzxasdKe-XXN7-T1ZZ2rc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=563fd92d9c22ff3b46cec435e2c15138159e2ec0", "width": 640, "height": 300}], "variants": {}, "id": "AGt6bxsbmM-0nQyYUtaaEsfO4Fc1-ls0Ub4VrKmg8ZY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b5fqb", "is_robot_indexable": true, "report_reasons": null, "author": "Data_Nerd1979", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b5fqb/get_pandas_to_work_at_scale_with_one_small_change/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b5fqb/get_pandas_to_work_at_scale_with_one_small_change/", "subreddit_subscribers": 1033087, "created_utc": 1693960210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I really like Philly as a city and am not a strong enough candidate for anything at Penn but drexels program especially their coop seems like it has strong ties with local companies. My only concern is whether the program is just a cash grab. I understand there are cheaper online programs but I do not perform well with online learning and am only looking at person learning opportunities.", "author_fullname": "t2_r9kqx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on Drexel for MSDS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b4pvq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693958389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really like Philly as a city and am not a strong enough candidate for anything at Penn but drexels program especially their coop seems like it has strong ties with local companies. My only concern is whether the program is just a cash grab. I understand there are cheaper online programs but I do not perform well with online learning and am only looking at person learning opportunities.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b4pvq", "is_robot_indexable": true, "report_reasons": null, "author": "Spitball2468", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b4pvq/thoughts_on_drexel_for_msds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b4pvq/thoughts_on_drexel_for_msds/", "subreddit_subscribers": 1033087, "created_utc": 1693958389.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}