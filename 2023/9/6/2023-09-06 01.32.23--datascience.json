{"kind": "Listing", "data": {"after": "t3_16apgfz", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There's always so much criticism of hiring processes in the tech world, from hating take home tests or the recent post complaining about what looks like a \\~5 minute task if you know SQL.\n\nI'm curious how everyone would realistically redesign / create their own application process since we're so critical of the existing ones. \n\nLet's say you're the hiring manager for a Data science role that you've benchmarked as needing someone with \\~1 to 2 years experience. The job role automatically closes after it's got 1000 applicants... which you get in about a day.\n\nHow do you handle those 1000 applicants? \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_4qacn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would YOU handle Data Science recruitment ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16aox1s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 96, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 96, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693922029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s always so much criticism of hiring processes in the tech world, from hating take home tests or the recent post complaining about what looks like a ~5 minute task if you know SQL.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious how everyone would realistically redesign / create their own application process since we&amp;#39;re so critical of the existing ones. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say you&amp;#39;re the hiring manager for a Data science role that you&amp;#39;ve benchmarked as needing someone with ~1 to 2 years experience. The job role automatically closes after it&amp;#39;s got 1000 applicants... which you get in about a day.&lt;/p&gt;\n\n&lt;p&gt;How do you handle those 1000 applicants? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16aox1s", "is_robot_indexable": true, "report_reasons": null, "author": "Littleish", "discussion_type": null, "num_comments": 86, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16aox1s/how_would_you_handle_data_science_recruitment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16aox1s/how_would_you_handle_data_science_recruitment/", "subreddit_subscribers": 1032696, "created_utc": 1693922029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work for a Fortune 500 company as a data engineer. I was promised when hired, that I would be a data scientist and get plenty of predictive modeling work. I have done some modeling work, but feel like there is no purpose to the work I am doing. We don't have much vision or leadership on the DS side and have a huge focus on DE. I have been in the position for about 1-1/2 year. It seems like we are doing a bunch of busy work to move the data into AWS, pay huge sums of money to Amazon, and get no business value out of the effort. There is a lot of talk about how great AWS is and how much of a game-changer this effort will be. Everyone on the team seems to think it's great and cool what we are doing, but I just cannot grasp the value and am extremely uninterested in the DE work. ", "author_fullname": "t2_9bjl9255", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hate my job - Waste of time?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16aop0z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 78, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 78, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693928982.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693921498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a Fortune 500 company as a data engineer. I was promised when hired, that I would be a data scientist and get plenty of predictive modeling work. I have done some modeling work, but feel like there is no purpose to the work I am doing. We don&amp;#39;t have much vision or leadership on the DS side and have a huge focus on DE. I have been in the position for about 1-1/2 year. It seems like we are doing a bunch of busy work to move the data into AWS, pay huge sums of money to Amazon, and get no business value out of the effort. There is a lot of talk about how great AWS is and how much of a game-changer this effort will be. Everyone on the team seems to think it&amp;#39;s great and cool what we are doing, but I just cannot grasp the value and am extremely uninterested in the DE work. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16aop0z", "is_robot_indexable": true, "report_reasons": null, "author": "Substantial-Lime7107", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16aop0z/hate_my_job_waste_of_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16aop0z/hate_my_job_waste_of_time/", "subreddit_subscribers": 1032696, "created_utc": 1693921498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Every time I've tested a SQL query and realized that some null somewhere broke my CONCAT statement, or run an R script and realized that I forgot to add \"na.rm = TRUE\" to a function somewhere, I wonder what was going through the development team's minds when they decided to build these tools these ways.\n\nWhy isn't ignoring nulls the default behavior?\n\nGenuinely curious if there's a logic to this that I haven't considered.", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELI5: Why are nulls not ignored by default?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16al6h4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693911891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every time I&amp;#39;ve tested a SQL query and realized that some null somewhere broke my CONCAT statement, or run an R script and realized that I forgot to add &amp;quot;na.rm = TRUE&amp;quot; to a function somewhere, I wonder what was going through the development team&amp;#39;s minds when they decided to build these tools these ways.&lt;/p&gt;\n\n&lt;p&gt;Why isn&amp;#39;t ignoring nulls the default behavior?&lt;/p&gt;\n\n&lt;p&gt;Genuinely curious if there&amp;#39;s a logic to this that I haven&amp;#39;t considered.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16al6h4", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16al6h4/eli5_why_are_nulls_not_ignored_by_default/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16al6h4/eli5_why_are_nulls_not_ignored_by_default/", "subreddit_subscribers": 1032696, "created_utc": 1693911891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Data science hobby projects I've built so far:\n\n\ud83c\udfda 2012 - \"Rent Scraper\" \u2014 A tool to find me the cheapest apartment to rent by scraping several websites every minute. (Bash, Python)\n\n\ud83d\uddde 2013 - \"Prionews\" \u2014 A system to summarize daily news in an unbiased manner. (Bash, SQL, Python)\n\n\ud83e\udd16 2014 - \"Jarvic\" \u2014 A self-learning chatbot. (Python)\n\n\ud83c\udfa8 2015 - \"Write-Here-Anything\" \u2014 A hard-to-describe website and art project. (JavaScript)\n\n\ud83c\udf10 2016 - \"Learn-Languages\" \u2014 A simple script that prints the top 1,000 most important words to learn in any language, based on the script of the Friends TV show. (Python)\n\n\ud83d\udda5 2017 - \"User-Generator\" \u2014 A Python script that simulates log/data creation for a mobile app, intended for educational purposes. (Python)\n\n\ud83d\udc6c 2018 - \"A/B Testing Redirect\" \u2014 Code to implement an A/B test without using third-party tools. (JavaScript + Python)\n\n\ud83d\udcc8 2019 - \"Simple User Log\" \u2014 A basic analytics tool designed to replace Google Analytics. (JavaScript + Python + Flask)\n\n\ud83d\udc68\u200d\ud83c\udfeb 2020 - \"Best Bet\" \u2014 A game that educates people about the concept of \"expected value.\" (Python + Flask + HTML)\n\n\u2618 2021 - \"Automated Gardener\" \u2014 A hardware project that automatically takes care of my plants. (Python + Bash + Raspberry Pi)\n\n\ud83d\udcb0 2022 - \"BitPanda\\_DCA\" \u2014 A simple automation tool that performs dollar-cost averaging on the Bitpanda platform for me. (Python + Bash)\n\n\ud83e\udd43 2023 - \"WhiskyReturns\" \u2014 A platform that collects data on whisky investments and displays them in a simple chart. (Python, Bash, SQL, HTML, APIs, etc.)\n\nMost of these projects are retired and offline, but they've been invaluable in teaching me about data science and coding. Building a hobby project is never a waste of time. You should start yours!", "author_fullname": "t2_11i3en", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My Last Decade of Data Science Hobby Projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16aid9m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693902224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data science hobby projects I&amp;#39;ve built so far:&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udfda 2012 - &amp;quot;Rent Scraper&amp;quot; \u2014 A tool to find me the cheapest apartment to rent by scraping several websites every minute. (Bash, Python)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\uddde 2013 - &amp;quot;Prionews&amp;quot; \u2014 A system to summarize daily news in an unbiased manner. (Bash, SQL, Python)&lt;/p&gt;\n\n&lt;p&gt;\ud83e\udd16 2014 - &amp;quot;Jarvic&amp;quot; \u2014 A self-learning chatbot. (Python)&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udfa8 2015 - &amp;quot;Write-Here-Anything&amp;quot; \u2014 A hard-to-describe website and art project. (JavaScript)&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udf10 2016 - &amp;quot;Learn-Languages&amp;quot; \u2014 A simple script that prints the top 1,000 most important words to learn in any language, based on the script of the Friends TV show. (Python)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udda5 2017 - &amp;quot;User-Generator&amp;quot; \u2014 A Python script that simulates log/data creation for a mobile app, intended for educational purposes. (Python)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udc6c 2018 - &amp;quot;A/B Testing Redirect&amp;quot; \u2014 Code to implement an A/B test without using third-party tools. (JavaScript + Python)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcc8 2019 - &amp;quot;Simple User Log&amp;quot; \u2014 A basic analytics tool designed to replace Google Analytics. (JavaScript + Python + Flask)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udc68\u200d\ud83c\udfeb 2020 - &amp;quot;Best Bet&amp;quot; \u2014 A game that educates people about the concept of &amp;quot;expected value.&amp;quot; (Python + Flask + HTML)&lt;/p&gt;\n\n&lt;p&gt;\u2618 2021 - &amp;quot;Automated Gardener&amp;quot; \u2014 A hardware project that automatically takes care of my plants. (Python + Bash + Raspberry Pi)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcb0 2022 - &amp;quot;BitPanda_DCA&amp;quot; \u2014 A simple automation tool that performs dollar-cost averaging on the Bitpanda platform for me. (Python + Bash)&lt;/p&gt;\n\n&lt;p&gt;\ud83e\udd43 2023 - &amp;quot;WhiskyReturns&amp;quot; \u2014 A platform that collects data on whisky investments and displays them in a simple chart. (Python, Bash, SQL, HTML, APIs, etc.)&lt;/p&gt;\n\n&lt;p&gt;Most of these projects are retired and offline, but they&amp;#39;ve been invaluable in teaching me about data science and coding. Building a hobby project is never a waste of time. You should start yours!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16aid9m", "is_robot_indexable": true, "report_reasons": null, "author": "mestitomi", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16aid9m/my_last_decade_of_data_science_hobby_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16aid9m/my_last_decade_of_data_science_hobby_projects/", "subreddit_subscribers": 1032696, "created_utc": 1693902224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello! I am 29, working at a Fortune 50 company in their modeling department, making decent money for a MCOL area. Recently I have realized, big job title, really high compensation doesn\u2019t really motivate me anymore. I still want to switch jobs and try to work on different kind of things but work life balance and healthy work environment has started to become number 1 priority for me. So much so that when I look for job after the current one, I am not willing to compromise my wlb despite the money. One of the reasons, it\u2019s getting difficult for me to think about switching jobs since the current one is the ideal job for me. \n\nI am sharing this to understand if I ma doing something really wrong and could be a career suicide. What do you guys think? Can I keep increasing my TC while having WLB?\n\nThanks!", "author_fullname": "t2_bv171ji2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I setting myself for an unsuccessful career if I have no motivation to climb the corporate ladder?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b194c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693950256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I am 29, working at a Fortune 50 company in their modeling department, making decent money for a MCOL area. Recently I have realized, big job title, really high compensation doesn\u2019t really motivate me anymore. I still want to switch jobs and try to work on different kind of things but work life balance and healthy work environment has started to become number 1 priority for me. So much so that when I look for job after the current one, I am not willing to compromise my wlb despite the money. One of the reasons, it\u2019s getting difficult for me to think about switching jobs since the current one is the ideal job for me. &lt;/p&gt;\n\n&lt;p&gt;I am sharing this to understand if I ma doing something really wrong and could be a career suicide. What do you guys think? Can I keep increasing my TC while having WLB?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b194c", "is_robot_indexable": true, "report_reasons": null, "author": "quite--average", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b194c/am_i_setting_myself_for_an_unsuccessful_career_if/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b194c/am_i_setting_myself_for_an_unsuccessful_career_if/", "subreddit_subscribers": 1032696, "created_utc": 1693950256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My job responsibilities are way below my skill level so I\u2019m losing motivation to keep learning. Job hoping doesn\u2019t fix this as every job is underwhelming.", "author_fullname": "t2_kcl3tfwe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle burnout?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16azemj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693946296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My job responsibilities are way below my skill level so I\u2019m losing motivation to keep learning. Job hoping doesn\u2019t fix this as every job is underwhelming.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16azemj", "is_robot_indexable": true, "report_reasons": null, "author": "NewEcho2940", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16azemj/how_do_you_handle_burnout/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16azemj/how_do_you_handle_burnout/", "subreddit_subscribers": 1032696, "created_utc": 1693946296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have read about renewable energy supply and energy demand forecasting. However, they are usually done separately. I am wondering if it makes sense to predict both. For instance, our university campus uses solar energy and fossil fuel energy (connected to the national grid). I want to predict the solar energy supply to see if it will be able to match the demand by itself (for instance, if it's intended for the lights of one building, if it will be able to supply that), but at the same time I want to predict the demand based on occupancy, etc. Has this been done before? Will this propagate errors? Thank you very much.", "author_fullname": "t2_81iatgrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Predicting both energy supply and demand in a university campus using hybrid energy source", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ac8uc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693885013.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693882320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have read about renewable energy supply and energy demand forecasting. However, they are usually done separately. I am wondering if it makes sense to predict both. For instance, our university campus uses solar energy and fossil fuel energy (connected to the national grid). I want to predict the solar energy supply to see if it will be able to match the demand by itself (for instance, if it&amp;#39;s intended for the lights of one building, if it will be able to supply that), but at the same time I want to predict the demand based on occupancy, etc. Has this been done before? Will this propagate errors? Thank you very much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ac8uc", "is_robot_indexable": true, "report_reasons": null, "author": "severinseverine", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ac8uc/predicting_both_energy_supply_and_demand_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ac8uc/predicting_both_energy_supply_and_demand_in_a/", "subreddit_subscribers": 1032696, "created_utc": 1693882320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nI recently created a CLI that lets you compile Jupyter notebooks into FastAPI apps. \n\nIt let\u2019s you write comments like \u201c\u201d\u201d@HTTP\u201d\u201d\u201d, \u201c\u201d\u201d@WS\u201d\u201d\u201d, and \u201c\u201d\u201d@SCHEDULE\u201d\u201d\u201d\u201d to expose your cells as http or websockets endpoints, or programmatically run your cells as scheduled time intervals for simple data pipelines.\n\nIt creates a /build folder with your dockerized code for easy running and deployment.\n\nhttps://github.com/neutrino-ai/neutrino-notebooks\n\nHope you find this helpful! I would appreciate any feedback", "author_fullname": "t2_2rm8fild", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A CLI that turns notebooks into FastAPI apps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16arf00", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693928015.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I recently created a CLI that lets you compile Jupyter notebooks into FastAPI apps. &lt;/p&gt;\n\n&lt;p&gt;It let\u2019s you write comments like \u201c\u201d\u201d@HTTP\u201d\u201d\u201d, \u201c\u201d\u201d@WS\u201d\u201d\u201d, and \u201c\u201d\u201d@SCHEDULE\u201d\u201d\u201d\u201d to expose your cells as http or websockets endpoints, or programmatically run your cells as scheduled time intervals for simple data pipelines.&lt;/p&gt;\n\n&lt;p&gt;It creates a /build folder with your dockerized code for easy running and deployment.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/neutrino-ai/neutrino-notebooks\"&gt;https://github.com/neutrino-ai/neutrino-notebooks&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hope you find this helpful! I would appreciate any feedback&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?auto=webp&amp;s=5394e103b0086b4eff4d06568f294956507f0ecb", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=43271c21e8a34542bc37693cfaf1dbacc436d23f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9701d58c16fd8ce19979b7db2dc7bdd8c459dd59", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=711a7236b9019520b595a3090b2f212b668efe75", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=03ab1ab20ef97b7eff2703ac8562037562678684", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=97db8e91a7af642bc87a0d9454c87ad4c0286dac", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/U8HOVmKOl15s3KWQoR3bZ3AKGrjeypS2GMMZx8AnwaU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ec562818e6bf8236d636bae3338627ea6dfd4584", "width": 1080, "height": 540}], "variants": {}, "id": "h0H4QH_NNA8eb-artCZYZeYRXgCLX9TCOO7CamwnaCk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16arf00", "is_robot_indexable": true, "report_reasons": null, "author": "gibbybutwithrandck", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16arf00/a_cli_that_turns_notebooks_into_fastapi_apps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16arf00/a_cli_that_turns_notebooks_into_fastapi_apps/", "subreddit_subscribers": 1032696, "created_utc": 1693928015.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently embarking on my first experience as a Data Scientist during my master's degree internship in a big and well known company, and I am also working on my thesis at the same time. As a newcomer to this field, I am facing several challenges. I persuaded my manager to let me tackle an ambitious deep learning project, but I have found myself struggling on my own to overcome various obstacles. Handling a complex project independently and dealing with structuring and managing code optimization has been challenging. I encountered difficulties collaborating with my team members and regret not seeking assistance from others.\n\nI do not have a background in computer science, and this is my very first experience in a corporate environment. This situation has been quite demotivating for me.\n\nHow can I bounce back, address my weaknesses, and position myself for success in a real Data Scientist role once I graduate?", "author_fullname": "t2_8xgr0r83", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bad first experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b08p5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693948081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently embarking on my first experience as a Data Scientist during my master&amp;#39;s degree internship in a big and well known company, and I am also working on my thesis at the same time. As a newcomer to this field, I am facing several challenges. I persuaded my manager to let me tackle an ambitious deep learning project, but I have found myself struggling on my own to overcome various obstacles. Handling a complex project independently and dealing with structuring and managing code optimization has been challenging. I encountered difficulties collaborating with my team members and regret not seeking assistance from others.&lt;/p&gt;\n\n&lt;p&gt;I do not have a background in computer science, and this is my very first experience in a corporate environment. This situation has been quite demotivating for me.&lt;/p&gt;\n\n&lt;p&gt;How can I bounce back, address my weaknesses, and position myself for success in a real Data Scientist role once I graduate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b08p5", "is_robot_indexable": true, "report_reasons": null, "author": "Weak_Two_6732", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b08p5/bad_first_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b08p5/bad_first_experience/", "subreddit_subscribers": 1032696, "created_utc": 1693948081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi  I'm geology fresh graduate and I'm taking Ai training program in Oil &amp; Gas sector, and now I'm working on a challenging dataset so the first and main issue i had is the high percentage of missing values in a list of well logs columns and each of them have a different percentage of missing values. And the other one is to achieve as high F1 score accuracy as possible using different classifications algorithms wether they were Ml or DL. So is there any certain algorithm i can use to achieve high F1 score or should i try them all?", "author_fullname": "t2_c0nfx4v9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Missing values", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16atdug", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693932565.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi  I&amp;#39;m geology fresh graduate and I&amp;#39;m taking Ai training program in Oil &amp;amp; Gas sector, and now I&amp;#39;m working on a challenging dataset so the first and main issue i had is the high percentage of missing values in a list of well logs columns and each of them have a different percentage of missing values. And the other one is to achieve as high F1 score accuracy as possible using different classifications algorithms wether they were Ml or DL. So is there any certain algorithm i can use to achieve high F1 score or should i try them all?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16atdug", "is_robot_indexable": true, "report_reasons": null, "author": "Many-Act6717", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16atdug/missing_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16atdug/missing_values/", "subreddit_subscribers": 1032696, "created_utc": 1693932565.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "so i am at the crossroads in my career, recently lost my job in customer support and since this career does not have many options\nin terms of growth and learning, i was wondering whether the data career is for me. i already saw a lot of yt videos about that, talked with people who finished bootcamps but am still not able to have a clear image wether this career is for me (or to say in other words - am i smart enough to go at least to mid-specialist in the future).\n\nmy question is - could you describe in simple words (for someone knowing just simple basic things) - a problem and a solution that mid-level specialist should be able to solve? an example of the problem you had and how you solved it would also give me an understanding of the complexity of this field. i hope this makes sense, since this would be the final question i have before making a decision. \n\nany tips, examples, opinions and insights are very welcome!", "author_fullname": "t2_471kp8ya", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "question for mid-level analysts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16aoxmu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693922070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so i am at the crossroads in my career, recently lost my job in customer support and since this career does not have many options\nin terms of growth and learning, i was wondering whether the data career is for me. i already saw a lot of yt videos about that, talked with people who finished bootcamps but am still not able to have a clear image wether this career is for me (or to say in other words - am i smart enough to go at least to mid-specialist in the future).&lt;/p&gt;\n\n&lt;p&gt;my question is - could you describe in simple words (for someone knowing just simple basic things) - a problem and a solution that mid-level specialist should be able to solve? an example of the problem you had and how you solved it would also give me an understanding of the complexity of this field. i hope this makes sense, since this would be the final question i have before making a decision. &lt;/p&gt;\n\n&lt;p&gt;any tips, examples, opinions and insights are very welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16aoxmu", "is_robot_indexable": true, "report_reasons": null, "author": "all-the-good1sRtaken", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16aoxmu/question_for_midlevel_analysts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16aoxmu/question_for_midlevel_analysts/", "subreddit_subscribers": 1032696, "created_utc": 1693922070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This is a word cloud (See picture), and it gives an idea of what the most common words are in a string.\n\nI want to get a quantified version of this, i.e. 20% of people think knowledge is important. However, I also want to automatically make synonyms equal. i.e.  quick = fast, bad = horrible. At the end of the day, I want to get a breakdown of what a group of people are saying.\n\nhttps://preview.redd.it/bqixl7289fmb1.jpg?width=927&amp;format=pjpg&amp;auto=webp&amp;s=eee160f4f117c15a9afae83baa1bf9a092f069f5\n\nI'm sure there are existing techniques/ libraries out there to do this. Any thoughts?", "author_fullname": "t2_vmnhefbj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting insights from word clouds? (Sentiment Analysis)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"bqixl7289fmb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/bqixl7289fmb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a4f3319cbcb0979b9772f8fdea66e389402f69fe"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/bqixl7289fmb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7dbd1831d19b90ec6155b5ab9f2979226da87478"}, {"y": 178, "x": 320, "u": "https://preview.redd.it/bqixl7289fmb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e688671c20eb4678ed9ca1b66574c20b8ab22af2"}, {"y": 357, "x": 640, "u": "https://preview.redd.it/bqixl7289fmb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=606bc880bcfc4ac63ad9471f378c6c6ff486b0bc"}], "s": {"y": 518, "x": 927, "u": "https://preview.redd.it/bqixl7289fmb1.jpg?width=927&amp;format=pjpg&amp;auto=webp&amp;s=eee160f4f117c15a9afae83baa1bf9a092f069f5"}, "id": "bqixl7289fmb1"}}, "name": "t3_16akhq4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CgLVIijj4i-Qzkq-Jfex_U9OGvSnLaiqSrFCzBys2Pc.jpg", "edited": 1693913066.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693909596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a word cloud (See picture), and it gives an idea of what the most common words are in a string.&lt;/p&gt;\n\n&lt;p&gt;I want to get a quantified version of this, i.e. 20% of people think knowledge is important. However, I also want to automatically make synonyms equal. i.e.  quick = fast, bad = horrible. At the end of the day, I want to get a breakdown of what a group of people are saying.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bqixl7289fmb1.jpg?width=927&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eee160f4f117c15a9afae83baa1bf9a092f069f5\"&gt;https://preview.redd.it/bqixl7289fmb1.jpg?width=927&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eee160f4f117c15a9afae83baa1bf9a092f069f5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure there are existing techniques/ libraries out there to do this. Any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16akhq4", "is_robot_indexable": true, "report_reasons": null, "author": "tootieloolie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16akhq4/getting_insights_from_word_clouds_sentiment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16akhq4/getting_insights_from_word_clouds_sentiment/", "subreddit_subscribers": 1032696, "created_utc": 1693909596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Currently we are discussing having a data science monorepo in our company as it will insure consistency in the deployment pipelines and it will make sharing files between projects easier.  \n\n\nHowever, we are concerned about one point, each data science project needs a lot of iterations and experiments to reach its final form. If the experiments and jupyter notebooks of all projects lie in the same repo, we are afraid it may make the repo heavy for cloning and laggy when working with it.   \n\n\nDo you think these concerns are valid? what is your experience with working with data science monorepos?", "author_fullname": "t2_42ammirh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "a question about having a monorepo for data science projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ajc39", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693905717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently we are discussing having a data science monorepo in our company as it will insure consistency in the deployment pipelines and it will make sharing files between projects easier.  &lt;/p&gt;\n\n&lt;p&gt;However, we are concerned about one point, each data science project needs a lot of iterations and experiments to reach its final form. If the experiments and jupyter notebooks of all projects lie in the same repo, we are afraid it may make the repo heavy for cloning and laggy when working with it.   &lt;/p&gt;\n\n&lt;p&gt;Do you think these concerns are valid? what is your experience with working with data science monorepos?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ajc39", "is_robot_indexable": true, "report_reasons": null, "author": "GreyWardenMage", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ajc39/a_question_about_having_a_monorepo_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ajc39/a_question_about_having_a_monorepo_for_data/", "subreddit_subscribers": 1032696, "created_utc": 1693905717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Totally Confused about what to do next, Career HELP\n\nI'm working in ml/dl/data science  field previously on applied domain now working in the same domain but instead of applied working on its software stack.\n\nI am unable to figure out what i really like\n\nI'm working in ml/dl/data science  field previously on applied domain now working in the same domain but instead of applied working on its software stack.\n\nI am unable to figure out what i really like\nAreas towards which i'm biased are\n 1. HPC with a focus on ML application.\n2. ML research. \n3. Applied ml.\n\nAlso i am constantly in loop of not getting satisfaction at a job role neither getting a good mentor either at job or in open source community.\n\nThis constant dilemma makes me applying to jobs again and again hearing nothing from the ones i really like, I'm trapped in a cycle of overthinking doing nothing but feeling like i will not be able to do anything.\n\nI need guidance i need some paths some light from experienced or who have gone through the same.\n\nNOT sure to how many people this tweet will reach and how many will respond to it but if you come across this tweet and you have an idea about the same, please HELP.\n\nI want to work in the research labs or in an environment where i can grow through mentorship but seems like i made a wrong switch and here things are not what i thought it would be like. \n\nWithout phd the big research labs seems unreachable slowly i'm losing hope and feels like will be trapped in a job and eventually in a role that i do not like.", "author_fullname": "t2_a5kzhn7l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Totally Confused about what to do next", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16afsb0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693893206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Totally Confused about what to do next, Career HELP&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working in ml/dl/data science  field previously on applied domain now working in the same domain but instead of applied working on its software stack.&lt;/p&gt;\n\n&lt;p&gt;I am unable to figure out what i really like&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working in ml/dl/data science  field previously on applied domain now working in the same domain but instead of applied working on its software stack.&lt;/p&gt;\n\n&lt;p&gt;I am unable to figure out what i really like\nAreas towards which i&amp;#39;m biased are\n 1. HPC with a focus on ML application.\n2. ML research. \n3. Applied ml.&lt;/p&gt;\n\n&lt;p&gt;Also i am constantly in loop of not getting satisfaction at a job role neither getting a good mentor either at job or in open source community.&lt;/p&gt;\n\n&lt;p&gt;This constant dilemma makes me applying to jobs again and again hearing nothing from the ones i really like, I&amp;#39;m trapped in a cycle of overthinking doing nothing but feeling like i will not be able to do anything.&lt;/p&gt;\n\n&lt;p&gt;I need guidance i need some paths some light from experienced or who have gone through the same.&lt;/p&gt;\n\n&lt;p&gt;NOT sure to how many people this tweet will reach and how many will respond to it but if you come across this tweet and you have an idea about the same, please HELP.&lt;/p&gt;\n\n&lt;p&gt;I want to work in the research labs or in an environment where i can grow through mentorship but seems like i made a wrong switch and here things are not what i thought it would be like. &lt;/p&gt;\n\n&lt;p&gt;Without phd the big research labs seems unreachable slowly i&amp;#39;m losing hope and feels like will be trapped in a job and eventually in a role that i do not like.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16afsb0", "is_robot_indexable": true, "report_reasons": null, "author": "Frosty_Work4827", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16afsb0/totally_confused_about_what_to_do_next/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16afsb0/totally_confused_about_what_to_do_next/", "subreddit_subscribers": 1032696, "created_utc": 1693893206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nI have a set of original frequency counts from a dataset and I have the same frequency counts but they are perturbed by adding noise to prevent some counts to be reported explicitly.\n\nWhat I am trying to assess is between original and perturbed count sets, how much information is lost when reporting original v/s perturb counts. \n\nIs there any formal test that I can perform in a typical situation like this?\nAre there any traditional statistical methods that I can extend to for this use-case\n\nFor e.g, something that came to my mind is using Root Mean Squared Deviation/ Error from linear regression between original and perturbed counts to get and R^2 as an estimate of deviation in perturbed counts from original counts.\nAgain, not an expert here so I dont know if that is scientifically and theoretically true.\n\nAny guidance is appreciated!\nThanks!", "author_fullname": "t2_9po1x00s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comparing original values with perturbed values to assess information loss", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16b5s7p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693961085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a set of original frequency counts from a dataset and I have the same frequency counts but they are perturbed by adding noise to prevent some counts to be reported explicitly.&lt;/p&gt;\n\n&lt;p&gt;What I am trying to assess is between original and perturbed count sets, how much information is lost when reporting original v/s perturb counts. &lt;/p&gt;\n\n&lt;p&gt;Is there any formal test that I can perform in a typical situation like this?\nAre there any traditional statistical methods that I can extend to for this use-case&lt;/p&gt;\n\n&lt;p&gt;For e.g, something that came to my mind is using Root Mean Squared Deviation/ Error from linear regression between original and perturbed counts to get and R&lt;sup&gt;2&lt;/sup&gt; as an estimate of deviation in perturbed counts from original counts.\nAgain, not an expert here so I dont know if that is scientifically and theoretically true.&lt;/p&gt;\n\n&lt;p&gt;Any guidance is appreciated!\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b5s7p", "is_robot_indexable": true, "report_reasons": null, "author": "HealthtoML", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b5s7p/comparing_original_values_with_perturbed_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b5s7p/comparing_original_values_with_perturbed_values/", "subreddit_subscribers": 1032696, "created_utc": 1693961085.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "[https://opendatascience.com/a-primer-to-scaling-pandas/](https://opendatascience.com/a-primer-to-scaling-pandas/)", "author_fullname": "t2_8glql2df", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get pandas to work at scale with one small change.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16b5fqb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693960210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://opendatascience.com/a-primer-to-scaling-pandas/\"&gt;https://opendatascience.com/a-primer-to-scaling-pandas/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JJ-UsASBX_8XOT-BSED5EiZzxasdKe-XXN7-T1ZZ2rc.jpg?auto=webp&amp;s=06fea5f82deca630b813e0d8985f4faac71ed99f", "width": 640, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/JJ-UsASBX_8XOT-BSED5EiZzxasdKe-XXN7-T1ZZ2rc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3095593a81ff0543cd34ec5c15f1a6eff286e6d5", "width": 108, "height": 50}, {"url": "https://external-preview.redd.it/JJ-UsASBX_8XOT-BSED5EiZzxasdKe-XXN7-T1ZZ2rc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=642a7a04fdda820458ff4c67dbd9b51e5ec6187c", "width": 216, "height": 101}, {"url": "https://external-preview.redd.it/JJ-UsASBX_8XOT-BSED5EiZzxasdKe-XXN7-T1ZZ2rc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5cad24f571a3c2311268581d584f7d2af25b0ec1", "width": 320, "height": 150}, {"url": "https://external-preview.redd.it/JJ-UsASBX_8XOT-BSED5EiZzxasdKe-XXN7-T1ZZ2rc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=563fd92d9c22ff3b46cec435e2c15138159e2ec0", "width": 640, "height": 300}], "variants": {}, "id": "AGt6bxsbmM-0nQyYUtaaEsfO4Fc1-ls0Ub4VrKmg8ZY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b5fqb", "is_robot_indexable": true, "report_reasons": null, "author": "Data_Nerd1979", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b5fqb/get_pandas_to_work_at_scale_with_one_small_change/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b5fqb/get_pandas_to_work_at_scale_with_one_small_change/", "subreddit_subscribers": 1032696, "created_utc": 1693960210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello Everyone,\n\nI've put together an [**Ultimate dbt-utils Cheat Sheet**](https://datacoves.com/post/dbt-utils-cheatsheet) that I believe can aid those utilizing dbt in their data science endeavors. \n\nThe dbt-utils package has:\n\n* SQL generators for effective data manipulation.\n* Data validation strategies.\n* Introspective macros for better data comprehension.\n\nI regularly share helpful content over at Datacoves. \n\nWould love to know if you find this resource helpful or if there are any other dbt areas you'd like a deep dive into!", "author_fullname": "t2_fosm1pwyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From me to you: Handy dbt-utils Cheat Sheet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16b4y7a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693958979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve put together an &lt;a href=\"https://datacoves.com/post/dbt-utils-cheatsheet\"&gt;&lt;strong&gt;Ultimate dbt-utils Cheat Sheet&lt;/strong&gt;&lt;/a&gt; that I believe can aid those utilizing dbt in their data science endeavors. &lt;/p&gt;\n\n&lt;p&gt;The dbt-utils package has:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SQL generators for effective data manipulation.&lt;/li&gt;\n&lt;li&gt;Data validation strategies.&lt;/li&gt;\n&lt;li&gt;Introspective macros for better data comprehension.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I regularly share helpful content over at Datacoves. &lt;/p&gt;\n\n&lt;p&gt;Would love to know if you find this resource helpful or if there are any other dbt areas you&amp;#39;d like a deep dive into!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?auto=webp&amp;s=c72294a445c689d26088117fc5fff4bb905f488d", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7997828bba9f07448ccf1bc5fc04e76033b71944", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7195abbc2a5d6cdc443796e0ed5f9086f845e24d", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33d24d3f4487eaf5c747d62b56db72b2c8b84ebf", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=545aeb79a24b1e25d483c5cd733b6ca413f25389", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7207869fedc316bad07e0b593dcb7995e5baae33", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=42828b93ff9d50484b7639ebe782f4de5c04e40c", "width": 1080, "height": 564}], "variants": {}, "id": "fRjQkbaAYceSqHYVE-NBwTYR2ZoPUS8MUARvVnek75k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b4y7a", "is_robot_indexable": true, "report_reasons": null, "author": "Data-Queen-Mayra", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b4y7a/from_me_to_you_handy_dbtutils_cheat_sheet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b4y7a/from_me_to_you_handy_dbtutils_cheat_sheet/", "subreddit_subscribers": 1032696, "created_utc": 1693958979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I really like Philly as a city and am not a strong enough candidate for anything at Penn but drexels program especially their coop seems like it has strong ties with local companies. My only concern is whether the program is just a cash grab. I understand there are cheaper online programs but I do not perform well with online learning and am only looking at person learning opportunities.", "author_fullname": "t2_r9kqx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on Drexel for MSDS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16b4pvq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693958389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really like Philly as a city and am not a strong enough candidate for anything at Penn but drexels program especially their coop seems like it has strong ties with local companies. My only concern is whether the program is just a cash grab. I understand there are cheaper online programs but I do not perform well with online learning and am only looking at person learning opportunities.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b4pvq", "is_robot_indexable": true, "report_reasons": null, "author": "Spitball2468", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b4pvq/thoughts_on_drexel_for_msds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b4pvq/thoughts_on_drexel_for_msds/", "subreddit_subscribers": 1032696, "created_utc": 1693958389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Join for season 2 of the #TrueDataOps Podcast with Bob Muglia, formerly the Chief Executive Officer of Snowflake. We will cover his new book, The DataPreneurs ([https://www.thedatapreneurs.com/](https://www.thedatapreneurs.com/)), the new age of AI, and all things Data related!\n\nRSVP via LinkedIn Live - [https://www.linkedin.com/events/bobmugila-truedataopspodcastep-7085395129217269760/theater/](https://www.linkedin.com/events/bobmugila-truedataopspodcastep-7085395129217269760/theater/)\n\n[#TrueDataOps Pocast - Bob Muglia](https://preview.redd.it/1hiljupvrimb1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=0e6332350d51b255090bf1800d082afc03beaedd)", "author_fullname": "t2_12li6zgs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bob Muglia (Formerly Snowflake CEO) - #TrueDataOps Podcast Ep.19", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"1hiljupvrimb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/1hiljupvrimb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=32d86bb89978fe26419b5c6853b3ed2141117c94"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/1hiljupvrimb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=565a1dfd648c7940f68c16cb55ba4698156843d1"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/1hiljupvrimb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9caedbdf808438cdf8917686bab1fbaf9b67ea83"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/1hiljupvrimb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3d93810b84757fff5c717ed55475ee2b3966141a"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/1hiljupvrimb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=81e02ffbcf986b8d86b433e499ff34ec4e150e83"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/1hiljupvrimb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f249ab459d4adc6d3f9f045813b7999a49659ec0"}], "s": {"y": 900, "x": 1600, "u": "https://preview.redd.it/1hiljupvrimb1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=0e6332350d51b255090bf1800d082afc03beaedd"}, "id": "1hiljupvrimb1"}}, "name": "t3_16b3ni3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XooPJrc0C3DdJnZliDOlkhRcw4XUfdMS7jCnZWp0WMo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693955736.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Join for season 2 of the #TrueDataOps Podcast with Bob Muglia, formerly the Chief Executive Officer of Snowflake. We will cover his new book, The DataPreneurs (&lt;a href=\"https://www.thedatapreneurs.com/\"&gt;https://www.thedatapreneurs.com/&lt;/a&gt;), the new age of AI, and all things Data related!&lt;/p&gt;\n\n&lt;p&gt;RSVP via LinkedIn Live - &lt;a href=\"https://www.linkedin.com/events/bobmugila-truedataopspodcastep-7085395129217269760/theater/\"&gt;https://www.linkedin.com/events/bobmugila-truedataopspodcastep-7085395129217269760/theater/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1hiljupvrimb1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e6332350d51b255090bf1800d082afc03beaedd\"&gt;#TrueDataOps Pocast - Bob Muglia&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b3ni3", "is_robot_indexable": true, "report_reasons": null, "author": "Dkreig", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b3ni3/bob_muglia_formerly_snowflake_ceo_truedataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b3ni3/bob_muglia_formerly_snowflake_ceo_truedataops/", "subreddit_subscribers": 1032696, "created_utc": 1693955736.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey data friends\u00a0\ud83d\udc4b\n\n  \nI'm super excited to to share a preview of some new stuff I'm working on: \u2728\u00a0Turntable Discover \u2728\n\nData teams struggle to keep their documentation up to date and actionable. Today they have to stitch together Notion docs, lineage tools, yaml / markdown files and maintain jobs to generate it.\n\nWe\u2019ve built a seamless way to ingest, discover, and share your warehouse's documentation all in one place. Discover is integrated with Github and dbt core, so you can get setup with a magical docs experience in only a few minutes including:\n\n\u26a1\ufe0f Super fast search - search across table names, column, descriptions and canonical sql to find the data you're looking for\n\n\u2728\ufe0f AI-powered semantic search - can't find a substring query that matches? Use semantic search to find data models that are tricky to find (ex: \"reps\" -&gt; \"sales people\")\n\n\ud83d\udd2cColumn-level lineage view - trace back the origins of a particular column and navigate across models with an inline column level lineage view\n\n\ud83d\udd17\u00a0One-click sharing - share documentation in one click with other teammates on the same OAuth domain\n\nWe\u2019re giving Discover to select teams in a private beta before rolling it out broadly. If you\u2019d like to try it out, you can DM, comment below, or sign up on our waitlist ([turntable.so](http://turntable.so/)) Looking forward to hearing your feedback \ud83d\ude4c\n\n&amp;#x200B;\n\nCheck out a quick demo of how it works here:\n\n[https://www.youtube.com/watch?v=sY0NefWRpKQ](https://www.youtube.com/watch?v=sY0NefWRpKQ)", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A fast, AI-powered data catalog for dbt Core", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b1mnq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693951094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey data friends\u00a0\ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m super excited to to share a preview of some new stuff I&amp;#39;m working on: \u2728\u00a0Turntable Discover \u2728&lt;/p&gt;\n\n&lt;p&gt;Data teams struggle to keep their documentation up to date and actionable. Today they have to stitch together Notion docs, lineage tools, yaml / markdown files and maintain jobs to generate it.&lt;/p&gt;\n\n&lt;p&gt;We\u2019ve built a seamless way to ingest, discover, and share your warehouse&amp;#39;s documentation all in one place. Discover is integrated with Github and dbt core, so you can get setup with a magical docs experience in only a few minutes including:&lt;/p&gt;\n\n&lt;p&gt;\u26a1\ufe0f Super fast search - search across table names, column, descriptions and canonical sql to find the data you&amp;#39;re looking for&lt;/p&gt;\n\n&lt;p&gt;\u2728\ufe0f AI-powered semantic search - can&amp;#39;t find a substring query that matches? Use semantic search to find data models that are tricky to find (ex: &amp;quot;reps&amp;quot; -&amp;gt; &amp;quot;sales people&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd2cColumn-level lineage view - trace back the origins of a particular column and navigate across models with an inline column level lineage view&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd17\u00a0One-click sharing - share documentation in one click with other teammates on the same OAuth domain&lt;/p&gt;\n\n&lt;p&gt;We\u2019re giving Discover to select teams in a private beta before rolling it out broadly. If you\u2019d like to try it out, you can DM, comment below, or sign up on our waitlist (&lt;a href=\"http://turntable.so/\"&gt;turntable.so&lt;/a&gt;) Looking forward to hearing your feedback \ud83d\ude4c&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Check out a quick demo of how it works here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=sY0NefWRpKQ\"&gt;https://www.youtube.com/watch?v=sY0NefWRpKQ&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b1mnq", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b1mnq/a_fast_aipowered_data_catalog_for_dbt_core/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b1mnq/a_fast_aipowered_data_catalog_for_dbt_core/", "subreddit_subscribers": 1032696, "created_utc": 1693951094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Ever wondered about the potential of R for diverse applications beyond statistics? Rafael Camargo, a Spatial Data Scientist at Quantis, deeply delves into Berlin's flourishing R User Group (RUG).\n\nThe blog also features Rafael's personal journey with R, from automating tasks to exploring machine learning, offering a rich perspective on the tool's versatility. \n\nThe Berlin RUG is actively looking for venue sponsors for their in-person events. It's a unique chance to align your brand with innovation and thought leadership in Data Science.\n\n\ud83d\udd17Read more: [https://www.r-consortium.org/blog/2023/09/05/spatial-data-science-using-r-in-berlin-germany](https://www.r-consortium.org/blog/2023/09/05/spatial-data-science-using-r-in-berlin-germany) \n\nLet's keep the spirit of collaboration and learning alive!", "author_fullname": "t2_6ow4kclla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unlocking R's Potential Beyond Stats: Inside Berlin's R User Group with Rafael Camargo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b0jp9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693948747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever wondered about the potential of R for diverse applications beyond statistics? Rafael Camargo, a Spatial Data Scientist at Quantis, deeply delves into Berlin&amp;#39;s flourishing R User Group (RUG).&lt;/p&gt;\n\n&lt;p&gt;The blog also features Rafael&amp;#39;s personal journey with R, from automating tasks to exploring machine learning, offering a rich perspective on the tool&amp;#39;s versatility. &lt;/p&gt;\n\n&lt;p&gt;The Berlin RUG is actively looking for venue sponsors for their in-person events. It&amp;#39;s a unique chance to align your brand with innovation and thought leadership in Data Science.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd17Read more: &lt;a href=\"https://www.r-consortium.org/blog/2023/09/05/spatial-data-science-using-r-in-berlin-germany\"&gt;https://www.r-consortium.org/blog/2023/09/05/spatial-data-science-using-r-in-berlin-germany&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s keep the spirit of collaboration and learning alive!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/M_7602wFXzkSJXA2lWBADY35BbVuEEi8w3O3r_rhxk0.jpg?auto=webp&amp;s=5f66f0f109e1ec1b9675818171d3cf3c3abb011a", "width": 536, "height": 322}, "resolutions": [{"url": "https://external-preview.redd.it/M_7602wFXzkSJXA2lWBADY35BbVuEEi8w3O3r_rhxk0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a1bc172172be63c21a412692c45873e62f18cabb", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/M_7602wFXzkSJXA2lWBADY35BbVuEEi8w3O3r_rhxk0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4001305e6c5c236efea8f24aa143856146258ff8", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/M_7602wFXzkSJXA2lWBADY35BbVuEEi8w3O3r_rhxk0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=80699aaf841b63637c9795be345d1c370f266f84", "width": 320, "height": 192}], "variants": {}, "id": "PbMwLAkOMtIFWEg3WA69xWXz5OQ8EkpX9n3Lh7jEQ9I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b0jp9", "is_robot_indexable": true, "report_reasons": null, "author": "Interesting_Chance31", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b0jp9/unlocking_rs_potential_beyond_stats_inside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b0jp9/unlocking_rs_potential_beyond_stats_inside/", "subreddit_subscribers": 1032696, "created_utc": 1693948747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello everyone! I have a Master\u2019s in Industrial Engineering and entered the data analysis field after a short trainee period. I work on creating workflows using Alteryx and Tableau dashboards. While I enjoy what I do, the rapid advancements in AI worry me, and I feel the need to further develop my skills, especially given my limited background in information technology.\n\nI\u2019m willing to invest time and have a strong desire to learn, but I don\u2019t want to waste time on things that won\u2019t pay off. I also worry that self-study doesn\u2019t provide a credential to prove my skills, making me consider a second Master\u2019s degree as perhaps essential. Given my background, eagerness to grow, and concerns about credentialing, what certifications or training would best help me transition into data science or data engineering? What skills or experiences offer the most bang for the buck? Are private projects published on GitHub a viable alternative to formal education?", "author_fullname": "t2_l5mlqax", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning from Data Analysis to Data Science or Engineering with a Background in Industrial Engineering. Need Advice!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b0bjh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693948672.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693948250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! I have a Master\u2019s in Industrial Engineering and entered the data analysis field after a short trainee period. I work on creating workflows using Alteryx and Tableau dashboards. While I enjoy what I do, the rapid advancements in AI worry me, and I feel the need to further develop my skills, especially given my limited background in information technology.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m willing to invest time and have a strong desire to learn, but I don\u2019t want to waste time on things that won\u2019t pay off. I also worry that self-study doesn\u2019t provide a credential to prove my skills, making me consider a second Master\u2019s degree as perhaps essential. Given my background, eagerness to grow, and concerns about credentialing, what certifications or training would best help me transition into data science or data engineering? What skills or experiences offer the most bang for the buck? Are private projects published on GitHub a viable alternative to formal education?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16b0bjh", "is_robot_indexable": true, "report_reasons": null, "author": "benjamin_chr", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16b0bjh/transitioning_from_data_analysis_to_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16b0bjh/transitioning_from_data_analysis_to_data_science/", "subreddit_subscribers": 1032696, "created_utc": 1693948250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " *Hi guys, i'm starting an email newsletter on practical uses with AI/ChatGPT. The following will be my content for my first email. Please let me know what you think! Any thoughts, refinements, etc.*\n\n[https://superpineapple.beehiiv.com/subscribe](https://superpineapple.beehiiv.com/subscribe)\n\n# Step-By-Step\n\nIt\u2019s not as simple as asking ChatGPT - \u201ctell me whether I should invest in Company A\u201d. You need a plan, and you need choose the right tools.\n\n### Getting Ready\n\n**Prerequisites**: ChatGPT Plus\n\n**Overall Objective:** You will want answers to the key questions below (\u201cObjectives\u201d), as well as anything you think are relevant to help your investment decision\n\n**Installation:** Install the following plugins with the GPT4 Model - \u201cCompany Transcript\u201d, \u201cPolygon\u201d, \u201cAI PDF\u201d, \u201cBrowser\u201d, \u201cBrowserOp\u201d\n\n### Phase 1 - First Hand Research\n\nPlugins: \u201cCompany Transcript\u201d, \u201cPolygon\u201d, \u201cAI PDF\u201d\n\n**Step 1 (Learn from Earning Transcripts):** Use \u201cCompany Transcript\u201d to get answers from recent earning calls. You can literally use prompts like *\u201cHow has X performed\u201d*, and *\u201cwhat is A\u2019s strategy for growth and profitability\u201d*\n\n**Step 2 (Learn from Annual / Quarterly Reports)**: Use \u201cPolygon\u201d pull data from recent annual/quarterly reports. Alternatively, you can use \u201cAI PDF\u201d. Get the pdf links on these company's investor relations site (e.g. [***AMD***](https://ir.amd.com/sec-filings/filter/annual-filings?utm_source=superpineapple.beehiiv.com&amp;utm_medium=newsletter&amp;utm_campaign=chatgpt-investment-research)), and query the annual/quarterly report more directly. Similar questions apply\n\nNote: You will probably need to ask follow-up and clarifying questions as you go along, but the idea is to have ChatGPT read these lengthy docs for you.\n\n### Phase 2 - Analyst Reports\n\nPlugins: \u201cBrowser and \u201cBrowserOp\u201d\n\n**Step 3 (Get your list of articles):** Use \u201cBrowser\u201d to get a list of articles from the internet that contain analyst reports and recommendations. An example - *\u201cFind me analyst articles on AMD that give an opinion on their current performance and future prospects. Summarize them\u201d*\n\n**Step 4 (Dig into the articles)**: Use \u201cBrowserOp\u201d to actually access the relevant articles and dig into details. My favorite prompt is as follows - *\u201cCan you use BrowserOp to access these articles and collate the information across all these articles. I would like the depth and reasons behind the recommendations. Present everything in a thesis / anti-thesis format as to whether I should* buy AMD stock\u201d\n\nNote: Follow up questions are definitely needed here as well. Try to have a conversation with ChatGPT to sense check the recommendations.\n\n# Objectives\n\nI like to be able to get a robust and detailed answer to these questions, to help with my investment decisions. Feel free to steal them as prompts!\n\n* **Business Model:** How does the company make money?\n* **Demand Dynamics:** Are its products or services in demand, and why?\n* **Historical Performance:** How has the company performed in the past?\n* **Leadership:** Are talented, experienced managers in charge?\n* **Growth Prospects:** Is the company positioned for growth and profitability?\n* **Financial Health:** How much debt does the company have?\n* **Industry Analysis:** How is the company\u2019s industry doing as a whole?\n* **Challenges:** What are the obstacles and challenges the company faces?\n* **Risks:** Does the company face any economic, political, or cultural risks?\n\nYou also want to know these metrics. Compare the company\u2019s metrics to the broader market, and their specific industry.\n\n* **EPS (Earnings Per Share)**\n* **P/E Ratio (Price to Earnings Ratio)**\n* **Price to Sales Ratio**\n* **Debt to Equity Ratio**", "author_fullname": "t2_8xwepztbe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ChatGPT + Investment Research", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ay5p6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693943515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Hi guys, i&amp;#39;m starting an email newsletter on practical uses with AI/ChatGPT. The following will be my content for my first email. Please let me know what you think! Any thoughts, refinements, etc.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://superpineapple.beehiiv.com/subscribe\"&gt;https://superpineapple.beehiiv.com/subscribe&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Step-By-Step&lt;/h1&gt;\n\n&lt;p&gt;It\u2019s not as simple as asking ChatGPT - \u201ctell me whether I should invest in Company A\u201d. You need a plan, and you need choose the right tools.&lt;/p&gt;\n\n&lt;h3&gt;Getting Ready&lt;/h3&gt;\n\n&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: ChatGPT Plus&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Overall Objective:&lt;/strong&gt; You will want answers to the key questions below (\u201cObjectives\u201d), as well as anything you think are relevant to help your investment decision&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Installation:&lt;/strong&gt; Install the following plugins with the GPT4 Model - \u201cCompany Transcript\u201d, \u201cPolygon\u201d, \u201cAI PDF\u201d, \u201cBrowser\u201d, \u201cBrowserOp\u201d&lt;/p&gt;\n\n&lt;h3&gt;Phase 1 - First Hand Research&lt;/h3&gt;\n\n&lt;p&gt;Plugins: \u201cCompany Transcript\u201d, \u201cPolygon\u201d, \u201cAI PDF\u201d&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 1 (Learn from Earning Transcripts):&lt;/strong&gt; Use \u201cCompany Transcript\u201d to get answers from recent earning calls. You can literally use prompts like &lt;em&gt;\u201cHow has X performed\u201d&lt;/em&gt;, and &lt;em&gt;\u201cwhat is A\u2019s strategy for growth and profitability\u201d&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 2 (Learn from Annual / Quarterly Reports)&lt;/strong&gt;: Use \u201cPolygon\u201d pull data from recent annual/quarterly reports. Alternatively, you can use \u201cAI PDF\u201d. Get the pdf links on these company&amp;#39;s investor relations site (e.g. &lt;a href=\"https://ir.amd.com/sec-filings/filter/annual-filings?utm_source=superpineapple.beehiiv.com&amp;amp;utm_medium=newsletter&amp;amp;utm_campaign=chatgpt-investment-research\"&gt;&lt;strong&gt;&lt;em&gt;AMD&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;), and query the annual/quarterly report more directly. Similar questions apply&lt;/p&gt;\n\n&lt;p&gt;Note: You will probably need to ask follow-up and clarifying questions as you go along, but the idea is to have ChatGPT read these lengthy docs for you.&lt;/p&gt;\n\n&lt;h3&gt;Phase 2 - Analyst Reports&lt;/h3&gt;\n\n&lt;p&gt;Plugins: \u201cBrowser and \u201cBrowserOp\u201d&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 3 (Get your list of articles):&lt;/strong&gt; Use \u201cBrowser\u201d to get a list of articles from the internet that contain analyst reports and recommendations. An example - &lt;em&gt;\u201cFind me analyst articles on AMD that give an opinion on their current performance and future prospects. Summarize them\u201d&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 4 (Dig into the articles)&lt;/strong&gt;: Use \u201cBrowserOp\u201d to actually access the relevant articles and dig into details. My favorite prompt is as follows - &lt;em&gt;\u201cCan you use BrowserOp to access these articles and collate the information across all these articles. I would like the depth and reasons behind the recommendations. Present everything in a thesis / anti-thesis format as to whether I should&lt;/em&gt; buy AMD stock\u201d&lt;/p&gt;\n\n&lt;p&gt;Note: Follow up questions are definitely needed here as well. Try to have a conversation with ChatGPT to sense check the recommendations.&lt;/p&gt;\n\n&lt;h1&gt;Objectives&lt;/h1&gt;\n\n&lt;p&gt;I like to be able to get a robust and detailed answer to these questions, to help with my investment decisions. Feel free to steal them as prompts!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Business Model:&lt;/strong&gt; How does the company make money?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Demand Dynamics:&lt;/strong&gt; Are its products or services in demand, and why?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Historical Performance:&lt;/strong&gt; How has the company performed in the past?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Leadership:&lt;/strong&gt; Are talented, experienced managers in charge?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Growth Prospects:&lt;/strong&gt; Is the company positioned for growth and profitability?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Financial Health:&lt;/strong&gt; How much debt does the company have?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Industry Analysis:&lt;/strong&gt; How is the company\u2019s industry doing as a whole?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Challenges:&lt;/strong&gt; What are the obstacles and challenges the company faces?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Risks:&lt;/strong&gt; Does the company face any economic, political, or cultural risks?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You also want to know these metrics. Compare the company\u2019s metrics to the broader market, and their specific industry.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;EPS (Earnings Per Share)&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;P/E Ratio (Price to Earnings Ratio)&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Price to Sales Ratio&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Debt to Equity Ratio&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?auto=webp&amp;s=88c7167e5ad8150d2f429e247a76802e8a1a6148", "width": 1199, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e1e1bd23c89f7ad647b1de6fde23ca337ffeacca", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400a1a2a825ef770ed8f56f3088f813cd2de9462", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e035677ebc7d242bebdf9e1b9fb8d252c9461f0", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=421af7e1068f8578afffb95812da82b11ae641fc", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5894c1be66680f613b52a21831ac258135a3159d", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/h06xYMyjcTVCu9Ycv4Y3J05qgVnUDmCzfZzMiI5Zr34.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f50e7e5a490938cb5bca5b92f55b1a2ce2614267", "width": 1080, "height": 1080}], "variants": {}, "id": "VImqdnKEmjE76MlFz5BQ0zc2dSL9Ya9-VTTUuw96kgE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ay5p6", "is_robot_indexable": true, "report_reasons": null, "author": "saasthom", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ay5p6/chatgpt_investment_research/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ay5p6/chatgpt_investment_research/", "subreddit_subscribers": 1032696, "created_utc": 1693943515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\u201cMCMC vs VI\u201d is no longer a discussion about your favourite Roman numeral. If you share my trepidation for model performance in the face of data sparsity, or you simply suffer from anxiety uncertainty, you might be tempted into the Bayesian world. Years later at the precipice of your career (and mental health degeneracy) you over-engineer probabilistic models so intractable that would stress Lord Bayes himself into stomach ulcers. The solution? Approximate inference, the true antihero to model simplification. I wrote a brief primer for those who enjoy maths and those who disdain it, in both cases it's impossible to avoid using maths while discussing Bayesian statistics so I kept it as light as I could.\n\nPS - This is a Reddit-friendly copypasta from my medium article, so if you're a visual person then head over there to get the visuals.\n\n**Bayesian Modelling**\n\nA closed-form solution to a machine learning model is one that can be written down on a sheet of paper using a finite number of standard mathematical operations. For example, linear models have closed-form solutions IF the design covariance matrix is invertible, otherwise we obtain a solution using iterative optimisation.\n\nBayesian models do not typically have exact closed-form solutions for their posterior distributions. One thing that typically helps is choosing simple models, Gaussian likelihood functions and conjugate priors. A prior distribution is said to be conjugate to a likelihood function if the resulting posterior belongs to the same distribution family as the prior.\n\nBayesian linear regression is a model that typically assumes Gaussian priors over both the regression coefficients and the likelihood function. When we update the prior with the observed data (using Bayes\u2019 theorem), the resulting posterior distribution for the regression coefficients will also follow a normal distribution. This can be written down analytically and sampled using standard methods in Python.\n\nConjugacy, however, does not always guarantee tractability. High-dimensional parameter spaces, hierarchical structures, non-Gaussian likelihoods with non-linear prior interactions can give rise to intractable integrals for the normalisation constant (which involve over the entire parameter space). This actually becomes prohibitive when want to build, say, a Multivariate Gaussian Linear Regression model with many predictors, or when we want to model count data using a Poisson likelihood and control for overfitting using on using a Laplace (non-conjugate) prior. Thankfully, a solution as old as the first computers comes to the rescue: Markov Chain Monte Carlo (MCMC).\n\n**Markov Chain Monte Carlo**\n\nMarkov Chain Monte Carlo can be described with enough mathematical jargon to send one fleeing back to first-derivative optimisers, so I\u2019ll skip the stomach ulcers and give an intuitive overview instead.\n\nGiven a probabilistic model parameterised by latent continuous random variables z, and observed values x, we can write down the known form for its probability density function P(z | x). If P(z | x) is intractable, we want to to generate an empirical distribution of samples based on a Markov chain that approximates the probability distribution.\n\nThis empirical distribution can then be used in place of the analytical solution to estimate posterior means, variances, quantiles, and other probabilistic summaries of the model parameters. The most important question: who is Markov and why are we talking about his chain?\n\nIn MCMC, a Markov chain is simply a sequence of samples where each sample is \u201cmemoryless\u201d, i.e. the probability of transitioning to the next sample depends only on the current sample and not on the previous history. This helps us reach a \u201cstationary\u201d distribution over samples, i.e. when we run the chain long enough, the probability P(0 &lt; z &lt; 1 | x)\\_{n} at iteration n and P(0 &lt; z &lt; 1 | x)\\_{m} at iteration m should be equal.\n\nWhat\u2019s amazing is that the distribution over samples from our Markov chain provides asymptotic exactness; MCMC converges to the true posterior distribution in the limit of infinite samples. How is this implemented in practice?\n\n**The Metropolis-Hastings (MH) Algorithm**\n\nMetropolis-Hastings (MH) is a specific type of (MCMC) algorithm ubiquitously used in approximate inference. The idea is to build a chain of samples with a proposal distribution that selects \u201cthe next\u201d sample based only on the \u201cthe current\u201d sample (remember the Markov principle).\n\nProposed samples with higher probabilities in our posterior are accepted into the chain more frequently and those with lower probabilities are rejected more often (don\u2019t make it into the chain). How do we capture the \u201ctails\u201d of our posterior if we\u2019re busy focusing on high probability regions?\n\nThis is where Metropolis-Hastings acceptance/rejection mechanism really shines:\n\nFor any given proposed sample, we define acceptance probability = min(1, \u03b1), where \u03b1 as the ratio of target and proposal distributions at proposed and current samples.\n\nNext comes the heart of the algorithm\u2019s exploration-exploitation mechanism: We generate a random number n in the domain \\[0,1\\]. If n \u2264 \u03b1, accept the new sample in the chain; if n &gt; \u03b1, keep the current sample and don\u2019t extend the chain.\n\nThe best bit? Samples with acceptance probabilities close to 1 are more likely to move the chain towards higher probability regions. Those with low acceptance probabilities can be accepted when 0 \u2264 n \u2264 \u03b1, exploring lower probability areas and avoiding local modes.\n\nWhat diagnostics do we run to check that MH successfully converged to a stationary empirical approximation to the posterior?\n\n**Trace Plots and Chain Mixing**\n\nA \u201ctrace plot\u201d allows us to inspect the chain by plotting accepted sample values for each iteration.\n\nWhat we\u2019re looking for is low autocorrelation between successive samples, and full exploration of the sample space characterised by high variance across moving windows of the trace. Chain 1 is an example of an ideal trace. Chain 2 initially has high autocorrelation and low variance but converges to stationarity after iteration t \\~1500. We discard the head segment t &lt; 1500 (so-called burn-in samples) since they\u2019re unlikely to be part of our target distribution.\n\nWhat about Chain 3? This trace demonstrates poor chain mixing, moving slowly across the parameter space between different regions of the distribution. One problem could be that we just haven\u2019t let the algorithm run long enough; but Model complexity increases with multimodality, high dimensionality and correlated parameters the asymptotic exactness guarantee of MCMC doesn\u2019t come with a tqdm, you could be waiting for quite a while. In these cases, we present the next best thing: variational inference.\n\n**Variatonal Inference**\n\nWhile MCMC offers asymptotic exactness around high dimensional distributions, it can be computationally intensive and impractical for complex distributions. Often we\u2019re just interested in a rough approximation to the posterior that scales well for deployment.\n\nVariational Inference (VI) frames the problem of approximating the posterior as an optimisation problem. Starting with a synthetic posterior Q(z | \u03bb) built from families of simpler distributions (known as the variational family), we optimise over parameters \u03bb that minimise the distance between the variational family and the true posterior P(z | x). This sounds cool but how do we choose the variational family? What even is a distance between distributions?\n\n**KL-Divergence**\n\nThe choice of Q(z | \u03bb) depends on the degree of flexibility required (increasing with the complexity of P(z | x)), but common choices are exponential or Gaussian distributions. To compare the \u201ccloseness\u201d of P and Q, we employ a similarity measure such as the Kullback-Leibler (KL) divergence:\n\nAlthough the KL divergence is asymmetric (DKL(Q||P) =/= DKL(P||Q)), it helps to quantify the difference between Q and P.\n\n\u201cBut we don\u2019t have a closed form solution for P(z|x)!\u201d I hear you exclaiming correctly. That\u2019s why we compute something called the Evidence Lower Bound (ELBO) instead: ELBO(\u03bb) = log( P(x) ) \u2212DKL(Q(z | \u03bb)||P(z | x)). This can be rearranged as ELBO(\u03bb) = E\u200b\\[log P(x, z)\\]\u2212E\u200b\\[log Q(z\u2223\u03bb)\\] helps us avoid that pesky intractable marginal integral.\n\nThus, maximizing ELBO is equivalent to minimizing the KL divergence, serves as an objective function we optimise using standard methods like co-ordinate ascent. Once \u03bb are optimised, the approximating distribution Q(z | \u03bb) serves as a surrogate for the true posterior. This approximation can then be used for downstream tasks like prediction, data imputation, or model interpretation.", "author_fullname": "t2_7h1mi6us", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tl;dr Approximate Inference methods made easy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16awjx5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693939860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\u201cMCMC vs VI\u201d is no longer a discussion about your favourite Roman numeral. If you share my trepidation for model performance in the face of data sparsity, or you simply suffer from anxiety uncertainty, you might be tempted into the Bayesian world. Years later at the precipice of your career (and mental health degeneracy) you over-engineer probabilistic models so intractable that would stress Lord Bayes himself into stomach ulcers. The solution? Approximate inference, the true antihero to model simplification. I wrote a brief primer for those who enjoy maths and those who disdain it, in both cases it&amp;#39;s impossible to avoid using maths while discussing Bayesian statistics so I kept it as light as I could.&lt;/p&gt;\n\n&lt;p&gt;PS - This is a Reddit-friendly copypasta from my medium article, so if you&amp;#39;re a visual person then head over there to get the visuals.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Bayesian Modelling&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A closed-form solution to a machine learning model is one that can be written down on a sheet of paper using a finite number of standard mathematical operations. For example, linear models have closed-form solutions IF the design covariance matrix is invertible, otherwise we obtain a solution using iterative optimisation.&lt;/p&gt;\n\n&lt;p&gt;Bayesian models do not typically have exact closed-form solutions for their posterior distributions. One thing that typically helps is choosing simple models, Gaussian likelihood functions and conjugate priors. A prior distribution is said to be conjugate to a likelihood function if the resulting posterior belongs to the same distribution family as the prior.&lt;/p&gt;\n\n&lt;p&gt;Bayesian linear regression is a model that typically assumes Gaussian priors over both the regression coefficients and the likelihood function. When we update the prior with the observed data (using Bayes\u2019 theorem), the resulting posterior distribution for the regression coefficients will also follow a normal distribution. This can be written down analytically and sampled using standard methods in Python.&lt;/p&gt;\n\n&lt;p&gt;Conjugacy, however, does not always guarantee tractability. High-dimensional parameter spaces, hierarchical structures, non-Gaussian likelihoods with non-linear prior interactions can give rise to intractable integrals for the normalisation constant (which involve over the entire parameter space). This actually becomes prohibitive when want to build, say, a Multivariate Gaussian Linear Regression model with many predictors, or when we want to model count data using a Poisson likelihood and control for overfitting using on using a Laplace (non-conjugate) prior. Thankfully, a solution as old as the first computers comes to the rescue: Markov Chain Monte Carlo (MCMC).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Markov Chain Monte Carlo&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Markov Chain Monte Carlo can be described with enough mathematical jargon to send one fleeing back to first-derivative optimisers, so I\u2019ll skip the stomach ulcers and give an intuitive overview instead.&lt;/p&gt;\n\n&lt;p&gt;Given a probabilistic model parameterised by latent continuous random variables z, and observed values x, we can write down the known form for its probability density function P(z | x). If P(z | x) is intractable, we want to to generate an empirical distribution of samples based on a Markov chain that approximates the probability distribution.&lt;/p&gt;\n\n&lt;p&gt;This empirical distribution can then be used in place of the analytical solution to estimate posterior means, variances, quantiles, and other probabilistic summaries of the model parameters. The most important question: who is Markov and why are we talking about his chain?&lt;/p&gt;\n\n&lt;p&gt;In MCMC, a Markov chain is simply a sequence of samples where each sample is \u201cmemoryless\u201d, i.e. the probability of transitioning to the next sample depends only on the current sample and not on the previous history. This helps us reach a \u201cstationary\u201d distribution over samples, i.e. when we run the chain long enough, the probability P(0 &amp;lt; z &amp;lt; 1 | x)_{n} at iteration n and P(0 &amp;lt; z &amp;lt; 1 | x)_{m} at iteration m should be equal.&lt;/p&gt;\n\n&lt;p&gt;What\u2019s amazing is that the distribution over samples from our Markov chain provides asymptotic exactness; MCMC converges to the true posterior distribution in the limit of infinite samples. How is this implemented in practice?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Metropolis-Hastings (MH) Algorithm&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Metropolis-Hastings (MH) is a specific type of (MCMC) algorithm ubiquitously used in approximate inference. The idea is to build a chain of samples with a proposal distribution that selects \u201cthe next\u201d sample based only on the \u201cthe current\u201d sample (remember the Markov principle).&lt;/p&gt;\n\n&lt;p&gt;Proposed samples with higher probabilities in our posterior are accepted into the chain more frequently and those with lower probabilities are rejected more often (don\u2019t make it into the chain). How do we capture the \u201ctails\u201d of our posterior if we\u2019re busy focusing on high probability regions?&lt;/p&gt;\n\n&lt;p&gt;This is where Metropolis-Hastings acceptance/rejection mechanism really shines:&lt;/p&gt;\n\n&lt;p&gt;For any given proposed sample, we define acceptance probability = min(1, \u03b1), where \u03b1 as the ratio of target and proposal distributions at proposed and current samples.&lt;/p&gt;\n\n&lt;p&gt;Next comes the heart of the algorithm\u2019s exploration-exploitation mechanism: We generate a random number n in the domain [0,1]. If n \u2264 \u03b1, accept the new sample in the chain; if n &amp;gt; \u03b1, keep the current sample and don\u2019t extend the chain.&lt;/p&gt;\n\n&lt;p&gt;The best bit? Samples with acceptance probabilities close to 1 are more likely to move the chain towards higher probability regions. Those with low acceptance probabilities can be accepted when 0 \u2264 n \u2264 \u03b1, exploring lower probability areas and avoiding local modes.&lt;/p&gt;\n\n&lt;p&gt;What diagnostics do we run to check that MH successfully converged to a stationary empirical approximation to the posterior?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Trace Plots and Chain Mixing&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A \u201ctrace plot\u201d allows us to inspect the chain by plotting accepted sample values for each iteration.&lt;/p&gt;\n\n&lt;p&gt;What we\u2019re looking for is low autocorrelation between successive samples, and full exploration of the sample space characterised by high variance across moving windows of the trace. Chain 1 is an example of an ideal trace. Chain 2 initially has high autocorrelation and low variance but converges to stationarity after iteration t ~1500. We discard the head segment t &amp;lt; 1500 (so-called burn-in samples) since they\u2019re unlikely to be part of our target distribution.&lt;/p&gt;\n\n&lt;p&gt;What about Chain 3? This trace demonstrates poor chain mixing, moving slowly across the parameter space between different regions of the distribution. One problem could be that we just haven\u2019t let the algorithm run long enough; but Model complexity increases with multimodality, high dimensionality and correlated parameters the asymptotic exactness guarantee of MCMC doesn\u2019t come with a tqdm, you could be waiting for quite a while. In these cases, we present the next best thing: variational inference.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Variatonal Inference&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;While MCMC offers asymptotic exactness around high dimensional distributions, it can be computationally intensive and impractical for complex distributions. Often we\u2019re just interested in a rough approximation to the posterior that scales well for deployment.&lt;/p&gt;\n\n&lt;p&gt;Variational Inference (VI) frames the problem of approximating the posterior as an optimisation problem. Starting with a synthetic posterior Q(z | \u03bb) built from families of simpler distributions (known as the variational family), we optimise over parameters \u03bb that minimise the distance between the variational family and the true posterior P(z | x). This sounds cool but how do we choose the variational family? What even is a distance between distributions?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;KL-Divergence&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The choice of Q(z | \u03bb) depends on the degree of flexibility required (increasing with the complexity of P(z | x)), but common choices are exponential or Gaussian distributions. To compare the \u201ccloseness\u201d of P and Q, we employ a similarity measure such as the Kullback-Leibler (KL) divergence:&lt;/p&gt;\n\n&lt;p&gt;Although the KL divergence is asymmetric (DKL(Q||P) =/= DKL(P||Q)), it helps to quantify the difference between Q and P.&lt;/p&gt;\n\n&lt;p&gt;\u201cBut we don\u2019t have a closed form solution for P(z|x)!\u201d I hear you exclaiming correctly. That\u2019s why we compute something called the Evidence Lower Bound (ELBO) instead: ELBO(\u03bb) = log( P(x) ) \u2212DKL(Q(z | \u03bb)||P(z | x)). This can be rearranged as ELBO(\u03bb) = E\u200b[log P(x, z)]\u2212E\u200b[log Q(z\u2223\u03bb)] helps us avoid that pesky intractable marginal integral.&lt;/p&gt;\n\n&lt;p&gt;Thus, maximizing ELBO is equivalent to minimizing the KL divergence, serves as an objective function we optimise using standard methods like co-ordinate ascent. Once \u03bb are optimised, the approximating distribution Q(z | \u03bb) serves as a surrogate for the true posterior. This approximation can then be used for downstream tasks like prediction, data imputation, or model interpretation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16awjx5", "is_robot_indexable": true, "report_reasons": null, "author": "SnooPaintings5866", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16awjx5/tldr_approximate_inference_methods_made_easy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16awjx5/tldr_approximate_inference_methods_made_easy/", "subreddit_subscribers": 1032696, "created_utc": 1693939860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello! So my background was in social science academically (economics from a top UK uni) and I\u2019ve done a law conversion after that and worked in legal related field in the UK. \n\nRecently I\u2019ve been missing the quantitative components in my degree after dealing with texts for so long, and looking to switch career into data. My economics degree was among the most quantitative economics degree in the UK and I\u2019ve done a fair bit of stats &amp; econometrics at the time, but haven\u2019t used them for a few years now. \n\nI\u2019ve enrolled in a part time bootcamp for coding atm, and I was wondering if there\u2019s any advice on what to focus/how to sell my background? Also wasn\u2019t sure how to phrase the skills from law to be relevant - if the legal experience was helpful at all? Thank you :)", "author_fullname": "t2_cp02o7wy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for someone from a social science background &amp; career change?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16apgfz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693923327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! So my background was in social science academically (economics from a top UK uni) and I\u2019ve done a law conversion after that and worked in legal related field in the UK. &lt;/p&gt;\n\n&lt;p&gt;Recently I\u2019ve been missing the quantitative components in my degree after dealing with texts for so long, and looking to switch career into data. My economics degree was among the most quantitative economics degree in the UK and I\u2019ve done a fair bit of stats &amp;amp; econometrics at the time, but haven\u2019t used them for a few years now. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve enrolled in a part time bootcamp for coding atm, and I was wondering if there\u2019s any advice on what to focus/how to sell my background? Also wasn\u2019t sure how to phrase the skills from law to be relevant - if the legal experience was helpful at all? Thank you :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16apgfz", "is_robot_indexable": true, "report_reasons": null, "author": "Mundane-Trade4453", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16apgfz/advice_for_someone_from_a_social_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16apgfz/advice_for_someone_from_a_social_science/", "subreddit_subscribers": 1032696, "created_utc": 1693923327.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}