{"kind": "Listing", "data": {"after": "t3_16qxxs5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'll be having a job interview in a few days and I think this question might come up and I personally don't know how to explain it to a layperson. Black box methods may come in handy one day, but I realized just now that I can't briefly explain how it works without making it sound like magic. What's your workaround for this? Have you been in a situation where you presented your results and you had to explain how neural networks operate in detail? Any similar experiences?", "author_fullname": "t2_bmbqthh3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For people in the industry, how do you explain the poor interpretability of some ML techniques to bosses who are not data scientists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qtywk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695549905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll be having a job interview in a few days and I think this question might come up and I personally don&amp;#39;t know how to explain it to a layperson. Black box methods may come in handy one day, but I realized just now that I can&amp;#39;t briefly explain how it works without making it sound like magic. What&amp;#39;s your workaround for this? Have you been in a situation where you presented your results and you had to explain how neural networks operate in detail? Any similar experiences?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qtywk", "is_robot_indexable": true, "report_reasons": null, "author": "krabbypatty-o-fish", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qtywk/for_people_in_the_industry_how_do_you_explain_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qtywk/for_people_in_the_industry_how_do_you_explain_the/", "subreddit_subscribers": 1059475, "created_utc": 1695549905.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Often you hear people saying that understanding the inner workings of models and algorithms is irrelevant and a waste of time. I am currently a MS student and struggle to understand some of the inner workings of things such as M-estimation for robust regression and I believe it\u2019s due to my poor statical background (CS undergrad). \n\nShould I put the time into going back and getting proper statistical credentials (I want to frankly) or is it a nice to have that isnt worth the time and money?", "author_fullname": "t2_pdclzxln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Poor statistical/Linear Algebra foundation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r1881", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695571158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Often you hear people saying that understanding the inner workings of models and algorithms is irrelevant and a waste of time. I am currently a MS student and struggle to understand some of the inner workings of things such as M-estimation for robust regression and I believe it\u2019s due to my poor statical background (CS undergrad). &lt;/p&gt;\n\n&lt;p&gt;Should I put the time into going back and getting proper statistical credentials (I want to frankly) or is it a nice to have that isnt worth the time and money?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r1881", "is_robot_indexable": true, "report_reasons": null, "author": "LongjumpingWheel11", "discussion_type": null, "num_comments": 23, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r1881/poor_statisticallinear_algebra_foundation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r1881/poor_statisticallinear_algebra_foundation/", "subreddit_subscribers": 1059475, "created_utc": 1695571158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been working in a data science Consulting startup as a data scientist. All I've done is write sql tables. I've started job hunting. I want to build AI products. What job description would that be? I know this sounds stupid but I don't want to be an analyst anymore", "author_fullname": "t2_5gr8xljt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do data scientists do anyway?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r5v0j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695582446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working in a data science Consulting startup as a data scientist. All I&amp;#39;ve done is write sql tables. I&amp;#39;ve started job hunting. I want to build AI products. What job description would that be? I know this sounds stupid but I don&amp;#39;t want to be an analyst anymore&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r5v0j", "is_robot_indexable": true, "report_reasons": null, "author": "wonko_the_sane__", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r5v0j/what_do_data_scientists_do_anyway/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r5v0j/what_do_data_scientists_do_anyway/", "subreddit_subscribers": 1059475, "created_utc": 1695582446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I browse through post often and general have limited understanding of what is being discussed. I can understand the very basics but the more indepth the convo goes the less I'm able to follow.\n\nI currently work as a BI developer, use SQL quite a bit and PowerBI, power automate.\n\nMy goal is to eventually dive into Data Science. At this time I would say I am no where close to being ready. I am enrolled amd planning to start Georgia Tech OMSA this upcoming spring. My question is.. did academics prepare you adequately to where you're able converse and function well as a DS?\n\nTIA!", "author_fullname": "t2_m8a5u0h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did academics prepare you for your role, or for the DS/ML/AI field?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qxfo0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695561104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I browse through post often and general have limited understanding of what is being discussed. I can understand the very basics but the more indepth the convo goes the less I&amp;#39;m able to follow.&lt;/p&gt;\n\n&lt;p&gt;I currently work as a BI developer, use SQL quite a bit and PowerBI, power automate.&lt;/p&gt;\n\n&lt;p&gt;My goal is to eventually dive into Data Science. At this time I would say I am no where close to being ready. I am enrolled amd planning to start Georgia Tech OMSA this upcoming spring. My question is.. did academics prepare you adequately to where you&amp;#39;re able converse and function well as a DS?&lt;/p&gt;\n\n&lt;p&gt;TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qxfo0", "is_robot_indexable": true, "report_reasons": null, "author": "AwkWORD47", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qxfo0/did_academics_prepare_you_for_your_role_or_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qxfo0/did_academics_prepare_you_for_your_role_or_for/", "subreddit_subscribers": 1059475, "created_utc": 1695561104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Enhancing your data analysis performance with Python's Numexpr and Pandas' eval/query functions \n\n This article was originally published on my personal blog [Data Leads Future.](https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/) \n\n&amp;#x200B;\n\n[ Use Numexpr to help me find the most livable city. Photo Credit: Created by Author, Canva ](https://preview.redd.it/29ec8ukgr5qb1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=238da4465da17bd301b49112d574444b16d4113c)\n\n This article will introduce you to the Python library [Numexpr](https://numexpr.readthedocs.io/en/latest/intro.html?ref=dataleadsfuture.com#), a tool that boosts the computational performance of Numpy Arrays. The eval and query methods of Pandas are also based on this library.\n\n This article also includes a hands-on weather data analysis project. \n\n By reading this article, you will understand the principles of Numexpr and how to use this powerful tool to speed up your calculations in reality. \n\n# Introduction \n\n# Recalling Numpy Arrays\n\n In a previous article discussing Numpy Arrays, I used a library example to explain why Numpy's Cache Locality is so efficient: \n\n[https://www.dataleadsfuture.com/python-lists-vs-numpy-arrays-a-deep-dive-into-memory-layout-and-performance-benefits/](https://www.dataleadsfuture.com/python-lists-vs-numpy-arrays-a-deep-dive-into-memory-layout-and-performance-benefits/)\n\n Each time you go to the library to search for materials, you take out a few books related to the content and place them next to your desk. \n\n This way, you can quickly check related materials without having to run to the shelf each time you need to read a book. \n\n This method saves a lot of time, especially when you need to consult many related books. \n\n In this scenario, the shelf is like your memory, the desk is equivalent to the CPU's L1 cache, and you, the reader, are the CPU's core. \n\n&amp;#x200B;\n\n[ When the CPU accesses RAM, the cache loads the entire cache line into the high-speed cache. Image by Author ](https://preview.redd.it/3k7gdxywr5qb1.png?width=625&amp;format=png&amp;auto=webp&amp;s=5e6a2ed6620cf23638d9c3f20e5f6f46a54e0049)\n\n### The limitations of Numpy\n\n Suppose you are unfortunate enough to encounter a demanding professor who wants you to take out Shakespeare and Tolstoy's works for a cross-comparison. \n\n At this point, taking out related books in advance will not work well. \n\n First, your desk space is limited and cannot hold all the books of these two masters at the same time, not to mention the reading notes that will be generated during the comparison process. \n\n Second, you're just one person, and comparing so many works would take too long. It would be nice if you could find a few more people to help. \n\n This is the current situation when we use Numpy to deal with large amounts of data: \n\n* The number of elements in the Array is too large to fit into the CPU's L1 cache.\n* Numpy's element-level operations are single-threaded and cannot utilize the computing power of multi-core CPUs.\n\n What should we do? \n\n Don't worry. When you really encounter a problem with too much data, you can call on our protagonist today, Numexpr, to help. \n\n## Understanding Numexpr: What and Why\n\n### How it works\n\n When Numpy encounters large arrays, element-wise calculations will experience two extremes. \n\n Let me give you an example to illustrate. Suppose there are two large Numpy ndarrays: \n\n    import numpy as np \n    import numexpr as ne  \n    \n    a = np.random.rand(100_000_000) \n    b = np.random.rand(100_000_000)\n\n When calculating the result of the expression a\\*\\*5 + 2 \\* b, there are generally two methods:\n\n One way is Numpy's vectorized calculation method, which uses two temporary arrays to store the results of a\\*\\*5 and 2\\*b separately.  \n\n    In: %timeit a**5 + 2 * b\n    \n    Out:2.11 s \u00b1 31.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n At this time, you have four arrays in your memory: a, b, a\\*\\*5, and 2 \\* b. This method will cause a lot of memory waste. \n\n Moreover, since each Array's size exceeds the CPU cache's capacity, it cannot use it well. \n\n Another way is to traverse each element in two arrays and calculate them separately. \n\n    c = np.empty(100_000_000, dtype=np.uint32)\n    \n    def calcu_elements(a, b, c):\n        for i in range(0, len(a), 1):\n            c[i] = a[i] ** 5 + 2 * b[i]\n            \n    %timeit calcu_elements(a, b, c)\n    \n    \n    Out: 24.6 s \u00b1 48.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n This method performs even worse. The calculation will be very slow because it cannot use vectorized calculations and only partially utilize the CPU cache. \n\n### Numexpr's calculation\n\n Numexpr commonly uses only one evaluate method. This method will receive an expression string each time and then compile it into bytecode using Python's compile method. \n\n Numexpr also has a virtual machine program. The virtual machine contains multiple vector registers, each using a chunk size of 4096. \n\n When Numexpr starts to calculate, it sends the data in one or more registers to the CPU's L1 cache each time. This way, there won't be a situation where the memory is too slow, and the CPU waits for data. \n\n At the same time, Numexpr's virtual machine is written in C, removing Python's GIL. It can utilize the computing power of multi-core CPUs. \n\n So, Numexpr is faster when calculating large arrays than using Numpy alone. We can make a comparison: \n\n    In:  %timeit ne.evaluate('a**5 + 2 * b')\n    Out: 258 ms \u00b1 14.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n### Summary of Numexpr's working principle\n\n Let's summarize the working principle of Numexpr and see why Numexpr is so fast: \n\n **Executing bytecode through a virtual machine.** Numexpr uses bytecode to execute expressions, which can fully utilize the [branch prediction](https://en.wikipedia.org/wiki/Branch_predictor?ref=dataleadsfuture.com) ability of the CPU, which is faster than using Python expressions. \n\n **Vectorized calculation.** Numexpr will use [SIMD (Single Instruction, Multiple Data)](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data?ref=dataleadsfuture.com) technology to improve computing efficiency significantly for the same operation on the data in each register. \n\n **Multi-core parallel computing.** Numexpr's virtual machine can decompose each task into multiple subtasks. They are executed in parallel on multiple CPU cores. \n\n **Less memory usage.** Unlike Numpy, which needs to generate intermediate arrays, Numexpr only loads a small amount of data when necessary, significantly reducing memory usage. \n\n&amp;#x200B;\n\n[ Workflow diagram of Numexpr. Image by Author ](https://preview.redd.it/46plaxk6t5qb1.png?width=863&amp;format=png&amp;auto=webp&amp;s=8c0bb7311c4fc743d720c2e0cec3c28c1ff98ed3)\n\n## Numexpr and Pandas: A Powerful Combination\n\n You might be wondering: We usually do data analysis with pandas. I understand the performance improvements Numexpr offers for Numpy, but does it have the same improvement for Pandas? \n\n The answer is Yes. \n\n The eval and query methods in pandas are implemented based on Numexpr. Let's look at some examples: \n\n### Pandas.eval for Cross-DataFrame operations\n\n When you have multiple pandas DataFrames, you can use pandas.eval to perform operations between DataFrame objects, for example: \n\n    import pandas as pd\n    \n    nrows, ncols = 1_000_000, 100\n    df1, df2, df3, df4 = (pd.DataFrame(rng.random((nrows, ncols))) for i in range(4))\n\n If you calculate the sum of these DataFrames using the traditional pandas method, the time consumed is: \n\n    In:  %timeit df1+df2+df3+df4\n    Out: 1.18 s \u00b1 65.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n You can also use pandas.eval for calculation. The time consumed is: \n\n The calculation of the eval version can improve performance by 50%, and the results are precisely the same: \n\n    In:  np.allclose(df1+df2+df3+df4, pd.eval('df1+df2+df3+df4'))\n    Out: True\n\n### DataFrame.eval for column-level operations\n\n Just like pandas.eval, DataFrame also has its own eval method. We can use this method for column-level operations within DataFrame, for example: \n\n    df = pd.DataFrame(rng.random((1000, 3)), columns=['A', 'B', 'C'])\n    \n    result1 = (df['A'] + df['B']) / (df['C'] - 1)\n    result2 = df.eval('(A + B) / (C - 1)')\n\n The results of using the traditional pandas method and the eval method are precisely the same: \n\n    In:  np.allclose(result1, result2)\n    Out: True\n\n Of course, you can also directly use the eval expression to add new columns to the DataFrame, which is very convenient: \n\n    df.eval('D = (A + B) / C', inplace=True)\n    df.head()\n\n[ Directly use the eval expression to add new columns. Image by Author ](https://preview.redd.it/ykotgj0ut5qb1.png?width=495&amp;format=png&amp;auto=webp&amp;s=f005b5782e751f2f5ea90fcddd8169cfcc2c5751)\n\n### Using DataFrame.query to quickly find data\n\n If the eval method of DataFrame executes comparison expressions, the returned result is a boolean result that meets the conditions. You need to use Mask Indexing to get the desired data: \n\n    mask = df.eval('(A &lt; 0.5) &amp; (B &lt; 0.5)')\n    result1 = df[mask]\n    result\n\n[ When filtering data only with DataFrame.query, it is necessary to use a boolean mask. Image by Author ](https://preview.redd.it/izwngwizt5qb1.png?width=469&amp;format=png&amp;auto=webp&amp;s=c7f0e4806977ccdcf3c588a67a6c913255f47091)\n\n The DataFrame.query method encapsulates this process, and you can directly obtain the desired data with the query method: \n\n    In:   result2 = df.query('A &lt; 0.5 and B &lt; 0.5')\n          np.allclose(result1, result2)\n    Out:  True\n\n When you need to use scalars in expressions, you can use the @  to indicate: \n\n    In:  Cmean = df['C'].mean()\n         result1 = df[(df.A &lt; Cmean) &amp; (df.B &lt; Cmean)]\n         result2 = df.query('A &lt; @Cmean and B &lt; @Cmean')\n         np.allclose(result1, result2)\n    Out: True\n\n This article was originally published on my personal blog [Data Leads Future.](https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/) ", "author_fullname": "t2_9r8ft2a0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring Numexpr: A Powerful Engine Behind Pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ykotgj0ut5qb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 39, "x": 108, "u": "https://preview.redd.it/ykotgj0ut5qb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45b2ace70cdc23065c16c4c7ecb49451067d5824"}, {"y": 78, "x": 216, "u": "https://preview.redd.it/ykotgj0ut5qb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ae1f3d972bbe64cc07341d2a7e32e66da793a7f"}, {"y": 115, "x": 320, "u": "https://preview.redd.it/ykotgj0ut5qb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=04db3f2e444bdb6b7cb6c9558407dc42561b74f2"}], "s": {"y": 179, "x": 495, "u": "https://preview.redd.it/ykotgj0ut5qb1.png?width=495&amp;format=png&amp;auto=webp&amp;s=f005b5782e751f2f5ea90fcddd8169cfcc2c5751"}, "id": "ykotgj0ut5qb1"}, "izwngwizt5qb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/izwngwizt5qb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2dadbb151f50fa5dbfbf14272d1860c079d94f89"}, {"y": 133, "x": 216, "u": "https://preview.redd.it/izwngwizt5qb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=184931d673c0227709ad80639f3cd20da70977d4"}, {"y": 197, "x": 320, "u": "https://preview.redd.it/izwngwizt5qb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ce4336dee4ce5c9bbedc261e3a277e72d6fa47e"}], "s": {"y": 289, "x": 469, "u": "https://preview.redd.it/izwngwizt5qb1.png?width=469&amp;format=png&amp;auto=webp&amp;s=c7f0e4806977ccdcf3c588a67a6c913255f47091"}, "id": "izwngwizt5qb1"}, "3k7gdxywr5qb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 45, "x": 108, "u": "https://preview.redd.it/3k7gdxywr5qb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a502f085a43ef5ac8f1f50599161b6d8ccfe1dc5"}, {"y": 91, "x": 216, "u": "https://preview.redd.it/3k7gdxywr5qb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fedccce3dc9866fbb52b62237eb1d362917e2eb3"}, {"y": 135, "x": 320, "u": "https://preview.redd.it/3k7gdxywr5qb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=77080ec0cc0d4b7202892c1035780ef145694726"}], "s": {"y": 264, "x": 625, "u": "https://preview.redd.it/3k7gdxywr5qb1.png?width=625&amp;format=png&amp;auto=webp&amp;s=5e6a2ed6620cf23638d9c3f20e5f6f46a54e0049"}, "id": "3k7gdxywr5qb1"}, "46plaxk6t5qb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/46plaxk6t5qb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d31e4cb8f3f2da53d3c291af37ecc489c082e5d"}, {"y": 152, "x": 216, "u": "https://preview.redd.it/46plaxk6t5qb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b292a546e21eea92bccc123befff21806f751ad"}, {"y": 226, "x": 320, "u": "https://preview.redd.it/46plaxk6t5qb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e63cb3e19ed9c368e8e297b1842854b459247ac6"}, {"y": 453, "x": 640, "u": "https://preview.redd.it/46plaxk6t5qb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=059bc7da3ab967ae8670b846e24da717ccfa6bb0"}], "s": {"y": 611, "x": 863, "u": "https://preview.redd.it/46plaxk6t5qb1.png?width=863&amp;format=png&amp;auto=webp&amp;s=8c0bb7311c4fc743d720c2e0cec3c28c1ff98ed3"}, "id": "46plaxk6t5qb1"}, "29ec8ukgr5qb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=827a05e29b961a5494a0a7bef09e05406e2e2d0e"}, {"y": 143, "x": 216, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7eda37aee46f7ac0e032a1d6986c84364e634243"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c421de1631ab305d9297cb110c5bc7e6f8fa84ee"}, {"y": 426, "x": 640, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=56aac32c3c80921dcb8d1294b3284c961c3fc4f4"}, {"y": 639, "x": 960, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4bf6231e0d1b715918aa8904b0dc7a8bdae05633"}, {"y": 719, "x": 1080, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a94c2abbd9a3ade8f23648b82905877f75b5bede"}], "s": {"y": 799, "x": 1200, "u": "https://preview.redd.it/29ec8ukgr5qb1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=238da4465da17bd301b49112d574444b16d4113c"}, "id": "29ec8ukgr5qb1"}}, "name": "t3_16qrxs4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ARL0vsBWFf7vstunNTahhEgCBgMGr53RAUJlwDu0dAc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1695542470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Enhancing your data analysis performance with Python&amp;#39;s Numexpr and Pandas&amp;#39; eval/query functions &lt;/p&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/\"&gt;Data Leads Future.&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/29ec8ukgr5qb1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=238da4465da17bd301b49112d574444b16d4113c\"&gt; Use Numexpr to help me find the most livable city. Photo Credit: Created by Author, Canva &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This article will introduce you to the Python library &lt;a href=\"https://numexpr.readthedocs.io/en/latest/intro.html?ref=dataleadsfuture.com#\"&gt;Numexpr&lt;/a&gt;, a tool that boosts the computational performance of Numpy Arrays. The eval and query methods of Pandas are also based on this library.&lt;/p&gt;\n\n&lt;p&gt;This article also includes a hands-on weather data analysis project. &lt;/p&gt;\n\n&lt;p&gt;By reading this article, you will understand the principles of Numexpr and how to use this powerful tool to speed up your calculations in reality. &lt;/p&gt;\n\n&lt;h1&gt;Introduction&lt;/h1&gt;\n\n&lt;h1&gt;Recalling Numpy Arrays&lt;/h1&gt;\n\n&lt;p&gt;In a previous article discussing Numpy Arrays, I used a library example to explain why Numpy&amp;#39;s Cache Locality is so efficient: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.dataleadsfuture.com/python-lists-vs-numpy-arrays-a-deep-dive-into-memory-layout-and-performance-benefits/\"&gt;https://www.dataleadsfuture.com/python-lists-vs-numpy-arrays-a-deep-dive-into-memory-layout-and-performance-benefits/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Each time you go to the library to search for materials, you take out a few books related to the content and place them next to your desk. &lt;/p&gt;\n\n&lt;p&gt;This way, you can quickly check related materials without having to run to the shelf each time you need to read a book. &lt;/p&gt;\n\n&lt;p&gt;This method saves a lot of time, especially when you need to consult many related books. &lt;/p&gt;\n\n&lt;p&gt;In this scenario, the shelf is like your memory, the desk is equivalent to the CPU&amp;#39;s L1 cache, and you, the reader, are the CPU&amp;#39;s core. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3k7gdxywr5qb1.png?width=625&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e6a2ed6620cf23638d9c3f20e5f6f46a54e0049\"&gt; When the CPU accesses RAM, the cache loads the entire cache line into the high-speed cache. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;The limitations of Numpy&lt;/h3&gt;\n\n&lt;p&gt;Suppose you are unfortunate enough to encounter a demanding professor who wants you to take out Shakespeare and Tolstoy&amp;#39;s works for a cross-comparison. &lt;/p&gt;\n\n&lt;p&gt;At this point, taking out related books in advance will not work well. &lt;/p&gt;\n\n&lt;p&gt;First, your desk space is limited and cannot hold all the books of these two masters at the same time, not to mention the reading notes that will be generated during the comparison process. &lt;/p&gt;\n\n&lt;p&gt;Second, you&amp;#39;re just one person, and comparing so many works would take too long. It would be nice if you could find a few more people to help. &lt;/p&gt;\n\n&lt;p&gt;This is the current situation when we use Numpy to deal with large amounts of data: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The number of elements in the Array is too large to fit into the CPU&amp;#39;s L1 cache.&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Numpy&amp;#39;s element-level operations are single-threaded and cannot utilize the computing power of multi-core CPUs.&lt;/p&gt;\n\n&lt;p&gt;What should we do? &lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t worry. When you really encounter a problem with too much data, you can call on our protagonist today, Numexpr, to help. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Understanding Numexpr: What and Why&lt;/h2&gt;\n\n&lt;h3&gt;How it works&lt;/h3&gt;\n\n&lt;p&gt;When Numpy encounters large arrays, element-wise calculations will experience two extremes. &lt;/p&gt;\n\n&lt;p&gt;Let me give you an example to illustrate. Suppose there are two large Numpy ndarrays: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import numpy as np \nimport numexpr as ne  \n\na = np.random.rand(100_000_000) \nb = np.random.rand(100_000_000)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;When calculating the result of the expression a**5 + 2 * b, there are generally two methods:&lt;/p&gt;\n\n&lt;p&gt;One way is Numpy&amp;#39;s vectorized calculation method, which uses two temporary arrays to store the results of a**5 and 2*b separately.  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In: %timeit a**5 + 2 * b\n\nOut:2.11 s \u00b1 31.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;At this time, you have four arrays in your memory: a, b, a**5, and 2 * b. This method will cause a lot of memory waste. &lt;/p&gt;\n\n&lt;p&gt;Moreover, since each Array&amp;#39;s size exceeds the CPU cache&amp;#39;s capacity, it cannot use it well. &lt;/p&gt;\n\n&lt;p&gt;Another way is to traverse each element in two arrays and calculate them separately. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;c = np.empty(100_000_000, dtype=np.uint32)\n\ndef calcu_elements(a, b, c):\n    for i in range(0, len(a), 1):\n        c[i] = a[i] ** 5 + 2 * b[i]\n\n%timeit calcu_elements(a, b, c)\n\n\nOut: 24.6 s \u00b1 48.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This method performs even worse. The calculation will be very slow because it cannot use vectorized calculations and only partially utilize the CPU cache. &lt;/p&gt;\n\n&lt;h3&gt;Numexpr&amp;#39;s calculation&lt;/h3&gt;\n\n&lt;p&gt;Numexpr commonly uses only one evaluate method. This method will receive an expression string each time and then compile it into bytecode using Python&amp;#39;s compile method. &lt;/p&gt;\n\n&lt;p&gt;Numexpr also has a virtual machine program. The virtual machine contains multiple vector registers, each using a chunk size of 4096. &lt;/p&gt;\n\n&lt;p&gt;When Numexpr starts to calculate, it sends the data in one or more registers to the CPU&amp;#39;s L1 cache each time. This way, there won&amp;#39;t be a situation where the memory is too slow, and the CPU waits for data. &lt;/p&gt;\n\n&lt;p&gt;At the same time, Numexpr&amp;#39;s virtual machine is written in C, removing Python&amp;#39;s GIL. It can utilize the computing power of multi-core CPUs. &lt;/p&gt;\n\n&lt;p&gt;So, Numexpr is faster when calculating large arrays than using Numpy alone. We can make a comparison: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:  %timeit ne.evaluate(&amp;#39;a**5 + 2 * b&amp;#39;)\nOut: 258 ms \u00b1 14.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;Summary of Numexpr&amp;#39;s working principle&lt;/h3&gt;\n\n&lt;p&gt;Let&amp;#39;s summarize the working principle of Numexpr and see why Numexpr is so fast: &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Executing bytecode through a virtual machine.&lt;/strong&gt; Numexpr uses bytecode to execute expressions, which can fully utilize the &lt;a href=\"https://en.wikipedia.org/wiki/Branch_predictor?ref=dataleadsfuture.com\"&gt;branch prediction&lt;/a&gt; ability of the CPU, which is faster than using Python expressions. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Vectorized calculation.&lt;/strong&gt; Numexpr will use &lt;a href=\"https://en.wikipedia.org/wiki/Single_instruction,_multiple_data?ref=dataleadsfuture.com\"&gt;SIMD (Single Instruction, Multiple Data)&lt;/a&gt; technology to improve computing efficiency significantly for the same operation on the data in each register. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Multi-core parallel computing.&lt;/strong&gt; Numexpr&amp;#39;s virtual machine can decompose each task into multiple subtasks. They are executed in parallel on multiple CPU cores. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Less memory usage.&lt;/strong&gt; Unlike Numpy, which needs to generate intermediate arrays, Numexpr only loads a small amount of data when necessary, significantly reducing memory usage. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/46plaxk6t5qb1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c0bb7311c4fc743d720c2e0cec3c28c1ff98ed3\"&gt; Workflow diagram of Numexpr. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Numexpr and Pandas: A Powerful Combination&lt;/h2&gt;\n\n&lt;p&gt;You might be wondering: We usually do data analysis with pandas. I understand the performance improvements Numexpr offers for Numpy, but does it have the same improvement for Pandas? &lt;/p&gt;\n\n&lt;p&gt;The answer is Yes. &lt;/p&gt;\n\n&lt;p&gt;The eval and query methods in pandas are implemented based on Numexpr. Let&amp;#39;s look at some examples: &lt;/p&gt;\n\n&lt;h3&gt;Pandas.eval for Cross-DataFrame operations&lt;/h3&gt;\n\n&lt;p&gt;When you have multiple pandas DataFrames, you can use pandas.eval to perform operations between DataFrame objects, for example: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import pandas as pd\n\nnrows, ncols = 1_000_000, 100\ndf1, df2, df3, df4 = (pd.DataFrame(rng.random((nrows, ncols))) for i in range(4))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;If you calculate the sum of these DataFrames using the traditional pandas method, the time consumed is: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:  %timeit df1+df2+df3+df4\nOut: 1.18 s \u00b1 65.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;You can also use pandas.eval for calculation. The time consumed is: &lt;/p&gt;\n\n&lt;p&gt;The calculation of the eval version can improve performance by 50%, and the results are precisely the same: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:  np.allclose(df1+df2+df3+df4, pd.eval(&amp;#39;df1+df2+df3+df4&amp;#39;))\nOut: True\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;DataFrame.eval for column-level operations&lt;/h3&gt;\n\n&lt;p&gt;Just like pandas.eval, DataFrame also has its own eval method. We can use this method for column-level operations within DataFrame, for example: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df = pd.DataFrame(rng.random((1000, 3)), columns=[&amp;#39;A&amp;#39;, &amp;#39;B&amp;#39;, &amp;#39;C&amp;#39;])\n\nresult1 = (df[&amp;#39;A&amp;#39;] + df[&amp;#39;B&amp;#39;]) / (df[&amp;#39;C&amp;#39;] - 1)\nresult2 = df.eval(&amp;#39;(A + B) / (C - 1)&amp;#39;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The results of using the traditional pandas method and the eval method are precisely the same: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:  np.allclose(result1, result2)\nOut: True\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Of course, you can also directly use the eval expression to add new columns to the DataFrame, which is very convenient: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df.eval(&amp;#39;D = (A + B) / C&amp;#39;, inplace=True)\ndf.head()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ykotgj0ut5qb1.png?width=495&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f005b5782e751f2f5ea90fcddd8169cfcc2c5751\"&gt; Directly use the eval expression to add new columns. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;Using DataFrame.query to quickly find data&lt;/h3&gt;\n\n&lt;p&gt;If the eval method of DataFrame executes comparison expressions, the returned result is a boolean result that meets the conditions. You need to use Mask Indexing to get the desired data: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;mask = df.eval(&amp;#39;(A &amp;lt; 0.5) &amp;amp; (B &amp;lt; 0.5)&amp;#39;)\nresult1 = df[mask]\nresult\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/izwngwizt5qb1.png?width=469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7f0e4806977ccdcf3c588a67a6c913255f47091\"&gt; When filtering data only with DataFrame.query, it is necessary to use a boolean mask. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The DataFrame.query method encapsulates this process, and you can directly obtain the desired data with the query method: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:   result2 = df.query(&amp;#39;A &amp;lt; 0.5 and B &amp;lt; 0.5&amp;#39;)\n      np.allclose(result1, result2)\nOut:  True\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;When you need to use scalars in expressions, you can use the @  to indicate: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:  Cmean = df[&amp;#39;C&amp;#39;].mean()\n     result1 = df[(df.A &amp;lt; Cmean) &amp;amp; (df.B &amp;lt; Cmean)]\n     result2 = df.query(&amp;#39;A &amp;lt; @Cmean and B &amp;lt; @Cmean&amp;#39;)\n     np.allclose(result1, result2)\nOut: True\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/\"&gt;Data Leads Future.&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?auto=webp&amp;s=4f4c5a16af6b6d5e954c2bd0d6ec11d253a3f16f", "width": 1387, "height": 924}, "resolutions": [{"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3216385c481210e323283ae6e03a16e14245f2a1", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6a3bbf3191c78cddfbd87af49e76d6667ca85fd", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=202ec313e2baa2ef1670e9cfd24d6c2560d1cd21", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ff3e49cdb7d8d0746f47c590240ce1fc893bc917", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=33dc2fd6c45c6a3cc3cbd721fe9ad6a9f5ab7779", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/TpHbi-LDIBl_Qie2W3bikMNbosa3_mKTOFYTA9OA7TA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf7b49d72586c5ef89b6b8f7dc0d9c7de09de4b8", "width": 1080, "height": 719}], "variants": {}, "id": "Bjw_Y7mZr_m-tfiX4fEkjjZrKlokqny6OjPxyctYkDg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qrxs4", "is_robot_indexable": true, "report_reasons": null, "author": "qtalen", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qrxs4/exploring_numexpr_a_powerful_engine_behind_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qrxs4/exploring_numexpr_a_powerful_engine_behind_pandas/", "subreddit_subscribers": 1059475, "created_utc": 1695542470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I employ existing widely used CNNs (e.g., `Xception`, `DenseNet169`, `InceptionV3`) via transfer-learning using Keras for a classification task of `10` classes. I've employed almost every available model but still experiencing overfitting as you can see via the attached accuracy-epoch graph, which was obtained during training. I re-calculate the weights of the layers that come with the existing models (*a.k.a.* base models). I've employed KerasTuner to optimize the hyperparameters such as the activation function, optimization algorithm, and learning rate. As a result of this task, `Adam` and `ReLU` were set as the optimization algorithm and activation function, respectively. To prevent overfitting, following the base model, I've added a `Dense` with `1,024` units and a `Dropout` layer with a dropout rate of `0.6` (kept that high to prevent overfitting) just before the final `Dense` layer with `softmax` activation function, which is solely responsible for the classification. I do use a 4-fold CV and the test set/train set ratio is `.3`. The total number of samples is `1,000` and each sample has a shape of `224x224x3`.\n\nHere are the scores that I've obtained on the test set:\n\nAcc: `72.5%`\n\nF1-Score: `72.287%`\n\nPrecision: `73.734%`\n\nRecall: `72.5%`\n\nAny recommendations to improve the test accuracy of the model are greatly appreciated. Please feel free to ask for any further information, which I might missed to note.\n\nMany thanks in advance for your time.\n\n*p.s. This task is a part of my own research project; not seeking help for homework or an exam.*\n\n[Accuracy-Epoch graph](https://preview.redd.it/ehqqfp1gc3qb1.png?width=750&amp;format=png&amp;auto=webp&amp;s=635ff0368530a400c271912e370aa7751b28c6de)", "author_fullname": "t2_rra8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I further improve the accuracy of my CNN based on transfer-learning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 102, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ehqqfp1gc3qb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 79, "x": 108, "u": "https://preview.redd.it/ehqqfp1gc3qb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e83284186f63764e965f0f1df35258730f30abec"}, {"y": 158, "x": 216, "u": "https://preview.redd.it/ehqqfp1gc3qb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=77999a2e263abae18b930dd0d4bd9059148c39ca"}, {"y": 234, "x": 320, "u": "https://preview.redd.it/ehqqfp1gc3qb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8c4fe86afbcfbaf6b5bc8338dbc498e993a1fe8e"}, {"y": 469, "x": 640, "u": "https://preview.redd.it/ehqqfp1gc3qb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5607c205ed1b934a2483d8aadd85c57f1b4a3a53"}], "s": {"y": 550, "x": 750, "u": "https://preview.redd.it/ehqqfp1gc3qb1.png?width=750&amp;format=png&amp;auto=webp&amp;s=635ff0368530a400c271912e370aa7751b28c6de"}, "id": "ehqqfp1gc3qb1"}}, "name": "t3_16qj0w2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Xm8SUClvOs6tY_0MCGHEpCFV0tuZnddsM2Oy3JzO490.jpg", "edited": 1695528340.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695513126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I employ existing widely used CNNs (e.g., &lt;code&gt;Xception&lt;/code&gt;, &lt;code&gt;DenseNet169&lt;/code&gt;, &lt;code&gt;InceptionV3&lt;/code&gt;) via transfer-learning using Keras for a classification task of &lt;code&gt;10&lt;/code&gt; classes. I&amp;#39;ve employed almost every available model but still experiencing overfitting as you can see via the attached accuracy-epoch graph, which was obtained during training. I re-calculate the weights of the layers that come with the existing models (&lt;em&gt;a.k.a.&lt;/em&gt; base models). I&amp;#39;ve employed KerasTuner to optimize the hyperparameters such as the activation function, optimization algorithm, and learning rate. As a result of this task, &lt;code&gt;Adam&lt;/code&gt; and &lt;code&gt;ReLU&lt;/code&gt; were set as the optimization algorithm and activation function, respectively. To prevent overfitting, following the base model, I&amp;#39;ve added a &lt;code&gt;Dense&lt;/code&gt; with &lt;code&gt;1,024&lt;/code&gt; units and a &lt;code&gt;Dropout&lt;/code&gt; layer with a dropout rate of &lt;code&gt;0.6&lt;/code&gt; (kept that high to prevent overfitting) just before the final &lt;code&gt;Dense&lt;/code&gt; layer with &lt;code&gt;softmax&lt;/code&gt; activation function, which is solely responsible for the classification. I do use a 4-fold CV and the test set/train set ratio is &lt;code&gt;.3&lt;/code&gt;. The total number of samples is &lt;code&gt;1,000&lt;/code&gt; and each sample has a shape of &lt;code&gt;224x224x3&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;Here are the scores that I&amp;#39;ve obtained on the test set:&lt;/p&gt;\n\n&lt;p&gt;Acc: &lt;code&gt;72.5%&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;F1-Score: &lt;code&gt;72.287%&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Precision: &lt;code&gt;73.734%&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Recall: &lt;code&gt;72.5%&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Any recommendations to improve the test accuracy of the model are greatly appreciated. Please feel free to ask for any further information, which I might missed to note.&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance for your time.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;p.s. This task is a part of my own research project; not seeking help for homework or an exam.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ehqqfp1gc3qb1.png?width=750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=635ff0368530a400c271912e370aa7751b28c6de\"&gt;Accuracy-Epoch graph&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qj0w2", "is_robot_indexable": true, "report_reasons": null, "author": "talhak", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qj0w2/how_can_i_further_improve_the_accuracy_of_my_cnn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qj0w2/how_can_i_further_improve_the_accuracy_of_my_cnn/", "subreddit_subscribers": 1059475, "created_utc": 1695513126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello guys, I needed some assistance with Forecasting a Time Series dataset using Neural Nets.\n\nFor context, the data has an annual seasonality with daily records of upto 5 years. Previously, I had used Statistical Methods like TBATS and DHR along with GARCH to transform and forecast.\n\nNow, I have been attempting to do the same using RNNs. I started off with GRU and LSTM with a few Hyperparameter Optimization Algorithms for the optimum depth, lookback window etc. only to end up with an eggregious yield of approx 10 MAPE.\n\nThus, I'd appreciate your input as to what can I improve be it an industry practise, some other insightful articles on the same or even alternative ML approaches!\n\nThanks in advance :)", "author_fullname": "t2_j6sk2g9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time Series Forecasting using RNN", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qnryv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695527752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I needed some assistance with Forecasting a Time Series dataset using Neural Nets.&lt;/p&gt;\n\n&lt;p&gt;For context, the data has an annual seasonality with daily records of upto 5 years. Previously, I had used Statistical Methods like TBATS and DHR along with GARCH to transform and forecast.&lt;/p&gt;\n\n&lt;p&gt;Now, I have been attempting to do the same using RNNs. I started off with GRU and LSTM with a few Hyperparameter Optimization Algorithms for the optimum depth, lookback window etc. only to end up with an eggregious yield of approx 10 MAPE.&lt;/p&gt;\n\n&lt;p&gt;Thus, I&amp;#39;d appreciate your input as to what can I improve be it an industry practise, some other insightful articles on the same or even alternative ML approaches!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qnryv", "is_robot_indexable": true, "report_reasons": null, "author": "magic_groovin", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qnryv/time_series_forecasting_using_rnn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qnryv/time_series_forecasting_using_rnn/", "subreddit_subscribers": 1059475, "created_utc": 1695527752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I find it to be a very useful tool.  However, only my current employer is using it.  If I change jobs, I won't have access to Alteryx anymore and my skill with deteriorate with non-use.     \nIt is unlike Excel where I'm sharpening my skills with it every place I work.  Any software is worth learning for your career, but we humans tend to forget knowledge what don't use.  Or I do at least, maybe it is just me.  Your thoughts?  \n", "author_fullname": "t2_duzivvbs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Alteryx a practical skill to learn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16r9lfj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695591476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I find it to be a very useful tool.  However, only my current employer is using it.  If I change jobs, I won&amp;#39;t have access to Alteryx anymore and my skill with deteriorate with non-use.&lt;br/&gt;\nIt is unlike Excel where I&amp;#39;m sharpening my skills with it every place I work.  Any software is worth learning for your career, but we humans tend to forget knowledge what don&amp;#39;t use.  Or I do at least, maybe it is just me.  Your thoughts?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r9lfj", "is_robot_indexable": true, "report_reasons": null, "author": "How_Much2", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r9lfj/is_alteryx_a_practical_skill_to_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r9lfj/is_alteryx_a_practical_skill_to_learn/", "subreddit_subscribers": 1059475, "created_utc": 1695591476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi Folks,\n\nFor context:\n\nI've been tasked with planning capacity for our helpdesk team at work.  There is it total 4 agents and they solve \\~50 tickets a month.  The business question I have been tasked with answering is \"At what point do we hire more agents.\n\nI have access to the data from Tableau for both tickets closed and created over a 2 year period, these are however separate data sets so will need to merge them together. \n\nI'm currently now familiar with any forecasting models but from my research might select ARIMA.\n\nAny advice on how to tackle this? currently also using chatgpt for some brainstorming. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_b3b9wr8r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "capacity forecasting planning advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r00gq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695568135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks,&lt;/p&gt;\n\n&lt;p&gt;For context:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been tasked with planning capacity for our helpdesk team at work.  There is it total 4 agents and they solve ~50 tickets a month.  The business question I have been tasked with answering is &amp;quot;At what point do we hire more agents.&lt;/p&gt;\n\n&lt;p&gt;I have access to the data from Tableau for both tickets closed and created over a 2 year period, these are however separate data sets so will need to merge them together. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently now familiar with any forecasting models but from my research might select ARIMA.&lt;/p&gt;\n\n&lt;p&gt;Any advice on how to tackle this? currently also using chatgpt for some brainstorming. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r00gq", "is_robot_indexable": true, "report_reasons": null, "author": "oaklandcruser", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r00gq/capacity_forecasting_planning_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r00gq/capacity_forecasting_planning_advice/", "subreddit_subscribers": 1059475, "created_utc": 1695568135.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am studying data science and from media and asking people who are already working in the field strated to be concerned about finding a DS job after graduation.\nSo I am picking a plan B besides my DS BSc but still want to work in DS in the long term so what would you recommend me \nSoftware Engineering or Data engineeing\nI know some people would say data analyst but I just don't know anything about the jobs and from reading job discribtions I feel like using excel and dashboards is a waste for all the Math and programming and ML I have been studying in the last 3 years", "author_fullname": "t2_h5su2eud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software engineering or data engineeing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qptlb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695534965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am studying data science and from media and asking people who are already working in the field strated to be concerned about finding a DS job after graduation.\nSo I am picking a plan B besides my DS BSc but still want to work in DS in the long term so what would you recommend me \nSoftware Engineering or Data engineeing\nI know some people would say data analyst but I just don&amp;#39;t know anything about the jobs and from reading job discribtions I feel like using excel and dashboards is a waste for all the Math and programming and ML I have been studying in the last 3 years&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qptlb", "is_robot_indexable": true, "report_reasons": null, "author": "Emotional-Rhubarb725", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qptlb/software_engineering_or_data_engineeing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qptlb/software_engineering_or_data_engineeing/", "subreddit_subscribers": 1059475, "created_utc": 1695534965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I've been assigned an AI/ML project, and I've identified that the data quality is not good. It's within a large organization, which makes it challenging to find a straightforward solution to the data quality problem. Personally, I'm feeling uncomfortable about proceeding further. Interestingly, my manager and other colleagues don't seem to share the same level of concern as I do. They are more inclined to continue the project and generate \"output\". Their primary worried about what to delivery to CIO. Given this situation, what would I do in my place?", "author_fullname": "t2_5fbmh3va", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you do when data quality is bad?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16ra88t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695592973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been assigned an AI/ML project, and I&amp;#39;ve identified that the data quality is not good. It&amp;#39;s within a large organization, which makes it challenging to find a straightforward solution to the data quality problem. Personally, I&amp;#39;m feeling uncomfortable about proceeding further. Interestingly, my manager and other colleagues don&amp;#39;t seem to share the same level of concern as I do. They are more inclined to continue the project and generate &amp;quot;output&amp;quot;. Their primary worried about what to delivery to CIO. Given this situation, what would I do in my place?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ra88t", "is_robot_indexable": true, "report_reasons": null, "author": "Excellent_Cost170", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ra88t/what_do_you_do_when_data_quality_is_bad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ra88t/what_do_you_do_when_data_quality_is_bad/", "subreddit_subscribers": 1059475, "created_utc": 1695592973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, I'm using AWS forcasting to forcast demand over different departments (fashion, home, elec, etc). Would it be suitable to do different forecasts for different departs that's have different related time series?\nSome related time series that I have tried reduce the WAPE on some departments but increase it on others", "author_fullname": "t2_7427v7db", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time series forcasting, when to split the target?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16ra2yz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695592632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m using AWS forcasting to forcast demand over different departments (fashion, home, elec, etc). Would it be suitable to do different forecasts for different departs that&amp;#39;s have different related time series?\nSome related time series that I have tried reduce the WAPE on some departments but increase it on others&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ra2yz", "is_robot_indexable": true, "report_reasons": null, "author": "Grovesy158", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ra2yz/time_series_forcasting_when_to_split_the_target/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ra2yz/time_series_forcasting_when_to_split_the_target/", "subreddit_subscribers": 1059475, "created_utc": 1695592632.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What is a more realistic and practical roadmap to become a self taught  data scientist based on your work experience.", "author_fullname": "t2_b68tvjss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Roadmap for ds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16r9iat", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695591268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is a more realistic and practical roadmap to become a self taught  data scientist based on your work experience.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r9iat", "is_robot_indexable": true, "report_reasons": null, "author": "t7Saitama", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r9iat/roadmap_for_ds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r9iat/roadmap_for_ds/", "subreddit_subscribers": 1059475, "created_utc": 1695591268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I see a large amount of relevant open source tools and libraries to assist in peripheral (not the actual data processing or modeling) areas of data science. I mean tools that make certain important tasks easier. For instance: kedro, hydra-conf, nannyml, streamlit, docker, devpod, black, ruff, pandera, mage, fugue, datapane, adn probably a lot more.\n\nWhat do you guys use for your data science project?", "author_fullname": "t2_hji49", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tools do you use on your data science projects from proof of concept to production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r7elg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695586274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see a large amount of relevant open source tools and libraries to assist in peripheral (not the actual data processing or modeling) areas of data science. I mean tools that make certain important tasks easier. For instance: kedro, hydra-conf, nannyml, streamlit, docker, devpod, black, ruff, pandera, mage, fugue, datapane, adn probably a lot more.&lt;/p&gt;\n\n&lt;p&gt;What do you guys use for your data science project?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r7elg", "is_robot_indexable": true, "report_reasons": null, "author": "vmgustavo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r7elg/what_tools_do_you_use_on_your_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r7elg/what_tools_do_you_use_on_your_data_science/", "subreddit_subscribers": 1059475, "created_utc": 1695586274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys, I've worked with a lot of clients that have poorly scoped AI/data science projects, and I thought of a business idea that might be really helpful. Would love your feedback\n\n* Most data science consultants have scoping questionnaires that they use for the project discovery process. But the idea is for this questionnaire to exist on its own\n* Basically, if clients might have a data science project, they would go to a website and complete a survey to better scope their idea. The survey will have mandatory fields - e.g. what kind of data do I have, what kind of data do I want, what kind of budget i have, etc.\n* This will allow clients to bring have a much better scoped project, BEFORE they even talk to any data science consultants. Much better engagement experience from the start for everyone\n* Target User: Small businesses that have data science needs but do not have in-house data scientists (will need an external data science consultant to come in and help them) \n* Revenue: Perhaps through commission from leads generated by data science consultants? Haven't really thought through this part, although I would hesitate to charge the client side at all", "author_fullname": "t2_8xwepztbe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Idea for a Tool - \"Define your data science project\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r6quo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695584659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I&amp;#39;ve worked with a lot of clients that have poorly scoped AI/data science projects, and I thought of a business idea that might be really helpful. Would love your feedback&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Most data science consultants have scoping questionnaires that they use for the project discovery process. But the idea is for this questionnaire to exist on its own&lt;/li&gt;\n&lt;li&gt;Basically, if clients might have a data science project, they would go to a website and complete a survey to better scope their idea. The survey will have mandatory fields - e.g. what kind of data do I have, what kind of data do I want, what kind of budget i have, etc.&lt;/li&gt;\n&lt;li&gt;This will allow clients to bring have a much better scoped project, BEFORE they even talk to any data science consultants. Much better engagement experience from the start for everyone&lt;/li&gt;\n&lt;li&gt;Target User: Small businesses that have data science needs but do not have in-house data scientists (will need an external data science consultant to come in and help them) &lt;/li&gt;\n&lt;li&gt;Revenue: Perhaps through commission from leads generated by data science consultants? Haven&amp;#39;t really thought through this part, although I would hesitate to charge the client side at all&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r6quo", "is_robot_indexable": true, "report_reasons": null, "author": "saasthom", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r6quo/idea_for_a_tool_define_your_data_science_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r6quo/idea_for_a_tool_define_your_data_science_project/", "subreddit_subscribers": 1059475, "created_utc": 1695584659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've found the troves of data from the department of education on undergraduate admissions. School acceptance rates, ACT / SATs, etc. \n\nIs there any such data for graduate schools or programs? For example, GRE / GMAT data, or simply acceptance rates. Any help would be greatly appreciated!", "author_fullname": "t2_lrgivisy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for graduate admissions data, DoED", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16r1z2f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695572951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve found the troves of data from the department of education on undergraduate admissions. School acceptance rates, ACT / SATs, etc. &lt;/p&gt;\n\n&lt;p&gt;Is there any such data for graduate schools or programs? For example, GRE / GMAT data, or simply acceptance rates. Any help would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r1z2f", "is_robot_indexable": true, "report_reasons": null, "author": "crimefog", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r1z2f/looking_for_graduate_admissions_data_doed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16r1z2f/looking_for_graduate_admissions_data_doed/", "subreddit_subscribers": 1059475, "created_utc": 1695572951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi\nI've wrote a CRM for shipyards, and other professionals that do boat maintenance.\n\nEach customer of this software will enter data about work orders, products costs and labour...\nThose data will be tied to boat makes, end customers and so on ...\n\nI'd like to be able to provide some useful data to the shipyards from this data. I'm pretty new to data analysis and don't know of there are tools that can help me to do so ?\nI.e. I can imagine when creating a new work order for some task (let's say an engine periodical maintenance), I could provide historical data about how much time it does take for this kind of task... or even when a special engine is concerned, this one is specifically harder to work with, so the  planned hour count should be higher and so on...\n\nIs there models that could be trained against the customer data to provide those features?\n\nSorry if it's in the wrong place or If my question seems dumb !\n\nThanks", "author_fullname": "t2_u3p6b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Writing a CRM : how to extract valued data to customers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qvvl0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695556386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi\nI&amp;#39;ve wrote a CRM for shipyards, and other professionals that do boat maintenance.&lt;/p&gt;\n\n&lt;p&gt;Each customer of this software will enter data about work orders, products costs and labour...\nThose data will be tied to boat makes, end customers and so on ...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to be able to provide some useful data to the shipyards from this data. I&amp;#39;m pretty new to data analysis and don&amp;#39;t know of there are tools that can help me to do so ?\nI.e. I can imagine when creating a new work order for some task (let&amp;#39;s say an engine periodical maintenance), I could provide historical data about how much time it does take for this kind of task... or even when a special engine is concerned, this one is specifically harder to work with, so the  planned hour count should be higher and so on...&lt;/p&gt;\n\n&lt;p&gt;Is there models that could be trained against the customer data to provide those features?&lt;/p&gt;\n\n&lt;p&gt;Sorry if it&amp;#39;s in the wrong place or If my question seems dumb !&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qvvl0", "is_robot_indexable": true, "report_reasons": null, "author": "Napo7", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qvvl0/writing_a_crm_how_to_extract_valued_data_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qvvl0/writing_a_crm_how_to_extract_valued_data_to/", "subreddit_subscribers": 1059475, "created_utc": 1695556386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am an aspiring data scientist currently pursuing a BCs in CS (2nd year). Lately, I've been actively searching for remote data science internships to gain practical experience and make my resume strong. I've come across several online virtual internship programs that are open to anyone. I am wondering if these opportunities are worth the time and if it's appropriate to include them in my work experience. Here are a few examples of such programs:\n\n1. [iNeuron](https://internship.ineuron.ai/)\n2. [OpenWeaver](https://community.openweaver.com/t/virtual-internship-in-data-science-apply-now/114521)\n3. [The Spark Foundation](https://internship.thesparksfoundation.info/)\n4. [Let's Grown More](https://letsgrowmore.in/vip/)", "author_fullname": "t2_6oagcr1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are these DS Virtual Internships good to get started?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qt2n3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695546597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an aspiring data scientist currently pursuing a BCs in CS (2nd year). Lately, I&amp;#39;ve been actively searching for remote data science internships to gain practical experience and make my resume strong. I&amp;#39;ve come across several online virtual internship programs that are open to anyone. I am wondering if these opportunities are worth the time and if it&amp;#39;s appropriate to include them in my work experience. Here are a few examples of such programs:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://internship.ineuron.ai/\"&gt;iNeuron&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://community.openweaver.com/t/virtual-internship-in-data-science-apply-now/114521\"&gt;OpenWeaver&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://internship.thesparksfoundation.info/\"&gt;The Spark Foundation&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://letsgrowmore.in/vip/\"&gt;Let&amp;#39;s Grown More&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qt2n3", "is_robot_indexable": true, "report_reasons": null, "author": "hashirbhatti", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qt2n3/are_these_ds_virtual_internships_good_to_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qt2n3/are_these_ds_virtual_internships_good_to_get/", "subreddit_subscribers": 1059475, "created_utc": 1695546597.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\nNavigating some unexpected twists in my data science journey and could really benefit from the collective wisdom of this community.\n\nFor about 2.5 years, I delved deep into predictive modeling within the heavy industry sector, leveraging my physics degree for a robust quantitative approach. Things took an unforeseen turn when my company closed its data science department. Though it was a jolt, I was offered a lifeline in the form of an analyst role within the fintech BaaS segment of our company. Here, I've been testing mobile apps, setting up APIs for clients, and gaining a different perspective.\n\nYet, as intriguing as fintech is, I've felt a drift from the heart of data science that once ignited my passion, and it's been a hurdle trying to relocate to a role that resonates more with my prior expertise.\n\nI'm at a juncture and could use some insights on these potential pathways:\n\n1. **Dive Deeper into Fintech**: Given my recent experiences, should I immerse myself further in fintech? I'm contemplating gaining a richer understanding of finance and then scouting for a data science role within this arena.\n2. **Physics and Data Science Fusion**: My love for physics remains undiminished. Could a focus on intertwining data science with physics be a promising avenue? Has anyone ventured this path and can share their experience?\n3. **Returning to Familiar Grounds or Exploring Fresh Horizons**: Is it more pragmatic to gravitate back to industries with which I'm acquainted or explore entirely new terrains where my data science prowess might be a good fit?\n\nYour experiences, advice, and reflections would be a beacon for me during this transitional phase.\n\nHeartfelt thanks for reading and offering your perspective!", "author_fullname": "t2_cyno6wxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Career Guidance: From Heavy Industry to Fintech, with a Physics Twist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qsg1r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695544336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;Navigating some unexpected twists in my data science journey and could really benefit from the collective wisdom of this community.&lt;/p&gt;\n\n&lt;p&gt;For about 2.5 years, I delved deep into predictive modeling within the heavy industry sector, leveraging my physics degree for a robust quantitative approach. Things took an unforeseen turn when my company closed its data science department. Though it was a jolt, I was offered a lifeline in the form of an analyst role within the fintech BaaS segment of our company. Here, I&amp;#39;ve been testing mobile apps, setting up APIs for clients, and gaining a different perspective.&lt;/p&gt;\n\n&lt;p&gt;Yet, as intriguing as fintech is, I&amp;#39;ve felt a drift from the heart of data science that once ignited my passion, and it&amp;#39;s been a hurdle trying to relocate to a role that resonates more with my prior expertise.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m at a juncture and could use some insights on these potential pathways:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Dive Deeper into Fintech&lt;/strong&gt;: Given my recent experiences, should I immerse myself further in fintech? I&amp;#39;m contemplating gaining a richer understanding of finance and then scouting for a data science role within this arena.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Physics and Data Science Fusion&lt;/strong&gt;: My love for physics remains undiminished. Could a focus on intertwining data science with physics be a promising avenue? Has anyone ventured this path and can share their experience?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Returning to Familiar Grounds or Exploring Fresh Horizons&lt;/strong&gt;: Is it more pragmatic to gravitate back to industries with which I&amp;#39;m acquainted or explore entirely new terrains where my data science prowess might be a good fit?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Your experiences, advice, and reflections would be a beacon for me during this transitional phase.&lt;/p&gt;\n\n&lt;p&gt;Heartfelt thanks for reading and offering your perspective!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qsg1r", "is_robot_indexable": true, "report_reasons": null, "author": "MachineSilly576", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qsg1r/seeking_career_guidance_from_heavy_industry_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qsg1r/seeking_career_guidance_from_heavy_industry_to/", "subreddit_subscribers": 1059475, "created_utc": 1695544336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am working on reproducing results claimed by an author in a research paper. \n\nI have followed the same process and have a model training and running.\n\nSo the MAE they claim is 5. And I am able to reproduce 23. \n\nI am a bit new to this, so when talking about reproducing results; how different can they be to say \"reproduce successful\"?", "author_fullname": "t2_123jiosa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comparing \"claimed\" results vs \"reproduced\" results.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qpup6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695535075.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on reproducing results claimed by an author in a research paper. &lt;/p&gt;\n\n&lt;p&gt;I have followed the same process and have a model training and running.&lt;/p&gt;\n\n&lt;p&gt;So the MAE they claim is 5. And I am able to reproduce 23. &lt;/p&gt;\n\n&lt;p&gt;I am a bit new to this, so when talking about reproducing results; how different can they be to say &amp;quot;reproduce successful&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qpup6", "is_robot_indexable": true, "report_reasons": null, "author": "Dump7", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qpup6/comparing_claimed_results_vs_reproduced_results/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qpup6/comparing_claimed_results_vs_reproduced_results/", "subreddit_subscribers": 1059475, "created_utc": 1695535075.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello everyone,\n\nI\u2019ve been trying to improve my SQL skills, specifically in writing queries involving joins and subqueries. I\u2019ve gone through several SQL tutorials on YouTube and while I feel like I understand the concepts being taught, I struggle when it comes to applying them in practice.\n\nWhen given a problem, I find it difficult to write the corresponding SQL queries. This has led me to question whether I\u2019ve truly grasped the theoretical and practical aspects of SQL.\n\nHas anyone else experienced this? How did you overcome it? Any advice or resources that could help me bridge this gap between understanding and application would be greatly appreciated.\n\nThank you!", "author_fullname": "t2_k37psi1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling with SQL Queries - Need Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qplm4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695534149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been trying to improve my SQL skills, specifically in writing queries involving joins and subqueries. I\u2019ve gone through several SQL tutorials on YouTube and while I feel like I understand the concepts being taught, I struggle when it comes to applying them in practice.&lt;/p&gt;\n\n&lt;p&gt;When given a problem, I find it difficult to write the corresponding SQL queries. This has led me to question whether I\u2019ve truly grasped the theoretical and practical aspects of SQL.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else experienced this? How did you overcome it? Any advice or resources that could help me bridge this gap between understanding and application would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qplm4", "is_robot_indexable": true, "report_reasons": null, "author": "PleaseJustStayAlive", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qplm4/struggling_with_sql_queries_need_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qplm4/struggling_with_sql_queries_need_advice/", "subreddit_subscribers": 1059475, "created_utc": 1695534149.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nHi All,\n\nIs there a way to collect real time data regarding Packaged food and its ingredients. I'm currently focussing on Indian packaged foods. Any sugestion would be extemely helpful.\n\nThanks a lot!", "author_fullname": "t2_3dgxudc7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Packaged food related ingredients and its proportions in a dataset", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16qo5es", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695529018.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;Is there a way to collect real time data regarding Packaged food and its ingredients. I&amp;#39;m currently focussing on Indian packaged foods. Any sugestion would be extemely helpful.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qo5es", "is_robot_indexable": true, "report_reasons": null, "author": "ashnel11", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qo5es/packaged_food_related_ingredients_and_its/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16qo5es/packaged_food_related_ingredients_and_its/", "subreddit_subscribers": 1059475, "created_utc": 1695529018.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_omm9izaz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just published my second blog on medium about feature scaling in machine learning please have a look", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 101, "top_awarded_type": null, "hide_score": false, "name": "t3_16r49mv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rdxAqa4GQx7SXopazXwuw2A2bNQIEcKP4eZPLPiX4rM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695578568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@harshsmj1504/understanding-feature-scaling-in-machine-learning-techniques-implementation-and-advantages-fd9065a349aa", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jARSXOFK7J-fZabW78b_vALuxQ8Q0QncKEe481t6IDY.jpg?auto=webp&amp;s=c980197e8163b852d1d559ba648479a4557dad2f", "width": 1080, "height": 786}, "resolutions": [{"url": "https://external-preview.redd.it/jARSXOFK7J-fZabW78b_vALuxQ8Q0QncKEe481t6IDY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bbfa4a8eb57342b787f93956600533a75d1ecba6", "width": 108, "height": 78}, {"url": "https://external-preview.redd.it/jARSXOFK7J-fZabW78b_vALuxQ8Q0QncKEe481t6IDY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=52a6e22c08c1b6adb96a25a9c148dec4d81d44a9", "width": 216, "height": 157}, {"url": "https://external-preview.redd.it/jARSXOFK7J-fZabW78b_vALuxQ8Q0QncKEe481t6IDY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72c8886e6f59f81f5aa1eb884ea2533bcf751258", "width": 320, "height": 232}, {"url": "https://external-preview.redd.it/jARSXOFK7J-fZabW78b_vALuxQ8Q0QncKEe481t6IDY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1208999cbfddb74786aeff4c269c898d2dd4c89f", "width": 640, "height": 465}, {"url": "https://external-preview.redd.it/jARSXOFK7J-fZabW78b_vALuxQ8Q0QncKEe481t6IDY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=407d5ca70527585b45bfff74597000ed5b87fd6e", "width": 960, "height": 698}, {"url": "https://external-preview.redd.it/jARSXOFK7J-fZabW78b_vALuxQ8Q0QncKEe481t6IDY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5525ea3b120b891165105bc6c520c5454f08f898", "width": 1080, "height": 786}], "variants": {}, "id": "6lDDNW4awFxdt0Byv15QeIDt1opzKJOUSyCSkp7wwBI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r49mv", "is_robot_indexable": true, "report_reasons": null, "author": "indusop", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r49mv/just_published_my_second_blog_on_medium_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@harshsmj1504/understanding-feature-scaling-in-machine-learning-techniques-implementation-and-advantages-fd9065a349aa", "subreddit_subscribers": 1059475, "created_utc": 1695578568.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Data science + Quantum physics = 21st century....", "author_fullname": "t2_lomnzv7f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quantum physics + Data Science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_16r08rn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6R8FvvvXIu9pDHZWNT-C1sXc2ufBlGDYcmcVlLrC_zs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695568704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "revealedge.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data science + Quantum physics = 21st century....&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.revealedge.com/quantum-physics-and-data-science-exploring-the-limitless-possibilities-of-a-dynamic-duo/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xSbrSMb-xZVx6wZpA6Wh5ox-Hkm-pG0uUcdU_AYvfR4.jpg?auto=webp&amp;s=8f6be4486c3a22e60abe7750303e3dd8c083a1ee", "width": 1920, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/xSbrSMb-xZVx6wZpA6Wh5ox-Hkm-pG0uUcdU_AYvfR4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eaae2e163ac98b9137e2dfdfbf649c753ad37c7f", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/xSbrSMb-xZVx6wZpA6Wh5ox-Hkm-pG0uUcdU_AYvfR4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97decbf0e37198f8ef1008c2cac36ac7b03cf10", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/xSbrSMb-xZVx6wZpA6Wh5ox-Hkm-pG0uUcdU_AYvfR4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7e24ce9daed211d440fcdd4d27427440dcbc0af", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/xSbrSMb-xZVx6wZpA6Wh5ox-Hkm-pG0uUcdU_AYvfR4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0ab0d8205d62e896ded4ac4f5d8dc852c639b9ac", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/xSbrSMb-xZVx6wZpA6Wh5ox-Hkm-pG0uUcdU_AYvfR4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ccb67bdbcee687ab3b2647b5b7bc3ba7d03740f", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/xSbrSMb-xZVx6wZpA6Wh5ox-Hkm-pG0uUcdU_AYvfR4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=821bac99d9fe40c568a32c2ae24f1296088cd198", "width": 1080, "height": 720}], "variants": {}, "id": "GGwDsOAGDZYDmIaYUZmeC7fEyC9sTcgaceukDD75LbU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16r08rn", "is_robot_indexable": true, "report_reasons": null, "author": "Epistemophilia3", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16r08rn/quantum_physics_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.revealedge.com/quantum-physics-and-data-science-exploring-the-limitless-possibilities-of-a-dynamic-duo/", "subreddit_subscribers": 1059475, "created_utc": 1695568704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_8o56ww5ld", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can someone please suggest me on how to autofit model in GARCH?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 95, "top_awarded_type": null, "hide_score": false, "name": "t3_16qxxs5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/98M-ZnRqgfMNjqn-i3RjTs0rBUUKykgRTV4KpXzLuWA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695562589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/4b0ig2c3i7qb1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/4b0ig2c3i7qb1.jpg?auto=webp&amp;s=74564800862b5d4322b6e7bbc499c8822654e73f", "width": 880, "height": 599}, "resolutions": [{"url": "https://preview.redd.it/4b0ig2c3i7qb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0bdb19846785e7298cd3c7fc3e40688459c3fffb", "width": 108, "height": 73}, {"url": "https://preview.redd.it/4b0ig2c3i7qb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d367b11a5dea4677d30d63e26b362ce2e9fdd50", "width": 216, "height": 147}, {"url": "https://preview.redd.it/4b0ig2c3i7qb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1dc3b2c1a4e2c5e5bd45162dcda6a103fde17b21", "width": 320, "height": 217}, {"url": "https://preview.redd.it/4b0ig2c3i7qb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=069f913636a95769f762e32b8c50420ea4290e6d", "width": 640, "height": 435}], "variants": {}, "id": "uDdKMkwJNvyrdCJRlfTcWQ_JQameUJroovnOKOAch08"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16qxxs5", "is_robot_indexable": true, "report_reasons": null, "author": "user_unidentified69", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16qxxs5/can_someone_please_suggest_me_on_how_to_autofit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/4b0ig2c3i7qb1.jpg", "subreddit_subscribers": 1059475, "created_utc": 1695562589.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}