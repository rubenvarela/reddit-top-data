{"kind": "Listing", "data": {"after": "t3_16ht8jd", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am tasked with designing the data infrastructure for my company from scratch in Azure. They have data from various ERP systems (mostly Microsoft Dynamics). They want to create reports using Power BI. \n\nPreviously I worked as a BI developer / data analyst, so I'm in a bit over my head. I'm currently going through the DP-203 certification material. Any other advice or interesting learning resources I can look at to better handle this project?", "author_fullname": "t2_7a8qzc12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data infrastructure from scratch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hlca1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694606997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am tasked with designing the data infrastructure for my company from scratch in Azure. They have data from various ERP systems (mostly Microsoft Dynamics). They want to create reports using Power BI. &lt;/p&gt;\n\n&lt;p&gt;Previously I worked as a BI developer / data analyst, so I&amp;#39;m in a bit over my head. I&amp;#39;m currently going through the DP-203 certification material. Any other advice or interesting learning resources I can look at to better handle this project?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hlca1", "is_robot_indexable": true, "report_reasons": null, "author": "vroemboem", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hlca1/data_infrastructure_from_scratch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hlca1/data_infrastructure_from_scratch/", "subreddit_subscribers": 128305, "created_utc": 1694606997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm wondering if a good portfolio (I don't know yet how good) and certificates can cover the 4 years gap I have. Knowing that I didn't choose to chill for 4 years (and it was far away from that anyway). Employers don't seem to care about why I was hit hard by life to the point of discontinuing my career. Anyway, I'm not asking for their pitty but I'm determined to make my profile succeed in getting me good jobs despite of the gap. I've been autolearning for a year now and I built some projects from scratch. How can I show my motivation by exposing these projects ?", "author_fullname": "t2_um2qwii8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to do with a 4 years gap in an IT resume", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hpyf2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694618742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering if a good portfolio (I don&amp;#39;t know yet how good) and certificates can cover the 4 years gap I have. Knowing that I didn&amp;#39;t choose to chill for 4 years (and it was far away from that anyway). Employers don&amp;#39;t seem to care about why I was hit hard by life to the point of discontinuing my career. Anyway, I&amp;#39;m not asking for their pitty but I&amp;#39;m determined to make my profile succeed in getting me good jobs despite of the gap. I&amp;#39;ve been autolearning for a year now and I built some projects from scratch. How can I show my motivation by exposing these projects ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16hpyf2", "is_robot_indexable": true, "report_reasons": null, "author": "NoChemical1223", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hpyf2/what_to_do_with_a_4_years_gap_in_an_it_resume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hpyf2/what_to_do_with_a_4_years_gap_in_an_it_resume/", "subreddit_subscribers": 128305, "created_utc": 1694618742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6lg1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte made huge progress on Postgres replication performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_16hqwpl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/kglnFOYWzScHGih_8jeM5cATAPWNE1KPTslylXHtl70.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694620926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/postgres-replication-performance-benchmark-airbyte-vs-fivetran", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?auto=webp&amp;s=056537f6f2992c8852f6cb95bbc4c00c1e229c51", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e03d950d72419b87f9d5aae966a4c768510ad3d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=51611433beec1d0c28d0a3b19f660558679b5c93", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ae364d8a384ecdcaa7b33b103f56ac454efde0e", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea1404c8fa73aa64041ee85644b9820989de6df0", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f13c773843bfa750b7b0898f0d5469e9d1693f65", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/AKXPMDZFH6bJKRjK6Jjb7PQX0IAdVCxTBAXJ-1Zz1lc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e18e53e06677f7d693f436caaa78f11e99b2664b", "width": 1080, "height": 565}], "variants": {}, "id": "WV3P-nqqnUiqFMV44_ujfp4HcBKISrl5GlfKY6P3V70"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16hqwpl", "is_robot_indexable": true, "report_reasons": null, "author": "evantahler", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hqwpl/airbyte_made_huge_progress_on_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/postgres-replication-performance-benchmark-airbyte-vs-fivetran", "subreddit_subscribers": 128305, "created_utc": 1694620926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I joined a data engineering team that the previous team of 5 had all left in the previous year with 4 walking out on the same day due to the work from home policy change.\n\nThey had replaced the team with me as  a lead.  1 sr software engineer who was a problem employee and 3 contract employees from south america that were all very young ... 22-24 ish.\n\nThe issue started with the problem employee refusing to answer the teams request for 2 months. She was the teams databricks admin and stalled the teams plans for 2 months as i was unable to bring data into databricks myself.\n\nI got pretty frustrated that this continued on for 2 months despite daily scrums.  like she would make up her own work that was all administrative. like submit support tickets for the team and just sit there and not do anything.  \n\ni have never seen anyone stonewall a group of people like that for 2 months and literally do nothing in return.\n\nThen we had one of the contract workers admit to admitting to not attending work after scrum to another employee for a month and do no work.  they took no action other than to require attendance during scrum.  the other two contractors were pretty sub par due to their age for a data engineering job and needed a ton of hand holding.\n\nI was a director of engineering paid 140k in a city with a high cost of living.   In my opinion the company was taking advantage by putting me into that situation and felt like they were just using me to maintain a dysfunctional product for existing clients before it was sunset.\n\nIt was a company with 50k employees. but i did not feel like this situation would improve due to the fact that i have never seen a company allow for someone to just do nothing for two months like that and abandon her role. \n\n i thought they had planned to sunset the product and were running out a replacement squad for the time being and i was just going to work my ass off to get beat up\n\nthoughts on this situation?", "author_fullname": "t2_grbdwzn7k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Leaving a role soon after hire, thoughts on this Data Engineering situaton?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hrg3c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694622163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I joined a data engineering team that the previous team of 5 had all left in the previous year with 4 walking out on the same day due to the work from home policy change.&lt;/p&gt;\n\n&lt;p&gt;They had replaced the team with me as  a lead.  1 sr software engineer who was a problem employee and 3 contract employees from south america that were all very young ... 22-24 ish.&lt;/p&gt;\n\n&lt;p&gt;The issue started with the problem employee refusing to answer the teams request for 2 months. She was the teams databricks admin and stalled the teams plans for 2 months as i was unable to bring data into databricks myself.&lt;/p&gt;\n\n&lt;p&gt;I got pretty frustrated that this continued on for 2 months despite daily scrums.  like she would make up her own work that was all administrative. like submit support tickets for the team and just sit there and not do anything.  &lt;/p&gt;\n\n&lt;p&gt;i have never seen anyone stonewall a group of people like that for 2 months and literally do nothing in return.&lt;/p&gt;\n\n&lt;p&gt;Then we had one of the contract workers admit to admitting to not attending work after scrum to another employee for a month and do no work.  they took no action other than to require attendance during scrum.  the other two contractors were pretty sub par due to their age for a data engineering job and needed a ton of hand holding.&lt;/p&gt;\n\n&lt;p&gt;I was a director of engineering paid 140k in a city with a high cost of living.   In my opinion the company was taking advantage by putting me into that situation and felt like they were just using me to maintain a dysfunctional product for existing clients before it was sunset.&lt;/p&gt;\n\n&lt;p&gt;It was a company with 50k employees. but i did not feel like this situation would improve due to the fact that i have never seen a company allow for someone to just do nothing for two months like that and abandon her role. &lt;/p&gt;\n\n&lt;p&gt;i thought they had planned to sunset the product and were running out a replacement squad for the time being and i was just going to work my ass off to get beat up&lt;/p&gt;\n\n&lt;p&gt;thoughts on this situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16hrg3c", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical-End-1524", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hrg3c/leaving_a_role_soon_after_hire_thoughts_on_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hrg3c/leaving_a_role_soon_after_hire_thoughts_on_this/", "subreddit_subscribers": 128305, "created_utc": 1694622163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have my final interview upcoming with a wells fargo panel. I have been preparing, but I wanted to ask if anyone has gone through the interview process and how did it go?\n\nAny advice would be appreciated. I am graduating soon and would love to have a job for when I graduate.", "author_fullname": "t2_ts5ccrmt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone interviewed at Wells Fargo for an entry level data engineer role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hyzx3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694639545.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have my final interview upcoming with a wells fargo panel. I have been preparing, but I wanted to ask if anyone has gone through the interview process and how did it go?&lt;/p&gt;\n\n&lt;p&gt;Any advice would be appreciated. I am graduating soon and would love to have a job for when I graduate.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16hyzx3", "is_robot_indexable": true, "report_reasons": null, "author": "Much-Series1120", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hyzx3/has_anyone_interviewed_at_wells_fargo_for_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hyzx3/has_anyone_interviewed_at_wells_fargo_for_an/", "subreddit_subscribers": 128305, "created_utc": 1694639545.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some best practices for developing data applications/pipelines using Spark? I'll be using PySpark, if there's any specific to that on top of Spark. Looking for any tips, from how to write code, to designing pipelines. A few I've learned from what I've gathered are to chain commands/functions in the code.\n\nI'm not very experienced in performance tuning and partitioning, so I'd love anything related to those topics as well!", "author_fullname": "t2_1k8fjdz5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some best practices for developing Spark data applications/pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hup03", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694629597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some best practices for developing data applications/pipelines using Spark? I&amp;#39;ll be using PySpark, if there&amp;#39;s any specific to that on top of Spark. Looking for any tips, from how to write code, to designing pipelines. A few I&amp;#39;ve learned from what I&amp;#39;ve gathered are to chain commands/functions in the code.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not very experienced in performance tuning and partitioning, so I&amp;#39;d love anything related to those topics as well!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hup03", "is_robot_indexable": true, "report_reasons": null, "author": "jbnpoc", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hup03/what_are_some_best_practices_for_developing_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hup03/what_are_some_best_practices_for_developing_spark/", "subreddit_subscribers": 128305, "created_utc": 1694629597.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So we have several tens of cronjobs in our k8s cluster which everyone has different schedule. We were discussing that maybe we should put most of the cronjobs into the Airflow because of easier rerunning failed jobs and because of easier to see history of runs. On the other hand it would mean we would have tens/hundreds of dags consisting of one single task as every one has different schedule. \n\nIs there any recommendation for such case? Yes or no and why?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cronjobs vs Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hugdx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694629043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we have several tens of cronjobs in our k8s cluster which everyone has different schedule. We were discussing that maybe we should put most of the cronjobs into the Airflow because of easier rerunning failed jobs and because of easier to see history of runs. On the other hand it would mean we would have tens/hundreds of dags consisting of one single task as every one has different schedule. &lt;/p&gt;\n\n&lt;p&gt;Is there any recommendation for such case? Yes or no and why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16hugdx", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hugdx/cronjobs_vs_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hugdx/cronjobs_vs_airflow/", "subreddit_subscribers": 128305, "created_utc": 1694629043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as a data engineer in data science, assisting scientists in building predicting models. I've been tasked with creating an inventory of features, along with their sources and derivations, used across our models and determine any overlap.\n\nOur typical process flow is as simple as this:\n\n    df = read_sql(open('query_from_warehouse).read()`)\n    df.columns = ['ID', 'Column1', 'Column2', ... )\n    # create features\n    df['feature1'] = np.where(df['Column1]=='some_value'],1,0)\n\nI'm hoping to find a library that can come at the very least, tell me how `feature1`  was derived. In a perfect world, it could tell me the feature came from the original `df` where I could associate it with a sql query. I know some sql parsing libraries exist I've yet to check out.\n\nFellow data engineers, Do you know of any tools that can accomplish this? If there's a word for this, can you tell me what I'm searching for?\n\nMany thanks in advance.", "author_fullname": "t2_16he60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python &amp; SQL Variable Definition / Inventory / Data Lineage / Derivations / Taxonomy Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hn402", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694611855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a data engineer in data science, assisting scientists in building predicting models. I&amp;#39;ve been tasked with creating an inventory of features, along with their sources and derivations, used across our models and determine any overlap.&lt;/p&gt;\n\n&lt;p&gt;Our typical process flow is as simple as this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df = read_sql(open(&amp;#39;query_from_warehouse).read()`)\ndf.columns = [&amp;#39;ID&amp;#39;, &amp;#39;Column1&amp;#39;, &amp;#39;Column2&amp;#39;, ... )\n# create features\ndf[&amp;#39;feature1&amp;#39;] = np.where(df[&amp;#39;Column1]==&amp;#39;some_value&amp;#39;],1,0)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;m hoping to find a library that can come at the very least, tell me how &lt;code&gt;feature1&lt;/code&gt;  was derived. In a perfect world, it could tell me the feature came from the original &lt;code&gt;df&lt;/code&gt; where I could associate it with a sql query. I know some sql parsing libraries exist I&amp;#39;ve yet to check out.&lt;/p&gt;\n\n&lt;p&gt;Fellow data engineers, Do you know of any tools that can accomplish this? If there&amp;#39;s a word for this, can you tell me what I&amp;#39;m searching for?&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hn402", "is_robot_indexable": true, "report_reasons": null, "author": "thefreakypeople", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hn402/python_sql_variable_definition_inventory_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hn402/python_sql_variable_definition_inventory_data/", "subreddit_subscribers": 128305, "created_utc": 1694611855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a Data Engineer located in Hong Kong with 4 years experience. Before this position I was an analyst but it's more a \"full-stacked\" position since I was involved in developing data pipeline and data modeling beside the analytic part.\n\nMy current team is small, only me and a DevOps Engineer design, develop and maintain a data platform. Although our platform are built based on AWS machines, most of components are picked from different open source projects instead of using cloud services (e.g self-managed Spark Application instead of EMR). The DevOps Engineer help setup the environment and infrastructure management. I'm responsible for pipeline development and optimization, pipeline orchestration, data modeling, data visualization. I also handle some basic DevOps jobs in case the DevOps Engineer is not available and it's really challenging. We also have a BA and a PM in the team, I need to work with team to understand the business requirement sometime.\n\nRecently I started discussion with different people and did some research in the data field in Hong Kong. It looks like my skillset is not match to the market since most of companies (around 7\\~8/10) use cloud service fully or enterprise data platform like Cloudera or Databricks. More importantly, it seems like you're not experienced if you didn't use any cloud service like AWS EMR before, even though I've a project in GitHub and profile in SO.\n\nThat makes me feel frustrated. I enjoy pipeline development and data modeling, also I wish my work can impact and contribute to the business, but i'm not sure the boundary between a Data Engineer and DevOps Engineer, especially I'm not familiar with the DevOps side like networking and IAM. On the other hand, it seems my skill set is not transferrable to my next job.\n\nI would like to ask:\n\n1. What do you think a Data Engineer should focus on? I prevent being jack of all trades but master of none.\n2. Is it worthing to spend time getting the cloud / Databricks certificates? Is it helpful when you applying the job? Should I spending more time on studying cloud service instead of the open source project?\n\nThanks for spending time on this post. I would love to know your advice and also your story on how do you grow in the role of Data Engineer.\n\nThank you.", "author_fullname": "t2_a7y0xzcr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ask for advice on the next move on Data Engineer role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hgvlq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694591596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a Data Engineer located in Hong Kong with 4 years experience. Before this position I was an analyst but it&amp;#39;s more a &amp;quot;full-stacked&amp;quot; position since I was involved in developing data pipeline and data modeling beside the analytic part.&lt;/p&gt;\n\n&lt;p&gt;My current team is small, only me and a DevOps Engineer design, develop and maintain a data platform. Although our platform are built based on AWS machines, most of components are picked from different open source projects instead of using cloud services (e.g self-managed Spark Application instead of EMR). The DevOps Engineer help setup the environment and infrastructure management. I&amp;#39;m responsible for pipeline development and optimization, pipeline orchestration, data modeling, data visualization. I also handle some basic DevOps jobs in case the DevOps Engineer is not available and it&amp;#39;s really challenging. We also have a BA and a PM in the team, I need to work with team to understand the business requirement sometime.&lt;/p&gt;\n\n&lt;p&gt;Recently I started discussion with different people and did some research in the data field in Hong Kong. It looks like my skillset is not match to the market since most of companies (around 7~8/10) use cloud service fully or enterprise data platform like Cloudera or Databricks. More importantly, it seems like you&amp;#39;re not experienced if you didn&amp;#39;t use any cloud service like AWS EMR before, even though I&amp;#39;ve a project in GitHub and profile in SO.&lt;/p&gt;\n\n&lt;p&gt;That makes me feel frustrated. I enjoy pipeline development and data modeling, also I wish my work can impact and contribute to the business, but i&amp;#39;m not sure the boundary between a Data Engineer and DevOps Engineer, especially I&amp;#39;m not familiar with the DevOps side like networking and IAM. On the other hand, it seems my skill set is not transferrable to my next job.&lt;/p&gt;\n\n&lt;p&gt;I would like to ask:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What do you think a Data Engineer should focus on? I prevent being jack of all trades but master of none.&lt;/li&gt;\n&lt;li&gt;Is it worthing to spend time getting the cloud / Databricks certificates? Is it helpful when you applying the job? Should I spending more time on studying cloud service instead of the open source project?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks for spending time on this post. I would love to know your advice and also your story on how do you grow in the role of Data Engineer.&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16hgvlq", "is_robot_indexable": true, "report_reasons": null, "author": "masamibb", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hgvlq/ask_for_advice_on_the_next_move_on_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hgvlq/ask_for_advice_on_the_next_move_on_data_engineer/", "subreddit_subscribers": 128305, "created_utc": 1694591596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm curious how people in the community are setting up vector embeddings pipelines to ingest large GBs of data at once.\n\n  \nWhen I was working for a LegalTech startup and we had to ingest millions of litigation documents into a single vector database collection, we used celery + kubernetes with GPU nodes to embed with an open source embedding model (sentence-transformers/sentence-t5-xxl) instead of OpenAI ADA. We eventually added Argo on top of it.\n\n  \nWhat other techniques do you see for scaling the pipeline? Where are you ingesting data from?\n\n  \nWe are building VectorFlow an open-source vector embedding pipeline that is containerized to run on kubernetes in any cloud and want to know what other features we should build next. Check out our Github repo:\u00a0[https://github.com/dgarnitz/vectorflow\u00a0to](https://github.com/dgarnitz/vectorflow%C2%A0to) install VectorFlow locally or t\\*ry it out in the playground (\\*[https://app.getvectorflow.com/](https://app.getvectorflow.com/)).", "author_fullname": "t2_8dgpjm0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Challenges with LLM + Vector searches with Large Data Volume", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hufnh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694628993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious how people in the community are setting up vector embeddings pipelines to ingest large GBs of data at once.&lt;/p&gt;\n\n&lt;p&gt;When I was working for a LegalTech startup and we had to ingest millions of litigation documents into a single vector database collection, we used celery + kubernetes with GPU nodes to embed with an open source embedding model (sentence-transformers/sentence-t5-xxl) instead of OpenAI ADA. We eventually added Argo on top of it.&lt;/p&gt;\n\n&lt;p&gt;What other techniques do you see for scaling the pipeline? Where are you ingesting data from?&lt;/p&gt;\n\n&lt;p&gt;We are building VectorFlow an open-source vector embedding pipeline that is containerized to run on kubernetes in any cloud and want to know what other features we should build next. Check out our Github repo:\u00a0&lt;a href=\"https://github.com/dgarnitz/vectorflow%C2%A0to\"&gt;https://github.com/dgarnitz/vectorflow\u00a0to&lt;/a&gt; install VectorFlow locally or t*ry it out in the playground (*&lt;a href=\"https://app.getvectorflow.com/\"&gt;https://app.getvectorflow.com/&lt;/a&gt;).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16hufnh", "is_robot_indexable": true, "report_reasons": null, "author": "Fast_Homework_3323", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hufnh/data_engineering_challenges_with_llm_vector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hufnh/data_engineering_challenges_with_llm_vector/", "subreddit_subscribers": 128305, "created_utc": 1694628993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello\n\nI am using redshift for a while now and i' got used to it not beeing very fast. I also only use one dc2.large node.\nHowever i have one Table with 1.3 Mio rows and around 20 columns. It is created by an INSERT INTO Clause so all Table configurations are Automatic. \nHowever when i try to group it by 2 columns and sum another column redshift's CPU is at 100% Performance and The query does Not complete (i Stopped after 30 mins).\nI think 1.3 Mio rows are Not very many and redshift should get the Job done but apparently i have to optimize Something. I tried vacuuming The Table already but it didnt lead to any improvements. Do you Guys have any Idea what i could do? Or is it that Just one node ist enough?", "author_fullname": "t2_as1vw8gf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift Times Out die to a simple group by?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hov7p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694616206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I am using redshift for a while now and i&amp;#39; got used to it not beeing very fast. I also only use one dc2.large node.\nHowever i have one Table with 1.3 Mio rows and around 20 columns. It is created by an INSERT INTO Clause so all Table configurations are Automatic. \nHowever when i try to group it by 2 columns and sum another column redshift&amp;#39;s CPU is at 100% Performance and The query does Not complete (i Stopped after 30 mins).\nI think 1.3 Mio rows are Not very many and redshift should get the Job done but apparently i have to optimize Something. I tried vacuuming The Table already but it didnt lead to any improvements. Do you Guys have any Idea what i could do? Or is it that Just one node ist enough?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hov7p", "is_robot_indexable": true, "report_reasons": null, "author": "One_Indication_6921", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hov7p/redshift_times_out_die_to_a_simple_group_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hov7p/redshift_times_out_die_to_a_simple_group_by/", "subreddit_subscribers": 128305, "created_utc": 1694616206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently in a role where I manage the data engineering team and of course the warehouse. We are going through some org and policy changes and one of the suggested policies is NO access to data, in OLTP or DWH. \nMy question is\u2026 is this normal? I\u2019ve been in tech (and data) for 10+ years and I\u2019ve never heard this edict before. RBAC, the concept of least access for role, data masking/obfuscation, yes\u2026 but just flat out not giving users access\u2026 I don\u2019t understand?", "author_fullname": "t2_9rbxc91v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should users have access", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hbaj7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694573365.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently in a role where I manage the data engineering team and of course the warehouse. We are going through some org and policy changes and one of the suggested policies is NO access to data, in OLTP or DWH. \nMy question is\u2026 is this normal? I\u2019ve been in tech (and data) for 10+ years and I\u2019ve never heard this edict before. RBAC, the concept of least access for role, data masking/obfuscation, yes\u2026 but just flat out not giving users access\u2026 I don\u2019t understand?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hbaj7", "is_robot_indexable": true, "report_reasons": null, "author": "AnnualDepth8843", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hbaj7/should_users_have_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hbaj7/should_users_have_access/", "subreddit_subscribers": 128305, "created_utc": 1694573365.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a DA whose in the early stages of my own consulting company and need to increase my DE skillset. I am interested in having someone walk through a basic data engineering project that includes gathering data in python through an API, ingesting it into a cloud instance of Snowflake, and setting it to automate the ingestion process. \n\nThis would happen via Google Meets at an agreed upon time. I am, of course, willing to pay for your time. Please DM me if interested.", "author_fullname": "t2_f5lvn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone willing to walk through a data engineering project from gathering data in python through an API to using that data to build a Snowflake database in the cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16h9imr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694568478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a DA whose in the early stages of my own consulting company and need to increase my DE skillset. I am interested in having someone walk through a basic data engineering project that includes gathering data in python through an API, ingesting it into a cloud instance of Snowflake, and setting it to automate the ingestion process. &lt;/p&gt;\n\n&lt;p&gt;This would happen via Google Meets at an agreed upon time. I am, of course, willing to pay for your time. Please DM me if interested.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16h9imr", "is_robot_indexable": true, "report_reasons": null, "author": "takemyderivative", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16h9imr/anyone_willing_to_walk_through_a_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16h9imr/anyone_willing_to_walk_through_a_data_engineering/", "subreddit_subscribers": 128305, "created_utc": 1694568478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've worked as a data scientist / engineer for the last 9 years but always at a scale a bit below where you really need distributed computing (i.e. SQL databases of a few terabytes). I'm interested in developing the skills that can take me to the next level of scale, but at my job we simply don't have that amount of data. Launching and running a cluster just for fun also seems like it would be a bit expensive. And if I'd want to make a shift to a senior data engineering role at this larger scale, they're going to want me to have some of this experience _before_ I get hired.\n\nWhat's a good way to expose myself to problems that I can solve with Kafka / Spark (i.e. I'm interested in streaming algorithms and mapreduce-like problems)? I'm wondering if there are (for example) open source geo datasets and public servers that you can do some work on (though obviously those cost money as well, so maybe I'm naive to think that).\n\nObviously I'm a bit new to this area so please do let me know if I said anything dumb :) I read \"Designing Data-Intensive Applications\" and have a decent grasp of CS fundamentals, but obviously there's some specialized expertise to be had here.", "author_fullname": "t2_23t9wnbf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to build experience in Kafka and Spark if not in a data engineering job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16i3u4l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694651454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve worked as a data scientist / engineer for the last 9 years but always at a scale a bit below where you really need distributed computing (i.e. SQL databases of a few terabytes). I&amp;#39;m interested in developing the skills that can take me to the next level of scale, but at my job we simply don&amp;#39;t have that amount of data. Launching and running a cluster just for fun also seems like it would be a bit expensive. And if I&amp;#39;d want to make a shift to a senior data engineering role at this larger scale, they&amp;#39;re going to want me to have some of this experience &lt;em&gt;before&lt;/em&gt; I get hired.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s a good way to expose myself to problems that I can solve with Kafka / Spark (i.e. I&amp;#39;m interested in streaming algorithms and mapreduce-like problems)? I&amp;#39;m wondering if there are (for example) open source geo datasets and public servers that you can do some work on (though obviously those cost money as well, so maybe I&amp;#39;m naive to think that).&lt;/p&gt;\n\n&lt;p&gt;Obviously I&amp;#39;m a bit new to this area so please do let me know if I said anything dumb :) I read &amp;quot;Designing Data-Intensive Applications&amp;quot; and have a decent grasp of CS fundamentals, but obviously there&amp;#39;s some specialized expertise to be had here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16i3u4l", "is_robot_indexable": true, "report_reasons": null, "author": "failarmyworm", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16i3u4l/how_to_build_experience_in_kafka_and_spark_if_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16i3u4l/how_to_build_experience_in_kafka_and_spark_if_not/", "subreddit_subscribers": 128305, "created_utc": 1694651454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j9jbk9ezl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing a Stream Processing System? This Article Has You Covered!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "name": "t3_16hoxjc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/vdAlFAyl1MlI8hKgnQRwdwQOB794pFfvbxQ6WaOAll8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694616351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@RisingWave_Engineering/choosing-a-stream-processing-system-this-article-has-you-covered-df046e71d862", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/t2u2X7yf0tj25bnQhxi6Xo7lkyTfy6yJYlwxhkr0hOA.jpg?auto=webp&amp;s=10fa2d5cc294c9ff3dfd51b7da90563fc8bafa81", "width": 1200, "height": 750}, "resolutions": [{"url": "https://external-preview.redd.it/t2u2X7yf0tj25bnQhxi6Xo7lkyTfy6yJYlwxhkr0hOA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab2ec2870532c87dba1f55ceacd7299e7de3909d", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/t2u2X7yf0tj25bnQhxi6Xo7lkyTfy6yJYlwxhkr0hOA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b3a8e6fd9154f83e153e4ce88d22648f4cc6f326", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/t2u2X7yf0tj25bnQhxi6Xo7lkyTfy6yJYlwxhkr0hOA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4356209bf740183a6fbcae45d06bb63241f70c0", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/t2u2X7yf0tj25bnQhxi6Xo7lkyTfy6yJYlwxhkr0hOA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=358e2d4914366b302331f3e7ba7df02790d7be6b", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/t2u2X7yf0tj25bnQhxi6Xo7lkyTfy6yJYlwxhkr0hOA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a52d31d8091e8d3eb29b674ab5bde1c683a4ad3", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/t2u2X7yf0tj25bnQhxi6Xo7lkyTfy6yJYlwxhkr0hOA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8823d19d8d31383239e812592debedbc30e5b785", "width": 1080, "height": 675}], "variants": {}, "id": "YwTl4puRGypP76HcEwAG9DDQsGynTIDtY_g1eVzj2Y4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16hoxjc", "is_robot_indexable": true, "report_reasons": null, "author": "ColinWodell", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hoxjc/choosing_a_stream_processing_system_this_article/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@RisingWave_Engineering/choosing-a-stream-processing-system-this-article-has-you-covered-df046e71d862", "subreddit_subscribers": 128305, "created_utc": 1694616351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to extract the API from Oracle Netsuite. Can someone please guide me? ", "author_fullname": "t2_dfjkwfqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Oracle Netsuite API to extract the data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hkdig", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694604076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to extract the API from Oracle Netsuite. Can someone please guide me? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hkdig", "is_robot_indexable": true, "report_reasons": null, "author": "Boss2508", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hkdig/oracle_netsuite_api_to_extract_the_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hkdig/oracle_netsuite_api_to_extract_the_data/", "subreddit_subscribers": 128305, "created_utc": 1694604076.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \n\nI have some raw tables in BigQuery that contains live data, with a cdc connected to it. \n\nI need to do some analytics queries for an API, and to reduce the compute time, I tought it would be a good idea to store everything in a incremental table using **dbt** to run my model each time the raw table gets an update. It works when testing, but I noticed each dbt run takes 10 sec and that's too long for me. What other options do I have? Thank you\n\nTried materialized views but it does not support my query. ", "author_fullname": "t2_40sshxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bigquery quick query.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hzeug", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694640496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I have some raw tables in BigQuery that contains live data, with a cdc connected to it. &lt;/p&gt;\n\n&lt;p&gt;I need to do some analytics queries for an API, and to reduce the compute time, I tought it would be a good idea to store everything in a incremental table using &lt;strong&gt;dbt&lt;/strong&gt; to run my model each time the raw table gets an update. It works when testing, but I noticed each dbt run takes 10 sec and that&amp;#39;s too long for me. What other options do I have? Thank you&lt;/p&gt;\n\n&lt;p&gt;Tried materialized views but it does not support my query. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hzeug", "is_robot_indexable": true, "report_reasons": null, "author": "LinweZ", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hzeug/bigquery_quick_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hzeug/bigquery_quick_query/", "subreddit_subscribers": 128305, "created_utc": 1694640496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\n[Interpolating bathymetry point dataset using python](https://preview.redd.it/r0kar94bl1ob1.png?width=579&amp;format=png&amp;auto=webp&amp;s=5c8318def79ec710e66262a8798ed6a44f58b287)\n\n[Interpolating bathymetry point dataset using python](https://spatial-dev.guru/2023/07/31/interpolating-bathymetry-point-dataset/)", "author_fullname": "t2_avt84u4i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interpolating bathymetry point dataset using python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "media_metadata": {"r0kar94bl1ob1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 84, "x": 108, "u": "https://preview.redd.it/r0kar94bl1ob1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cdaa0e7a97e4305853bdc423ebed05e4dacd4e7f"}, {"y": 168, "x": 216, "u": "https://preview.redd.it/r0kar94bl1ob1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c34ff1006fba71eb8faf790439f55e79b7cd06d0"}, {"y": 250, "x": 320, "u": "https://preview.redd.it/r0kar94bl1ob1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7bc02bf1a5afd1dae0160d1b7b854f67bc0514a"}], "s": {"y": 453, "x": 579, "u": "https://preview.redd.it/r0kar94bl1ob1.png?width=579&amp;format=png&amp;auto=webp&amp;s=5c8318def79ec710e66262a8798ed6a44f58b287"}, "id": "r0kar94bl1ob1"}}, "name": "t3_16hq7w5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jClx3X7WaOtwOX6nI2hW7emqQsKDfLLMP_PckjmEhno.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1694619352.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r0kar94bl1ob1.png?width=579&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c8318def79ec710e66262a8798ed6a44f58b287\"&gt;Interpolating bathymetry point dataset using python&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://spatial-dev.guru/2023/07/31/interpolating-bathymetry-point-dataset/\"&gt;Interpolating bathymetry point dataset using python&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/S2275Z_UMEPbiKImFbLlXX9h39FlXNTlcb6989F63yA.jpg?auto=webp&amp;s=f39e97e0b90fcbaefbae3ec238427e62993e320c", "width": 579, "height": 453}, "resolutions": [{"url": "https://external-preview.redd.it/S2275Z_UMEPbiKImFbLlXX9h39FlXNTlcb6989F63yA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f830d5d1374f9dbc3935d6b442c42f9a792edea", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/S2275Z_UMEPbiKImFbLlXX9h39FlXNTlcb6989F63yA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=631694a70e28d6f7bb3c6f4089fd377236c36d39", "width": 216, "height": 168}, {"url": "https://external-preview.redd.it/S2275Z_UMEPbiKImFbLlXX9h39FlXNTlcb6989F63yA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7bcf0bebba2340e734f5f201184344b41e475bc", "width": 320, "height": 250}], "variants": {}, "id": "gXs2eFzpnnyKOsdl5OI3jff2o2ddJeEQgbGsCoKon8A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16hq7w5", "is_robot_indexable": true, "report_reasons": null, "author": "iamgeoknight", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hq7w5/interpolating_bathymetry_point_dataset_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hq7w5/interpolating_bathymetry_point_dataset_using/", "subreddit_subscribers": 128305, "created_utc": 1694619352.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am using ADF for an ETL job and I use one generic pipelines per database type (PostgreSQL, DB2, SqlServer). The data is copied from tables in respective database and saved in a data lake (which I call bronze). In a table called MetaBronze I have specified servernames, connectionsstrings and what tables to be copied as well as in which container and under what filepath in the data lake the copy should be stored. I also mark which date this was done so I can reload by updating the date to an earlier one.\n\nIt is a bit like the following approach: [https://techcommunity.microsoft.com/t5/fasttrack-for-azure/metadata-driven-pipelines-for-microsoft-fabric/ba-p/3891651](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/metadata-driven-pipelines-for-microsoft-fabric/ba-p/3891651)\n\nSo far so good since I have a one-to-one relationship between source and destination. However, between my bronze layer and silver layer there is a many-to-many relationship. A specific table (table here is a parquet file copied from the source) from bronze can go to many different tables in the silver layer and multiple tables from bronze can go to one table in the silver layer.\n\nHow should I design this?\n\nBetween bronze and silver we need to keep track of the max date of the files that we used to build the tables in silver since some of the tables are updated incrementally.\n\nLastly between Silver and Gold there is a many-to-one relationship.\n\nCurrently I plan on making one table for each step: MetaBronze, MetaSilver, MetaGold and then an Mapping table that maps Id's between the steps. However I am not sure this is a good Idea.\n\nThanks In advance! Any tip on reading material would be much appreciated as well as suggestions on how this should be designed.\n\nEdit:\n\nFollowing is a rough draft on how I imagine the design should be (obviously there will be much more columns in each table, but in this draft I wanted to address the many-to-many issue). Hope this sketch makes it easier to understand what I want to achieve.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/u414cv9pr0ob1.png?width=1505&amp;format=png&amp;auto=webp&amp;s=6c2fcb893d374b23cc41546417a1ba8b3a71ddad\n\n&amp;#x200B;", "author_fullname": "t2_7ckv97pys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to design a metadata table (or tables) to handle many-to-many relationship between source and destination", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 60, "top_awarded_type": null, "hide_score": false, "media_metadata": {"u414cv9pr0ob1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 46, "x": 108, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aad7b6f4a0ee92e60d53513589e3e2918f9467c5"}, {"y": 93, "x": 216, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=48582d554781d4421c97d18359afa95a5d9467c5"}, {"y": 138, "x": 320, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e58d7f1a13e46fb2830a22fe0581779cefa0bbd"}, {"y": 277, "x": 640, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f597040652b7cdbe8e1e461a7a1a99d2399344c9"}, {"y": 416, "x": 960, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2fe524ed95dd878197548aaf4bdafed89adb9b50"}, {"y": 468, "x": 1080, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ca4bb7e7c16043d1b3417aba60e90dc2a734b9a8"}], "s": {"y": 653, "x": 1505, "u": "https://preview.redd.it/u414cv9pr0ob1.png?width=1505&amp;format=png&amp;auto=webp&amp;s=6c2fcb893d374b23cc41546417a1ba8b3a71ddad"}, "id": "u414cv9pr0ob1"}}, "name": "t3_16hjoc6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6yNDHhukBxPTebG3Gr50l4nN9dR28VzyHUVaJ9Gbd2g.jpg", "edited": 1694609372.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694601755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am using ADF for an ETL job and I use one generic pipelines per database type (PostgreSQL, DB2, SqlServer). The data is copied from tables in respective database and saved in a data lake (which I call bronze). In a table called MetaBronze I have specified servernames, connectionsstrings and what tables to be copied as well as in which container and under what filepath in the data lake the copy should be stored. I also mark which date this was done so I can reload by updating the date to an earlier one.&lt;/p&gt;\n\n&lt;p&gt;It is a bit like the following approach: &lt;a href=\"https://techcommunity.microsoft.com/t5/fasttrack-for-azure/metadata-driven-pipelines-for-microsoft-fabric/ba-p/3891651\"&gt;https://techcommunity.microsoft.com/t5/fasttrack-for-azure/metadata-driven-pipelines-for-microsoft-fabric/ba-p/3891651&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So far so good since I have a one-to-one relationship between source and destination. However, between my bronze layer and silver layer there is a many-to-many relationship. A specific table (table here is a parquet file copied from the source) from bronze can go to many different tables in the silver layer and multiple tables from bronze can go to one table in the silver layer.&lt;/p&gt;\n\n&lt;p&gt;How should I design this?&lt;/p&gt;\n\n&lt;p&gt;Between bronze and silver we need to keep track of the max date of the files that we used to build the tables in silver since some of the tables are updated incrementally.&lt;/p&gt;\n\n&lt;p&gt;Lastly between Silver and Gold there is a many-to-one relationship.&lt;/p&gt;\n\n&lt;p&gt;Currently I plan on making one table for each step: MetaBronze, MetaSilver, MetaGold and then an Mapping table that maps Id&amp;#39;s between the steps. However I am not sure this is a good Idea.&lt;/p&gt;\n\n&lt;p&gt;Thanks In advance! Any tip on reading material would be much appreciated as well as suggestions on how this should be designed.&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;Following is a rough draft on how I imagine the design should be (obviously there will be much more columns in each table, but in this draft I wanted to address the many-to-many issue). Hope this sketch makes it easier to understand what I want to achieve.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u414cv9pr0ob1.png?width=1505&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6c2fcb893d374b23cc41546417a1ba8b3a71ddad\"&gt;https://preview.redd.it/u414cv9pr0ob1.png?width=1505&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6c2fcb893d374b23cc41546417a1ba8b3a71ddad&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hjoc6", "is_robot_indexable": true, "report_reasons": null, "author": "Ygolopot272", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hjoc6/how_to_design_a_metadata_table_or_tables_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hjoc6/how_to_design_a_metadata_table_or_tables_to/", "subreddit_subscribers": 128305, "created_utc": 1694601755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to process either Excel files, API data, or CSV's that have different forms of similar data. The raw files will get saved as they are in S3. In this layer, we will also convert all of these to parquet.\n\nIn the next layer, we are going to take these parquet files, conform them to the same schema, and normalize any data into the appropriate staging tables.\n\nIn the final layer, we will use these normalized tables to denormalize and create any business level, easy-to-query tables.\n\nSo in S3, we'll save layer 1 data in the raw folder, layer 2 data in the stage folder, and layer 3 data in the bi folder. Is this organization reasonable? I feel like the first layer's conversion of the raw data into parquet is some form of transformation and therefore not \"raw.\" But maybe it's just semantics?", "author_fullname": "t2_5pjz5m35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Raw vs Stage vs BI - Planning Things Correctly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16i3vab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694651539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to process either Excel files, API data, or CSV&amp;#39;s that have different forms of similar data. The raw files will get saved as they are in S3. In this layer, we will also convert all of these to parquet.&lt;/p&gt;\n\n&lt;p&gt;In the next layer, we are going to take these parquet files, conform them to the same schema, and normalize any data into the appropriate staging tables.&lt;/p&gt;\n\n&lt;p&gt;In the final layer, we will use these normalized tables to denormalize and create any business level, easy-to-query tables.&lt;/p&gt;\n\n&lt;p&gt;So in S3, we&amp;#39;ll save layer 1 data in the raw folder, layer 2 data in the stage folder, and layer 3 data in the bi folder. Is this organization reasonable? I feel like the first layer&amp;#39;s conversion of the raw data into parquet is some form of transformation and therefore not &amp;quot;raw.&amp;quot; But maybe it&amp;#39;s just semantics?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16i3vab", "is_robot_indexable": true, "report_reasons": null, "author": "maraskooknah", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16i3vab/raw_vs_stage_vs_bi_planning_things_correctly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16i3vab/raw_vs_stage_vs_bi_planning_things_correctly/", "subreddit_subscribers": 128305, "created_utc": 1694651539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am using python and graph.Microsoft.com \nTo connect to SharePoint and get the URL of all files available in a folder. My the URL is not accepting any requests( failed to reach the end point) \n\nAnyone has share some sample code which can help me with this requirement", "author_fullname": "t2_ihm17vcju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get the files URL from a folder in SharePoint", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hzhau", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694640640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using python and graph.Microsoft.com \nTo connect to SharePoint and get the URL of all files available in a folder. My the URL is not accepting any requests( failed to reach the end point) &lt;/p&gt;\n\n&lt;p&gt;Anyone has share some sample code which can help me with this requirement&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16hzhau", "is_robot_indexable": true, "report_reasons": null, "author": "krivamsh007", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hzhau/get_the_files_url_from_a_folder_in_sharepoint/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hzhau/get_the_files_url_from_a_folder_in_sharepoint/", "subreddit_subscribers": 128305, "created_utc": 1694640640.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have a good primer or advice on best practices for data governance in medical &amp; related fields? Trying to make sure I have an ok sense of how the industry approaches that problem in advance of a possible career pivot towards it. \n\n&amp;#x200B;", "author_fullname": "t2_3uhq8bcd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HIPAA and data storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hz43t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694639814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have a good primer or advice on best practices for data governance in medical &amp;amp; related fields? Trying to make sure I have an ok sense of how the industry approaches that problem in advance of a possible career pivot towards it. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16hz43t", "is_robot_indexable": true, "report_reasons": null, "author": "TheBankTank", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hz43t/hipaa_and_data_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hz43t/hipaa_and_data_storage/", "subreddit_subscribers": 128305, "created_utc": 1694639814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,\n\nI need help haha...\n\nI have to design a dwh which its data source is tracking data.\n\nThe raw data schema is some fields, one of them event name (cardinality ~500, increacing in time while the app is developed). Another field is a  parameters dictionary with variable number of keys  (depending on event name) from 6 to 50 keys each.\n\nThe data is to do standar analytics (nbr of user that use this feature, etc).\n\nMy question is what should i do to prepare the data for the analysts?\n\nNormalizing all from the begining would end in a very big table with 500 columns. Also it would vary in time.\n\nHave some other ideas, but really nothing that makes me thing, yes, this is the way.\n\nDoes somebody has any resource, blog post, or experience I can use to inspire or as insignt?\n\nThanks!", "author_fullname": "t2_64tza4m8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data tracking normalization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16hvx47", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694632365.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;I need help haha...&lt;/p&gt;\n\n&lt;p&gt;I have to design a dwh which its data source is tracking data.&lt;/p&gt;\n\n&lt;p&gt;The raw data schema is some fields, one of them event name (cardinality ~500, increacing in time while the app is developed). Another field is a  parameters dictionary with variable number of keys  (depending on event name) from 6 to 50 keys each.&lt;/p&gt;\n\n&lt;p&gt;The data is to do standar analytics (nbr of user that use this feature, etc).&lt;/p&gt;\n\n&lt;p&gt;My question is what should i do to prepare the data for the analysts?&lt;/p&gt;\n\n&lt;p&gt;Normalizing all from the begining would end in a very big table with 500 columns. Also it would vary in time.&lt;/p&gt;\n\n&lt;p&gt;Have some other ideas, but really nothing that makes me thing, yes, this is the way.&lt;/p&gt;\n\n&lt;p&gt;Does somebody has any resource, blog post, or experience I can use to inspire or as insignt?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16hvx47", "is_robot_indexable": true, "report_reasons": null, "author": "AndroidePsicokiller", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hvx47/data_tracking_normalization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16hvx47/data_tracking_normalization/", "subreddit_subscribers": 128305, "created_utc": 1694632365.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_mbuz666f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How we built a Streaming SQL Engine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_16hvso0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XLkl3kC5RjCIAQCiqsIya4l1fgyvIWSGQFGiQ-nN--M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694632085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.epsio.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.epsio.io/how-to-create-a-streaming-sql-engine-96e23994e0dd", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Q_VogusdbAlwFUZZICrXzmigT3VLnQRxXv5RjSo78m4.jpg?auto=webp&amp;s=2638e6b717a1eab2e66a52afadad38285afd1476", "width": 1200, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/Q_VogusdbAlwFUZZICrXzmigT3VLnQRxXv5RjSo78m4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=971c63f44e00612f61c53bb75e2b8dda35a8b118", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Q_VogusdbAlwFUZZICrXzmigT3VLnQRxXv5RjSo78m4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f9e691874a924dc1562f5ed92e6c82653cb5da9", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Q_VogusdbAlwFUZZICrXzmigT3VLnQRxXv5RjSo78m4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa5b67e1800d23d35702c093a84eee1faeaaece3", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/Q_VogusdbAlwFUZZICrXzmigT3VLnQRxXv5RjSo78m4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3bbb4c4f324ca0a53c533e32735fb6235b83c0e1", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/Q_VogusdbAlwFUZZICrXzmigT3VLnQRxXv5RjSo78m4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=92473ede888d3433740cd26c84af1a7fdb4d6d20", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/Q_VogusdbAlwFUZZICrXzmigT3VLnQRxXv5RjSo78m4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7b89db777104a6a5f8b55c224f487face955fa9c", "width": 1080, "height": 810}], "variants": {}, "id": "ScR3TmXvQjwmPHqH8dvevbEUuRL70CqPIPf_ofRJNEI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16hvso0", "is_robot_indexable": true, "report_reasons": null, "author": "dkgs19982", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16hvso0/how_we_built_a_streaming_sql_engine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.epsio.io/how-to-create-a-streaming-sql-engine-96e23994e0dd", "subreddit_subscribers": 128305, "created_utc": 1694632085.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm hitting an issue which appears so fundamental, I'm surprised I can't find any resources about it.\n\nWhen fivetran syncs our postgres (CDC) data to snowflake, from a highlevel it:  loops through each table, loads the updated data into a snowflake \"staging\" table, then updates the underlying \"live\" table.  If we were syncing 3 tables, it'd roughly look like:  \n\n\n* copy\\_records \"posts\\_users\"\n* insert\\_records \"posts\\_users\"\n* copy\\_records \"posts\"\n* insert\\_records \"posts\"\n* copy\\_records \"users\"\n* insert\\_records \"users\"\n\n&amp;#x200B;\n\nThe issue: Once that first 'insert\\_records \"posts\\_users\"' completes, the data in snowflake will be inconsistent, because \"post\\_users\" will be new data, but \"posts\" and \"users\" will be old data.  We have a few hundred postgres tables, so this time window is about 3 minutes.\n\n&amp;#x200B;\n\n* I would love if fivetran supported blue/green loading (table rename, SWAP), so that the data is always consistent and read-able.\n* If not, it'd be nice if they batched the \"copy\\_records\" steps together, then did the \"insert\\_records\" steps together, so the total time was less.\n\n&amp;#x200B;\n\nI can try to work around the issue in the orchestration layer, but that assumes:  \n\n\n* the downstream steps need to finish before the next fivetran postgres sync can start\n* a team who should have access to the raw data (vs bronze/gold) might get inconsistent data\n\n&amp;#x200B;\n\nRandom idea:\n\n* when i kickoff a \"dbt run\" pass in a timestamp of \"last known consistent state\", and use snowflake timetravel in all dbt queries (override 'source macro')???  would this be super slow/horrible?\n\n&amp;#x200B;\n\nAm I totally missing the boat here?  Any recommendations?  Thanks so much for reading this far....  \n\n\nfull context: fivetran postgres wal log w/ history mode, dbt, snowflake", "author_fullname": "t2_1qwjj2kn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Consistency during Fivetran Load", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ht8jd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694626264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m hitting an issue which appears so fundamental, I&amp;#39;m surprised I can&amp;#39;t find any resources about it.&lt;/p&gt;\n\n&lt;p&gt;When fivetran syncs our postgres (CDC) data to snowflake, from a highlevel it:  loops through each table, loads the updated data into a snowflake &amp;quot;staging&amp;quot; table, then updates the underlying &amp;quot;live&amp;quot; table.  If we were syncing 3 tables, it&amp;#39;d roughly look like:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;copy_records &amp;quot;posts_users&amp;quot;&lt;/li&gt;\n&lt;li&gt;insert_records &amp;quot;posts_users&amp;quot;&lt;/li&gt;\n&lt;li&gt;copy_records &amp;quot;posts&amp;quot;&lt;/li&gt;\n&lt;li&gt;insert_records &amp;quot;posts&amp;quot;&lt;/li&gt;\n&lt;li&gt;copy_records &amp;quot;users&amp;quot;&lt;/li&gt;\n&lt;li&gt;insert_records &amp;quot;users&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The issue: Once that first &amp;#39;insert_records &amp;quot;posts_users&amp;quot;&amp;#39; completes, the data in snowflake will be inconsistent, because &amp;quot;post_users&amp;quot; will be new data, but &amp;quot;posts&amp;quot; and &amp;quot;users&amp;quot; will be old data.  We have a few hundred postgres tables, so this time window is about 3 minutes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I would love if fivetran supported blue/green loading (table rename, SWAP), so that the data is always consistent and read-able.&lt;/li&gt;\n&lt;li&gt;If not, it&amp;#39;d be nice if they batched the &amp;quot;copy_records&amp;quot; steps together, then did the &amp;quot;insert_records&amp;quot; steps together, so the total time was less.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I can try to work around the issue in the orchestration layer, but that assumes:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the downstream steps need to finish before the next fivetran postgres sync can start&lt;/li&gt;\n&lt;li&gt;a team who should have access to the raw data (vs bronze/gold) might get inconsistent data&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Random idea:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;when i kickoff a &amp;quot;dbt run&amp;quot; pass in a timestamp of &amp;quot;last known consistent state&amp;quot;, and use snowflake timetravel in all dbt queries (override &amp;#39;source macro&amp;#39;)???  would this be super slow/horrible?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Am I totally missing the boat here?  Any recommendations?  Thanks so much for reading this far....  &lt;/p&gt;\n\n&lt;p&gt;full context: fivetran postgres wal log w/ history mode, dbt, snowflake&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ht8jd", "is_robot_indexable": true, "report_reasons": null, "author": "tomhallett", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ht8jd/data_consistency_during_fivetran_load/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ht8jd/data_consistency_during_fivetran_load/", "subreddit_subscribers": 128305, "created_utc": 1694626264.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}