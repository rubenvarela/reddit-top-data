{"kind": "Listing", "data": {"after": "t3_16tzcjh", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "  \n\nMy Stats:  \nAzure Engineering Experience: 2 years on and off  \nStudied: 30 days \u2013 Ave 3 hours/day  \nScored: 892/1000\n\nExam Covers: Data Factory, Synapse, Stream Analytics, Event Hubs, Databricks, Data Lake Storage, SQL/Scala\n\nResources I paid for:   \nAlan Rodrigues \u2013 Udemy - DP-203 Course - 21 hours long - Updated for 2023. Very good course, but not enough to pass the exam. \n\nMeasure Up Test Prep Questions\n\nYou will get questions on:  \nParquet Files, Data Lake Gen2 storage, Slowly Changing Dimension Tables (SCDs), Fact &amp; Dimension tables, Synapse Distributions (Hash, Replicated, Round-Robin), Synapse Partitions &amp; Indexes, Security Options. I\u2019d say half the questions involved Synapse. \n\nFinal Tips:\n\n\u2022 Make sure you\u2019re getting near 100% on test preps before you attempt the exam.\n\n\u2022 Have Azure open when you do the test prep questions, to practice for real.\n\n\u2022 If you feel you\u2019re not ready, re-schedule the exam, which is free.\n\n\u2022 Book the exam in advance - Stopped me from procrastinating or chickening out.\n\n\u2022 Organize your time, I needed 3 hours/day for 4 weeks to prepare. Depends on your skill level.\n\n\u2022 Read the question &amp; answer options first. Then look at the case study. Helps you find the answer quicker. \n\nHere are some free resources.\n\nFree Test Prep Questions:  \n [https://www.analystlaunch.com/c/dp203-test-prep-download-landing](https://www.analystlaunch.com/c/dp203-test-prep-download-landing)\n\nVideo on passing the exam:  \n [https://www.youtube.com/watch?v=fRR5FLrv398](https://www.youtube.com/watch?v=fRR5FLrv398)\n\nSitting your first Microsoft exam at home:  \n [https://www.youtube.com/watch?v=hV\\_DpwpzNZI&amp;t=1s](https://www.youtube.com/watch?v=hV_DpwpzNZI&amp;t=1s)\n\nGood luck.", "author_fullname": "t2_144pid", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I recently passed the DP-203, here are my notes to prepare", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16thbm7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695811920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My Stats:&lt;br/&gt;\nAzure Engineering Experience: 2 years on and off&lt;br/&gt;\nStudied: 30 days \u2013 Ave 3 hours/day&lt;br/&gt;\nScored: 892/1000&lt;/p&gt;\n\n&lt;p&gt;Exam Covers: Data Factory, Synapse, Stream Analytics, Event Hubs, Databricks, Data Lake Storage, SQL/Scala&lt;/p&gt;\n\n&lt;p&gt;Resources I paid for:&lt;br/&gt;\nAlan Rodrigues \u2013 Udemy - DP-203 Course - 21 hours long - Updated for 2023. Very good course, but not enough to pass the exam. &lt;/p&gt;\n\n&lt;p&gt;Measure Up Test Prep Questions&lt;/p&gt;\n\n&lt;p&gt;You will get questions on:&lt;br/&gt;\nParquet Files, Data Lake Gen2 storage, Slowly Changing Dimension Tables (SCDs), Fact &amp;amp; Dimension tables, Synapse Distributions (Hash, Replicated, Round-Robin), Synapse Partitions &amp;amp; Indexes, Security Options. I\u2019d say half the questions involved Synapse. &lt;/p&gt;\n\n&lt;p&gt;Final Tips:&lt;/p&gt;\n\n&lt;p&gt;\u2022 Make sure you\u2019re getting near 100% on test preps before you attempt the exam.&lt;/p&gt;\n\n&lt;p&gt;\u2022 Have Azure open when you do the test prep questions, to practice for real.&lt;/p&gt;\n\n&lt;p&gt;\u2022 If you feel you\u2019re not ready, re-schedule the exam, which is free.&lt;/p&gt;\n\n&lt;p&gt;\u2022 Book the exam in advance - Stopped me from procrastinating or chickening out.&lt;/p&gt;\n\n&lt;p&gt;\u2022 Organize your time, I needed 3 hours/day for 4 weeks to prepare. Depends on your skill level.&lt;/p&gt;\n\n&lt;p&gt;\u2022 Read the question &amp;amp; answer options first. Then look at the case study. Helps you find the answer quicker. &lt;/p&gt;\n\n&lt;p&gt;Here are some free resources.&lt;/p&gt;\n\n&lt;p&gt;Free Test Prep Questions:&lt;br/&gt;\n &lt;a href=\"https://www.analystlaunch.com/c/dp203-test-prep-download-landing\"&gt;https://www.analystlaunch.com/c/dp203-test-prep-download-landing&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Video on passing the exam:&lt;br/&gt;\n &lt;a href=\"https://www.youtube.com/watch?v=fRR5FLrv398\"&gt;https://www.youtube.com/watch?v=fRR5FLrv398&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sitting your first Microsoft exam at home:&lt;br/&gt;\n &lt;a href=\"https://www.youtube.com/watch?v=hV_DpwpzNZI&amp;amp;t=1s\"&gt;https://www.youtube.com/watch?v=hV_DpwpzNZI&amp;amp;t=1s&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Good luck.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/h6wnfWcQ3YtgQNPie3idtMjoJC5FM1FV4T14guiaWMY.jpg?auto=webp&amp;s=3976944633dabc5557d2590773413fb971dcb141", "width": 800, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/h6wnfWcQ3YtgQNPie3idtMjoJC5FM1FV4T14guiaWMY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=be7c917b3417d86e923ffb71081bf5eab499349f", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/h6wnfWcQ3YtgQNPie3idtMjoJC5FM1FV4T14guiaWMY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1971f20837cef88239efff3d89277f985bcae6dc", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/h6wnfWcQ3YtgQNPie3idtMjoJC5FM1FV4T14guiaWMY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=35b3bf2c272d05e0ea0edd59dd71cc7eefc93489", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/h6wnfWcQ3YtgQNPie3idtMjoJC5FM1FV4T14guiaWMY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d95f4b972d047c571d8ee2e410901f7a1736f9f", "width": 640, "height": 640}], "variants": {}, "id": "UFwUkDMkNVaWB_fVqVW8W81v34KS8PnII6eyck02VAY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16thbm7", "is_robot_indexable": true, "report_reasons": null, "author": "dojogreen", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16thbm7/i_recently_passed_the_dp203_here_are_my_notes_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16thbm7/i_recently_passed_the_dp203_here_are_my_notes_to/", "subreddit_subscribers": 130809, "created_utc": 1695811920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am always wondering how can a data engineer know, and then convince product team or business that their architecture or design is correct? There is not exactly an exact science to data engineering. There are companies which are happy using out of the box tools and make it work, and there are engineers putting up with complex legacy codebases using microservices et al. Besides reading up and doing courses trying to get as close as possible to best practices, your architecture suggestions are subject to either lessons learnt from experience (takes 1-2 years for a big data project to take real shape) or googling and doing the best possible back of the envelope tradeoffs. While other non technical team members expect you to give them a silver bullet or quickly lose trust in you. How do you establish this confidence in a solution without seeming like a tinkerer?", "author_fullname": "t2_rov69023", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you \"know\" your architecture is correct?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16u1sxr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695862478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am always wondering how can a data engineer know, and then convince product team or business that their architecture or design is correct? There is not exactly an exact science to data engineering. There are companies which are happy using out of the box tools and make it work, and there are engineers putting up with complex legacy codebases using microservices et al. Besides reading up and doing courses trying to get as close as possible to best practices, your architecture suggestions are subject to either lessons learnt from experience (takes 1-2 years for a big data project to take real shape) or googling and doing the best possible back of the envelope tradeoffs. While other non technical team members expect you to give them a silver bullet or quickly lose trust in you. How do you establish this confidence in a solution without seeming like a tinkerer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16u1sxr", "is_robot_indexable": true, "report_reasons": null, "author": "CutSubstantial7320", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16u1sxr/how_do_you_know_your_architecture_is_correct/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16u1sxr/how_do_you_know_your_architecture_is_correct/", "subreddit_subscribers": 130809, "created_utc": 1695862478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most of us would have observed recently that the companies are moving to cloud for data engineering. Be it Azure,  gcp AWS or databricks cloud, they provide managed cluster platforms.Since they are making the data engineers life easier by integrating job optimisation tools,  managed airflow, orchestrationservices, CI CD and what not.  But when it comes to on-premise, currently most of these are being manage by data engineers and some intermediate level coding is required.\nI would like to understand from the experienced data engineers in this group is --  in a few years down the lene, say 5 years, the dependency on data engineers will decrease for the type tasks mentioned above ?\nOf course, data engineers would be doing ETL by writing python sql scripts and that is not going to go away but if we keep this part side, what is the future of the rest of the tasks?\n I think there are some big organisations trying to move to the cloud and in the near future, they would need tremendous support from the data engineers and this theerw would be a high demand in the short run.\n\nThanks.", "author_fullname": "t2_b8ynqpo5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scope of Data Engineering in future", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16tnvsu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695829199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most of us would have observed recently that the companies are moving to cloud for data engineering. Be it Azure,  gcp AWS or databricks cloud, they provide managed cluster platforms.Since they are making the data engineers life easier by integrating job optimisation tools,  managed airflow, orchestrationservices, CI CD and what not.  But when it comes to on-premise, currently most of these are being manage by data engineers and some intermediate level coding is required.\nI would like to understand from the experienced data engineers in this group is --  in a few years down the lene, say 5 years, the dependency on data engineers will decrease for the type tasks mentioned above ?\nOf course, data engineers would be doing ETL by writing python sql scripts and that is not going to go away but if we keep this part side, what is the future of the rest of the tasks?\n I think there are some big organisations trying to move to the cloud and in the near future, they would need tremendous support from the data engineers and this theerw would be a high demand in the short run.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16tnvsu", "is_robot_indexable": true, "report_reasons": null, "author": "ajeetyadav_", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tnvsu/scope_of_data_engineering_in_future/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16tnvsu/scope_of_data_engineering_in_future/", "subreddit_subscribers": 130809, "created_utc": 1695829199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Good afternoon readers,\n\nWant to keep this to the point as this is a longer post so I am going to jump right in to my situation. I recently signed at a large consulting firm as a Jr. DE (perfect for me, very exiting) but my current scope of responsibility seems way over my head, maybe I had a mistach in expectations I am unsure.  \n\n\nTo get directly to the ask, our team has a Tableau enviroment that various users have eyes on. The data sources are populated from various parts of the Navy (my client). Currently, roughly 40 different excel files are used to refresh data sources and it is a completely manual process. In other words, user goes to website, user downloads more recent data, user transforms said data manually, user refreshes data source on our Tableau server. This process is repeated more or less from varoius stakeholders and many web portals.\n\nSo far, to me everything makes sense, the ask is to build a database and streamline the data management process (build data pipeline). I would say I am strong but no expert in SQL but so far within my means. Two major problems exists.\n\n**Problem 1: Moving Data from SQL to Tableau Server**\n\nSince this work is done in a secure space, Tableau does not support the functionality of connecting directly to our database (a pSQL enviroment). I have gotten Tableau techs on the phone and asked them directly, a direct connection between these two services simply cannot be done in a secure enviroment. I have also had discussions with our cloud dev team and managers and asked them how to streamline this data, they are not helpful.  More recently, I have been poking around with the Tableau Rest API. However the only data sources it can ingest are tableau files (.tds, .tde, .pbix). So I am unsure if it is possible to export data from SQL and manage that conversion.\n\nSo I am essentially at a stand still, I do not see a way to move data from my pSQL enviroment to Tableau server. This essentially makes the ask impossible, but maybe I am just young and naive. My intuition tells me surely there is a way to integrate these two programs, but I see none. Thoughts here are greatly appreicated.\n\n**Problem 2: Data Ingestion**\n\nThis is a much smaller problem for me and advise on problem 1 is more impactful, but I figure since I am posting to put this out there. I am unsure how to ingest data to a SQL enviroment. Our cloud leads tell me a transition to Azure pSQL is happening soon, so I am guessing there is some type of UI tool that I can instruct users to 'drop' excel files that I can write code against to transform/write to our SQL enviroment. But this is a guess and ideas on data ingestion would be great.\n\nI have previously tried tried to ask the owners of the raw data sources that our users are pulling data from to see if they have an API or if I can integrate with their backend, I am just left on read. \n\n**Carreer**\n\nI just do not know what to tell my manager, he wants updates and everything I explore to solve these problems leads nowhere. It feels like I am on the wrong project and that not much more optimization can be done. But I want to leave it to the experts that I am sure exists on this sub.\n\nAm I perhaps in over my head as far as problem solving/skill level? I was hoping my first gig I would have an architecture already in place that I could develop/expand on, but perhaps that expecation is wrong. \n\nShould I tell my managers I do not love this contract? This feels bad as a Jr. I want to prove myself, not fun away from the first gig they trusted me with.\n\n&amp;#x200B;\n\nMuch love to any and all readers and I appreicate your thoughts in advance.\n\nSigned,  \nA Jr. DE in need\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_2saw9ds3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jr. Data Engineer who is in over his head, looking for advice from more senior members.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16tv2io", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695846453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good afternoon readers,&lt;/p&gt;\n\n&lt;p&gt;Want to keep this to the point as this is a longer post so I am going to jump right in to my situation. I recently signed at a large consulting firm as a Jr. DE (perfect for me, very exiting) but my current scope of responsibility seems way over my head, maybe I had a mistach in expectations I am unsure.  &lt;/p&gt;\n\n&lt;p&gt;To get directly to the ask, our team has a Tableau enviroment that various users have eyes on. The data sources are populated from various parts of the Navy (my client). Currently, roughly 40 different excel files are used to refresh data sources and it is a completely manual process. In other words, user goes to website, user downloads more recent data, user transforms said data manually, user refreshes data source on our Tableau server. This process is repeated more or less from varoius stakeholders and many web portals.&lt;/p&gt;\n\n&lt;p&gt;So far, to me everything makes sense, the ask is to build a database and streamline the data management process (build data pipeline). I would say I am strong but no expert in SQL but so far within my means. Two major problems exists.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem 1: Moving Data from SQL to Tableau Server&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Since this work is done in a secure space, Tableau does not support the functionality of connecting directly to our database (a pSQL enviroment). I have gotten Tableau techs on the phone and asked them directly, a direct connection between these two services simply cannot be done in a secure enviroment. I have also had discussions with our cloud dev team and managers and asked them how to streamline this data, they are not helpful.  More recently, I have been poking around with the Tableau Rest API. However the only data sources it can ingest are tableau files (.tds, .tde, .pbix). So I am unsure if it is possible to export data from SQL and manage that conversion.&lt;/p&gt;\n\n&lt;p&gt;So I am essentially at a stand still, I do not see a way to move data from my pSQL enviroment to Tableau server. This essentially makes the ask impossible, but maybe I am just young and naive. My intuition tells me surely there is a way to integrate these two programs, but I see none. Thoughts here are greatly appreicated.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem 2: Data Ingestion&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a much smaller problem for me and advise on problem 1 is more impactful, but I figure since I am posting to put this out there. I am unsure how to ingest data to a SQL enviroment. Our cloud leads tell me a transition to Azure pSQL is happening soon, so I am guessing there is some type of UI tool that I can instruct users to &amp;#39;drop&amp;#39; excel files that I can write code against to transform/write to our SQL enviroment. But this is a guess and ideas on data ingestion would be great.&lt;/p&gt;\n\n&lt;p&gt;I have previously tried tried to ask the owners of the raw data sources that our users are pulling data from to see if they have an API or if I can integrate with their backend, I am just left on read. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Carreer&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I just do not know what to tell my manager, he wants updates and everything I explore to solve these problems leads nowhere. It feels like I am on the wrong project and that not much more optimization can be done. But I want to leave it to the experts that I am sure exists on this sub.&lt;/p&gt;\n\n&lt;p&gt;Am I perhaps in over my head as far as problem solving/skill level? I was hoping my first gig I would have an architecture already in place that I could develop/expand on, but perhaps that expecation is wrong. &lt;/p&gt;\n\n&lt;p&gt;Should I tell my managers I do not love this contract? This feels bad as a Jr. I want to prove myself, not fun away from the first gig they trusted me with.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Much love to any and all readers and I appreicate your thoughts in advance.&lt;/p&gt;\n\n&lt;p&gt;Signed,&lt;br/&gt;\nA Jr. DE in need&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16tv2io", "is_robot_indexable": true, "report_reasons": null, "author": "likely-", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tv2io/jr_data_engineer_who_is_in_over_his_head_looking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16tv2io/jr_data_engineer_who_is_in_over_his_head_looking/", "subreddit_subscribers": 130809, "created_utc": 1695846453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TL;DR: I love writing complex SQL queries to gather data for someone else to analyze. What job functions should I look at in the Data world?\n\nI have been a process engineer at a defense contractor for close to a decade, and I have found that my heart is not truly in it.\n\nAt my company, I have become an SQL expert, and many engineers and managers will reach out to me to acquire processed data sets that they can then use to track yield, throughput, or other trends. Oftentimes this requires creating a dashboard, but then it's up to them to analyze the data in the dashboard to determine what can be learned from it or what actions should be taken based on it. \n\nOur raw data resides in different servers and tables that I can access through SSMS, but it can require some very complicated queries to process the raw data into whatever form my customer is asking for. I absolutely love the days where I sit and write SQL all day to join it all together and make sure it's clean and ready for the customer.\n\nI recently learned about the Data job functions and wondered if this may be a career path that I should go down. However, I'm having trouble determining where exactly I would best fit based on my interests. I know I *don't* want to perform statistical analysis or modeling. I don't care what the data actually says, I just want to gather it from the various tables and clean it and process it into something that's user-friendly. I actually don't even care about the dashboards that much; usually it's just a means to an end to hand the data off to the customer. I would be just fine if my end product was the query text itself or a View on the database or something like that.\n\nWhen I look at Data Engineering projects via YouTube, a lot of time is spent gathering data from various sources like APIs, so I'm not sure how much time a Data Engineer spends writing complex SQL queries.\n\nAny advice on where you think I fit best in the Data fields would be much appreciated.", "author_fullname": "t2_8wmp3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do I fit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16tm2uk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695833811.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695824855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: I love writing complex SQL queries to gather data for someone else to analyze. What job functions should I look at in the Data world?&lt;/p&gt;\n\n&lt;p&gt;I have been a process engineer at a defense contractor for close to a decade, and I have found that my heart is not truly in it.&lt;/p&gt;\n\n&lt;p&gt;At my company, I have become an SQL expert, and many engineers and managers will reach out to me to acquire processed data sets that they can then use to track yield, throughput, or other trends. Oftentimes this requires creating a dashboard, but then it&amp;#39;s up to them to analyze the data in the dashboard to determine what can be learned from it or what actions should be taken based on it. &lt;/p&gt;\n\n&lt;p&gt;Our raw data resides in different servers and tables that I can access through SSMS, but it can require some very complicated queries to process the raw data into whatever form my customer is asking for. I absolutely love the days where I sit and write SQL all day to join it all together and make sure it&amp;#39;s clean and ready for the customer.&lt;/p&gt;\n\n&lt;p&gt;I recently learned about the Data job functions and wondered if this may be a career path that I should go down. However, I&amp;#39;m having trouble determining where exactly I would best fit based on my interests. I know I &lt;em&gt;don&amp;#39;t&lt;/em&gt; want to perform statistical analysis or modeling. I don&amp;#39;t care what the data actually says, I just want to gather it from the various tables and clean it and process it into something that&amp;#39;s user-friendly. I actually don&amp;#39;t even care about the dashboards that much; usually it&amp;#39;s just a means to an end to hand the data off to the customer. I would be just fine if my end product was the query text itself or a View on the database or something like that.&lt;/p&gt;\n\n&lt;p&gt;When I look at Data Engineering projects via YouTube, a lot of time is spent gathering data from various sources like APIs, so I&amp;#39;m not sure how much time a Data Engineer spends writing complex SQL queries.&lt;/p&gt;\n\n&lt;p&gt;Any advice on where you think I fit best in the Data fields would be much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16tm2uk", "is_robot_indexable": true, "report_reasons": null, "author": "bluetrench", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tm2uk/where_do_i_fit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16tm2uk/where_do_i_fit/", "subreddit_subscribers": 130809, "created_utc": 1695824855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have lots of data generated by source systems that don't have their own backend OLTP database, they just push flat files out. These are easy to consume into delta tables. Delta Lake and the lake house are new territory and even the sales engineers we've been talking to from databricks have seemed a bit non-committal iffy/squirmy when I ask them about referential constraints. I know delta lake doesn't have physically enforced constraints. I assumed they just had high performance multi-step wrappers that effectively do the same thing. I'd like to NOT have to develop that myself, I'd rather just use an OLTP in that situation\n\nMy question for the community is, has anyone used the databricks implementation of ['referential' constraints](https://docs.databricks.com/en/tables/constraints.html) extensively? What were the major challenges/issues? How much custom hacking was it?\n\nOur use cases aren't very complicated. In most cases we're just bulk adding data to tables, and sometimes bulk overwriting. Tables have relations to other tables based on keys. But we don't have any situations like delete or update record from table A and then cascade that to other downstream tables etc. ", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake 'Referential' Constraints", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ttgcz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695842702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have lots of data generated by source systems that don&amp;#39;t have their own backend OLTP database, they just push flat files out. These are easy to consume into delta tables. Delta Lake and the lake house are new territory and even the sales engineers we&amp;#39;ve been talking to from databricks have seemed a bit non-committal iffy/squirmy when I ask them about referential constraints. I know delta lake doesn&amp;#39;t have physically enforced constraints. I assumed they just had high performance multi-step wrappers that effectively do the same thing. I&amp;#39;d like to NOT have to develop that myself, I&amp;#39;d rather just use an OLTP in that situation&lt;/p&gt;\n\n&lt;p&gt;My question for the community is, has anyone used the databricks implementation of &lt;a href=\"https://docs.databricks.com/en/tables/constraints.html\"&gt;&amp;#39;referential&amp;#39; constraints&lt;/a&gt; extensively? What were the major challenges/issues? How much custom hacking was it?&lt;/p&gt;\n\n&lt;p&gt;Our use cases aren&amp;#39;t very complicated. In most cases we&amp;#39;re just bulk adding data to tables, and sometimes bulk overwriting. Tables have relations to other tables based on keys. But we don&amp;#39;t have any situations like delete or update record from table A and then cascade that to other downstream tables etc. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?auto=webp&amp;s=9dd59568b8579947f05ce66ee028655ef14e64d6", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99613d282007d0bcc41947bc7f0846da94adca04", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400ef45c57444e53fb95c1358e9a0b6419c3112e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ed83d9a6c1afb35b8be4de3f85b722298d1c3d6", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=768e111879e31b88e5a61b81d8d367edaa5e5351", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2a359111feb6e4d3ffa529f6614614a63914c4e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6e5d40f18830851f93eb2158f465da573a5df80", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ttgcz", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ttgcz/delta_lake_referential_constraints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ttgcz/delta_lake_referential_constraints/", "subreddit_subscribers": 130809, "created_utc": 1695842702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Observability for Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "name": "t3_16thtp5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jsJlUb0PErnZBq5Ynmga2-8jvFwCXbk8BY7ZLhQ5N2A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695813514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@jatin_solanki/data-observability-for-data-engineers-6035898db6b1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?auto=webp&amp;s=1d02e183083e4073e68845ead1706a813ba69b77", "width": 1200, "height": 611}, "resolutions": [{"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cef1ef2e72af279e50b7f878a1997b835e0bace1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=94c137e16ee29f4a25b737d13f2125b761ad14ff", "width": 216, "height": 109}, {"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=47af5272d2421f6d3f0ee3cad05e67e44317e1ae", "width": 320, "height": 162}, {"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e06526383b4262a8e8c924cebcc4f75c0651e935", "width": 640, "height": 325}, {"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8f2f9852a7ea11872456519c5ed7c23e3c086dca", "width": 960, "height": 488}, {"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8260deafd5457c053ab4f84053f213391413e13a", "width": 1080, "height": 549}], "variants": {}, "id": "K4cOA1zbl1M9mnT-Vo35IS6P0fPDMY51O2l9-SYi9gk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16thtp5", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16thtp5/data_observability_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@jatin_solanki/data-observability-for-data-engineers-6035898db6b1", "subreddit_subscribers": 130809, "created_utc": 1695813514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If I am trying to standardize KPIs for the company, what layer should they be in? I want to build a core dimensional model that can support many use cases. Should I build a generic dimensional model then create a reporting table/view that will store all the KPIs based on the fact tables? Like marketing KPI view and finance KPI view.", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fact Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16to8b0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695830029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I am trying to standardize KPIs for the company, what layer should they be in? I want to build a core dimensional model that can support many use cases. Should I build a generic dimensional model then create a reporting table/view that will store all the KPIs based on the fact tables? Like marketing KPI view and finance KPI view.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16to8b0", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16to8b0/fact_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16to8b0/fact_tables/", "subreddit_subscribers": 130809, "created_utc": 1695830029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At work, we work with a very  bureaucratic organization that provides data to us only in the form of scheduled emails with spreadsheets in a zip file. Attempting to convince them to connect to an API is futile. \n\nAnyways, I used python to collect all the outlook attachments, load the zipped files as temp files, read the data into data frames and push to our server database. it\u2019s janky as hell and relies on the most insecure method of data transfer(email) , but it works dammit!\n\nAnyone else put up with some seriously janky data engineering? Be honest. I cannot be the only one.", "author_fullname": "t2_6hsp2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extremely Janky Data Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ty7xj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695853609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At work, we work with a very  bureaucratic organization that provides data to us only in the form of scheduled emails with spreadsheets in a zip file. Attempting to convince them to connect to an API is futile. &lt;/p&gt;\n\n&lt;p&gt;Anyways, I used python to collect all the outlook attachments, load the zipped files as temp files, read the data into data frames and push to our server database. it\u2019s janky as hell and relies on the most insecure method of data transfer(email) , but it works dammit!&lt;/p&gt;\n\n&lt;p&gt;Anyone else put up with some seriously janky data engineering? Be honest. I cannot be the only one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ty7xj", "is_robot_indexable": true, "report_reasons": null, "author": "suitupyo", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ty7xj/extremely_janky_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ty7xj/extremely_janky_data_pipeline/", "subreddit_subscribers": 130809, "created_utc": 1695853609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, excited to announce the addition of image embeddings for semantic similarity search to VectorFlow, the only high volume open source embedding pipeline. Now you can embed a high volume of images quickly with minimal effort and search them using Vectorflow. This will empower a wide range of applications, from e-commerce product searches to manufacturing defect detection.\n\nWe built this to support multi-modal AI applications, since LLMs don\u2019t exist in a vacuum.\n\nIf you are thinking about adding images to your LLM workflows or computer vision systems, we would love to hear from you to learn more about the problems you are facing and see if VectorFlow can help!\n\nCheck out our Open Source repo - [https://github.com/dgarnitz/vectorflow](https://github.com/dgarnitz/vectorflow)", "author_fullname": "t2_8dgpjm0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-Modal Vector Embeddings at Scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16u02q5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695858046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, excited to announce the addition of image embeddings for semantic similarity search to VectorFlow, the only high volume open source embedding pipeline. Now you can embed a high volume of images quickly with minimal effort and search them using Vectorflow. This will empower a wide range of applications, from e-commerce product searches to manufacturing defect detection.&lt;/p&gt;\n\n&lt;p&gt;We built this to support multi-modal AI applications, since LLMs don\u2019t exist in a vacuum.&lt;/p&gt;\n\n&lt;p&gt;If you are thinking about adding images to your LLM workflows or computer vision systems, we would love to hear from you to learn more about the problems you are facing and see if VectorFlow can help!&lt;/p&gt;\n\n&lt;p&gt;Check out our Open Source repo - &lt;a href=\"https://github.com/dgarnitz/vectorflow\"&gt;https://github.com/dgarnitz/vectorflow&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?auto=webp&amp;s=a3924c1e78e3388c312311040c0d9588a49a552b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12668e0c9df66f8a4e36e1bbfdaae95629eb179c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=79cebafd5274a4a70f307a69d343a88277af0b4f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=90beff0d5c6b0de9c63582a34fd72a5fab7c586c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f64b2da41f1bb0ee7bb7fbfee9000c3c48704020", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d4740a2e00b09c07c401ed0e0538edda00fecb7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6b2dedbc6df54bef7cca1234f1c8452b207e0420", "width": 1080, "height": 540}], "variants": {}, "id": "ihzaBOYBFOFvmz1TpFTKGK5GDTWIxDqGf83GqYavM9o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16u02q5", "is_robot_indexable": true, "report_reasons": null, "author": "Fast_Homework_3323", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16u02q5/multimodal_vector_embeddings_at_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16u02q5/multimodal_vector_embeddings_at_scale/", "subreddit_subscribers": 130809, "created_utc": 1695858046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Platform - Unifying Data Observability, Governance and Catalog.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_16thyd0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tEhRK_XXJVr34mqSCLPiQMWHWJiQAPVbPIZntztB9eo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695813924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "decube.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.decube.io/post/new-data-governance-essential-for-modern-data-stack", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?auto=webp&amp;s=f1f521f3c967ebcb706a1fab5f4a2ef3bfadb2e6", "width": 1921, "height": 1081}, "resolutions": [{"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e4e7881ce8d2a85531d8e76cbed2ee58bcfbb34", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=62e16cd13c8804b8aa9eb8983ecde4b815ca2365", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=63199051e205758060efd19c21e43efee0e58c93", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d15b48eeb712f324f2f3038f603aa155538b6f07", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7152f682c34305a47379ea4d292aea0ee74463d6", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=30e0180d96682a54bdaeb824612ce8d5cd4eecb9", "width": 1080, "height": 607}], "variants": {}, "id": "3Orb3HTH8Vbo1PYnkNdreENhh84Slt5-NKiruAit2L4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16thyd0", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16thyd0/data_platform_unifying_data_observability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.decube.io/post/new-data-governance-essential-for-modern-data-stack", "subreddit_subscribers": 130809, "created_utc": 1695813924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This will be my first job as a DE (I am a data scientist specialized in the retail industry with extensive training in warehouse and ETL).\n\nThe position is security-oriented for a fintech. I don't have much info but I think they seek to comply with certain legal regulations that require deleting information from time to time as well as minimizing the risk due to PII (personally identifiable information).\n\nAny advice would be greatly appreciated.", "author_fullname": "t2_802hx5xi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice from data engineers specialized in security", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16u02bl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695858020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This will be my first job as a DE (I am a data scientist specialized in the retail industry with extensive training in warehouse and ETL).&lt;/p&gt;\n\n&lt;p&gt;The position is security-oriented for a fintech. I don&amp;#39;t have much info but I think they seek to comply with certain legal regulations that require deleting information from time to time as well as minimizing the risk due to PII (personally identifiable information).&lt;/p&gt;\n\n&lt;p&gt;Any advice would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16u02bl", "is_robot_indexable": true, "report_reasons": null, "author": "NationOfSheeps", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16u02bl/advice_from_data_engineers_specialized_in_security/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16u02bl/advice_from_data_engineers_specialized_in_security/", "subreddit_subscribers": 130809, "created_utc": 1695858020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have lots of data generated by source systems that don't have their own backend OLTP database, they just push flat files out. These are easy to consume into delta tables. Delta Lake and the lake house are new territory and even the sales engineers we've been talking to from databricks have seemed a bit non-committal iffy/squirmy when I ask them about referential constraints. I know delta lake doesn't have physically enforced constraints. I assumed they just had high performance multi-step wrappers that effectively do the same thing. I'd like to NOT have to develop that myself, I'd rather just use an OLTP in that situation\n\nMy question for the community is, has anyone used the databricks implementation of ['referential' constraints](https://docs.databricks.com/en/tables/constraints.html) extensively? What were the major challenges/issues? How much custom hacking was it?\n\nOur use cases aren't very complicated. In most cases we're just bulk adding data to tables, and sometimes bulk overwriting. Tables have relations to other tables based on keys. But we don't have any situations like delete or update record from table A and then cascade that to other downstream tables etc. ", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake 'Referential' Constraints", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ttg92", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695842695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have lots of data generated by source systems that don&amp;#39;t have their own backend OLTP database, they just push flat files out. These are easy to consume into delta tables. Delta Lake and the lake house are new territory and even the sales engineers we&amp;#39;ve been talking to from databricks have seemed a bit non-committal iffy/squirmy when I ask them about referential constraints. I know delta lake doesn&amp;#39;t have physically enforced constraints. I assumed they just had high performance multi-step wrappers that effectively do the same thing. I&amp;#39;d like to NOT have to develop that myself, I&amp;#39;d rather just use an OLTP in that situation&lt;/p&gt;\n\n&lt;p&gt;My question for the community is, has anyone used the databricks implementation of &lt;a href=\"https://docs.databricks.com/en/tables/constraints.html\"&gt;&amp;#39;referential&amp;#39; constraints&lt;/a&gt; extensively? What were the major challenges/issues? How much custom hacking was it?&lt;/p&gt;\n\n&lt;p&gt;Our use cases aren&amp;#39;t very complicated. In most cases we&amp;#39;re just bulk adding data to tables, and sometimes bulk overwriting. Tables have relations to other tables based on keys. But we don&amp;#39;t have any situations like delete or update record from table A and then cascade that to other downstream tables etc. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?auto=webp&amp;s=9dd59568b8579947f05ce66ee028655ef14e64d6", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99613d282007d0bcc41947bc7f0846da94adca04", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400ef45c57444e53fb95c1358e9a0b6419c3112e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ed83d9a6c1afb35b8be4de3f85b722298d1c3d6", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=768e111879e31b88e5a61b81d8d367edaa5e5351", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2a359111feb6e4d3ffa529f6614614a63914c4e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6e5d40f18830851f93eb2158f465da573a5df80", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ttg92", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ttg92/delta_lake_referential_constraints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ttg92/delta_lake_referential_constraints/", "subreddit_subscribers": 130809, "created_utc": 1695842695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to execute a few notebooks in loop from a parent notebook using the dbutils.notebook.run and trying to capture the details of the child notebook like run ID and job ID. I tried using the dbutils get_context but that only gives me the context of the parent notebook. I don't know how to associate or get the details for the child notebooks. I've searched the forums but nothing seems to work. Any suggestions would be highly appreciated.", "author_fullname": "t2_5tnbm4y7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I get the execution context of a child notebook from a parent notebook in Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16trv8e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695838560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to execute a few notebooks in loop from a parent notebook using the dbutils.notebook.run and trying to capture the details of the child notebook like run ID and job ID. I tried using the dbutils get_context but that only gives me the context of the parent notebook. I don&amp;#39;t know how to associate or get the details for the child notebooks. I&amp;#39;ve searched the forums but nothing seems to work. Any suggestions would be highly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16trv8e", "is_robot_indexable": true, "report_reasons": null, "author": "TheViper1994", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16trv8e/how_do_i_get_the_execution_context_of_a_child/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16trv8e/how_do_i_get_the_execution_context_of_a_child/", "subreddit_subscribers": 130809, "created_utc": 1695838560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jcps4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fuzzy Matching Images and Text", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_16tr1qa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/55icYUDg9a_vh8FxBoOcQCvZsBgeWUqVh9nj5NQMghs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695836486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "databricks.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.databricks.com/blog/using-images-and-metadata-product-fuzzy-matching-zingg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?auto=webp&amp;s=bf8e5c2c1b93f1afb1d0506255fa89366061473e", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1e11f18d5b10168513fe156f015b7dffe3219fe", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f1cd07a69024d55254c8665d619e6d56a852214", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff32ca9b9a0197e48bf7579bf2a46eec1b2adfec", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d9d63b2a1dc792b84f7b26a0bfd77c3c86d2289", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=23df7a72e4955e7a218a656ca4bcf9ab41c52243", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0dc360080e1e154220739d39acd7e0808cd58ac3", "width": 1080, "height": 565}], "variants": {}, "id": "Y5WW65fCuqgNmZ3YIsL2Kz58c0X6q-jcV6ATBhrRJUM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16tr1qa", "is_robot_indexable": true, "report_reasons": null, "author": "sonalg", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tr1qa/fuzzy_matching_images_and_text/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.databricks.com/blog/using-images-and-metadata-product-fuzzy-matching-zingg", "subreddit_subscribers": 130809, "created_utc": 1695836486.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\nCurrently working in a startup and I am the only one who is working in Data Science or Data Engineering task. Joined in May 2023 as an intern.\nNow,My CTO has asked me to take interview of senior DE, these guys have around 3-7 yrs of work exp, I am very much confuses what to ask!\nCan you guys tell me! What are the fundamentals need to be asked", "author_fullname": "t2_i3smbco5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fresher taking interview of senior DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16tpb3v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695832502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,\nCurrently working in a startup and I am the only one who is working in Data Science or Data Engineering task. Joined in May 2023 as an intern.\nNow,My CTO has asked me to take interview of senior DE, these guys have around 3-7 yrs of work exp, I am very much confuses what to ask!\nCan you guys tell me! What are the fundamentals need to be asked&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16tpb3v", "is_robot_indexable": true, "report_reasons": null, "author": "No_Woodpecker_3267", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tpb3v/fresher_taking_interview_of_senior_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16tpb3v/fresher_taking_interview_of_senior_de/", "subreddit_subscribers": 130809, "created_utc": 1695832502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I was asked to create a data warehouse, and I'm leaning toward the following ELT solution.\n\n    [Read Replica of Source Database]  \n    \n    \u2193\u00a0-- using Amazon DMS  \n    \n    [S3 bucket]   \n    \n    \u2193 \u00a0-- using Snowflake  \n    \n    [Snowflake] \n\nI would first load all the data into Snowflake and do transformations and processing all within Snowflake.\n\nMy questions are:\n\n* What are your thoughts on this ELT setup?\n* If there are hundreds of tables in the source database, do I need load queries for each table?\n* How should I orchestrate the schedules? Do you use Snowflake tasks, or another service like Airflow?", "author_fullname": "t2_98ijstb5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would like to get your feedback on my data warehousing solution using Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16todpd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695830387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was asked to create a data warehouse, and I&amp;#39;m leaning toward the following ELT solution.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[Read Replica of Source Database]  \n\n\u2193\u00a0-- using Amazon DMS  \n\n[S3 bucket]   \n\n\u2193 \u00a0-- using Snowflake  \n\n[Snowflake] \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I would first load all the data into Snowflake and do transformations and processing all within Snowflake.&lt;/p&gt;\n\n&lt;p&gt;My questions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are your thoughts on this ELT setup?&lt;/li&gt;\n&lt;li&gt;If there are hundreds of tables in the source database, do I need load queries for each table?&lt;/li&gt;\n&lt;li&gt;How should I orchestrate the schedules? Do you use Snowflake tasks, or another service like Airflow?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16todpd", "is_robot_indexable": true, "report_reasons": null, "author": "Specialist_Dig2115", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16todpd/would_like_to_get_your_feedback_on_my_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16todpd/would_like_to_get_your_feedback_on_my_data/", "subreddit_subscribers": 130809, "created_utc": 1695830387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Consistency is one of the most important parts of an operational system.\n\nYou don\u2019t want to take an automated action based on incorrect intermediate results. Imagine getting charged an overdraft fee when you never actually overdrafted, just because of an eventually consistent stream processor!\n\nIn this blog post, Frank McSherry himself breaks down how Materialize achieves strong consistency guarantees, so you can act upon results with confidence: https://materialize.com/blog/operational-consistency/\n\n(Oh, and if you happen to be at the Current conference in San Jose, come to the Materialize booth to chat more about why consistency is so critical to operationalizing your data)", "author_fullname": "t2_5p00kusf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The importance of data consistency for business operations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ti3zi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695814414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Consistency is one of the most important parts of an operational system.&lt;/p&gt;\n\n&lt;p&gt;You don\u2019t want to take an automated action based on incorrect intermediate results. Imagine getting charged an overdraft fee when you never actually overdrafted, just because of an eventually consistent stream processor!&lt;/p&gt;\n\n&lt;p&gt;In this blog post, Frank McSherry himself breaks down how Materialize achieves strong consistency guarantees, so you can act upon results with confidence: &lt;a href=\"https://materialize.com/blog/operational-consistency/\"&gt;https://materialize.com/blog/operational-consistency/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;(Oh, and if you happen to be at the Current conference in San Jose, come to the Materialize booth to chat more about why consistency is so critical to operationalizing your data)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16ti3zi", "is_robot_indexable": true, "report_reasons": null, "author": "Chuck-Alt-Delete", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ti3zi/the_importance_of_data_consistency_for_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ti3zi/the_importance_of_data_consistency_for_business/", "subreddit_subscribers": 130809, "created_utc": 1695814414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working as data modeller for 2 yrs. It's my first job. My work is to data model for the data coming into datalake ( for migration and new ). I am not sure if I can make this as my career. Does big data data modelling have future ? Or should I have to search jobs in coding ? ( I like coding and have intermediate knowledge in python, c and begginer in Java)\n\nAny input would be appreciated \ud83d\ude2d as I don't have any senior mentor for me. I don't know if I am doing it right.", "author_fullname": "t2_77h4mj8t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does data modelling have a future ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16ubs3r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695895165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working as data modeller for 2 yrs. It&amp;#39;s my first job. My work is to data model for the data coming into datalake ( for migration and new ). I am not sure if I can make this as my career. Does big data data modelling have future ? Or should I have to search jobs in coding ? ( I like coding and have intermediate knowledge in python, c and begginer in Java)&lt;/p&gt;\n\n&lt;p&gt;Any input would be appreciated \ud83d\ude2d as I don&amp;#39;t have any senior mentor for me. I don&amp;#39;t know if I am doing it right.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16ubs3r", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Trash_69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ubs3r/does_data_modelling_have_a_future/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ubs3r/does_data_modelling_have_a_future/", "subreddit_subscribers": 130809, "created_utc": 1695895165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company ingests events from IoT devices and provides services based on these data.\n\nHistorical events are stored in the cloud as Parquet files, and usually queried with Spark for one-time analyses, ETLs, dashboards etc.\n\nParquet files are currently partitioned by event date, and this is very convenient for most use-cases. But for others, it would be optimal if data were partitioned by device ID. \n\nDo I need to create two separate Parquet datasets, \"events\\_by\\_date\" and  \"events\\_by\\_deviceID\" to handle this? Are there any tools, databases, or file formats that can handle automatically the fact that I need to have multiple \"indexes\" in my big data?\n\nThanks", "author_fullname": "t2_eu3nz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tool for \"indexing\" big data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16ubawx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695893406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company ingests events from IoT devices and provides services based on these data.&lt;/p&gt;\n\n&lt;p&gt;Historical events are stored in the cloud as Parquet files, and usually queried with Spark for one-time analyses, ETLs, dashboards etc.&lt;/p&gt;\n\n&lt;p&gt;Parquet files are currently partitioned by event date, and this is very convenient for most use-cases. But for others, it would be optimal if data were partitioned by device ID. &lt;/p&gt;\n\n&lt;p&gt;Do I need to create two separate Parquet datasets, &amp;quot;events_by_date&amp;quot; and  &amp;quot;events_by_deviceID&amp;quot; to handle this? Are there any tools, databases, or file formats that can handle automatically the fact that I need to have multiple &amp;quot;indexes&amp;quot; in my big data?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ubawx", "is_robot_indexable": true, "report_reasons": null, "author": "valhallavagen", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ubawx/best_tool_for_indexing_big_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ubawx/best_tool_for_indexing_big_data/", "subreddit_subscribers": 130809, "created_utc": 1695893406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Attention all data enthusiasts!  We are excited to announce the upcoming event of the Data Zen Greece community: \"Joining the Greek Data Community.\"   \n \n\n[Register now!](http://datazen.top/qa2iy)\n\n  \n October 12th, 2023,  15:00 GMT+3\n\nhttps://preview.redd.it/chldgp4qiyqb1.png?width=4800&amp;format=png&amp;auto=webp&amp;s=f61019786cabc0831e7724d71751a0ea75e51efe", "author_fullname": "t2_8rp73p0k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Joining the Greek Data Community: Growth and Opportunities", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"chldgp4qiyqb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/chldgp4qiyqb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=67d270d6bccfb9b3754df54ee8fca818be2d5f04"}, {"y": 113, "x": 216, "u": "https://preview.redd.it/chldgp4qiyqb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1545d53214bc1b899e93d2ae449d87627846708"}, {"y": 168, "x": 320, "u": "https://preview.redd.it/chldgp4qiyqb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a83bcc01134341967ead6e628f9ac23a7be66842"}, {"y": 336, "x": 640, "u": "https://preview.redd.it/chldgp4qiyqb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4c6ea5bbeff81d9240eeeb5292066d429f36446b"}, {"y": 504, "x": 960, "u": "https://preview.redd.it/chldgp4qiyqb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=86504322e5d13cddcd7cbe0ec4db5ec6eaabdf63"}, {"y": 567, "x": 1080, "u": "https://preview.redd.it/chldgp4qiyqb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7dee74b7c4b9a0c7c01fe88bb5b38d07836431fb"}], "s": {"y": 2520, "x": 4800, "u": "https://preview.redd.it/chldgp4qiyqb1.png?width=4800&amp;format=png&amp;auto=webp&amp;s=f61019786cabc0831e7724d71751a0ea75e51efe"}, "id": "chldgp4qiyqb1"}}, "name": "t3_16uadei", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uc-HyoQxYLzOjjvGb_IzvbSbisnLFnNe9oTIzfUf4Jw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695889827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Attention all data enthusiasts!  We are excited to announce the upcoming event of the Data Zen Greece community: &amp;quot;Joining the Greek Data Community.&amp;quot;   &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://datazen.top/qa2iy\"&gt;Register now!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;October 12th, 2023,  15:00 GMT+3&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/chldgp4qiyqb1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f61019786cabc0831e7724d71751a0ea75e51efe\"&gt;https://preview.redd.it/chldgp4qiyqb1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f61019786cabc0831e7724d71751a0ea75e51efe&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16uadei", "is_robot_indexable": true, "report_reasons": null, "author": "alexdby", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16uadei/joining_the_greek_data_community_growth_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16uadei/joining_the_greek_data_community_growth_and/", "subreddit_subscribers": 130809, "created_utc": 1695889827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1bnhotlu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Jobs for Beginners to Consider", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16u9hih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1695886306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kanger.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://kanger.dev/article/data-engineering-jobs", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16u9hih", "is_robot_indexable": true, "report_reasons": null, "author": "skj8", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16u9hih/data_engineering_jobs_for_beginners_to_consider/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://kanger.dev/article/data-engineering-jobs", "subreddit_subscribers": 130809, "created_utc": 1695886306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_u9bnox3q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Top 4 Azure Resources for Data Engineers to Master. Do you agree with my list or would you pick some other 4 resources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_16u8a37", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/GMBM-c9FfSA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Top Azure Resources for Data Engineers to Learn in 2023\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Top Azure Resources for Data Engineers to Learn in 2023", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/GMBM-c9FfSA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Top Azure Resources for Data Engineers to Learn in 2023\"&gt;&lt;/iframe&gt;", "author_name": "Aleksi Partanen Tech", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/GMBM-c9FfSA/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@AleksiPartanenTech"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/GMBM-c9FfSA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Top Azure Resources for Data Engineers to Learn in 2023\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/16u8a37", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4JcGEhs_g5syE_HM8RaqbbtxrrpIJeXFLKdVcRxpnfM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695881871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/GMBM-c9FfSA", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/d5tVau_NnDZ7IhoDbHhelEiEFavU3ITY5OfMn1fUEwg.jpg?auto=webp&amp;s=c554e77e892e0443f2789053e0087284ba58b0ea", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/d5tVau_NnDZ7IhoDbHhelEiEFavU3ITY5OfMn1fUEwg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=97e312b8583385d1c14929870c762ab059adbca3", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/d5tVau_NnDZ7IhoDbHhelEiEFavU3ITY5OfMn1fUEwg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9de8e14f244cf0442bd3f7e58472d5e958afc899", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/d5tVau_NnDZ7IhoDbHhelEiEFavU3ITY5OfMn1fUEwg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3bac270d924d82029c26ec59c02276647b835687", "width": 320, "height": 240}], "variants": {}, "id": "43a7JKCAJFBJK7EmStSKeNTYXolETs3giYGYOKhj9Ec"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16u8a37", "is_robot_indexable": true, "report_reasons": null, "author": "aleks1ck", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16u8a37/top_4_azure_resources_for_data_engineers_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/GMBM-c9FfSA", "subreddit_subscribers": 130809, "created_utc": 1695881871.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Top Azure Resources for Data Engineers to Learn in 2023", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/GMBM-c9FfSA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Top Azure Resources for Data Engineers to Learn in 2023\"&gt;&lt;/iframe&gt;", "author_name": "Aleksi Partanen Tech", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/GMBM-c9FfSA/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@AleksiPartanenTech"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a BI team lead hoping to delve a bit more into data engineering. I have many of the skills required for a proper data engineering project, but I have trouble actually completing one. I think if I had a deadline it would help me tremendously. \n\nI'm looking for a single-project bootcamp type thing that is fairly open ended, intensive, short, and ideally involves some sort of cloud infrastructure. And most importantly, one that will put a fire under my @$$ so I actually finish it. \n\nAny suggestions are welcome.", "author_fullname": "t2_am2gg89e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a short but intensive DEng bootcamp", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16u7tlz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695880266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a BI team lead hoping to delve a bit more into data engineering. I have many of the skills required for a proper data engineering project, but I have trouble actually completing one. I think if I had a deadline it would help me tremendously. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a single-project bootcamp type thing that is fairly open ended, intensive, short, and ideally involves some sort of cloud infrastructure. And most importantly, one that will put a fire under my @$$ so I actually finish it. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16u7tlz", "is_robot_indexable": true, "report_reasons": null, "author": "Whiskeystring", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16u7tlz/looking_for_a_short_but_intensive_deng_bootcamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16u7tlz/looking_for_a_short_but_intensive_deng_bootcamp/", "subreddit_subscribers": 130809, "created_utc": 1695880266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a BA. Have always been one. But I am getting another BA opportunity from a Software Company which a huge DE team. The only reason for me to switch to this cpany would be the hope that someday they might let me move to their DE team and thats what I ultimately want - to break into the DE market. \n\nAm I being delusional? I am working on personal DE projects.", "author_fullname": "t2_5zamw97i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any success stories for switching to DE within same organization?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16tzcjh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695856252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a BA. Have always been one. But I am getting another BA opportunity from a Software Company which a huge DE team. The only reason for me to switch to this cpany would be the hope that someday they might let me move to their DE team and thats what I ultimately want - to break into the DE market. &lt;/p&gt;\n\n&lt;p&gt;Am I being delusional? I am working on personal DE projects.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16tzcjh", "is_robot_indexable": true, "report_reasons": null, "author": "abhishek16x", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tzcjh/any_success_stories_for_switching_to_de_within/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16tzcjh/any_success_stories_for_switching_to_de_within/", "subreddit_subscribers": 130809, "created_utc": 1695856252.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}