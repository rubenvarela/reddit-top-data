{"kind": "Listing", "data": {"after": "t3_16thyd0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "  \n\nMy Stats:  \nAzure Engineering Experience: 2 years on and off  \nStudied: 30 days \u2013 Ave 3 hours/day  \nScored: 892/1000\n\nExam Covers: Data Factory, Synapse, Stream Analytics, Event Hubs, Databricks, Data Lake Storage, SQL/Scala\n\nResources I paid for:   \nAlan Rodrigues \u2013 Udemy - DP-203 Course - 21 hours long - Updated for 2023. Very good course, but not enough to pass the exam. \n\nMeasure Up Test Prep Questions\n\nYou will get questions on:  \nParquet Files, Data Lake Gen2 storage, Slowly Changing Dimension Tables (SCDs), Fact &amp; Dimension tables, Synapse Distributions (Hash, Replicated, Round-Robin), Synapse Partitions &amp; Indexes, Security Options. I\u2019d say half the questions involved Synapse. \n\nFinal Tips:\n\n\u2022 Make sure you\u2019re getting near 100% on test preps before you attempt the exam.\n\n\u2022 Have Azure open when you do the test prep questions, to practice for real.\n\n\u2022 If you feel you\u2019re not ready, re-schedule the exam, which is free.\n\n\u2022 Book the exam in advance - Stopped me from procrastinating or chickening out.\n\n\u2022 Organize your time, I needed 3 hours/day for 4 weeks to prepare. Depends on your skill level.\n\n\u2022 Read the question &amp; answer options first. Then look at the case study. Helps you find the answer quicker. \n\nHere are some free resources.\n\nFree Test Prep Questions:  \n [https://www.analystlaunch.com/c/dp203-test-prep-download-landing](https://www.analystlaunch.com/c/dp203-test-prep-download-landing)\n\nVideo on passing the exam:  \n [https://www.youtube.com/watch?v=fRR5FLrv398](https://www.youtube.com/watch?v=fRR5FLrv398)\n\nSitting your first Microsoft exam at home:  \n [https://www.youtube.com/watch?v=hV\\_DpwpzNZI&amp;t=1s](https://www.youtube.com/watch?v=hV_DpwpzNZI&amp;t=1s)\n\nGood luck.", "author_fullname": "t2_144pid", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I recently passed the DP-203, here are my notes to prepare", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16thbm7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 65, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 65, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695811920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My Stats:&lt;br/&gt;\nAzure Engineering Experience: 2 years on and off&lt;br/&gt;\nStudied: 30 days \u2013 Ave 3 hours/day&lt;br/&gt;\nScored: 892/1000&lt;/p&gt;\n\n&lt;p&gt;Exam Covers: Data Factory, Synapse, Stream Analytics, Event Hubs, Databricks, Data Lake Storage, SQL/Scala&lt;/p&gt;\n\n&lt;p&gt;Resources I paid for:&lt;br/&gt;\nAlan Rodrigues \u2013 Udemy - DP-203 Course - 21 hours long - Updated for 2023. Very good course, but not enough to pass the exam. &lt;/p&gt;\n\n&lt;p&gt;Measure Up Test Prep Questions&lt;/p&gt;\n\n&lt;p&gt;You will get questions on:&lt;br/&gt;\nParquet Files, Data Lake Gen2 storage, Slowly Changing Dimension Tables (SCDs), Fact &amp;amp; Dimension tables, Synapse Distributions (Hash, Replicated, Round-Robin), Synapse Partitions &amp;amp; Indexes, Security Options. I\u2019d say half the questions involved Synapse. &lt;/p&gt;\n\n&lt;p&gt;Final Tips:&lt;/p&gt;\n\n&lt;p&gt;\u2022 Make sure you\u2019re getting near 100% on test preps before you attempt the exam.&lt;/p&gt;\n\n&lt;p&gt;\u2022 Have Azure open when you do the test prep questions, to practice for real.&lt;/p&gt;\n\n&lt;p&gt;\u2022 If you feel you\u2019re not ready, re-schedule the exam, which is free.&lt;/p&gt;\n\n&lt;p&gt;\u2022 Book the exam in advance - Stopped me from procrastinating or chickening out.&lt;/p&gt;\n\n&lt;p&gt;\u2022 Organize your time, I needed 3 hours/day for 4 weeks to prepare. Depends on your skill level.&lt;/p&gt;\n\n&lt;p&gt;\u2022 Read the question &amp;amp; answer options first. Then look at the case study. Helps you find the answer quicker. &lt;/p&gt;\n\n&lt;p&gt;Here are some free resources.&lt;/p&gt;\n\n&lt;p&gt;Free Test Prep Questions:&lt;br/&gt;\n &lt;a href=\"https://www.analystlaunch.com/c/dp203-test-prep-download-landing\"&gt;https://www.analystlaunch.com/c/dp203-test-prep-download-landing&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Video on passing the exam:&lt;br/&gt;\n &lt;a href=\"https://www.youtube.com/watch?v=fRR5FLrv398\"&gt;https://www.youtube.com/watch?v=fRR5FLrv398&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sitting your first Microsoft exam at home:&lt;br/&gt;\n &lt;a href=\"https://www.youtube.com/watch?v=hV_DpwpzNZI&amp;amp;t=1s\"&gt;https://www.youtube.com/watch?v=hV_DpwpzNZI&amp;amp;t=1s&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Good luck.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/h6wnfWcQ3YtgQNPie3idtMjoJC5FM1FV4T14guiaWMY.jpg?auto=webp&amp;s=3976944633dabc5557d2590773413fb971dcb141", "width": 800, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/h6wnfWcQ3YtgQNPie3idtMjoJC5FM1FV4T14guiaWMY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=be7c917b3417d86e923ffb71081bf5eab499349f", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/h6wnfWcQ3YtgQNPie3idtMjoJC5FM1FV4T14guiaWMY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1971f20837cef88239efff3d89277f985bcae6dc", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/h6wnfWcQ3YtgQNPie3idtMjoJC5FM1FV4T14guiaWMY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=35b3bf2c272d05e0ea0edd59dd71cc7eefc93489", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/h6wnfWcQ3YtgQNPie3idtMjoJC5FM1FV4T14guiaWMY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d95f4b972d047c571d8ee2e410901f7a1736f9f", "width": 640, "height": 640}], "variants": {}, "id": "UFwUkDMkNVaWB_fVqVW8W81v34KS8PnII6eyck02VAY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16thbm7", "is_robot_indexable": true, "report_reasons": null, "author": "dojogreen", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16thbm7/i_recently_passed_the_dp203_here_are_my_notes_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16thbm7/i_recently_passed_the_dp203_here_are_my_notes_to/", "subreddit_subscribers": 130782, "created_utc": 1695811920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most of us would have observed recently that the companies are moving to cloud for data engineering. Be it Azure,  gcp AWS or databricks cloud, they provide managed cluster platforms.Since they are making the data engineers life easier by integrating job optimisation tools,  managed airflow, orchestrationservices, CI CD and what not.  But when it comes to on-premise, currently most of these are being manage by data engineers and some intermediate level coding is required.\nI would like to understand from the experienced data engineers in this group is --  in a few years down the lene, say 5 years, the dependency on data engineers will decrease for the type tasks mentioned above ?\nOf course, data engineers would be doing ETL by writing python sql scripts and that is not going to go away but if we keep this part side, what is the future of the rest of the tasks?\n I think there are some big organisations trying to move to the cloud and in the near future, they would need tremendous support from the data engineers and this theerw would be a high demand in the short run.\n\nThanks.", "author_fullname": "t2_b8ynqpo5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scope of Data Engineering in future", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16tnvsu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695829199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most of us would have observed recently that the companies are moving to cloud for data engineering. Be it Azure,  gcp AWS or databricks cloud, they provide managed cluster platforms.Since they are making the data engineers life easier by integrating job optimisation tools,  managed airflow, orchestrationservices, CI CD and what not.  But when it comes to on-premise, currently most of these are being manage by data engineers and some intermediate level coding is required.\nI would like to understand from the experienced data engineers in this group is --  in a few years down the lene, say 5 years, the dependency on data engineers will decrease for the type tasks mentioned above ?\nOf course, data engineers would be doing ETL by writing python sql scripts and that is not going to go away but if we keep this part side, what is the future of the rest of the tasks?\n I think there are some big organisations trying to move to the cloud and in the near future, they would need tremendous support from the data engineers and this theerw would be a high demand in the short run.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16tnvsu", "is_robot_indexable": true, "report_reasons": null, "author": "ajeetyadav_", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tnvsu/scope_of_data_engineering_in_future/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16tnvsu/scope_of_data_engineering_in_future/", "subreddit_subscribers": 130782, "created_utc": 1695829199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know you're thinking it's an over-reaction and it probably is... but I decided to just apply to all the roles available in California (where I would be happy to continue living) or remote roles.  I sent like 100 applications today which I know isn't the best method but I don't have many other options.\n\nTwo of the applications I sent today had the recruiter download my resume and every single one after just has... no activity at all.\n\nI think I'm just triggered because I've never felt unwanted in this field before, having had success a lot before.  I'm also financially secure for some time.\n\nBut man... is the economy bad?  How are there so many open roles (there seem to be hundreds of new DE posts every day) but there is no activity?  Was taking a year gap to get the surgery I needed a mistake?\n\nFor reference I have 6 YOE including a MAANG company.", "author_fullname": "t2_imajwwpcb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Started my job search after medical leave today, I am already depressed.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16u3k1g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695867061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know you&amp;#39;re thinking it&amp;#39;s an over-reaction and it probably is... but I decided to just apply to all the roles available in California (where I would be happy to continue living) or remote roles.  I sent like 100 applications today which I know isn&amp;#39;t the best method but I don&amp;#39;t have many other options.&lt;/p&gt;\n\n&lt;p&gt;Two of the applications I sent today had the recruiter download my resume and every single one after just has... no activity at all.&lt;/p&gt;\n\n&lt;p&gt;I think I&amp;#39;m just triggered because I&amp;#39;ve never felt unwanted in this field before, having had success a lot before.  I&amp;#39;m also financially secure for some time.&lt;/p&gt;\n\n&lt;p&gt;But man... is the economy bad?  How are there so many open roles (there seem to be hundreds of new DE posts every day) but there is no activity?  Was taking a year gap to get the surgery I needed a mistake?&lt;/p&gt;\n\n&lt;p&gt;For reference I have 6 YOE including a MAANG company.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16u3k1g", "is_robot_indexable": true, "report_reasons": null, "author": "al-hamal", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16u3k1g/started_my_job_search_after_medical_leave_today_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16u3k1g/started_my_job_search_after_medical_leave_today_i/", "subreddit_subscribers": 130782, "created_utc": 1695867061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am always wondering how can a data engineer know, and then convince product team or business that their architecture or design is correct? There is not exactly an exact science to data engineering. There are companies which are happy using out of the box tools and make it work, and there are engineers putting up with complex legacy codebases using microservices et al. Besides reading up and doing courses trying to get as close as possible to best practices, your architecture suggestions are subject to either lessons learnt from experience (takes 1-2 years for a big data project to take real shape) or googling and doing the best possible back of the envelope tradeoffs. While other non technical team members expect you to give them a silver bullet or quickly lose trust in you. How do you establish this confidence in a solution without seeming like a tinkerer?", "author_fullname": "t2_rov69023", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you \"know\" your architecture is correct?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16u1sxr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695862478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am always wondering how can a data engineer know, and then convince product team or business that their architecture or design is correct? There is not exactly an exact science to data engineering. There are companies which are happy using out of the box tools and make it work, and there are engineers putting up with complex legacy codebases using microservices et al. Besides reading up and doing courses trying to get as close as possible to best practices, your architecture suggestions are subject to either lessons learnt from experience (takes 1-2 years for a big data project to take real shape) or googling and doing the best possible back of the envelope tradeoffs. While other non technical team members expect you to give them a silver bullet or quickly lose trust in you. How do you establish this confidence in a solution without seeming like a tinkerer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16u1sxr", "is_robot_indexable": true, "report_reasons": null, "author": "CutSubstantial7320", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16u1sxr/how_do_you_know_your_architecture_is_correct/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16u1sxr/how_do_you_know_your_architecture_is_correct/", "subreddit_subscribers": 130782, "created_utc": 1695862478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Good afternoon readers,\n\nWant to keep this to the point as this is a longer post so I am going to jump right in to my situation. I recently signed at a large consulting firm as a Jr. DE (perfect for me, very exiting) but my current scope of responsibility seems way over my head, maybe I had a mistach in expectations I am unsure.  \n\n\nTo get directly to the ask, our team has a Tableau enviroment that various users have eyes on. The data sources are populated from various parts of the Navy (my client). Currently, roughly 40 different excel files are used to refresh data sources and it is a completely manual process. In other words, user goes to website, user downloads more recent data, user transforms said data manually, user refreshes data source on our Tableau server. This process is repeated more or less from varoius stakeholders and many web portals.\n\nSo far, to me everything makes sense, the ask is to build a database and streamline the data management process (build data pipeline). I would say I am strong but no expert in SQL but so far within my means. Two major problems exists.\n\n**Problem 1: Moving Data from SQL to Tableau Server**\n\nSince this work is done in a secure space, Tableau does not support the functionality of connecting directly to our database (a pSQL enviroment). I have gotten Tableau techs on the phone and asked them directly, a direct connection between these two services simply cannot be done in a secure enviroment. I have also had discussions with our cloud dev team and managers and asked them how to streamline this data, they are not helpful.  More recently, I have been poking around with the Tableau Rest API. However the only data sources it can ingest are tableau files (.tds, .tde, .pbix). So I am unsure if it is possible to export data from SQL and manage that conversion.\n\nSo I am essentially at a stand still, I do not see a way to move data from my pSQL enviroment to Tableau server. This essentially makes the ask impossible, but maybe I am just young and naive. My intuition tells me surely there is a way to integrate these two programs, but I see none. Thoughts here are greatly appreicated.\n\n**Problem 2: Data Ingestion**\n\nThis is a much smaller problem for me and advise on problem 1 is more impactful, but I figure since I am posting to put this out there. I am unsure how to ingest data to a SQL enviroment. Our cloud leads tell me a transition to Azure pSQL is happening soon, so I am guessing there is some type of UI tool that I can instruct users to 'drop' excel files that I can write code against to transform/write to our SQL enviroment. But this is a guess and ideas on data ingestion would be great.\n\nI have previously tried tried to ask the owners of the raw data sources that our users are pulling data from to see if they have an API or if I can integrate with their backend, I am just left on read. \n\n**Carreer**\n\nI just do not know what to tell my manager, he wants updates and everything I explore to solve these problems leads nowhere. It feels like I am on the wrong project and that not much more optimization can be done. But I want to leave it to the experts that I am sure exists on this sub.\n\nAm I perhaps in over my head as far as problem solving/skill level? I was hoping my first gig I would have an architecture already in place that I could develop/expand on, but perhaps that expecation is wrong. \n\nShould I tell my managers I do not love this contract? This feels bad as a Jr. I want to prove myself, not fun away from the first gig they trusted me with.\n\n&amp;#x200B;\n\nMuch love to any and all readers and I appreicate your thoughts in advance.\n\nSigned,  \nA Jr. DE in need\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_2saw9ds3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jr. Data Engineer who is in over his head, looking for advice from more senior members.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16tv2io", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695846453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good afternoon readers,&lt;/p&gt;\n\n&lt;p&gt;Want to keep this to the point as this is a longer post so I am going to jump right in to my situation. I recently signed at a large consulting firm as a Jr. DE (perfect for me, very exiting) but my current scope of responsibility seems way over my head, maybe I had a mistach in expectations I am unsure.  &lt;/p&gt;\n\n&lt;p&gt;To get directly to the ask, our team has a Tableau enviroment that various users have eyes on. The data sources are populated from various parts of the Navy (my client). Currently, roughly 40 different excel files are used to refresh data sources and it is a completely manual process. In other words, user goes to website, user downloads more recent data, user transforms said data manually, user refreshes data source on our Tableau server. This process is repeated more or less from varoius stakeholders and many web portals.&lt;/p&gt;\n\n&lt;p&gt;So far, to me everything makes sense, the ask is to build a database and streamline the data management process (build data pipeline). I would say I am strong but no expert in SQL but so far within my means. Two major problems exists.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem 1: Moving Data from SQL to Tableau Server&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Since this work is done in a secure space, Tableau does not support the functionality of connecting directly to our database (a pSQL enviroment). I have gotten Tableau techs on the phone and asked them directly, a direct connection between these two services simply cannot be done in a secure enviroment. I have also had discussions with our cloud dev team and managers and asked them how to streamline this data, they are not helpful.  More recently, I have been poking around with the Tableau Rest API. However the only data sources it can ingest are tableau files (.tds, .tde, .pbix). So I am unsure if it is possible to export data from SQL and manage that conversion.&lt;/p&gt;\n\n&lt;p&gt;So I am essentially at a stand still, I do not see a way to move data from my pSQL enviroment to Tableau server. This essentially makes the ask impossible, but maybe I am just young and naive. My intuition tells me surely there is a way to integrate these two programs, but I see none. Thoughts here are greatly appreicated.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem 2: Data Ingestion&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a much smaller problem for me and advise on problem 1 is more impactful, but I figure since I am posting to put this out there. I am unsure how to ingest data to a SQL enviroment. Our cloud leads tell me a transition to Azure pSQL is happening soon, so I am guessing there is some type of UI tool that I can instruct users to &amp;#39;drop&amp;#39; excel files that I can write code against to transform/write to our SQL enviroment. But this is a guess and ideas on data ingestion would be great.&lt;/p&gt;\n\n&lt;p&gt;I have previously tried tried to ask the owners of the raw data sources that our users are pulling data from to see if they have an API or if I can integrate with their backend, I am just left on read. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Carreer&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I just do not know what to tell my manager, he wants updates and everything I explore to solve these problems leads nowhere. It feels like I am on the wrong project and that not much more optimization can be done. But I want to leave it to the experts that I am sure exists on this sub.&lt;/p&gt;\n\n&lt;p&gt;Am I perhaps in over my head as far as problem solving/skill level? I was hoping my first gig I would have an architecture already in place that I could develop/expand on, but perhaps that expecation is wrong. &lt;/p&gt;\n\n&lt;p&gt;Should I tell my managers I do not love this contract? This feels bad as a Jr. I want to prove myself, not fun away from the first gig they trusted me with.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Much love to any and all readers and I appreicate your thoughts in advance.&lt;/p&gt;\n\n&lt;p&gt;Signed,&lt;br/&gt;\nA Jr. DE in need&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16tv2io", "is_robot_indexable": true, "report_reasons": null, "author": "likely-", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tv2io/jr_data_engineer_who_is_in_over_his_head_looking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16tv2io/jr_data_engineer_who_is_in_over_his_head_looking/", "subreddit_subscribers": 130782, "created_utc": 1695846453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TL;DR: I love writing complex SQL queries to gather data for someone else to analyze. What job functions should I look at in the Data world?\n\nI have been a process engineer at a defense contractor for close to a decade, and I have found that my heart is not truly in it.\n\nAt my company, I have become an SQL expert, and many engineers and managers will reach out to me to acquire processed data sets that they can then use to track yield, throughput, or other trends. Oftentimes this requires creating a dashboard, but then it's up to them to analyze the data in the dashboard to determine what can be learned from it or what actions should be taken based on it. \n\nOur raw data resides in different servers and tables that I can access through SSMS, but it can require some very complicated queries to process the raw data into whatever form my customer is asking for. I absolutely love the days where I sit and write SQL all day to join it all together and make sure it's clean and ready for the customer.\n\nI recently learned about the Data job functions and wondered if this may be a career path that I should go down. However, I'm having trouble determining where exactly I would best fit based on my interests. I know I *don't* want to perform statistical analysis or modeling. I don't care what the data actually says, I just want to gather it from the various tables and clean it and process it into something that's user-friendly. I actually don't even care about the dashboards that much; usually it's just a means to an end to hand the data off to the customer. I would be just fine if my end product was the query text itself or a View on the database or something like that.\n\nWhen I look at Data Engineering projects via YouTube, a lot of time is spent gathering data from various sources like APIs, so I'm not sure how much time a Data Engineer spends writing complex SQL queries.\n\nAny advice on where you think I fit best in the Data fields would be much appreciated.", "author_fullname": "t2_8wmp3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do I fit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16tm2uk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695833811.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695824855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: I love writing complex SQL queries to gather data for someone else to analyze. What job functions should I look at in the Data world?&lt;/p&gt;\n\n&lt;p&gt;I have been a process engineer at a defense contractor for close to a decade, and I have found that my heart is not truly in it.&lt;/p&gt;\n\n&lt;p&gt;At my company, I have become an SQL expert, and many engineers and managers will reach out to me to acquire processed data sets that they can then use to track yield, throughput, or other trends. Oftentimes this requires creating a dashboard, but then it&amp;#39;s up to them to analyze the data in the dashboard to determine what can be learned from it or what actions should be taken based on it. &lt;/p&gt;\n\n&lt;p&gt;Our raw data resides in different servers and tables that I can access through SSMS, but it can require some very complicated queries to process the raw data into whatever form my customer is asking for. I absolutely love the days where I sit and write SQL all day to join it all together and make sure it&amp;#39;s clean and ready for the customer.&lt;/p&gt;\n\n&lt;p&gt;I recently learned about the Data job functions and wondered if this may be a career path that I should go down. However, I&amp;#39;m having trouble determining where exactly I would best fit based on my interests. I know I &lt;em&gt;don&amp;#39;t&lt;/em&gt; want to perform statistical analysis or modeling. I don&amp;#39;t care what the data actually says, I just want to gather it from the various tables and clean it and process it into something that&amp;#39;s user-friendly. I actually don&amp;#39;t even care about the dashboards that much; usually it&amp;#39;s just a means to an end to hand the data off to the customer. I would be just fine if my end product was the query text itself or a View on the database or something like that.&lt;/p&gt;\n\n&lt;p&gt;When I look at Data Engineering projects via YouTube, a lot of time is spent gathering data from various sources like APIs, so I&amp;#39;m not sure how much time a Data Engineer spends writing complex SQL queries.&lt;/p&gt;\n\n&lt;p&gt;Any advice on where you think I fit best in the Data fields would be much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16tm2uk", "is_robot_indexable": true, "report_reasons": null, "author": "bluetrench", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tm2uk/where_do_i_fit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16tm2uk/where_do_i_fit/", "subreddit_subscribers": 130782, "created_utc": 1695824855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Observability for Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "name": "t3_16thtp5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jsJlUb0PErnZBq5Ynmga2-8jvFwCXbk8BY7ZLhQ5N2A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695813514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@jatin_solanki/data-observability-for-data-engineers-6035898db6b1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?auto=webp&amp;s=1d02e183083e4073e68845ead1706a813ba69b77", "width": 1200, "height": 611}, "resolutions": [{"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cef1ef2e72af279e50b7f878a1997b835e0bace1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=94c137e16ee29f4a25b737d13f2125b761ad14ff", "width": 216, "height": 109}, {"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=47af5272d2421f6d3f0ee3cad05e67e44317e1ae", "width": 320, "height": 162}, {"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e06526383b4262a8e8c924cebcc4f75c0651e935", "width": 640, "height": 325}, {"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8f2f9852a7ea11872456519c5ed7c23e3c086dca", "width": 960, "height": 488}, {"url": "https://external-preview.redd.it/xeAEnusWjKIlmgooyKDlTbPH8aBkKDKVVqGBavDOlU4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8260deafd5457c053ab4f84053f213391413e13a", "width": 1080, "height": 549}], "variants": {}, "id": "K4cOA1zbl1M9mnT-Vo35IS6P0fPDMY51O2l9-SYi9gk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16thtp5", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16thtp5/data_observability_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@jatin_solanki/data-observability-for-data-engineers-6035898db6b1", "subreddit_subscribers": 130782, "created_utc": 1695813514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6mi7burq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "All Major Software Architecture Patterns Explained in 7 Minutes | Meaning, Design, Models &amp; Examples", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_16teo04", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ZTVAs9cNo30?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"All Major Software Architecture Patterns Explained in 7 Minutes | Meaning, Design, Models &amp;amp; Examples\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "All Major Software Architecture Patterns Explained in 7 Minutes | Meaning, Design, Models &amp; Examples", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ZTVAs9cNo30?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"All Major Software Architecture Patterns Explained in 7 Minutes | Meaning, Design, Models &amp;amp; Examples\"&gt;&lt;/iframe&gt;", "author_name": "Learn with Whiteboard", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/ZTVAs9cNo30/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@learnwithwhiteboard"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ZTVAs9cNo30?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"All Major Software Architecture Patterns Explained in 7 Minutes | Meaning, Design, Models &amp;amp; Examples\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/16teo04", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/oLegyj3Uv11bB4NPR_jSZirJ2oL6q-MsmZl8RPxzzpw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695802074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=ZTVAs9cNo30", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hxEGQDnO5aAH0po80iHagbCdmD-liBQ9AfAw0ANhg9k.jpg?auto=webp&amp;s=976f8411c69155f0fef9636025a226057d8b70da", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/hxEGQDnO5aAH0po80iHagbCdmD-liBQ9AfAw0ANhg9k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5bb9f772739cf636e313c3661b25081daff7f65c", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/hxEGQDnO5aAH0po80iHagbCdmD-liBQ9AfAw0ANhg9k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da7e928964e51a3a32db425b8fec237a0a5e5be9", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/hxEGQDnO5aAH0po80iHagbCdmD-liBQ9AfAw0ANhg9k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7d5eefca42eab4dea557f659db0e32df212759c", "width": 320, "height": 240}], "variants": {}, "id": "vnUdDEB9r0yGB72hK8usF-7DoKT7eEnRKOt6ILbDKC0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16teo04", "is_robot_indexable": true, "report_reasons": null, "author": "smart_brand_builder", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16teo04/all_major_software_architecture_patterns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=ZTVAs9cNo30", "subreddit_subscribers": 130782, "created_utc": 1695802074.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "All Major Software Architecture Patterns Explained in 7 Minutes | Meaning, Design, Models &amp; Examples", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ZTVAs9cNo30?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"All Major Software Architecture Patterns Explained in 7 Minutes | Meaning, Design, Models &amp;amp; Examples\"&gt;&lt;/iframe&gt;", "author_name": "Learn with Whiteboard", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/ZTVAs9cNo30/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@learnwithwhiteboard"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At work, we work with a very  bureaucratic organization that provides data to us only in the form of scheduled emails with spreadsheets in a zip file. Attempting to convince them to connect to an API is futile. \n\nAnyways, I used python to collect all the outlook attachments, load the zipped files as temp files, read the data into data frames and push to our server database. it\u2019s janky as hell and relies on the most insecure method of data transfer(email) , but it works dammit!\n\nAnyone else put up with some seriously janky data engineering? Be honest. I cannot be the only one.", "author_fullname": "t2_6hsp2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extremely Janky Data Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ty7xj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695853609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At work, we work with a very  bureaucratic organization that provides data to us only in the form of scheduled emails with spreadsheets in a zip file. Attempting to convince them to connect to an API is futile. &lt;/p&gt;\n\n&lt;p&gt;Anyways, I used python to collect all the outlook attachments, load the zipped files as temp files, read the data into data frames and push to our server database. it\u2019s janky as hell and relies on the most insecure method of data transfer(email) , but it works dammit!&lt;/p&gt;\n\n&lt;p&gt;Anyone else put up with some seriously janky data engineering? Be honest. I cannot be the only one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ty7xj", "is_robot_indexable": true, "report_reasons": null, "author": "suitupyo", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ty7xj/extremely_janky_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ty7xj/extremely_janky_data_pipeline/", "subreddit_subscribers": 130782, "created_utc": 1695853609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have lots of data generated by source systems that don't have their own backend OLTP database, they just push flat files out. These are easy to consume into delta tables. Delta Lake and the lake house are new territory and even the sales engineers we've been talking to from databricks have seemed a bit non-committal iffy/squirmy when I ask them about referential constraints. I know delta lake doesn't have physically enforced constraints. I assumed they just had high performance multi-step wrappers that effectively do the same thing. I'd like to NOT have to develop that myself, I'd rather just use an OLTP in that situation\n\nMy question for the community is, has anyone used the databricks implementation of ['referential' constraints](https://docs.databricks.com/en/tables/constraints.html) extensively? What were the major challenges/issues? How much custom hacking was it?\n\nOur use cases aren't very complicated. In most cases we're just bulk adding data to tables, and sometimes bulk overwriting. Tables have relations to other tables based on keys. But we don't have any situations like delete or update record from table A and then cascade that to other downstream tables etc. ", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake 'Referential' Constraints", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ttgcz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695842702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have lots of data generated by source systems that don&amp;#39;t have their own backend OLTP database, they just push flat files out. These are easy to consume into delta tables. Delta Lake and the lake house are new territory and even the sales engineers we&amp;#39;ve been talking to from databricks have seemed a bit non-committal iffy/squirmy when I ask them about referential constraints. I know delta lake doesn&amp;#39;t have physically enforced constraints. I assumed they just had high performance multi-step wrappers that effectively do the same thing. I&amp;#39;d like to NOT have to develop that myself, I&amp;#39;d rather just use an OLTP in that situation&lt;/p&gt;\n\n&lt;p&gt;My question for the community is, has anyone used the databricks implementation of &lt;a href=\"https://docs.databricks.com/en/tables/constraints.html\"&gt;&amp;#39;referential&amp;#39; constraints&lt;/a&gt; extensively? What were the major challenges/issues? How much custom hacking was it?&lt;/p&gt;\n\n&lt;p&gt;Our use cases aren&amp;#39;t very complicated. In most cases we&amp;#39;re just bulk adding data to tables, and sometimes bulk overwriting. Tables have relations to other tables based on keys. But we don&amp;#39;t have any situations like delete or update record from table A and then cascade that to other downstream tables etc. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?auto=webp&amp;s=9dd59568b8579947f05ce66ee028655ef14e64d6", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99613d282007d0bcc41947bc7f0846da94adca04", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400ef45c57444e53fb95c1358e9a0b6419c3112e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ed83d9a6c1afb35b8be4de3f85b722298d1c3d6", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=768e111879e31b88e5a61b81d8d367edaa5e5351", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2a359111feb6e4d3ffa529f6614614a63914c4e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6e5d40f18830851f93eb2158f465da573a5df80", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ttgcz", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ttgcz/delta_lake_referential_constraints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ttgcz/delta_lake_referential_constraints/", "subreddit_subscribers": 130782, "created_utc": 1695842702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If I am trying to standardize KPIs for the company, what layer should they be in? I want to build a core dimensional model that can support many use cases. Should I build a generic dimensional model then create a reporting table/view that will store all the KPIs based on the fact tables? Like marketing KPI view and finance KPI view.", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fact Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16to8b0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695830029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I am trying to standardize KPIs for the company, what layer should they be in? I want to build a core dimensional model that can support many use cases. Should I build a generic dimensional model then create a reporting table/view that will store all the KPIs based on the fact tables? Like marketing KPI view and finance KPI view.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16to8b0", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16to8b0/fact_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16to8b0/fact_tables/", "subreddit_subscribers": 130782, "created_utc": 1695830029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, excited to announce the addition of image embeddings for semantic similarity search to VectorFlow, the only high volume open source embedding pipeline. Now you can embed a high volume of images quickly with minimal effort and search them using Vectorflow. This will empower a wide range of applications, from e-commerce product searches to manufacturing defect detection.\n\nWe built this to support multi-modal AI applications, since LLMs don\u2019t exist in a vacuum.\n\nIf you are thinking about adding images to your LLM workflows or computer vision systems, we would love to hear from you to learn more about the problems you are facing and see if VectorFlow can help!\n\nCheck out our Open Source repo - [https://github.com/dgarnitz/vectorflow](https://github.com/dgarnitz/vectorflow)", "author_fullname": "t2_8dgpjm0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-Modal Vector Embeddings at Scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16u02q5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695858046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, excited to announce the addition of image embeddings for semantic similarity search to VectorFlow, the only high volume open source embedding pipeline. Now you can embed a high volume of images quickly with minimal effort and search them using Vectorflow. This will empower a wide range of applications, from e-commerce product searches to manufacturing defect detection.&lt;/p&gt;\n\n&lt;p&gt;We built this to support multi-modal AI applications, since LLMs don\u2019t exist in a vacuum.&lt;/p&gt;\n\n&lt;p&gt;If you are thinking about adding images to your LLM workflows or computer vision systems, we would love to hear from you to learn more about the problems you are facing and see if VectorFlow can help!&lt;/p&gt;\n\n&lt;p&gt;Check out our Open Source repo - &lt;a href=\"https://github.com/dgarnitz/vectorflow\"&gt;https://github.com/dgarnitz/vectorflow&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?auto=webp&amp;s=a3924c1e78e3388c312311040c0d9588a49a552b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12668e0c9df66f8a4e36e1bbfdaae95629eb179c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=79cebafd5274a4a70f307a69d343a88277af0b4f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=90beff0d5c6b0de9c63582a34fd72a5fab7c586c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f64b2da41f1bb0ee7bb7fbfee9000c3c48704020", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d4740a2e00b09c07c401ed0e0538edda00fecb7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/gz9goWfmEhaWr8H0ySdzVZ1JGlfab0iYbz0wgLupVr4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6b2dedbc6df54bef7cca1234f1c8452b207e0420", "width": 1080, "height": 540}], "variants": {}, "id": "ihzaBOYBFOFvmz1TpFTKGK5GDTWIxDqGf83GqYavM9o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16u02q5", "is_robot_indexable": true, "report_reasons": null, "author": "Fast_Homework_3323", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16u02q5/multimodal_vector_embeddings_at_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16u02q5/multimodal_vector_embeddings_at_scale/", "subreddit_subscribers": 130782, "created_utc": 1695858046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This will be my first job as a DE (I am a data scientist specialized in the retail industry with extensive training in warehouse and ETL).\n\nThe position is security-oriented for a fintech. I don't have much info but I think they seek to comply with certain legal regulations that require deleting information from time to time as well as minimizing the risk due to PII (personally identifiable information).\n\nAny advice would be greatly appreciated.", "author_fullname": "t2_802hx5xi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice from data engineers specialized in security", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16u02bl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695858020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This will be my first job as a DE (I am a data scientist specialized in the retail industry with extensive training in warehouse and ETL).&lt;/p&gt;\n\n&lt;p&gt;The position is security-oriented for a fintech. I don&amp;#39;t have much info but I think they seek to comply with certain legal regulations that require deleting information from time to time as well as minimizing the risk due to PII (personally identifiable information).&lt;/p&gt;\n\n&lt;p&gt;Any advice would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16u02bl", "is_robot_indexable": true, "report_reasons": null, "author": "NationOfSheeps", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16u02bl/advice_from_data_engineers_specialized_in_security/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16u02bl/advice_from_data_engineers_specialized_in_security/", "subreddit_subscribers": 130782, "created_utc": 1695858020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have lots of data generated by source systems that don't have their own backend OLTP database, they just push flat files out. These are easy to consume into delta tables. Delta Lake and the lake house are new territory and even the sales engineers we've been talking to from databricks have seemed a bit non-committal iffy/squirmy when I ask them about referential constraints. I know delta lake doesn't have physically enforced constraints. I assumed they just had high performance multi-step wrappers that effectively do the same thing. I'd like to NOT have to develop that myself, I'd rather just use an OLTP in that situation\n\nMy question for the community is, has anyone used the databricks implementation of ['referential' constraints](https://docs.databricks.com/en/tables/constraints.html) extensively? What were the major challenges/issues? How much custom hacking was it?\n\nOur use cases aren't very complicated. In most cases we're just bulk adding data to tables, and sometimes bulk overwriting. Tables have relations to other tables based on keys. But we don't have any situations like delete or update record from table A and then cascade that to other downstream tables etc. ", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake 'Referential' Constraints", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ttg92", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695842695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have lots of data generated by source systems that don&amp;#39;t have their own backend OLTP database, they just push flat files out. These are easy to consume into delta tables. Delta Lake and the lake house are new territory and even the sales engineers we&amp;#39;ve been talking to from databricks have seemed a bit non-committal iffy/squirmy when I ask them about referential constraints. I know delta lake doesn&amp;#39;t have physically enforced constraints. I assumed they just had high performance multi-step wrappers that effectively do the same thing. I&amp;#39;d like to NOT have to develop that myself, I&amp;#39;d rather just use an OLTP in that situation&lt;/p&gt;\n\n&lt;p&gt;My question for the community is, has anyone used the databricks implementation of &lt;a href=\"https://docs.databricks.com/en/tables/constraints.html\"&gt;&amp;#39;referential&amp;#39; constraints&lt;/a&gt; extensively? What were the major challenges/issues? How much custom hacking was it?&lt;/p&gt;\n\n&lt;p&gt;Our use cases aren&amp;#39;t very complicated. In most cases we&amp;#39;re just bulk adding data to tables, and sometimes bulk overwriting. Tables have relations to other tables based on keys. But we don&amp;#39;t have any situations like delete or update record from table A and then cascade that to other downstream tables etc. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?auto=webp&amp;s=9dd59568b8579947f05ce66ee028655ef14e64d6", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99613d282007d0bcc41947bc7f0846da94adca04", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400ef45c57444e53fb95c1358e9a0b6419c3112e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ed83d9a6c1afb35b8be4de3f85b722298d1c3d6", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=768e111879e31b88e5a61b81d8d367edaa5e5351", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2a359111feb6e4d3ffa529f6614614a63914c4e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6e5d40f18830851f93eb2158f465da573a5df80", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ttg92", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ttg92/delta_lake_referential_constraints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ttg92/delta_lake_referential_constraints/", "subreddit_subscribers": 130782, "created_utc": 1695842695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to execute a few notebooks in loop from a parent notebook using the dbutils.notebook.run and trying to capture the details of the child notebook like run ID and job ID. I tried using the dbutils get_context but that only gives me the context of the parent notebook. I don't know how to associate or get the details for the child notebooks. I've searched the forums but nothing seems to work. Any suggestions would be highly appreciated.", "author_fullname": "t2_5tnbm4y7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I get the execution context of a child notebook from a parent notebook in Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16trv8e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695838560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to execute a few notebooks in loop from a parent notebook using the dbutils.notebook.run and trying to capture the details of the child notebook like run ID and job ID. I tried using the dbutils get_context but that only gives me the context of the parent notebook. I don&amp;#39;t know how to associate or get the details for the child notebooks. I&amp;#39;ve searched the forums but nothing seems to work. Any suggestions would be highly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16trv8e", "is_robot_indexable": true, "report_reasons": null, "author": "TheViper1994", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16trv8e/how_do_i_get_the_execution_context_of_a_child/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16trv8e/how_do_i_get_the_execution_context_of_a_child/", "subreddit_subscribers": 130782, "created_utc": 1695838560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jcps4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fuzzy Matching Images and Text", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_16tr1qa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/55icYUDg9a_vh8FxBoOcQCvZsBgeWUqVh9nj5NQMghs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695836486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "databricks.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.databricks.com/blog/using-images-and-metadata-product-fuzzy-matching-zingg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?auto=webp&amp;s=bf8e5c2c1b93f1afb1d0506255fa89366061473e", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1e11f18d5b10168513fe156f015b7dffe3219fe", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f1cd07a69024d55254c8665d619e6d56a852214", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff32ca9b9a0197e48bf7579bf2a46eec1b2adfec", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d9d63b2a1dc792b84f7b26a0bfd77c3c86d2289", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=23df7a72e4955e7a218a656ca4bcf9ab41c52243", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/H4iudY_kHprASzNavlQij4RSySMgjWwCL_sJGgiPNBI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0dc360080e1e154220739d39acd7e0808cd58ac3", "width": 1080, "height": 565}], "variants": {}, "id": "Y5WW65fCuqgNmZ3YIsL2Kz58c0X6q-jcV6ATBhrRJUM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16tr1qa", "is_robot_indexable": true, "report_reasons": null, "author": "sonalg", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tr1qa/fuzzy_matching_images_and_text/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.databricks.com/blog/using-images-and-metadata-product-fuzzy-matching-zingg", "subreddit_subscribers": 130782, "created_utc": 1695836486.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\nCurrently working in a startup and I am the only one who is working in Data Science or Data Engineering task. Joined in May 2023 as an intern.\nNow,My CTO has asked me to take interview of senior DE, these guys have around 3-7 yrs of work exp, I am very much confuses what to ask!\nCan you guys tell me! What are the fundamentals need to be asked", "author_fullname": "t2_i3smbco5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fresher taking interview of senior DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16tpb3v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695832502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,\nCurrently working in a startup and I am the only one who is working in Data Science or Data Engineering task. Joined in May 2023 as an intern.\nNow,My CTO has asked me to take interview of senior DE, these guys have around 3-7 yrs of work exp, I am very much confuses what to ask!\nCan you guys tell me! What are the fundamentals need to be asked&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16tpb3v", "is_robot_indexable": true, "report_reasons": null, "author": "No_Woodpecker_3267", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tpb3v/fresher_taking_interview_of_senior_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16tpb3v/fresher_taking_interview_of_senior_de/", "subreddit_subscribers": 130782, "created_utc": 1695832502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I was asked to create a data warehouse, and I'm leaning toward the following ELT solution.\n\n    [Read Replica of Source Database]  \n    \n    \u2193\u00a0-- using Amazon DMS  \n    \n    [S3 bucket]   \n    \n    \u2193 \u00a0-- using Snowflake  \n    \n    [Snowflake] \n\nI would first load all the data into Snowflake and do transformations and processing all within Snowflake.\n\nMy questions are:\n\n* What are your thoughts on this ELT setup?\n* If there are hundreds of tables in the source database, do I need load queries for each table?\n* How should I orchestrate the schedules? Do you use Snowflake tasks, or another service like Airflow?", "author_fullname": "t2_98ijstb5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would like to get your feedback on my data warehousing solution using Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16todpd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695830387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was asked to create a data warehouse, and I&amp;#39;m leaning toward the following ELT solution.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[Read Replica of Source Database]  \n\n\u2193\u00a0-- using Amazon DMS  \n\n[S3 bucket]   \n\n\u2193 \u00a0-- using Snowflake  \n\n[Snowflake] \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I would first load all the data into Snowflake and do transformations and processing all within Snowflake.&lt;/p&gt;\n\n&lt;p&gt;My questions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are your thoughts on this ELT setup?&lt;/li&gt;\n&lt;li&gt;If there are hundreds of tables in the source database, do I need load queries for each table?&lt;/li&gt;\n&lt;li&gt;How should I orchestrate the schedules? Do you use Snowflake tasks, or another service like Airflow?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16todpd", "is_robot_indexable": true, "report_reasons": null, "author": "Specialist_Dig2115", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16todpd/would_like_to_get_your_feedback_on_my_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16todpd/would_like_to_get_your_feedback_on_my_data/", "subreddit_subscribers": 130782, "created_utc": 1695830387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Consistency is one of the most important parts of an operational system.\n\nYou don\u2019t want to take an automated action based on incorrect intermediate results. Imagine getting charged an overdraft fee when you never actually overdrafted, just because of an eventually consistent stream processor!\n\nIn this blog post, Frank McSherry himself breaks down how Materialize achieves strong consistency guarantees, so you can act upon results with confidence: https://materialize.com/blog/operational-consistency/\n\n(Oh, and if you happen to be at the Current conference in San Jose, come to the Materialize booth to chat more about why consistency is so critical to operationalizing your data)", "author_fullname": "t2_5p00kusf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The importance of data consistency for business operations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ti3zi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695814414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Consistency is one of the most important parts of an operational system.&lt;/p&gt;\n\n&lt;p&gt;You don\u2019t want to take an automated action based on incorrect intermediate results. Imagine getting charged an overdraft fee when you never actually overdrafted, just because of an eventually consistent stream processor!&lt;/p&gt;\n\n&lt;p&gt;In this blog post, Frank McSherry himself breaks down how Materialize achieves strong consistency guarantees, so you can act upon results with confidence: &lt;a href=\"https://materialize.com/blog/operational-consistency/\"&gt;https://materialize.com/blog/operational-consistency/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;(Oh, and if you happen to be at the Current conference in San Jose, come to the Materialize booth to chat more about why consistency is so critical to operationalizing your data)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16ti3zi", "is_robot_indexable": true, "report_reasons": null, "author": "Chuck-Alt-Delete", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ti3zi/the_importance_of_data_consistency_for_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ti3zi/the_importance_of_data_consistency_for_business/", "subreddit_subscribers": 130782, "created_utc": 1695814414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a BA. Have always been one. But I am getting another BA opportunity from a Software Company which a huge DE team. The only reason for me to switch to this cpany would be the hope that someday they might let me move to their DE team and thats what I ultimately want - to break into the DE market. \n\nAm I being delusional? I am working on personal DE projects.", "author_fullname": "t2_5zamw97i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any success stories for switching to DE within same organization?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16tzcjh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695856252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a BA. Have always been one. But I am getting another BA opportunity from a Software Company which a huge DE team. The only reason for me to switch to this cpany would be the hope that someday they might let me move to their DE team and thats what I ultimately want - to break into the DE market. &lt;/p&gt;\n\n&lt;p&gt;Am I being delusional? I am working on personal DE projects.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16tzcjh", "is_robot_indexable": true, "report_reasons": null, "author": "abhishek16x", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tzcjh/any_success_stories_for_switching_to_de_within/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16tzcjh/any_success_stories_for_switching_to_de_within/", "subreddit_subscribers": 130782, "created_utc": 1695856252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data analyst (2YE) turned DE at a start-up. We have internal stakeholders who want data from several sources for dashboards and metrics. I am familiar with writing API scripts and all that but it is a lot of work for a solo DE so I stumbled upon Airbyte (SLT doesn't want an expensive tool). \n\nShould I create these integrations write to an S3 and TL them (using Glue or MWAA) over to Redshift? Or should I just import directly into Redshift? Wondering what the pros and cons of each method are. Also, does Airbyte offer incremental loads of some sort?   \n\n\nI'm interested in hearing about some successful design patterns with AWS and Airbyte for internal/dashboarding. ", "author_fullname": "t2_geamh3dd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Started New Job - Is this a good design pattern?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16tyw82", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695855180.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data analyst (2YE) turned DE at a start-up. We have internal stakeholders who want data from several sources for dashboards and metrics. I am familiar with writing API scripts and all that but it is a lot of work for a solo DE so I stumbled upon Airbyte (SLT doesn&amp;#39;t want an expensive tool). &lt;/p&gt;\n\n&lt;p&gt;Should I create these integrations write to an S3 and TL them (using Glue or MWAA) over to Redshift? Or should I just import directly into Redshift? Wondering what the pros and cons of each method are. Also, does Airbyte offer incremental loads of some sort?   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in hearing about some successful design patterns with AWS and Airbyte for internal/dashboarding. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16tyw82", "is_robot_indexable": true, "report_reasons": null, "author": "Emergency_Estate_866", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tyw82/started_new_job_is_this_a_good_design_pattern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16tyw82/started_new_job_is_this_a_good_design_pattern/", "subreddit_subscribers": 130782, "created_utc": 1695855180.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am currently trying to decide if I should accept an offer that I received from my summer internship. \n\nDuring my internship, I was hired as a data engineer intern (I was specifically looking for data engineering internship as it is a field that I'm passionate about want to get into). The company where I worked is a public, global software company. I have BS in CS degree in T20 CS schools in US, as well as 4 YOE software engineering experiences, and currently pursuing MS in Data Analysis in T5 CS school - with a goal of becoming a data engineer. \n\nOnce I started my internship, I was assigned to a Business Intelligence team, and was assigned to work on dashboarding works (GCP, Looker etc). I didn't get a chance to work on data pipelinining or ingestion. I was confused but did my best to do the best I can - which I received good feedback and that turned into a full time offer to join the team. During my internship my manager also made sure with me to talk to some of the senior DEs in my team so that I can gain some insights, but the amount of technical work that I did was pretty minimal (intermediate-advanced level SQL for maintaining the data model). \n\nThe offer turned out I'll be mostly continuing the dashboarding work and sounds like the team is right now in need of me to fully devote to the data visualization projects (and also added that the current DE team is very full). In addition, the location that they're able to offer is not ideal for me as the heat in TX caused me skin rashes and overall my mood was significantly affected. \n\nMy manager left the team after my internship and the new manager seems like would prefer me to be a fully BI Developer for dashboards. I really liked working with my manager which was honestly a big factor for me to even consider BI role so I am pretty sad about it as well. \n\nI'm not sure if it is wise for me to take the offer given that 1) I'll be mostly working as a BI Developer (not able to actively develop my SWE skills / CS degree) / 2) at a location that I had such a hard time adjusting to. \n\nMy career goal is to get into data engineering and to develop my technical skills. I don't mind getting more business related works, however, the team seems to be not willing to let me explore DE works at this point. \n\nAm I getting too picky on my choice given this market? However, I am afraid this will make it even harder to advance into DE roles in the future. \n\nAny suggestions or guidance would be really appreciated!", "author_fullname": "t2_l0szgqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this job offer OK?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16tx26a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695864787.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695851034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am currently trying to decide if I should accept an offer that I received from my summer internship. &lt;/p&gt;\n\n&lt;p&gt;During my internship, I was hired as a data engineer intern (I was specifically looking for data engineering internship as it is a field that I&amp;#39;m passionate about want to get into). The company where I worked is a public, global software company. I have BS in CS degree in T20 CS schools in US, as well as 4 YOE software engineering experiences, and currently pursuing MS in Data Analysis in T5 CS school - with a goal of becoming a data engineer. &lt;/p&gt;\n\n&lt;p&gt;Once I started my internship, I was assigned to a Business Intelligence team, and was assigned to work on dashboarding works (GCP, Looker etc). I didn&amp;#39;t get a chance to work on data pipelinining or ingestion. I was confused but did my best to do the best I can - which I received good feedback and that turned into a full time offer to join the team. During my internship my manager also made sure with me to talk to some of the senior DEs in my team so that I can gain some insights, but the amount of technical work that I did was pretty minimal (intermediate-advanced level SQL for maintaining the data model). &lt;/p&gt;\n\n&lt;p&gt;The offer turned out I&amp;#39;ll be mostly continuing the dashboarding work and sounds like the team is right now in need of me to fully devote to the data visualization projects (and also added that the current DE team is very full). In addition, the location that they&amp;#39;re able to offer is not ideal for me as the heat in TX caused me skin rashes and overall my mood was significantly affected. &lt;/p&gt;\n\n&lt;p&gt;My manager left the team after my internship and the new manager seems like would prefer me to be a fully BI Developer for dashboards. I really liked working with my manager which was honestly a big factor for me to even consider BI role so I am pretty sad about it as well. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if it is wise for me to take the offer given that 1) I&amp;#39;ll be mostly working as a BI Developer (not able to actively develop my SWE skills / CS degree) / 2) at a location that I had such a hard time adjusting to. &lt;/p&gt;\n\n&lt;p&gt;My career goal is to get into data engineering and to develop my technical skills. I don&amp;#39;t mind getting more business related works, however, the team seems to be not willing to let me explore DE works at this point. &lt;/p&gt;\n\n&lt;p&gt;Am I getting too picky on my choice given this market? However, I am afraid this will make it even harder to advance into DE roles in the future. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions or guidance would be really appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16tx26a", "is_robot_indexable": true, "report_reasons": null, "author": "PythonIsGreat", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16tx26a/is_this_job_offer_ok/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16tx26a/is_this_job_offer_ok/", "subreddit_subscribers": 130782, "created_utc": 1695851034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, community,\n\nI'm currently trying to understand of RAG and contemporary large language model (LLM) architectures. Specifically, I'm investigating whether the Llamaindex library supports real-time data streaming and if it offers any functionality for auto-re-indexing or incremental indexing based on an updated data corpus. For example, through APIs or Kafka/debezium?\n\nIf not Llamaindex, have you used any alternatives for such use cases? Thanks!", "author_fullname": "t2_n0ywlxbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Llamaindex work on a real-time stream of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16twvbn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695850596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, community,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently trying to understand of RAG and contemporary large language model (LLM) architectures. Specifically, I&amp;#39;m investigating whether the Llamaindex library supports real-time data streaming and if it offers any functionality for auto-re-indexing or incremental indexing based on an updated data corpus. For example, through APIs or Kafka/debezium?&lt;/p&gt;\n\n&lt;p&gt;If not Llamaindex, have you used any alternatives for such use cases? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16twvbn", "is_robot_indexable": true, "report_reasons": null, "author": "muditjps", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16twvbn/does_llamaindex_work_on_a_realtime_stream_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16twvbn/does_llamaindex_work_on_a_realtime_stream_of_data/", "subreddit_subscribers": 130782, "created_utc": 1695850596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In simplification, target architecture include followig components:\n- Talend for data ingestion to AWS S3 [brozne, silver]\n- Snowflake with dbt as Data Warehouse [gold]\n\nWhat are there possible tools for managing extraction and load data to S3?\n\nAny experience to share?\n\nThanks in advance.", "author_fullname": "t2_h1utybam", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is best ELT tool for load data to AWS S3 ingestion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ttnli", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695843160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In simplification, target architecture include followig components:\n- Talend for data ingestion to AWS S3 [brozne, silver]\n- Snowflake with dbt as Data Warehouse [gold]&lt;/p&gt;\n\n&lt;p&gt;What are there possible tools for managing extraction and load data to S3?&lt;/p&gt;\n\n&lt;p&gt;Any experience to share?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ttnli", "is_robot_indexable": true, "report_reasons": null, "author": "Savings_Problem1972", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ttnli/what_is_best_elt_tool_for_load_data_to_aws_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ttnli/what_is_best_elt_tool_for_load_data_to_aws_s3/", "subreddit_subscribers": 130782, "created_utc": 1695843160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Platform - Unifying Data Observability, Governance and Catalog.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_16thyd0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tEhRK_XXJVr34mqSCLPiQMWHWJiQAPVbPIZntztB9eo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695813924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "decube.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.decube.io/post/new-data-governance-essential-for-modern-data-stack", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?auto=webp&amp;s=f1f521f3c967ebcb706a1fab5f4a2ef3bfadb2e6", "width": 1921, "height": 1081}, "resolutions": [{"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e4e7881ce8d2a85531d8e76cbed2ee58bcfbb34", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=62e16cd13c8804b8aa9eb8983ecde4b815ca2365", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=63199051e205758060efd19c21e43efee0e58c93", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d15b48eeb712f324f2f3038f603aa155538b6f07", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7152f682c34305a47379ea4d292aea0ee74463d6", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/njHA9qSeoIk7_SAhskWSHg6wzyFcr5a-ytXOvUzwAZE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=30e0180d96682a54bdaeb824612ce8d5cd4eecb9", "width": 1080, "height": 607}], "variants": {}, "id": "3Orb3HTH8Vbo1PYnkNdreENhh84Slt5-NKiruAit2L4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16thyd0", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16thyd0/data_platform_unifying_data_observability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.decube.io/post/new-data-governance-essential-for-modern-data-stack", "subreddit_subscribers": 130782, "created_utc": 1695813924.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}