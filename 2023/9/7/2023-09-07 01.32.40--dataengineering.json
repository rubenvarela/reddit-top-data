{"kind": "Listing", "data": {"after": "t3_16bjqig", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When a web dev wants to showcase their project, they can easily build one and deploy it in the cloud and a prospect employer can easily appreciate it by diving into the web application itself. While in data engineering, all I see to showcase a project is to create architectural diagrams, enrich README file (yes, in github), and creating online dashboards (a means to an end to highlight the \u201cdata\u201d part but how about the engineering part?). I mean, compared to web dev projects, DE projects will need the user/employer\u2019s time and effort to get a feel of the project.\n\nAny better way we can do to showcase our portfolio/projects?", "author_fullname": "t2_5owlarij", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Projects are Hard to Showcase than Web Development Projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16be6f8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693987222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When a web dev wants to showcase their project, they can easily build one and deploy it in the cloud and a prospect employer can easily appreciate it by diving into the web application itself. While in data engineering, all I see to showcase a project is to create architectural diagrams, enrich README file (yes, in github), and creating online dashboards (a means to an end to highlight the \u201cdata\u201d part but how about the engineering part?). I mean, compared to web dev projects, DE projects will need the user/employer\u2019s time and effort to get a feel of the project.&lt;/p&gt;\n\n&lt;p&gt;Any better way we can do to showcase our portfolio/projects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16be6f8", "is_robot_indexable": true, "report_reasons": null, "author": "_Dark_mage", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16be6f8/data_engineering_projects_are_hard_to_showcase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16be6f8/data_engineering_projects_are_hard_to_showcase/", "subreddit_subscribers": 127154, "created_utc": 1693987222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been banging my head against the wall for two days, trying to get formulas to pick up column names, which seemed to have the right data type.\n\nThey came with n dashes instead of hyphens!!!!", "author_fullname": "t2_gs67ogqx6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It\u2019s an n-dash!!!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bl4y2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694008960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been banging my head against the wall for two days, trying to get formulas to pick up column names, which seemed to have the right data type.&lt;/p&gt;\n\n&lt;p&gt;They came with n dashes instead of hyphens!!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bl4y2", "is_robot_indexable": true, "report_reasons": null, "author": "LittleBiggle", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bl4y2/its_an_ndash/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bl4y2/its_an_ndash/", "subreddit_subscribers": 127154, "created_utc": 1694008960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_owff7qyq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I built an AI-powered tool that can turn any website into an API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 94, "top_awarded_type": null, "hide_score": false, "name": "t3_16brzy2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/d8azdr7diomb1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1598, "scrubber_media_url": "https://v.redd.it/d8azdr7diomb1/DASH_96.mp4", "dash_url": "https://v.redd.it/d8azdr7diomb1/DASHPlaylist.mpd?a=1696642360%2CMDQyY2ZmMjNmMjc5ZWExZjUxYmZkODY2MjM1OWYyMGNkMTI0NmY3MjJlMmQwZjg5MWU1MWU5OTJjNjE2NTg3MA%3D%3D&amp;v=1&amp;f=sd", "duration": 130, "hls_url": "https://v.redd.it/d8azdr7diomb1/HLSPlaylist.m3u8?a=1696642360%2CZWRhZmVkZWY5NjA5YjEzMDE5MmMxYzRhNGJiNTE2NGRiODM0MTBkMTlkMTMyNDAyYmJlMGM2ODRjZDBjNGJlZQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-wGropRbBsTw5PkWvR6tE6YUX2EFttT1PsIcKew8sVg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694025191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/d8azdr7diomb1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?format=pjpg&amp;auto=webp&amp;s=f93f8ee3fdd671a7d984571ea5b1fbdeaa1e673b", "width": 1598, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4dd7b5deeab38e44ccca169c3cdb9b8d3aa4e583", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e208178b6ace95bca7bb5cefe28bebcc0a9ceee6", "width": 216, "height": 145}, {"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a484d3eb1209307a3e4987349431e53412918d16", "width": 320, "height": 216}, {"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=32dc84092d7ed3cbf49ec9c712fe0f148743ef03", "width": 640, "height": 432}, {"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ca4d1ae8a06f7f43d0fb6dcddb737da857a15ff9", "width": 960, "height": 648}, {"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=49b0d028d1b13f869fe03cbaffb86971218dfef0", "width": 1080, "height": 729}], "variants": {}, "id": "5QRgmU7Upi9cdWw454kxztDnZriTUxVoIOvs7oWXNAI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16brzy2", "is_robot_indexable": true, "report_reasons": null, "author": "madredditscientist", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16brzy2/i_built_an_aipowered_tool_that_can_turn_any/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/d8azdr7diomb1", "subreddit_subscribers": 127154, "created_utc": 1694025191.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/d8azdr7diomb1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1598, "scrubber_media_url": "https://v.redd.it/d8azdr7diomb1/DASH_96.mp4", "dash_url": "https://v.redd.it/d8azdr7diomb1/DASHPlaylist.mpd?a=1696642360%2CMDQyY2ZmMjNmMjc5ZWExZjUxYmZkODY2MjM1OWYyMGNkMTI0NmY3MjJlMmQwZjg5MWU1MWU5OTJjNjE2NTg3MA%3D%3D&amp;v=1&amp;f=sd", "duration": 130, "hls_url": "https://v.redd.it/d8azdr7diomb1/HLSPlaylist.m3u8?a=1696642360%2CZWRhZmVkZWY5NjA5YjEzMDE5MmMxYzRhNGJiNTE2NGRiODM0MTBkMTlkMTMyNDAyYmJlMGM2ODRjZDBjNGJlZQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9ua96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Birmingham City Council goes under after Oracle disaster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_16bk4yo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/vcZxnOpZqHgDRaDOx7DN_lBgQOc42wHJleKZUPorfu4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694006412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "theregister.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.theregister.com/2023/09/05/birmingham_city_council_oracle/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?auto=webp&amp;s=0a305e828a30a6f0c71c29fe7128261795b84be7", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da6738dd559dcd27f5c6395b87b24e35f9f0f90b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e13687e35641b792bc1b4618600421e2f2219c07", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2870609fd4788bdd796d62d476bd37c54e234394", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=762a32e0a6fbe457317ae2f4ca52c22a5139c69f", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=db21c86d150204fb0faf5d99a1319f2829ee8638", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fff2e47b3400ab0679156b32901cef4a6faa7a26", "width": 1080, "height": 565}], "variants": {}, "id": "x_FLdS7YQetbIrTzI0wUDbZvSjWYi10bdLE8R6sRSNA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16bk4yo", "is_robot_indexable": true, "report_reasons": null, "author": "uluvboobs", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bk4yo/birmingham_city_council_goes_under_after_oracle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.theregister.com/2023/09/05/birmingham_city_council_oracle/", "subreddit_subscribers": 127154, "created_utc": 1694006412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I have transitioned from an analyst to DE role over the last 4 years thanks to this amazing community and have been working as a DE (more of an analytics engineer) for around a year. My organization uses airflow on GKE. While working on a project recently, our team ran into some dependency problems and were recommended the use of KubernetesPodOperator. I dont know much about Kubernetes and want to learn it fast to at least get up and running with the operator first and then to learn more about kubernetes eventually. \n\n&amp;#x200B;\n\nHere is what I am trying to learn first: \n\n1. Hosting a container image on a private repo. \n2. Use this image in the podoperator. \n3. handling authentication to gcp so i can read from and write to bigquery tables.\n\nHow would you guys recommend to get up and running? Also I learn best via videos but I couldn't find something in video format around what I am trying to do.", "author_fullname": "t2_na539", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need resource recommendations for learning Kubernetes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bhwo5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694000263.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I have transitioned from an analyst to DE role over the last 4 years thanks to this amazing community and have been working as a DE (more of an analytics engineer) for around a year. My organization uses airflow on GKE. While working on a project recently, our team ran into some dependency problems and were recommended the use of KubernetesPodOperator. I dont know much about Kubernetes and want to learn it fast to at least get up and running with the operator first and then to learn more about kubernetes eventually. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here is what I am trying to learn first: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Hosting a container image on a private repo. &lt;/li&gt;\n&lt;li&gt;Use this image in the podoperator. &lt;/li&gt;\n&lt;li&gt;handling authentication to gcp so i can read from and write to bigquery tables.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;How would you guys recommend to get up and running? Also I learn best via videos but I couldn&amp;#39;t find something in video format around what I am trying to do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16bhwo5", "is_robot_indexable": true, "report_reasons": null, "author": "acid4207", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bhwo5/need_resource_recommendations_for_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bhwo5/need_resource_recommendations_for_learning/", "subreddit_subscribers": 127154, "created_utc": 1694000263.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know that data engineering tends to be more focused on the warehousing and pipeline side of things, but recently, being asked in a couple of interviews that I've been in about maintaining indexes of tables, creation and set up of new tables, SSRS, almost like they are looking for a database analyst or a DBA. I was wondering if anyone has any experience with that because index maintenance can be very different I think than what a typical data engineer would do, wouldn't it?", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have any resources on index maintenance, or DBA related stuff?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bi7jz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694001190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that data engineering tends to be more focused on the warehousing and pipeline side of things, but recently, being asked in a couple of interviews that I&amp;#39;ve been in about maintaining indexes of tables, creation and set up of new tables, SSRS, almost like they are looking for a database analyst or a DBA. I was wondering if anyone has any experience with that because index maintenance can be very different I think than what a typical data engineer would do, wouldn&amp;#39;t it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bi7jz", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bi7jz/does_anyone_have_any_resources_on_index/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bi7jz/does_anyone_have_any_resources_on_index/", "subreddit_subscribers": 127154, "created_utc": 1694001190.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Here's my usecase.  \n\n* We have a highly concurrent transactional workload on our DBs.\n* Data in different DBs are related to each other in some way or the other and communication between DBs is done via APIs. \n* Right now, we perform CDC via AWS DMS and store an exact replica of all our dbs in a separate DB [lets call this master]. Due to the nature of our data we aren't able to capture/recreate PK/FK or indexes on the master db.\n* We've build 100's of dashboards and reports on top of this master DB. \n* We need a more performant system than our current master db for dashboarding purposes / application backend purposes   \n   \n\n\n\n\nHere's what I've tried so far.     \n\n*  Hybrid OLAP/OLTP solutions like CitusData -&gt; didn't work for us due to the extra management required to maintain the shards and more importantly the \"No foreign keys across distributed schemas.\" restriction. \n* Hydra DB [Link to Github](https://github.com/hydradatabase/hydra) -&gt; Worked well for aggregated query usecases but not for queries that build reports. Also, data insertion and updation is abyssmal on columnar dbs. \n* Clickhouse -&gt; loaded bulk data onto clickhouse and things work well but a large portion of the reports will have to be rebuilt.\n\n\nAny insights as to what I can do here?\n\n\nedit:   \nEnd Goal:   We need a more performant system than our current master db", "author_fullname": "t2_lrdo4bnz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC from postgres to postgres.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bf3a4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693993925.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693990633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s my usecase.  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We have a highly concurrent transactional workload on our DBs.&lt;/li&gt;\n&lt;li&gt;Data in different DBs are related to each other in some way or the other and communication between DBs is done via APIs. &lt;/li&gt;\n&lt;li&gt;Right now, we perform CDC via AWS DMS and store an exact replica of all our dbs in a separate DB [lets call this master]. Due to the nature of our data we aren&amp;#39;t able to capture/recreate PK/FK or indexes on the master db.&lt;/li&gt;\n&lt;li&gt;We&amp;#39;ve build 100&amp;#39;s of dashboards and reports on top of this master DB. &lt;/li&gt;\n&lt;li&gt;We need a more performant system than our current master db for dashboarding purposes / application backend purposes&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s what I&amp;#39;ve tried so far.     &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; Hybrid OLAP/OLTP solutions like CitusData -&amp;gt; didn&amp;#39;t work for us due to the extra management required to maintain the shards and more importantly the &amp;quot;No foreign keys across distributed schemas.&amp;quot; restriction. &lt;/li&gt;\n&lt;li&gt;Hydra DB &lt;a href=\"https://github.com/hydradatabase/hydra\"&gt;Link to Github&lt;/a&gt; -&amp;gt; Worked well for aggregated query usecases but not for queries that build reports. Also, data insertion and updation is abyssmal on columnar dbs. &lt;/li&gt;\n&lt;li&gt;Clickhouse -&amp;gt; loaded bulk data onto clickhouse and things work well but a large portion of the reports will have to be rebuilt.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any insights as to what I can do here?&lt;/p&gt;\n\n&lt;p&gt;edit:&lt;br/&gt;\nEnd Goal:   We need a more performant system than our current master db&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NAftzDs_QZTTYoNQDmOEcnmdBhfCsvMABzECAlBXz0E.jpg?auto=webp&amp;s=cb8fae1e4bd021364cb7d63509619278c16cdaa7", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/NAftzDs_QZTTYoNQDmOEcnmdBhfCsvMABzECAlBXz0E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d5eca145b2c18ace24d22857718d5115f6be9d7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/NAftzDs_QZTTYoNQDmOEcnmdBhfCsvMABzECAlBXz0E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b15c6918e67eeb0558b3a23d499f30e80e45e8a0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/NAftzDs_QZTTYoNQDmOEcnmdBhfCsvMABzECAlBXz0E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbb148b5b93ca592a12966cbe469585fd964cf36", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/NAftzDs_QZTTYoNQDmOEcnmdBhfCsvMABzECAlBXz0E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dc0b8db3fe878e6dc834a9e75fda707bd2be9838", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/NAftzDs_QZTTYoNQDmOEcnmdBhfCsvMABzECAlBXz0E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0915cd9c9696aee370edbf6eebc38d87ab269cdd", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/NAftzDs_QZTTYoNQDmOEcnmdBhfCsvMABzECAlBXz0E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b15ea48853321892d85e65756e3bb40060c90765", "width": 1080, "height": 540}], "variants": {}, "id": "j3a7KYMSc5BpKUsERA6h0wMphrco9B7kT4Uv5TEizKA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bf3a4", "is_robot_indexable": true, "report_reasons": null, "author": "WhollyConfused96", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bf3a4/cdc_from_postgres_to_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bf3a4/cdc_from_postgres_to_postgres/", "subreddit_subscribers": 127154, "created_utc": 1693990633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I regularly work with a variety of data sources and provide static plots to internal customers.  \n\nI'm familiar with tools like Plotly to create interactive html files, but this requires my handling of the data. I'd like to automate this a bit, but I'm not sure what tools are available for interactive plotting AND data selection. \n\nBasically, I have data that may come from \\*.sie files, \\*.blf, \\*.asc files, \\*.csv files, or other assorted data acquisition tools. Pretty much all of it is time series data.\n\nI think I'd like to create a web interface to view this, but I'm not sure what interactive tools exist to manipulate the channels. A single file may have 50 channels of data. \n\nItems that have crossed my mind:\n\n* An sql server with a database for each datafile. PowerBI to view?\n* Auto-generated web pages from Bokeh, but how do I get user selectable channels?\n* Jupyterhub/Jupyterlab notebooks? \n\n&amp;#x200B;\n\nWhat paid tools exist for engineering?\n\n* I've seen [Aqira](https://www.hbkworld.com/en/products/software/analysis-simulation/durability/aqira-standardize-global-engineering-processes) , but most tools seem to be standalone. \n\nFor reference, an idea of the data source might be a DAQ (data acquisition) tool like\n\n[https://astronovainc.com/our-businesses/test-measurement/](https://astronovainc.com/our-businesses/test-measurement/) \n\n&amp;#x200B;\n\nIf I'm posted in the wrong thread, my apologies. I'm just not really sure where to ask this.", "author_fullname": "t2_3si1p2jc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Providing interactive plots to internal customers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bpt8i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1694020198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I regularly work with a variety of data sources and provide static plots to internal customers.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m familiar with tools like Plotly to create interactive html files, but this requires my handling of the data. I&amp;#39;d like to automate this a bit, but I&amp;#39;m not sure what tools are available for interactive plotting AND data selection. &lt;/p&gt;\n\n&lt;p&gt;Basically, I have data that may come from *.sie files, *.blf, *.asc files, *.csv files, or other assorted data acquisition tools. Pretty much all of it is time series data.&lt;/p&gt;\n\n&lt;p&gt;I think I&amp;#39;d like to create a web interface to view this, but I&amp;#39;m not sure what interactive tools exist to manipulate the channels. A single file may have 50 channels of data. &lt;/p&gt;\n\n&lt;p&gt;Items that have crossed my mind:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;An sql server with a database for each datafile. PowerBI to view?&lt;/li&gt;\n&lt;li&gt;Auto-generated web pages from Bokeh, but how do I get user selectable channels?&lt;/li&gt;\n&lt;li&gt;Jupyterhub/Jupyterlab notebooks? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What paid tools exist for engineering?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ve seen &lt;a href=\"https://www.hbkworld.com/en/products/software/analysis-simulation/durability/aqira-standardize-global-engineering-processes\"&gt;Aqira&lt;/a&gt; , but most tools seem to be standalone. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For reference, an idea of the data source might be a DAQ (data acquisition) tool like&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://astronovainc.com/our-businesses/test-measurement/\"&gt;https://astronovainc.com/our-businesses/test-measurement/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;m posted in the wrong thread, my apologies. I&amp;#39;m just not really sure where to ask this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?auto=webp&amp;s=3a7ccc4fed70f5fe339e0cd18b0f298f562a0dfb", "width": 1200, "height": 645}, "resolutions": [{"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed7e248fa443fbf4a40cf842d4a3550c1e9c4f2b", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d974297f87b1449299a9ff49a4fbe2b7c19cad3", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=71b78ff8bd0bdf95c43fe90bdcd56d413f6f38e8", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c7476c27f219886ac57aafaa693750c5eee321ad", "width": 640, "height": 344}, {"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=487c05c704ee6414bc406fe96d59fafdf86ebc0b", "width": 960, "height": 516}, {"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b253fbe84c01e1aab3039c90fb616770eb90b2f", "width": 1080, "height": 580}], "variants": {}, "id": "VMbu5D8d3p88J0L0CDm3qmZUYWV6_kEFk-qS4gJuykM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bpt8i", "is_robot_indexable": true, "report_reasons": null, "author": "Substantial_Coyote91", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bpt8i/providing_interactive_plots_to_internal_customers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bpt8i/providing_interactive_plots_to_internal_customers/", "subreddit_subscribers": 127154, "created_utc": 1694020198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TLDR: For those of you who use airflow to coordinate data pipelines between clouds, what are some patterns you support and what might I stay away from? \n\nHey all,\nI've been doing some orchestrator deep dives for an upcoming long haul data lake/data warehousing initiative and have evaluated a few technologies thusfar, ultimately settling on airflow for the PoC use case thanks to the tool already being in use by the enterprise in other teams. Although I was *extremely* intrigued by Dagster, the talent pool and larger code base to draw from internally made airflow the more attractive option in the near term at least. However, the issue I am facing for the organization is that the other teams that currently use airflow have had the luxury of being single cloud teams, AWS or Azure respective to where their data resides. In contrast, this project will require us to handle coordination across both providers. \n\nAs an example workflow to go off, imagine a json file landing in s3 and a different json file landing in adls from a different source at the same time. I'll need to join this data together down the line in a cloud data warehouse such as Snowflake or an extract of records from both sources. I would need to coordinate some data validity checks with great expectations or similar before promoting the data past the raw lake layer and materializing it in a columnar format such as parquet in the lake serving layer. Then, I'd need to do the insert to the data warehouse layer which may be in Azure or AWS. Is there enough IO and egress possible in such a workload that it would make sense to have both airflow coordination hubs live separately? Or would I want to create an endpoint to trigger jobs in another dag in the other instance? If the data generally lands in one cloud vs the other for serving, would this change your answer?\n\nMore generally, where do you keep your codebases and how do you fo cicd for your dags on azure vs aws? I'd imagine it must get complex if you separate the codebases to be cloud specific.\n\nLastly, is one managed airflow cloud offering better than the other? Should I just keep it as raw container deployment to kubernetes to stay cloud agnostic? Any strong opinions on other implementations such as Astronomer or other airflow SaaS offerings?\n\nAny thoughts would really help me eliminate some design choices as we start out, thanks for your insights as always.", "author_fullname": "t2_uqrd0850", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Users: How do you best achieve cross-cloud deployment/orchestration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16b76m5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693964790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: For those of you who use airflow to coordinate data pipelines between clouds, what are some patterns you support and what might I stay away from? &lt;/p&gt;\n\n&lt;p&gt;Hey all,\nI&amp;#39;ve been doing some orchestrator deep dives for an upcoming long haul data lake/data warehousing initiative and have evaluated a few technologies thusfar, ultimately settling on airflow for the PoC use case thanks to the tool already being in use by the enterprise in other teams. Although I was &lt;em&gt;extremely&lt;/em&gt; intrigued by Dagster, the talent pool and larger code base to draw from internally made airflow the more attractive option in the near term at least. However, the issue I am facing for the organization is that the other teams that currently use airflow have had the luxury of being single cloud teams, AWS or Azure respective to where their data resides. In contrast, this project will require us to handle coordination across both providers. &lt;/p&gt;\n\n&lt;p&gt;As an example workflow to go off, imagine a json file landing in s3 and a different json file landing in adls from a different source at the same time. I&amp;#39;ll need to join this data together down the line in a cloud data warehouse such as Snowflake or an extract of records from both sources. I would need to coordinate some data validity checks with great expectations or similar before promoting the data past the raw lake layer and materializing it in a columnar format such as parquet in the lake serving layer. Then, I&amp;#39;d need to do the insert to the data warehouse layer which may be in Azure or AWS. Is there enough IO and egress possible in such a workload that it would make sense to have both airflow coordination hubs live separately? Or would I want to create an endpoint to trigger jobs in another dag in the other instance? If the data generally lands in one cloud vs the other for serving, would this change your answer?&lt;/p&gt;\n\n&lt;p&gt;More generally, where do you keep your codebases and how do you fo cicd for your dags on azure vs aws? I&amp;#39;d imagine it must get complex if you separate the codebases to be cloud specific.&lt;/p&gt;\n\n&lt;p&gt;Lastly, is one managed airflow cloud offering better than the other? Should I just keep it as raw container deployment to kubernetes to stay cloud agnostic? Any strong opinions on other implementations such as Astronomer or other airflow SaaS offerings?&lt;/p&gt;\n\n&lt;p&gt;Any thoughts would really help me eliminate some design choices as we start out, thanks for your insights as always.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16b76m5", "is_robot_indexable": true, "report_reasons": null, "author": "IncognitoEmployee", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16b76m5/airflow_users_how_do_you_best_achieve_crosscloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16b76m5/airflow_users_how_do_you_best_achieve_crosscloud/", "subreddit_subscribers": 127154, "created_utc": 1693964790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data engineer setting up dbt for a group of analysts and data scientists. The naming conventions for transformational layers suggested in the dbt docs are:\n\n1. staging\n2. intermediate\n3. marts\n\nTbh, I don't really like these names. Especially staging -- its generic and conflicts with the term staging as a reference to the environment. In past projects (that did not use dbt), I used different naming conventions, but were functionally the same as dbt's in terms of organization:\n\n1. crbo (common reporting business objects)\n   1. this could could be prefixed as stripe\\_crbo, and is almost always suffixed with something like \\_transactions or \\_refunds\n2. core\n   1. also can be prefixed or suffixed in the same manner as crbo\n3. any name that describes the data set, and then its aggregation level\n   1. i.e. stripe\\_transactions\\_by\\_country\\_plan\n\nHow do you structure your dbt layers and what naming conventions do you use? I will probably start out with dbt's conventions, but am interested in other approaches!", "author_fullname": "t2_1p505jz8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Layer Naming Conventions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bulr0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694031133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data engineer setting up dbt for a group of analysts and data scientists. The naming conventions for transformational layers suggested in the dbt docs are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;staging&lt;/li&gt;\n&lt;li&gt;intermediate&lt;/li&gt;\n&lt;li&gt;marts&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Tbh, I don&amp;#39;t really like these names. Especially staging -- its generic and conflicts with the term staging as a reference to the environment. In past projects (that did not use dbt), I used different naming conventions, but were functionally the same as dbt&amp;#39;s in terms of organization:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;crbo (common reporting business objects)\n\n&lt;ol&gt;\n&lt;li&gt;this could could be prefixed as stripe_crbo, and is almost always suffixed with something like _transactions or _refunds&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;core\n\n&lt;ol&gt;\n&lt;li&gt;also can be prefixed or suffixed in the same manner as crbo&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;any name that describes the data set, and then its aggregation level\n\n&lt;ol&gt;\n&lt;li&gt;i.e. stripe_transactions_by_country_plan&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;How do you structure your dbt layers and what naming conventions do you use? I will probably start out with dbt&amp;#39;s conventions, but am interested in other approaches!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bulr0", "is_robot_indexable": true, "report_reasons": null, "author": "Fredonia1988", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bulr0/dbt_layer_naming_conventions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bulr0/dbt_layer_naming_conventions/", "subreddit_subscribers": 127154, "created_utc": 1694031133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey data engineering,\n\nWanted to reach out to the community to see how other teams are solving UAT / QC process and challenges when using DBT. \n\nBackground:\n\nOne of the main challenges we run into is that we're often developing net new data sources, updating existing based on feature requests, or updating existing based on bugs found by the business/BI team. A frequent problem is that we will onboard a new data source all the way to production and then a month later the business finally reviews the source and requests changes. \n\nWe already have dev and prod targets configured in our DBT profiles and our CI/CD runs with a --target prod. The --target dev is primarily used by devs locally against a different database. Our current branching strategy is feature -&gt; develop -&gt; master with the --target prod only running on master branch.\n\nProposed Ideas:\n\nWe're thinking this could likely be solved by using a feature -&gt; develop -&gt; uat -&gt; master branching strategy. The biggest drawback here is that is a lot of PRs that have to happen in order to get code to production. And because we ultimately need different models in different databases based on the stage of the development lifecycle we're in, this seems like an \"all or nothing\" approach unless we start cherry-picking or commits to include in merges.\n\nAlternative idea: Use the DBT tagging feature to add tags=\\[\"uat\"\\] for models that still need review/sign-off by the business  and then add an --exclude tag:uat from our normal DBT runs. This way we don't have to increase the complexity of our branching process and could easily tell which step a model is in. The UAT models could either be run by using a --select tag:uat from local systems or via CI/CD.\n\nAsk:\n\nSo data engineering, how do you solve this issue? What does your process look like for going to production? How do you (or do you) wait until you get business sign off even if all dev work has been completed? Any thoughts/approaches appreciated!\n\nThanks!", "author_fullname": "t2_dvuofczh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle UAT / business QC with DBT projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bs3z4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694025452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey data engineering,&lt;/p&gt;\n\n&lt;p&gt;Wanted to reach out to the community to see how other teams are solving UAT / QC process and challenges when using DBT. &lt;/p&gt;\n\n&lt;p&gt;Background:&lt;/p&gt;\n\n&lt;p&gt;One of the main challenges we run into is that we&amp;#39;re often developing net new data sources, updating existing based on feature requests, or updating existing based on bugs found by the business/BI team. A frequent problem is that we will onboard a new data source all the way to production and then a month later the business finally reviews the source and requests changes. &lt;/p&gt;\n\n&lt;p&gt;We already have dev and prod targets configured in our DBT profiles and our CI/CD runs with a --target prod. The --target dev is primarily used by devs locally against a different database. Our current branching strategy is feature -&amp;gt; develop -&amp;gt; master with the --target prod only running on master branch.&lt;/p&gt;\n\n&lt;p&gt;Proposed Ideas:&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re thinking this could likely be solved by using a feature -&amp;gt; develop -&amp;gt; uat -&amp;gt; master branching strategy. The biggest drawback here is that is a lot of PRs that have to happen in order to get code to production. And because we ultimately need different models in different databases based on the stage of the development lifecycle we&amp;#39;re in, this seems like an &amp;quot;all or nothing&amp;quot; approach unless we start cherry-picking or commits to include in merges.&lt;/p&gt;\n\n&lt;p&gt;Alternative idea: Use the DBT tagging feature to add tags=[&amp;quot;uat&amp;quot;] for models that still need review/sign-off by the business  and then add an --exclude tag:uat from our normal DBT runs. This way we don&amp;#39;t have to increase the complexity of our branching process and could easily tell which step a model is in. The UAT models could either be run by using a --select tag:uat from local systems or via CI/CD.&lt;/p&gt;\n\n&lt;p&gt;Ask:&lt;/p&gt;\n\n&lt;p&gt;So data engineering, how do you solve this issue? What does your process look like for going to production? How do you (or do you) wait until you get business sign off even if all dev work has been completed? Any thoughts/approaches appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bs3z4", "is_robot_indexable": true, "report_reasons": null, "author": "I_Blame_DevOps", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bs3z4/how_do_you_handle_uat_business_qc_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bs3z4/how_do_you_handle_uat_business_qc_with_dbt/", "subreddit_subscribers": 127154, "created_utc": 1694025452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm doing some data orchestration and have tried numerous approaches for data manipulation and querying. The most \"user-friendly\" I've found for moderate-sized data (up to a few 10s of GB per table) is Duckdb. I use parquet files with views over them to establish and maintain a schema. However, a significant downside of this approach is that the database allows only one writer at a time. The alternative I've been using is a :memory: database, but then I lose the visibility into the schema. An additional challenge is that with performant data orchestration, accessing a database file can be a devops challenge (eg., for multiple kubernetes jobs). Systems like Trino rely on external systems like a hive metadata catalog. How do you use duckdb to build a data warehouse/access a data lake, but avoid the single database living on disk challenges?  \n\n\nAs background, I'm a researcher and use this work to support my biomedical data science work. I'm not looking to build a production environment and want to support minimal infrastructure (within reason). ", "author_fullname": "t2_66wcvsmn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "With all the possibilities for storage, compute, and catalog separation, what works for you?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bliex", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1694010136.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694009908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing some data orchestration and have tried numerous approaches for data manipulation and querying. The most &amp;quot;user-friendly&amp;quot; I&amp;#39;ve found for moderate-sized data (up to a few 10s of GB per table) is Duckdb. I use parquet files with views over them to establish and maintain a schema. However, a significant downside of this approach is that the database allows only one writer at a time. The alternative I&amp;#39;ve been using is a :memory: database, but then I lose the visibility into the schema. An additional challenge is that with performant data orchestration, accessing a database file can be a devops challenge (eg., for multiple kubernetes jobs). Systems like Trino rely on external systems like a hive metadata catalog. How do you use duckdb to build a data warehouse/access a data lake, but avoid the single database living on disk challenges?  &lt;/p&gt;\n\n&lt;p&gt;As background, I&amp;#39;m a researcher and use this work to support my biomedical data science work. I&amp;#39;m not looking to build a production environment and want to support minimal infrastructure (within reason). &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bliex", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo-56267", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bliex/with_all_the_possibilities_for_storage_compute/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bliex/with_all_the_possibilities_for_storage_compute/", "subreddit_subscribers": 127154, "created_utc": 1694009908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is data engineering hiring ramping up all of the sudden? Could be that LinkedIn changed the recruiter search tool algo I suppose. Curious if anyone else is seeing the same.", "author_fullname": "t2_jyzw4d7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Huge influx of recruiters messaging me on LinkedIn starting September 1st despite nothing changing on my LinkedIn\u2026 anyone else?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16c0p6h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694045281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is data engineering hiring ramping up all of the sudden? Could be that LinkedIn changed the recruiter search tool algo I suppose. Curious if anyone else is seeing the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16c0p6h", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Positive-7272", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16c0p6h/huge_influx_of_recruiters_messaging_me_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16c0p6h/huge_influx_of_recruiters_messaging_me_on/", "subreddit_subscribers": 127154, "created_utc": 1694045281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any better way to convert (my)SQL dump to parquet than spinning up fresh db instance, restoring the dump and then using something like pyarrow to query and store the data to parquet? We are getting sql dumps but would like to create a parquet for easier analysis", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL dump to parquet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bqtnu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694022521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any better way to convert (my)SQL dump to parquet than spinning up fresh db instance, restoring the dump and then using something like pyarrow to query and store the data to parquet? We are getting sql dumps but would like to create a parquet for easier analysis&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bqtnu", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bqtnu/sql_dump_to_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bqtnu/sql_dump_to_parquet/", "subreddit_subscribers": 127154, "created_utc": 1694022521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The easy and fast way to solve dependency conflicts in Apache Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_16blbc8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/mWQa5mWpMZ4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"The ExternalPythonOperator: No more dependency conflicts in Apache Airflow\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "The ExternalPythonOperator: No more dependency conflicts in Apache Airflow", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/mWQa5mWpMZ4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"The ExternalPythonOperator: No more dependency conflicts in Apache Airflow\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/mWQa5mWpMZ4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/mWQa5mWpMZ4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"The ExternalPythonOperator: No more dependency conflicts in Apache Airflow\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/16blbc8", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bJm4imjkjDioNZVCTqP-RQCbA_q0pm8XRb8UonCTM_g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694009405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/mWQa5mWpMZ4", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/z5G-zx8FO9pgBzwwPiY1Y_SoPszozeKmAtE1VO8JAtw.jpg?auto=webp&amp;s=857265f0cde73937923b860f20e1e9003df37b1f", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/z5G-zx8FO9pgBzwwPiY1Y_SoPszozeKmAtE1VO8JAtw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=69f35dfaed80eada77c1100640d189b09527c215", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/z5G-zx8FO9pgBzwwPiY1Y_SoPszozeKmAtE1VO8JAtw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4501a0079078a5aa7f658970dada1dbe302ae4e", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/z5G-zx8FO9pgBzwwPiY1Y_SoPszozeKmAtE1VO8JAtw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2c06db25507e1a6c69537ca517896e605c8fc67", "width": 320, "height": 240}], "variants": {}, "id": "NUN2weHdZfK2_NtrA-azV1PCNn7Os1j7nYZCgBI9iq0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16blbc8", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16blbc8/the_easy_and_fast_way_to_solve_dependency/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/mWQa5mWpMZ4", "subreddit_subscribers": 127154, "created_utc": 1694009405.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "The ExternalPythonOperator: No more dependency conflicts in Apache Airflow", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/mWQa5mWpMZ4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"The ExternalPythonOperator: No more dependency conflicts in Apache Airflow\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/mWQa5mWpMZ4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to implement my first data pipeline. I have 7 python scripts that extract and clean api data, then upload to sql server. I\u2019m trying to run these scripts every evening. Should I use Airflow or Alteryx to facilitate this?", "author_fullname": "t2_t45bb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alteryx or airflow for simple API pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bt2oe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694027665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to implement my first data pipeline. I have 7 python scripts that extract and clean api data, then upload to sql server. I\u2019m trying to run these scripts every evening. Should I use Airflow or Alteryx to facilitate this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16bt2oe", "is_robot_indexable": true, "report_reasons": null, "author": "giantdickinmyface", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bt2oe/alteryx_or_airflow_for_simple_api_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bt2oe/alteryx_or_airflow_for_simple_api_pipeline/", "subreddit_subscribers": 127154, "created_utc": 1694027665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nThis is a throwaway from the Midwest USA.\n\nI would like you opinions on what my next move should be. I am looking for a position as a data engineer primarily to improve on my skills in Python and because I like to build systems. I have an engineering degree but not in data or computer engineering so I'm self taught in those regards. These past few months I've been interviewing with a few companies and one of them has recently sent me an offer. I cannot give the name of the company but it is a world leader in the manufacturing sector and I was offered roughly $80k in the midwest. There are three more companies that I am interviewing for and two of them have given me coding tests that should be completed this week. One of those companies is a smaller company working in clean energy and the other is a fortune 500 also in manufacturing. I know that the other two companies would pay about $15k - $25k more per year. That is mostly because I was unprepared in salary negotiations and said the number I was comfortable with instead of fair market value for my area and because the fortune 500 had their salary band posted.\n\nMy question is should I accept my first and only offer while finishing the interviews for the other three companies or just inform them that I have accepted an offer elsewhere and thank them for their time? All these positions are remote and two have headquarters within 2hrs drive of me. Also, and this is huge, I have a felony that is over 9 yrs old but I used to have problems with my background check. My records used to get mixed up because some companies only cross referenced first and last names. I was recently let go from my previous employer and I think the BG check was why, I was a data engineer for them. It was a non-violent offence. As you can imagine getting a job has been hellish for me so please do not judge me on my previous offense.\n\n&amp;#x200B;\n\nThanks", "author_fullname": "t2_hodibk40d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Advice PLZ. Should I continue interviewing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bo4e8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694016228.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;This is a throwaway from the Midwest USA.&lt;/p&gt;\n\n&lt;p&gt;I would like you opinions on what my next move should be. I am looking for a position as a data engineer primarily to improve on my skills in Python and because I like to build systems. I have an engineering degree but not in data or computer engineering so I&amp;#39;m self taught in those regards. These past few months I&amp;#39;ve been interviewing with a few companies and one of them has recently sent me an offer. I cannot give the name of the company but it is a world leader in the manufacturing sector and I was offered roughly $80k in the midwest. There are three more companies that I am interviewing for and two of them have given me coding tests that should be completed this week. One of those companies is a smaller company working in clean energy and the other is a fortune 500 also in manufacturing. I know that the other two companies would pay about $15k - $25k more per year. That is mostly because I was unprepared in salary negotiations and said the number I was comfortable with instead of fair market value for my area and because the fortune 500 had their salary band posted.&lt;/p&gt;\n\n&lt;p&gt;My question is should I accept my first and only offer while finishing the interviews for the other three companies or just inform them that I have accepted an offer elsewhere and thank them for their time? All these positions are remote and two have headquarters within 2hrs drive of me. Also, and this is huge, I have a felony that is over 9 yrs old but I used to have problems with my background check. My records used to get mixed up because some companies only cross referenced first and last names. I was recently let go from my previous employer and I think the BG check was why, I was a data engineer for them. It was a non-violent offence. As you can imagine getting a job has been hellish for me so please do not judge me on my previous offense.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16bo4e8", "is_robot_indexable": true, "report_reasons": null, "author": "Just-Example-598", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bo4e8/career_advice_plz_should_i_continue_interviewing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bo4e8/career_advice_plz_should_i_continue_interviewing/", "subreddit_subscribers": 127154, "created_utc": 1694016228.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Saw this on LinkedIn, it is simple, but seems handy when trying to figure out the arguments for dbt-utils macros\n\nhttps://datacoves.com/post/dbt-utils-cheatsheet", "author_fullname": "t2_vx67of8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt-utils macros cheat sheet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bnjw2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1694014895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saw this on LinkedIn, it is simple, but seems handy when trying to figure out the arguments for dbt-utils macros&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://datacoves.com/post/dbt-utils-cheatsheet\"&gt;https://datacoves.com/post/dbt-utils-cheatsheet&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?auto=webp&amp;s=c72294a445c689d26088117fc5fff4bb905f488d", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7997828bba9f07448ccf1bc5fc04e76033b71944", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7195abbc2a5d6cdc443796e0ed5f9086f845e24d", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33d24d3f4487eaf5c747d62b56db72b2c8b84ebf", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=545aeb79a24b1e25d483c5cd733b6ca413f25389", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7207869fedc316bad07e0b593dcb7995e5baae33", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=42828b93ff9d50484b7639ebe782f4de5c04e40c", "width": 1080, "height": 564}], "variants": {}, "id": "fRjQkbaAYceSqHYVE-NBwTYR2ZoPUS8MUARvVnek75k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16bnjw2", "is_robot_indexable": true, "report_reasons": null, "author": "Hot_Map_7868", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bnjw2/dbtutils_macros_cheat_sheet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bnjw2/dbtutils_macros_cheat_sheet/", "subreddit_subscribers": 127154, "created_utc": 1694014895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an SQL query that I have written, and would like to test in somewhere online for practice. It includes creation of a database. Is there any online free platform I can do this? Create a db then call some SELECT queries and a few more stuff? Thank you in advance", "author_fullname": "t2_7w9il3uc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need of a free online platform to test SQL query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bduox", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693986022.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an SQL query that I have written, and would like to test in somewhere online for practice. It includes creation of a database. Is there any online free platform I can do this? Create a db then call some SELECT queries and a few more stuff? Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16bduox", "is_robot_indexable": true, "report_reasons": null, "author": "Kelvinmwendwa", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bduox/need_of_a_free_online_platform_to_test_sql_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bduox/need_of_a_free_online_platform_to_test_sql_query/", "subreddit_subscribers": 127154, "created_utc": 1693986022.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say I have 50 tables which each need 2 tasks in a workflow. I currently have a initial bulk load workflow with the 100 tasks required. How/would this change with a near real time requirement? Workflow for each table? Continuously run the workflow I already have? Just trying to understand what best practice is here. My apologies in advance if these are dumb questions. I don't have much experience with orchestration. Thanks", "author_fullname": "t2_feymqzcjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Workflow Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bxhcj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694037536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say I have 50 tables which each need 2 tasks in a workflow. I currently have a initial bulk load workflow with the 100 tasks required. How/would this change with a near real time requirement? Workflow for each table? Continuously run the workflow I already have? Just trying to understand what best practice is here. My apologies in advance if these are dumb questions. I don&amp;#39;t have much experience with orchestration. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16bxhcj", "is_robot_indexable": true, "report_reasons": null, "author": "TheConSpooky", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bxhcj/databricks_workflow_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bxhcj/databricks_workflow_question/", "subreddit_subscribers": 127154, "created_utc": 1694037536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j68228u1s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Developing an Elo Based, Data-Driven Ranking System for 2v2 Multiplayer Games", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_16bvy7e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9gfc6HTFq8-nNnNMoMJBOeCQVc9o1ZcyzxfdIWTYyMk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694034075.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@lazarekolebka/developing-an-elo-based-data-driven-ranking-system-for-2v2-multiplayer-games-7689f7d42a53", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?auto=webp&amp;s=c0bd3da4d3611b3e1117c469f22addc41b5df10a", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec62a37864fa95077a312b7028165478361ca38f", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ff7a20bd2e5b26d2ac66339264d6e921c7162479", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=16b4d444ad60041d878c8baeb1c319b3897d2ab9", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd5f2bac301ffdd333d8cb2e53464935edc756b2", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e7d8b8fd5b708813c03f8c9e45b64c30f461dd5", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=52f1bddb3d8d712d063ab62ad3c33e4053ef987a", "width": 1080, "height": 720}], "variants": {}, "id": "LatcCAz20Bz3Y65ssYat9EM3WANhjc5gqXqo7blclXA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "16bvy7e", "is_robot_indexable": true, "report_reasons": null, "author": "Bulky-Violinist7187", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bvy7e/developing_an_elo_based_datadriven_ranking_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@lazarekolebka/developing-an-elo-based-data-driven-ranking-system-for-2v2-multiplayer-games-7689f7d42a53", "subreddit_subscribers": 127154, "created_utc": 1694034075.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. I'm new to Airflow and I just want to know if I'm on the right track.  \nI have an ETL project, with each step in its directory (extract, transform, load), and each directory has a Python script that runs in a Docker container. In other words, I have a docker-compose.yml for each stage, and I simply run 'docker-compose up' one by one to run each stage.  \nNow I want to automate this with Airflow. To do that, I'm running Airflow in Docker as shown in the documentation.  \nThen, to execute each stage, I was planning to use DockerOperators. However, in all the examples and tutorials I've seen, they call a pre-built image and then run it. What I wanted to do is to execute 'docker-compose up --build' for each directory without the need to build the image beforehand and publish it.  \nBut it seems that I need to mount the docker socket inside the Airflow container for this to make it work.  \nDoes that make sense, or am I making it too complicated? ", "author_fullname": "t2_y5ux0rs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "inquiry about Apache Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bp0iq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694018306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I&amp;#39;m new to Airflow and I just want to know if I&amp;#39;m on the right track.&lt;br/&gt;\nI have an ETL project, with each step in its directory (extract, transform, load), and each directory has a Python script that runs in a Docker container. In other words, I have a docker-compose.yml for each stage, and I simply run &amp;#39;docker-compose up&amp;#39; one by one to run each stage.&lt;br/&gt;\nNow I want to automate this with Airflow. To do that, I&amp;#39;m running Airflow in Docker as shown in the documentation.&lt;br/&gt;\nThen, to execute each stage, I was planning to use DockerOperators. However, in all the examples and tutorials I&amp;#39;ve seen, they call a pre-built image and then run it. What I wanted to do is to execute &amp;#39;docker-compose up --build&amp;#39; for each directory without the need to build the image beforehand and publish it.&lt;br/&gt;\nBut it seems that I need to mount the docker socket inside the Airflow container for this to make it work.&lt;br/&gt;\nDoes that make sense, or am I making it too complicated? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16bp0iq", "is_robot_indexable": true, "report_reasons": null, "author": "johnthepostman", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bp0iq/inquiry_about_apache_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bp0iq/inquiry_about_apache_airflow/", "subreddit_subscribers": 127154, "created_utc": 1694018306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone able to help to compile the list of mirena IUD complaints by category &amp; # to the FDA? Is this a public doc record request? Pls &amp; Thank you!", "author_fullname": "t2_7t3g1vqch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data help\u2014FDA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bo28a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694016096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone able to help to compile the list of mirena IUD complaints by category &amp;amp; # to the FDA? Is this a public doc record request? Pls &amp;amp; Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16bo28a", "is_robot_indexable": true, "report_reasons": null, "author": "NYCBoston", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bo28a/data_helpfda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bo28a/data_helpfda/", "subreddit_subscribers": 127154, "created_utc": 1694016096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey data friends\u00a0\ud83d\udc4b\n\nI'm super excited to to share a preview of some new stuff I'm working on: \u2728\u00a0Turntable Discover \u2728\n\nData teams struggle to keep their documentation up to date and actionable. Today they have to stitch together Notion docs, lineage tools, yaml / markdown files and maintain jobs to generate it.\n\nWe\u2019ve built a seamless way to ingest, discover, and share your warehouse's documentation all in one place. Discover is integrated with Github and dbt core, so you can get setup with a magical docs experience in only a few minutes including:\n\n\u26a1\ufe0f Super fast search - search across table names, column, descriptions and canonical sql to find the data you're looking for\n\n\u2728\ufe0f AI-powered semantic search - can't find a substring query that matches? Use semantic search to find data models that are tricky to find (ex: \"reps\" -&gt; \"sales people\")\n\n\ud83d\udd2cColumn-level lineage view - trace back the origins of a particular column and navigate across models with an inline column level lineage view\n\n\ud83d\udd17\u00a0One-click sharing - share documentation in one click with other teammates on the same OAuth domain\n\nWe\u2019re giving Discover to select teams in a private beta before rolling it out broadly. If you\u2019d like to try it out, you can DM, comment below, or sign up on our waitlist ([turntable.so](http://turntable.so/)) Looking forward to hearing your feedback \ud83d\ude4c\n\nCheck out a quick demo of how it works here:\n\n[https://www.youtube.com/watch?v=sY0NefWRpKQ](https://www.youtube.com/watch?v=sY0NefWRpKQ)", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A fast, shareable, and AI-powered docs catalog for dbt Core", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bmnos", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694012748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey data friends\u00a0\ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m super excited to to share a preview of some new stuff I&amp;#39;m working on: \u2728\u00a0Turntable Discover \u2728&lt;/p&gt;\n\n&lt;p&gt;Data teams struggle to keep their documentation up to date and actionable. Today they have to stitch together Notion docs, lineage tools, yaml / markdown files and maintain jobs to generate it.&lt;/p&gt;\n\n&lt;p&gt;We\u2019ve built a seamless way to ingest, discover, and share your warehouse&amp;#39;s documentation all in one place. Discover is integrated with Github and dbt core, so you can get setup with a magical docs experience in only a few minutes including:&lt;/p&gt;\n\n&lt;p&gt;\u26a1\ufe0f Super fast search - search across table names, column, descriptions and canonical sql to find the data you&amp;#39;re looking for&lt;/p&gt;\n\n&lt;p&gt;\u2728\ufe0f AI-powered semantic search - can&amp;#39;t find a substring query that matches? Use semantic search to find data models that are tricky to find (ex: &amp;quot;reps&amp;quot; -&amp;gt; &amp;quot;sales people&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd2cColumn-level lineage view - trace back the origins of a particular column and navigate across models with an inline column level lineage view&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd17\u00a0One-click sharing - share documentation in one click with other teammates on the same OAuth domain&lt;/p&gt;\n\n&lt;p&gt;We\u2019re giving Discover to select teams in a private beta before rolling it out broadly. If you\u2019d like to try it out, you can DM, comment below, or sign up on our waitlist (&lt;a href=\"http://turntable.so/\"&gt;turntable.so&lt;/a&gt;) Looking forward to hearing your feedback \ud83d\ude4c&lt;/p&gt;\n\n&lt;p&gt;Check out a quick demo of how it works here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=sY0NefWRpKQ\"&gt;https://www.youtube.com/watch?v=sY0NefWRpKQ&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16bmnos", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bmnos/a_fast_shareable_and_aipowered_docs_catalog_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bmnos/a_fast_shareable_and_aipowered_docs_catalog_for/", "subreddit_subscribers": 127154, "created_utc": 1694012748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_o09cwtfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE to the BE team after the big schema migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 103, "top_awarded_type": null, "hide_score": false, "name": "t3_16bjqig", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PaY4eaVor0K5qG41vH_ZKoKhOtNusjQVTtbnmMv2FIE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694005337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/bvrdkvelvmmb1.gif", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?format=png8&amp;s=ff9de109865c651f7a8ed9795d2a53334fe2ee51", "width": 640, "height": 472}, "resolutions": [{"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=36f94cd92967e78a6586c2f6fa77e6c8b010efed", "width": 108, "height": 79}, {"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=caaefe3779c68171a688a26586446824fdb654a9", "width": 216, "height": 159}, {"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=5b861f2d2f0154a418c2980251961e9424b07dba", "width": 320, "height": 236}, {"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=6aec22bb58983925a3eafa493cce92f9a848b148", "width": 640, "height": 472}], "variants": {"gif": {"source": {"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?s=29aaf21322f9583d9f6e68cae0a60672654735d4", "width": 640, "height": 472}, "resolutions": [{"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?width=108&amp;crop=smart&amp;s=a5170cc07d2258a43e7097c50781c155d2553756", "width": 108, "height": 79}, {"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?width=216&amp;crop=smart&amp;s=d7d66852ad170042a8ca0fdd8e4dd080d90b76b3", "width": 216, "height": 159}, {"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?width=320&amp;crop=smart&amp;s=47ad4e52aefd16f84bf423e172e5cb3c00ee57e2", "width": 320, "height": 236}, {"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?width=640&amp;crop=smart&amp;s=87dcb98fd60f033a59a445337419662bd1b74405", "width": 640, "height": 472}]}, "mp4": {"source": {"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?format=mp4&amp;s=7b846c84129be35c57de79c9b4e14a6d4ccce9d8", "width": 640, "height": 472}, "resolutions": [{"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?width=108&amp;format=mp4&amp;s=d37102420eefd332828fab0e0632d16626a8b65e", "width": 108, "height": 79}, {"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?width=216&amp;format=mp4&amp;s=26d42ef36a2c0138fb92e6899e8a5dfb183b1691", "width": 216, "height": 159}, {"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?width=320&amp;format=mp4&amp;s=8926639bd61401f8491a031c4eee07dbccfb6217", "width": 320, "height": 236}, {"url": "https://preview.redd.it/bvrdkvelvmmb1.gif?width=640&amp;format=mp4&amp;s=ff45360aefcd9ff9f38641bd212832b170b9b53e", "width": 640, "height": 472}]}}, "id": "hV2JAAQc5W_ojAgiOOE6Pzjv0S82dVPELZ2tZ8l2Hc0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "16bjqig", "is_robot_indexable": true, "report_reasons": null, "author": "itty-bitty-birdy-tb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bjqig/de_to_the_be_team_after_the_big_schema_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/bvrdkvelvmmb1.gif", "subreddit_subscribers": 127154, "created_utc": 1694005337.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}