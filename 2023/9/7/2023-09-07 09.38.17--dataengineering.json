{"kind": "Listing", "data": {"after": "t3_16bnjw2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_owff7qyq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I built an AI-powered tool that can turn any website into an API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 94, "top_awarded_type": null, "hide_score": false, "name": "t3_16brzy2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/d8azdr7diomb1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1598, "scrubber_media_url": "https://v.redd.it/d8azdr7diomb1/DASH_96.mp4", "dash_url": "https://v.redd.it/d8azdr7diomb1/DASHPlaylist.mpd?a=1696671497%2CNTI4ZDA2Y2Q1Y2ZmYWU0ZmI2Yzc1NmQwNTdmMzY4ZjVlZWFjYzFiNWQ4NGEyOTk0OTYwNzVmYmQyZmE5NzQxMQ%3D%3D&amp;v=1&amp;f=sd", "duration": 130, "hls_url": "https://v.redd.it/d8azdr7diomb1/HLSPlaylist.m3u8?a=1696671497%2CNTE3MjUzOWQxNzE1ZDI1ZmQ4OWMxYTI0ZmJmYWZiOTg2MjEwYmUzODRhMzY2Njc0NDhlYjFjMzMxN2Q1OTZlNg%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-wGropRbBsTw5PkWvR6tE6YUX2EFttT1PsIcKew8sVg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694025191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/d8azdr7diomb1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?format=pjpg&amp;auto=webp&amp;s=f93f8ee3fdd671a7d984571ea5b1fbdeaa1e673b", "width": 1598, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4dd7b5deeab38e44ccca169c3cdb9b8d3aa4e583", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e208178b6ace95bca7bb5cefe28bebcc0a9ceee6", "width": 216, "height": 145}, {"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a484d3eb1209307a3e4987349431e53412918d16", "width": 320, "height": 216}, {"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=32dc84092d7ed3cbf49ec9c712fe0f148743ef03", "width": 640, "height": 432}, {"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ca4d1ae8a06f7f43d0fb6dcddb737da857a15ff9", "width": 960, "height": 648}, {"url": "https://external-preview.redd.it/ZOuFJbCs5Cpd4iFQ26ghgiEUW6fEyfTnrtxlvI772AA.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=49b0d028d1b13f869fe03cbaffb86971218dfef0", "width": 1080, "height": 729}], "variants": {}, "id": "5QRgmU7Upi9cdWw454kxztDnZriTUxVoIOvs7oWXNAI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16brzy2", "is_robot_indexable": true, "report_reasons": null, "author": "madredditscientist", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16brzy2/i_built_an_aipowered_tool_that_can_turn_any/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/d8azdr7diomb1", "subreddit_subscribers": 127199, "created_utc": 1694025191.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/d8azdr7diomb1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1598, "scrubber_media_url": "https://v.redd.it/d8azdr7diomb1/DASH_96.mp4", "dash_url": "https://v.redd.it/d8azdr7diomb1/DASHPlaylist.mpd?a=1696671497%2CNTI4ZDA2Y2Q1Y2ZmYWU0ZmI2Yzc1NmQwNTdmMzY4ZjVlZWFjYzFiNWQ4NGEyOTk0OTYwNzVmYmQyZmE5NzQxMQ%3D%3D&amp;v=1&amp;f=sd", "duration": 130, "hls_url": "https://v.redd.it/d8azdr7diomb1/HLSPlaylist.m3u8?a=1696671497%2CNTE3MjUzOWQxNzE1ZDI1ZmQ4OWMxYTI0ZmJmYWZiOTg2MjEwYmUzODRhMzY2Njc0NDhlYjFjMzMxN2Q1OTZlNg%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is data engineering hiring ramping up all of the sudden? Could be that LinkedIn changed the recruiter search tool algo I suppose. Curious if anyone else is seeing the same.", "author_fullname": "t2_jyzw4d7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Huge influx of recruiters messaging me on LinkedIn starting September 1st despite nothing changing on my LinkedIn\u2026 anyone else?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16c0p6h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694045281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is data engineering hiring ramping up all of the sudden? Could be that LinkedIn changed the recruiter search tool algo I suppose. Curious if anyone else is seeing the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16c0p6h", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Positive-7272", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16c0p6h/huge_influx_of_recruiters_messaging_me_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16c0p6h/huge_influx_of_recruiters_messaging_me_on/", "subreddit_subscribers": 127199, "created_utc": 1694045281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been banging my head against the wall for two days, trying to get formulas to pick up column names, which seemed to have the right data type.\n\nThey came with n dashes instead of hyphens!!!!", "author_fullname": "t2_gs67ogqx6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It\u2019s an n-dash!!!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bl4y2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694008960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been banging my head against the wall for two days, trying to get formulas to pick up column names, which seemed to have the right data type.&lt;/p&gt;\n\n&lt;p&gt;They came with n dashes instead of hyphens!!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bl4y2", "is_robot_indexable": true, "report_reasons": null, "author": "LittleBiggle", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bl4y2/its_an_ndash/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bl4y2/its_an_ndash/", "subreddit_subscribers": 127199, "created_utc": 1694008960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I have transitioned from an analyst to DE role over the last 4 years thanks to this amazing community and have been working as a DE (more of an analytics engineer) for around a year. My organization uses airflow on GKE. While working on a project recently, our team ran into some dependency problems and were recommended the use of KubernetesPodOperator. I dont know much about Kubernetes and want to learn it fast to at least get up and running with the operator first and then to learn more about kubernetes eventually. \n\n&amp;#x200B;\n\nHere is what I am trying to learn first: \n\n1. Hosting a container image on a private repo. \n2. Use this image in the podoperator. \n3. handling authentication to gcp so i can read from and write to bigquery tables.\n\nHow would you guys recommend to get up and running? Also I learn best via videos but I couldn't find something in video format around what I am trying to do.", "author_fullname": "t2_na539", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need resource recommendations for learning Kubernetes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bhwo5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694000263.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I have transitioned from an analyst to DE role over the last 4 years thanks to this amazing community and have been working as a DE (more of an analytics engineer) for around a year. My organization uses airflow on GKE. While working on a project recently, our team ran into some dependency problems and were recommended the use of KubernetesPodOperator. I dont know much about Kubernetes and want to learn it fast to at least get up and running with the operator first and then to learn more about kubernetes eventually. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here is what I am trying to learn first: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Hosting a container image on a private repo. &lt;/li&gt;\n&lt;li&gt;Use this image in the podoperator. &lt;/li&gt;\n&lt;li&gt;handling authentication to gcp so i can read from and write to bigquery tables.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;How would you guys recommend to get up and running? Also I learn best via videos but I couldn&amp;#39;t find something in video format around what I am trying to do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16bhwo5", "is_robot_indexable": true, "report_reasons": null, "author": "acid4207", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bhwo5/need_resource_recommendations_for_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bhwo5/need_resource_recommendations_for_learning/", "subreddit_subscribers": 127199, "created_utc": 1694000263.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9ua96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Birmingham City Council goes under after Oracle disaster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_16bk4yo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/vcZxnOpZqHgDRaDOx7DN_lBgQOc42wHJleKZUPorfu4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694006412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "theregister.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.theregister.com/2023/09/05/birmingham_city_council_oracle/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?auto=webp&amp;s=0a305e828a30a6f0c71c29fe7128261795b84be7", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da6738dd559dcd27f5c6395b87b24e35f9f0f90b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e13687e35641b792bc1b4618600421e2f2219c07", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2870609fd4788bdd796d62d476bd37c54e234394", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=762a32e0a6fbe457317ae2f4ca52c22a5139c69f", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=db21c86d150204fb0faf5d99a1319f2829ee8638", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/YQIdF1Voe-1vS2ML4At80bSKyOhPwit0rVQ9yRvZJ5Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fff2e47b3400ab0679156b32901cef4a6faa7a26", "width": 1080, "height": 565}], "variants": {}, "id": "x_FLdS7YQetbIrTzI0wUDbZvSjWYi10bdLE8R6sRSNA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16bk4yo", "is_robot_indexable": true, "report_reasons": null, "author": "uluvboobs", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bk4yo/birmingham_city_council_goes_under_after_oracle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.theregister.com/2023/09/05/birmingham_city_council_oracle/", "subreddit_subscribers": 127199, "created_utc": 1694006412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know that data engineering tends to be more focused on the warehousing and pipeline side of things, but recently, being asked in a couple of interviews that I've been in about maintaining indexes of tables, creation and set up of new tables, SSRS, almost like they are looking for a database analyst or a DBA. I was wondering if anyone has any experience with that because index maintenance can be very different I think than what a typical data engineer would do, wouldn't it?", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have any resources on index maintenance, or DBA related stuff?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bi7jz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694001190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that data engineering tends to be more focused on the warehousing and pipeline side of things, but recently, being asked in a couple of interviews that I&amp;#39;ve been in about maintaining indexes of tables, creation and set up of new tables, SSRS, almost like they are looking for a database analyst or a DBA. I was wondering if anyone has any experience with that because index maintenance can be very different I think than what a typical data engineer would do, wouldn&amp;#39;t it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bi7jz", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bi7jz/does_anyone_have_any_resources_on_index/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bi7jz/does_anyone_have_any_resources_on_index/", "subreddit_subscribers": 127199, "created_utc": 1694001190.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my job as a Data Analyst we receive the same data types from each new client. For example, customers, suppliers, etc. We perform ETL and import cleaned and transformed data into our SQL Server. \n\nEach client sends us their data through SFTP in Excel files which my team and I have to manually pick out the columns we need for each data type. \n\nMy question is how do you best standardize the data we receive from clients? Knowing that you know which specific columns you need for each data type?\n\nMy initial idea is to ask clients to fill out a data collection workbook of the columns we specifically need for each data type. But this solution doesn\u2019t seem optimal.\n\nI also want to use SSIS to help automate the ETL process but I haven\u2019t delved in too much on how to use that tool yet.", "author_fullname": "t2_pz85y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to standardize data ingestion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16c2yqn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1694051709.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694051312.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my job as a Data Analyst we receive the same data types from each new client. For example, customers, suppliers, etc. We perform ETL and import cleaned and transformed data into our SQL Server. &lt;/p&gt;\n\n&lt;p&gt;Each client sends us their data through SFTP in Excel files which my team and I have to manually pick out the columns we need for each data type. &lt;/p&gt;\n\n&lt;p&gt;My question is how do you best standardize the data we receive from clients? Knowing that you know which specific columns you need for each data type?&lt;/p&gt;\n\n&lt;p&gt;My initial idea is to ask clients to fill out a data collection workbook of the columns we specifically need for each data type. But this solution doesn\u2019t seem optimal.&lt;/p&gt;\n\n&lt;p&gt;I also want to use SSIS to help automate the ETL process but I haven\u2019t delved in too much on how to use that tool yet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16c2yqn", "is_robot_indexable": true, "report_reasons": null, "author": "imperialka", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16c2yqn/how_to_standardize_data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16c2yqn/how_to_standardize_data_ingestion/", "subreddit_subscribers": 127199, "created_utc": 1694051312.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I regularly work with a variety of data sources and provide static plots to internal customers.  \n\nI'm familiar with tools like Plotly to create interactive html files, but this requires my handling of the data. I'd like to automate this a bit, but I'm not sure what tools are available for interactive plotting AND data selection. \n\nBasically, I have data that may come from \\*.sie files, \\*.blf, \\*.asc files, \\*.csv files, or other assorted data acquisition tools. Pretty much all of it is time series data.\n\nI think I'd like to create a web interface to view this, but I'm not sure what interactive tools exist to manipulate the channels. A single file may have 50 channels of data. \n\nItems that have crossed my mind:\n\n* An sql server with a database for each datafile. PowerBI to view?\n* Auto-generated web pages from Bokeh, but how do I get user selectable channels?\n* Jupyterhub/Jupyterlab notebooks? \n\n&amp;#x200B;\n\nWhat paid tools exist for engineering?\n\n* I've seen [Aqira](https://www.hbkworld.com/en/products/software/analysis-simulation/durability/aqira-standardize-global-engineering-processes) , but most tools seem to be standalone. \n\nFor reference, an idea of the data source might be a DAQ (data acquisition) tool like\n\n[https://astronovainc.com/our-businesses/test-measurement/](https://astronovainc.com/our-businesses/test-measurement/) \n\n&amp;#x200B;\n\nIf I'm posted in the wrong thread, my apologies. I'm just not really sure where to ask this.", "author_fullname": "t2_3si1p2jc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Providing interactive plots to internal customers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bpt8i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1694020198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I regularly work with a variety of data sources and provide static plots to internal customers.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m familiar with tools like Plotly to create interactive html files, but this requires my handling of the data. I&amp;#39;d like to automate this a bit, but I&amp;#39;m not sure what tools are available for interactive plotting AND data selection. &lt;/p&gt;\n\n&lt;p&gt;Basically, I have data that may come from *.sie files, *.blf, *.asc files, *.csv files, or other assorted data acquisition tools. Pretty much all of it is time series data.&lt;/p&gt;\n\n&lt;p&gt;I think I&amp;#39;d like to create a web interface to view this, but I&amp;#39;m not sure what interactive tools exist to manipulate the channels. A single file may have 50 channels of data. &lt;/p&gt;\n\n&lt;p&gt;Items that have crossed my mind:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;An sql server with a database for each datafile. PowerBI to view?&lt;/li&gt;\n&lt;li&gt;Auto-generated web pages from Bokeh, but how do I get user selectable channels?&lt;/li&gt;\n&lt;li&gt;Jupyterhub/Jupyterlab notebooks? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What paid tools exist for engineering?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ve seen &lt;a href=\"https://www.hbkworld.com/en/products/software/analysis-simulation/durability/aqira-standardize-global-engineering-processes\"&gt;Aqira&lt;/a&gt; , but most tools seem to be standalone. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For reference, an idea of the data source might be a DAQ (data acquisition) tool like&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://astronovainc.com/our-businesses/test-measurement/\"&gt;https://astronovainc.com/our-businesses/test-measurement/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;m posted in the wrong thread, my apologies. I&amp;#39;m just not really sure where to ask this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?auto=webp&amp;s=3a7ccc4fed70f5fe339e0cd18b0f298f562a0dfb", "width": 1200, "height": 645}, "resolutions": [{"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed7e248fa443fbf4a40cf842d4a3550c1e9c4f2b", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d974297f87b1449299a9ff49a4fbe2b7c19cad3", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=71b78ff8bd0bdf95c43fe90bdcd56d413f6f38e8", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c7476c27f219886ac57aafaa693750c5eee321ad", "width": 640, "height": 344}, {"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=487c05c704ee6414bc406fe96d59fafdf86ebc0b", "width": 960, "height": 516}, {"url": "https://external-preview.redd.it/4eLZD7HyxLM1_DBss6OI4a7ojEFIxsv3AZmJhUH6VVg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b253fbe84c01e1aab3039c90fb616770eb90b2f", "width": 1080, "height": 580}], "variants": {}, "id": "VMbu5D8d3p88J0L0CDm3qmZUYWV6_kEFk-qS4gJuykM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bpt8i", "is_robot_indexable": true, "report_reasons": null, "author": "Substantial_Coyote91", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bpt8i/providing_interactive_plots_to_internal_customers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bpt8i/providing_interactive_plots_to_internal_customers/", "subreddit_subscribers": 127199, "created_utc": 1694020198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any better way to convert (my)SQL dump to parquet than spinning up fresh db instance, restoring the dump and then using something like pyarrow to query and store the data to parquet? We are getting sql dumps but would like to create a parquet for easier analysis", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL dump to parquet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bqtnu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694022521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any better way to convert (my)SQL dump to parquet than spinning up fresh db instance, restoring the dump and then using something like pyarrow to query and store the data to parquet? We are getting sql dumps but would like to create a parquet for easier analysis&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bqtnu", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bqtnu/sql_dump_to_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bqtnu/sql_dump_to_parquet/", "subreddit_subscribers": 127199, "created_utc": 1694022521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm doing some data orchestration and have tried numerous approaches for data manipulation and querying. The most \"user-friendly\" I've found for moderate-sized data (up to a few 10s of GB per table) is Duckdb. I use parquet files with views over them to establish and maintain a schema. However, a significant downside of this approach is that the database allows only one writer at a time. The alternative I've been using is a :memory: database, but then I lose the visibility into the schema. An additional challenge is that with performant data orchestration, accessing a database file can be a devops challenge (eg., for multiple kubernetes jobs). Systems like Trino rely on external systems like a hive metadata catalog. How do you use duckdb to build a data warehouse/access a data lake, but avoid the single database living on disk challenges?  \n\n\nAs background, I'm a researcher and use this work to support my biomedical data science work. I'm not looking to build a production environment and want to support minimal infrastructure (within reason). ", "author_fullname": "t2_66wcvsmn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "With all the possibilities for storage, compute, and catalog separation, what works for you?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bliex", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1694010136.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694009908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing some data orchestration and have tried numerous approaches for data manipulation and querying. The most &amp;quot;user-friendly&amp;quot; I&amp;#39;ve found for moderate-sized data (up to a few 10s of GB per table) is Duckdb. I use parquet files with views over them to establish and maintain a schema. However, a significant downside of this approach is that the database allows only one writer at a time. The alternative I&amp;#39;ve been using is a :memory: database, but then I lose the visibility into the schema. An additional challenge is that with performant data orchestration, accessing a database file can be a devops challenge (eg., for multiple kubernetes jobs). Systems like Trino rely on external systems like a hive metadata catalog. How do you use duckdb to build a data warehouse/access a data lake, but avoid the single database living on disk challenges?  &lt;/p&gt;\n\n&lt;p&gt;As background, I&amp;#39;m a researcher and use this work to support my biomedical data science work. I&amp;#39;m not looking to build a production environment and want to support minimal infrastructure (within reason). &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bliex", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo-56267", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bliex/with_all_the_possibilities_for_storage_compute/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bliex/with_all_the_possibilities_for_storage_compute/", "subreddit_subscribers": 127199, "created_utc": 1694009908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The easy and fast way to solve dependency conflicts in Apache Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_16blbc8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/mWQa5mWpMZ4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"The ExternalPythonOperator: No more dependency conflicts in Apache Airflow\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "The ExternalPythonOperator: No more dependency conflicts in Apache Airflow", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/mWQa5mWpMZ4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"The ExternalPythonOperator: No more dependency conflicts in Apache Airflow\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/mWQa5mWpMZ4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/mWQa5mWpMZ4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"The ExternalPythonOperator: No more dependency conflicts in Apache Airflow\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/16blbc8", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bJm4imjkjDioNZVCTqP-RQCbA_q0pm8XRb8UonCTM_g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694009405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/mWQa5mWpMZ4", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/z5G-zx8FO9pgBzwwPiY1Y_SoPszozeKmAtE1VO8JAtw.jpg?auto=webp&amp;s=857265f0cde73937923b860f20e1e9003df37b1f", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/z5G-zx8FO9pgBzwwPiY1Y_SoPszozeKmAtE1VO8JAtw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=69f35dfaed80eada77c1100640d189b09527c215", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/z5G-zx8FO9pgBzwwPiY1Y_SoPszozeKmAtE1VO8JAtw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4501a0079078a5aa7f658970dada1dbe302ae4e", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/z5G-zx8FO9pgBzwwPiY1Y_SoPszozeKmAtE1VO8JAtw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2c06db25507e1a6c69537ca517896e605c8fc67", "width": 320, "height": 240}], "variants": {}, "id": "NUN2weHdZfK2_NtrA-azV1PCNn7Os1j7nYZCgBI9iq0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16blbc8", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16blbc8/the_easy_and_fast_way_to_solve_dependency/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/mWQa5mWpMZ4", "subreddit_subscribers": 127199, "created_utc": 1694009405.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "The ExternalPythonOperator: No more dependency conflicts in Apache Airflow", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/mWQa5mWpMZ4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"The ExternalPythonOperator: No more dependency conflicts in Apache Airflow\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/mWQa5mWpMZ4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company's data community is pretty much split in two. Over the past year, the majority of the company (lead by AWS ProServe) worked to build up a data mesh architecture with a custom platform used to find and share data in AWS (handling cross account roles and related infrastructure), Data storage is predominantly S3 and compute is predominantly Redshift Serverless. They're also in the final stages of buying a commercial catalog for data governance.\n\nHonestly, *A LOT* of effort was spent on this development, but at the end of the day, the custom platform isn't differentiating, and there's still a lot of manual work needed to connect to the data to run compute on it. Another half of the data community recently decided that they needed a simper out of the box solution, and decided to PoC Databricks, and they'll be building out a data lake with the Unity Catalog, nothing custom, just an out of the box implementation. \n\nHowever, I'm trying to premptivly think of how these two systems can integrate with each other? I'm very experienced in AWS, but much less-so in Databricks... I can easily imagine how S3 data could be shared with a Databricks consuming account, however, from my understanding, Databricks needs files in it's data lake to be of type Delta Lake. Does this mean that we should encourage all data producers to store their data as Delta Lake files? Doing the conversion on an as-needed basis might lead to data duplication and data silos.\n\nAlso, what challenges arise from using Unity Catalog alongside a second data catalog? It seems like a bad idea, but maybe a two-state-solution like this could work? I'm curious if anyone else ever worked around a split ecosystem like this? ", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How easily would Databricks integrate into a mostly AWS backed data ecosystem?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16c4yrt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694056902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company&amp;#39;s data community is pretty much split in two. Over the past year, the majority of the company (lead by AWS ProServe) worked to build up a data mesh architecture with a custom platform used to find and share data in AWS (handling cross account roles and related infrastructure), Data storage is predominantly S3 and compute is predominantly Redshift Serverless. They&amp;#39;re also in the final stages of buying a commercial catalog for data governance.&lt;/p&gt;\n\n&lt;p&gt;Honestly, &lt;em&gt;A LOT&lt;/em&gt; of effort was spent on this development, but at the end of the day, the custom platform isn&amp;#39;t differentiating, and there&amp;#39;s still a lot of manual work needed to connect to the data to run compute on it. Another half of the data community recently decided that they needed a simper out of the box solution, and decided to PoC Databricks, and they&amp;#39;ll be building out a data lake with the Unity Catalog, nothing custom, just an out of the box implementation. &lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m trying to premptivly think of how these two systems can integrate with each other? I&amp;#39;m very experienced in AWS, but much less-so in Databricks... I can easily imagine how S3 data could be shared with a Databricks consuming account, however, from my understanding, Databricks needs files in it&amp;#39;s data lake to be of type Delta Lake. Does this mean that we should encourage all data producers to store their data as Delta Lake files? Doing the conversion on an as-needed basis might lead to data duplication and data silos.&lt;/p&gt;\n\n&lt;p&gt;Also, what challenges arise from using Unity Catalog alongside a second data catalog? It seems like a bad idea, but maybe a two-state-solution like this could work? I&amp;#39;m curious if anyone else ever worked around a split ecosystem like this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16c4yrt", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16c4yrt/how_easily_would_databricks_integrate_into_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16c4yrt/how_easily_would_databricks_integrate_into_a/", "subreddit_subscribers": 127199, "created_utc": 1694056902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data architect for my company and have recently been pulled in to a dept I've not worked with before that is using ADF pipelines for data ingestion through to our CRM and Analytics dbs.  I noticed that there are tables on a Azure SQL Server that have no primary/foreign keys, and no indexes at all. Is this common practice?  I've looked at the pipelines, and yes there is some use of databricks to create dataframes off of the data in the tables, but I still feel like this might be slightly inefficient. I don't have a big DE background, so I figured I'd check with you all. Common practice because you can gain efficiencies through creating python scripts to pull and manage the data from the SQL dbs?  For clarification, these are not temp tables. These are production tables attempting to manage millions of records a day.", "author_fullname": "t2_2v1p3nx2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Un-indexed Azure SQL Server tables used in ADF Pipelines (Is this common?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16c3ezp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694052533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data architect for my company and have recently been pulled in to a dept I&amp;#39;ve not worked with before that is using ADF pipelines for data ingestion through to our CRM and Analytics dbs.  I noticed that there are tables on a Azure SQL Server that have no primary/foreign keys, and no indexes at all. Is this common practice?  I&amp;#39;ve looked at the pipelines, and yes there is some use of databricks to create dataframes off of the data in the tables, but I still feel like this might be slightly inefficient. I don&amp;#39;t have a big DE background, so I figured I&amp;#39;d check with you all. Common practice because you can gain efficiencies through creating python scripts to pull and manage the data from the SQL dbs?  For clarification, these are not temp tables. These are production tables attempting to manage millions of records a day.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Architect", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16c3ezp", "is_robot_indexable": true, "report_reasons": null, "author": "No-Current-7884", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16c3ezp/unindexed_azure_sql_server_tables_used_in_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16c3ezp/unindexed_azure_sql_server_tables_used_in_adf/", "subreddit_subscribers": 127199, "created_utc": 1694052533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data engineer setting up dbt for a group of analysts and data scientists. The naming conventions for transformational layers suggested in the dbt docs are:\n\n1. staging\n2. intermediate\n3. marts\n\nTbh, I don't really like these names. Especially staging -- its generic and conflicts with the term staging as a reference to the environment. In past projects (that did not use dbt), I used different naming conventions, but were functionally the same as dbt's in terms of organization:\n\n1. crbo (common reporting business objects)\n   1. this could could be prefixed as stripe\\_crbo, and is almost always suffixed with something like \\_transactions or \\_refunds\n2. core\n   1. also can be prefixed or suffixed in the same manner as crbo\n3. any name that describes the data set, and then its aggregation level\n   1. i.e. stripe\\_transactions\\_by\\_country\\_plan\n\nHow do you structure your dbt layers and what naming conventions do you use? I will probably start out with dbt's conventions, but am interested in other approaches!", "author_fullname": "t2_1p505jz8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Layer Naming Conventions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bulr0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694031133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data engineer setting up dbt for a group of analysts and data scientists. The naming conventions for transformational layers suggested in the dbt docs are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;staging&lt;/li&gt;\n&lt;li&gt;intermediate&lt;/li&gt;\n&lt;li&gt;marts&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Tbh, I don&amp;#39;t really like these names. Especially staging -- its generic and conflicts with the term staging as a reference to the environment. In past projects (that did not use dbt), I used different naming conventions, but were functionally the same as dbt&amp;#39;s in terms of organization:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;crbo (common reporting business objects)\n\n&lt;ol&gt;\n&lt;li&gt;this could could be prefixed as stripe_crbo, and is almost always suffixed with something like _transactions or _refunds&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;core\n\n&lt;ol&gt;\n&lt;li&gt;also can be prefixed or suffixed in the same manner as crbo&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;any name that describes the data set, and then its aggregation level\n\n&lt;ol&gt;\n&lt;li&gt;i.e. stripe_transactions_by_country_plan&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;How do you structure your dbt layers and what naming conventions do you use? I will probably start out with dbt&amp;#39;s conventions, but am interested in other approaches!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bulr0", "is_robot_indexable": true, "report_reasons": null, "author": "Fredonia1988", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bulr0/dbt_layer_naming_conventions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bulr0/dbt_layer_naming_conventions/", "subreddit_subscribers": 127199, "created_utc": 1694031133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey data engineering,\n\nWanted to reach out to the community to see how other teams are solving UAT / QC process and challenges when using DBT. \n\nBackground:\n\nOne of the main challenges we run into is that we're often developing net new data sources, updating existing based on feature requests, or updating existing based on bugs found by the business/BI team. A frequent problem is that we will onboard a new data source all the way to production and then a month later the business finally reviews the source and requests changes. \n\nWe already have dev and prod targets configured in our DBT profiles and our CI/CD runs with a --target prod. The --target dev is primarily used by devs locally against a different database. Our current branching strategy is feature -&gt; develop -&gt; master with the --target prod only running on master branch.\n\nProposed Ideas:\n\nWe're thinking this could likely be solved by using a feature -&gt; develop -&gt; uat -&gt; master branching strategy. The biggest drawback here is that is a lot of PRs that have to happen in order to get code to production. And because we ultimately need different models in different databases based on the stage of the development lifecycle we're in, this seems like an \"all or nothing\" approach unless we start cherry-picking or commits to include in merges.\n\nAlternative idea: Use the DBT tagging feature to add tags=\\[\"uat\"\\] for models that still need review/sign-off by the business  and then add an --exclude tag:uat from our normal DBT runs. This way we don't have to increase the complexity of our branching process and could easily tell which step a model is in. The UAT models could either be run by using a --select tag:uat from local systems or via CI/CD.\n\nAsk:\n\nSo data engineering, how do you solve this issue? What does your process look like for going to production? How do you (or do you) wait until you get business sign off even if all dev work has been completed? Any thoughts/approaches appreciated!\n\nThanks!", "author_fullname": "t2_dvuofczh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle UAT / business QC with DBT projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bs3z4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694025452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey data engineering,&lt;/p&gt;\n\n&lt;p&gt;Wanted to reach out to the community to see how other teams are solving UAT / QC process and challenges when using DBT. &lt;/p&gt;\n\n&lt;p&gt;Background:&lt;/p&gt;\n\n&lt;p&gt;One of the main challenges we run into is that we&amp;#39;re often developing net new data sources, updating existing based on feature requests, or updating existing based on bugs found by the business/BI team. A frequent problem is that we will onboard a new data source all the way to production and then a month later the business finally reviews the source and requests changes. &lt;/p&gt;\n\n&lt;p&gt;We already have dev and prod targets configured in our DBT profiles and our CI/CD runs with a --target prod. The --target dev is primarily used by devs locally against a different database. Our current branching strategy is feature -&amp;gt; develop -&amp;gt; master with the --target prod only running on master branch.&lt;/p&gt;\n\n&lt;p&gt;Proposed Ideas:&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re thinking this could likely be solved by using a feature -&amp;gt; develop -&amp;gt; uat -&amp;gt; master branching strategy. The biggest drawback here is that is a lot of PRs that have to happen in order to get code to production. And because we ultimately need different models in different databases based on the stage of the development lifecycle we&amp;#39;re in, this seems like an &amp;quot;all or nothing&amp;quot; approach unless we start cherry-picking or commits to include in merges.&lt;/p&gt;\n\n&lt;p&gt;Alternative idea: Use the DBT tagging feature to add tags=[&amp;quot;uat&amp;quot;] for models that still need review/sign-off by the business  and then add an --exclude tag:uat from our normal DBT runs. This way we don&amp;#39;t have to increase the complexity of our branching process and could easily tell which step a model is in. The UAT models could either be run by using a --select tag:uat from local systems or via CI/CD.&lt;/p&gt;\n\n&lt;p&gt;Ask:&lt;/p&gt;\n\n&lt;p&gt;So data engineering, how do you solve this issue? What does your process look like for going to production? How do you (or do you) wait until you get business sign off even if all dev work has been completed? Any thoughts/approaches appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bs3z4", "is_robot_indexable": true, "report_reasons": null, "author": "I_Blame_DevOps", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bs3z4/how_do_you_handle_uat_business_qc_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bs3z4/how_do_you_handle_uat_business_qc_with_dbt/", "subreddit_subscribers": 127199, "created_utc": 1694025452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello r/dataengineering!\n\nI'm on a quest to find the best managed EL tools out there. Our home-grown Python scripts have been a significant source of headaches, and with a small team, self-hosting just isn't viable for us. We are keenly interested in cloud-based solutions to make our life easier.\n\nSo far, here's what's on our radar:\n\n* **Fivetran:** It appears fairly production-ready and robust, but I have reservations about it being a proprietary system. Additionally, the costs seem to rise significantly given the relatively high active row count (IoT business).\n* **Airbyte:** While it seems promising, I've observed numerous issues on their GitHub. Moreover, they're in the midst of rolling out a major update with their V2 destinations.\n* **Meltano:** I recently discovered they have a \"Meltano Cloud\" offering currently in its open beta. This could be a potential game-changer, but I would love to hear experiences from anyone who has used it.\n\nGiven how rapidly the tech landscape changes, I'm sure there might be some gems out there I'm unaware of in 2023. Any insights, recommendations, or experiences with the aforementioned tools (or others) would be hugely appreciated!\n\nThanks in advance!", "author_fullname": "t2_89x2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Worthwhile managed EL (Extract-Load) tools in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16cagf7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694075654.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on a quest to find the best managed EL tools out there. Our home-grown Python scripts have been a significant source of headaches, and with a small team, self-hosting just isn&amp;#39;t viable for us. We are keenly interested in cloud-based solutions to make our life easier.&lt;/p&gt;\n\n&lt;p&gt;So far, here&amp;#39;s what&amp;#39;s on our radar:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Fivetran:&lt;/strong&gt; It appears fairly production-ready and robust, but I have reservations about it being a proprietary system. Additionally, the costs seem to rise significantly given the relatively high active row count (IoT business).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Airbyte:&lt;/strong&gt; While it seems promising, I&amp;#39;ve observed numerous issues on their GitHub. Moreover, they&amp;#39;re in the midst of rolling out a major update with their V2 destinations.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Meltano:&lt;/strong&gt; I recently discovered they have a &amp;quot;Meltano Cloud&amp;quot; offering currently in its open beta. This could be a potential game-changer, but I would love to hear experiences from anyone who has used it.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Given how rapidly the tech landscape changes, I&amp;#39;m sure there might be some gems out there I&amp;#39;m unaware of in 2023. Any insights, recommendations, or experiences with the aforementioned tools (or others) would be hugely appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16cagf7", "is_robot_indexable": true, "report_reasons": null, "author": "Pranasas", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16cagf7/worthwhile_managed_el_extractload_tools_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16cagf7/worthwhile_managed_el_extractload_tools_in_2023/", "subreddit_subscribers": 127199, "created_utc": 1694075654.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to implement my first data pipeline. I have 7 python scripts that extract and clean api data, then upload to sql server. I\u2019m trying to run these scripts every evening. Should I use Airflow or Alteryx to facilitate this?", "author_fullname": "t2_t45bb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alteryx or airflow for simple API pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bt2oe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694027665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to implement my first data pipeline. I have 7 python scripts that extract and clean api data, then upload to sql server. I\u2019m trying to run these scripts every evening. Should I use Airflow or Alteryx to facilitate this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16bt2oe", "is_robot_indexable": true, "report_reasons": null, "author": "giantdickinmyface", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bt2oe/alteryx_or_airflow_for_simple_api_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bt2oe/alteryx_or_airflow_for_simple_api_pipeline/", "subreddit_subscribers": 127199, "created_utc": 1694027665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I was asked to create an Audit Trail on the Master table where all the information about a specific product is stored , so I used a trigger to capture all the changes that were made on the table , the trigger was working perfectly in the testing environment but in production during the night time something happened and all the process were slowed and stopped , the db owner deleted the trigger which in turn deleted all the log history of the trigger, so no I am unable to figure out why that happened, can you guys please guide me", "author_fullname": "t2_4p2t0v0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using a trigger not the way to capture Audit Trail?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16c7ouo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694065538.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I was asked to create an Audit Trail on the Master table where all the information about a specific product is stored , so I used a trigger to capture all the changes that were made on the table , the trigger was working perfectly in the testing environment but in production during the night time something happened and all the process were slowed and stopped , the db owner deleted the trigger which in turn deleted all the log history of the trigger, so no I am unable to figure out why that happened, can you guys please guide me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16c7ouo", "is_robot_indexable": true, "report_reasons": null, "author": "omghag18", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16c7ouo/using_a_trigger_not_the_way_to_capture_audit_trail/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16c7ouo/using_a_trigger_not_the_way_to_capture_audit_trail/", "subreddit_subscribers": 127199, "created_utc": 1694065538.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys,\nIs there a way to find out who executed which query at a workspace level? Is there any logs which can be read into a dataframe to extract this  information? I can see the users in query history tab within the UI so is it possible pull this info into a dataframe?\nThank you!", "author_fullname": "t2_f1s7yw3kw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks - Identify who executed the query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16c61uw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694060162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys,\nIs there a way to find out who executed which query at a workspace level? Is there any logs which can be read into a dataframe to extract this  information? I can see the users in query history tab within the UI so is it possible pull this info into a dataframe?\nThank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16c61uw", "is_robot_indexable": true, "report_reasons": null, "author": "fusebox12345", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16c61uw/databricks_identify_who_executed_the_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16c61uw/databricks_identify_who_executed_the_query/", "subreddit_subscribers": 127199, "created_utc": 1694060162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say I have 50 tables which each need 2 tasks in a workflow. I currently have a initial bulk load workflow with the 100 tasks required. How/would this change with a near real time requirement? Workflow for each table? Continuously run the workflow I already have? Just trying to understand what best practice is here. My apologies in advance if these are dumb questions. I don't have much experience with orchestration. Thanks", "author_fullname": "t2_feymqzcjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Workflow Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bxhcj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694037536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say I have 50 tables which each need 2 tasks in a workflow. I currently have a initial bulk load workflow with the 100 tasks required. How/would this change with a near real time requirement? Workflow for each table? Continuously run the workflow I already have? Just trying to understand what best practice is here. My apologies in advance if these are dumb questions. I don&amp;#39;t have much experience with orchestration. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16bxhcj", "is_robot_indexable": true, "report_reasons": null, "author": "TheConSpooky", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bxhcj/databricks_workflow_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bxhcj/databricks_workflow_question/", "subreddit_subscribers": 127199, "created_utc": 1694037536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j68228u1s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Developing an Elo Based, Data-Driven Ranking System for 2v2 Multiplayer Games", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_16bvy7e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9gfc6HTFq8-nNnNMoMJBOeCQVc9o1ZcyzxfdIWTYyMk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694034075.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@lazarekolebka/developing-an-elo-based-data-driven-ranking-system-for-2v2-multiplayer-games-7689f7d42a53", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?auto=webp&amp;s=c0bd3da4d3611b3e1117c469f22addc41b5df10a", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec62a37864fa95077a312b7028165478361ca38f", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ff7a20bd2e5b26d2ac66339264d6e921c7162479", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=16b4d444ad60041d878c8baeb1c319b3897d2ab9", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd5f2bac301ffdd333d8cb2e53464935edc756b2", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e7d8b8fd5b708813c03f8c9e45b64c30f461dd5", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=52f1bddb3d8d712d063ab62ad3c33e4053ef987a", "width": 1080, "height": 720}], "variants": {}, "id": "LatcCAz20Bz3Y65ssYat9EM3WANhjc5gqXqo7blclXA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16bvy7e", "is_robot_indexable": true, "report_reasons": null, "author": "Bulky-Violinist7187", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bvy7e/developing_an_elo_based_datadriven_ranking_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@lazarekolebka/developing-an-elo-based-data-driven-ranking-system-for-2v2-multiplayer-games-7689f7d42a53", "subreddit_subscribers": 127199, "created_utc": 1694034075.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. I'm new to Airflow and I just want to know if I'm on the right track.  \nI have an ETL project, with each step in its directory (extract, transform, load), and each directory has a Python script that runs in a Docker container. In other words, I have a docker-compose.yml for each stage, and I simply run 'docker-compose up' one by one to run each stage.  \nNow I want to automate this with Airflow. To do that, I'm running Airflow in Docker as shown in the documentation.  \nThen, to execute each stage, I was planning to use DockerOperators. However, in all the examples and tutorials I've seen, they call a pre-built image and then run it. What I wanted to do is to execute 'docker-compose up --build' for each directory without the need to build the image beforehand and publish it.  \nBut it seems that I need to mount the docker socket inside the Airflow container for this to make it work.  \nDoes that make sense, or am I making it too complicated? ", "author_fullname": "t2_y5ux0rs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "inquiry about Apache Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bp0iq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694018306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I&amp;#39;m new to Airflow and I just want to know if I&amp;#39;m on the right track.&lt;br/&gt;\nI have an ETL project, with each step in its directory (extract, transform, load), and each directory has a Python script that runs in a Docker container. In other words, I have a docker-compose.yml for each stage, and I simply run &amp;#39;docker-compose up&amp;#39; one by one to run each stage.&lt;br/&gt;\nNow I want to automate this with Airflow. To do that, I&amp;#39;m running Airflow in Docker as shown in the documentation.&lt;br/&gt;\nThen, to execute each stage, I was planning to use DockerOperators. However, in all the examples and tutorials I&amp;#39;ve seen, they call a pre-built image and then run it. What I wanted to do is to execute &amp;#39;docker-compose up --build&amp;#39; for each directory without the need to build the image beforehand and publish it.&lt;br/&gt;\nBut it seems that I need to mount the docker socket inside the Airflow container for this to make it work.&lt;br/&gt;\nDoes that make sense, or am I making it too complicated? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16bp0iq", "is_robot_indexable": true, "report_reasons": null, "author": "johnthepostman", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bp0iq/inquiry_about_apache_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bp0iq/inquiry_about_apache_airflow/", "subreddit_subscribers": 127199, "created_utc": 1694018306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nThis is a throwaway from the Midwest USA.\n\nI would like you opinions on what my next move should be. I am looking for a position as a data engineer primarily to improve on my skills in Python and because I like to build systems. I have an engineering degree but not in data or computer engineering so I'm self taught in those regards. These past few months I've been interviewing with a few companies and one of them has recently sent me an offer. I cannot give the name of the company but it is a world leader in the manufacturing sector and I was offered roughly $80k in the midwest. There are three more companies that I am interviewing for and two of them have given me coding tests that should be completed this week. One of those companies is a smaller company working in clean energy and the other is a fortune 500 also in manufacturing. I know that the other two companies would pay about $15k - $25k more per year. That is mostly because I was unprepared in salary negotiations and said the number I was comfortable with instead of fair market value for my area and because the fortune 500 had their salary band posted.\n\nMy question is should I accept my first and only offer while finishing the interviews for the other three companies or just inform them that I have accepted an offer elsewhere and thank them for their time? All these positions are remote and two have headquarters within 2hrs drive of me. Also, and this is huge, I have a felony that is over 9 yrs old but I used to have problems with my background check. My records used to get mixed up because some companies only cross referenced first and last names. I was recently let go from my previous employer and I think the BG check was why, I was a data engineer for them. It was a non-violent offence. As you can imagine getting a job has been hellish for me so please do not judge me on my previous offense.\n\n&amp;#x200B;\n\nThanks", "author_fullname": "t2_hodibk40d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Advice PLZ. Should I continue interviewing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bo4e8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694016228.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;This is a throwaway from the Midwest USA.&lt;/p&gt;\n\n&lt;p&gt;I would like you opinions on what my next move should be. I am looking for a position as a data engineer primarily to improve on my skills in Python and because I like to build systems. I have an engineering degree but not in data or computer engineering so I&amp;#39;m self taught in those regards. These past few months I&amp;#39;ve been interviewing with a few companies and one of them has recently sent me an offer. I cannot give the name of the company but it is a world leader in the manufacturing sector and I was offered roughly $80k in the midwest. There are three more companies that I am interviewing for and two of them have given me coding tests that should be completed this week. One of those companies is a smaller company working in clean energy and the other is a fortune 500 also in manufacturing. I know that the other two companies would pay about $15k - $25k more per year. That is mostly because I was unprepared in salary negotiations and said the number I was comfortable with instead of fair market value for my area and because the fortune 500 had their salary band posted.&lt;/p&gt;\n\n&lt;p&gt;My question is should I accept my first and only offer while finishing the interviews for the other three companies or just inform them that I have accepted an offer elsewhere and thank them for their time? All these positions are remote and two have headquarters within 2hrs drive of me. Also, and this is huge, I have a felony that is over 9 yrs old but I used to have problems with my background check. My records used to get mixed up because some companies only cross referenced first and last names. I was recently let go from my previous employer and I think the BG check was why, I was a data engineer for them. It was a non-violent offence. As you can imagine getting a job has been hellish for me so please do not judge me on my previous offense.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16bo4e8", "is_robot_indexable": true, "report_reasons": null, "author": "Just-Example-598", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bo4e8/career_advice_plz_should_i_continue_interviewing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bo4e8/career_advice_plz_should_i_continue_interviewing/", "subreddit_subscribers": 127199, "created_utc": 1694016228.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone able to help to compile the list of mirena IUD complaints by category &amp; # to the FDA? Is this a public doc record request? Pls &amp; Thank you!", "author_fullname": "t2_7t3g1vqch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data help\u2014FDA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bo28a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694016096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone able to help to compile the list of mirena IUD complaints by category &amp;amp; # to the FDA? Is this a public doc record request? Pls &amp;amp; Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16bo28a", "is_robot_indexable": true, "report_reasons": null, "author": "NYCBoston", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bo28a/data_helpfda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bo28a/data_helpfda/", "subreddit_subscribers": 127199, "created_utc": 1694016096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Saw this on LinkedIn, it is simple, but seems handy when trying to figure out the arguments for dbt-utils macros\n\nhttps://datacoves.com/post/dbt-utils-cheatsheet", "author_fullname": "t2_vx67of8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt-utils macros cheat sheet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bnjw2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1694014895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saw this on LinkedIn, it is simple, but seems handy when trying to figure out the arguments for dbt-utils macros&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://datacoves.com/post/dbt-utils-cheatsheet\"&gt;https://datacoves.com/post/dbt-utils-cheatsheet&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?auto=webp&amp;s=c72294a445c689d26088117fc5fff4bb905f488d", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7997828bba9f07448ccf1bc5fc04e76033b71944", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7195abbc2a5d6cdc443796e0ed5f9086f845e24d", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33d24d3f4487eaf5c747d62b56db72b2c8b84ebf", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=545aeb79a24b1e25d483c5cd733b6ca413f25389", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7207869fedc316bad07e0b593dcb7995e5baae33", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/MR_Bvoy9qfm0qvUoHBj721WQcnmlyH-U6UzwBgkw76U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=42828b93ff9d50484b7639ebe782f4de5c04e40c", "width": 1080, "height": 564}], "variants": {}, "id": "fRjQkbaAYceSqHYVE-NBwTYR2ZoPUS8MUARvVnek75k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16bnjw2", "is_robot_indexable": true, "report_reasons": null, "author": "Hot_Map_7868", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bnjw2/dbtutils_macros_cheat_sheet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bnjw2/dbtutils_macros_cheat_sheet/", "subreddit_subscribers": 127199, "created_utc": 1694014895.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}