{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is data engineering hiring ramping up all of the sudden? Could be that LinkedIn changed the recruiter search tool algo I suppose. Curious if anyone else is seeing the same.", "author_fullname": "t2_jyzw4d7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Huge influx of recruiters messaging me on LinkedIn starting September 1st despite nothing changing on my LinkedIn\u2026 anyone else?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16c0p6h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694045281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is data engineering hiring ramping up all of the sudden? Could be that LinkedIn changed the recruiter search tool algo I suppose. Curious if anyone else is seeing the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16c0p6h", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Positive-7272", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16c0p6h/huge_influx_of_recruiters_messaging_me_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16c0p6h/huge_influx_of_recruiters_messaging_me_on/", "subreddit_subscribers": 127274, "created_utc": 1694045281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello r/dataengineering!\n\nI'm on a quest to find the best managed EL tools out there. Our home-grown Python scripts have been a significant source of headaches, and with a small team, self-hosting just isn't viable for us. We are keenly interested in cloud-based solutions to make our life easier.\n\nSo far, here's what's on our radar:\n\n* **Fivetran:** It appears fairly production-ready and robust, but I have reservations about it being a proprietary system. Additionally, the costs seem to rise significantly given the relatively high active row count (IoT business).\n* **Airbyte:** While it seems promising, I've observed numerous issues on their GitHub. Moreover, they're in the midst of rolling out a major update with their V2 destinations.\n* **Meltano:** I recently discovered they have a \"Meltano Cloud\" offering currently in its open beta. This could be a potential game-changer, but I would love to hear experiences from anyone who has used it.\n\nGiven how rapidly the tech landscape changes, I'm sure there might be some gems out there I'm unaware of in 2023. Any insights, recommendations, or experiences with the aforementioned tools (or others) would be hugely appreciated!\n\nThanks in advance!", "author_fullname": "t2_89x2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Worthwhile managed EL (Extract-Load) tools in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16cagf7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694075654.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on a quest to find the best managed EL tools out there. Our home-grown Python scripts have been a significant source of headaches, and with a small team, self-hosting just isn&amp;#39;t viable for us. We are keenly interested in cloud-based solutions to make our life easier.&lt;/p&gt;\n\n&lt;p&gt;So far, here&amp;#39;s what&amp;#39;s on our radar:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Fivetran:&lt;/strong&gt; It appears fairly production-ready and robust, but I have reservations about it being a proprietary system. Additionally, the costs seem to rise significantly given the relatively high active row count (IoT business).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Airbyte:&lt;/strong&gt; While it seems promising, I&amp;#39;ve observed numerous issues on their GitHub. Moreover, they&amp;#39;re in the midst of rolling out a major update with their V2 destinations.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Meltano:&lt;/strong&gt; I recently discovered they have a &amp;quot;Meltano Cloud&amp;quot; offering currently in its open beta. This could be a potential game-changer, but I would love to hear experiences from anyone who has used it.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Given how rapidly the tech landscape changes, I&amp;#39;m sure there might be some gems out there I&amp;#39;m unaware of in 2023. Any insights, recommendations, or experiences with the aforementioned tools (or others) would be hugely appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16cagf7", "is_robot_indexable": true, "report_reasons": null, "author": "Pranasas", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16cagf7/worthwhile_managed_el_extractload_tools_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16cagf7/worthwhile_managed_el_extractload_tools_in_2023/", "subreddit_subscribers": 127274, "created_utc": 1694075654.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my job as a Data Analyst we receive the same data types from each new client. For example, customers, suppliers, etc. We perform ETL and import cleaned and transformed data into our SQL Server. \n\nEach client sends us their data through SFTP in Excel files which my team and I have to manually pick out the columns we need for each data type. \n\nMy question is how do you best standardize the data we receive from clients? Knowing that you know which specific columns you need for each data type?\n\nMy initial idea is to ask clients to fill out a data collection workbook of the columns we specifically need for each data type. But this solution doesn\u2019t seem optimal.\n\nI also want to use SSIS to help automate the ETL process but I haven\u2019t delved in too much on how to use that tool yet.", "author_fullname": "t2_pz85y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to standardize data ingestion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16c2yqn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1694051709.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694051312.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my job as a Data Analyst we receive the same data types from each new client. For example, customers, suppliers, etc. We perform ETL and import cleaned and transformed data into our SQL Server. &lt;/p&gt;\n\n&lt;p&gt;Each client sends us their data through SFTP in Excel files which my team and I have to manually pick out the columns we need for each data type. &lt;/p&gt;\n\n&lt;p&gt;My question is how do you best standardize the data we receive from clients? Knowing that you know which specific columns you need for each data type?&lt;/p&gt;\n\n&lt;p&gt;My initial idea is to ask clients to fill out a data collection workbook of the columns we specifically need for each data type. But this solution doesn\u2019t seem optimal.&lt;/p&gt;\n\n&lt;p&gt;I also want to use SSIS to help automate the ETL process but I haven\u2019t delved in too much on how to use that tool yet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16c2yqn", "is_robot_indexable": true, "report_reasons": null, "author": "imperialka", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16c2yqn/how_to_standardize_data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16c2yqn/how_to_standardize_data_ingestion/", "subreddit_subscribers": 127274, "created_utc": 1694051312.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company's data community is pretty much split in two. Over the past year, the majority of the company (lead by AWS ProServe) worked to build up a data mesh architecture with a custom platform used to find and share data in AWS (handling cross account roles and related infrastructure), Data storage is predominantly S3 and compute is predominantly Redshift Serverless. They're also in the final stages of buying a commercial catalog for data governance.\n\nHonestly, *A LOT* of effort was spent on this development, but at the end of the day, the custom platform isn't differentiating, and there's still a lot of manual work needed to connect to the data to run compute on it. Another half of the data community recently decided that they needed a simper out of the box solution, and decided to PoC Databricks, and they'll be building out a data lake with the Unity Catalog, nothing custom, just an out of the box implementation. \n\nHowever, I'm trying to premptivly think of how these two systems can integrate with each other? I'm very experienced in AWS, but much less-so in Databricks... I can easily imagine how S3 data could be shared with a Databricks consuming account, however, from my understanding, Databricks needs files in it's data lake to be of type Delta Lake. Does this mean that we should encourage all data producers to store their data as Delta Lake files? Doing the conversion on an as-needed basis might lead to data duplication and data silos.\n\nAlso, what challenges arise from using Unity Catalog alongside a second data catalog? It seems like a bad idea, but maybe a two-state-solution like this could work? I'm curious if anyone else ever worked around a split ecosystem like this? ", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How easily would Databricks integrate into a mostly AWS backed data ecosystem?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16c4yrt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694056902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company&amp;#39;s data community is pretty much split in two. Over the past year, the majority of the company (lead by AWS ProServe) worked to build up a data mesh architecture with a custom platform used to find and share data in AWS (handling cross account roles and related infrastructure), Data storage is predominantly S3 and compute is predominantly Redshift Serverless. They&amp;#39;re also in the final stages of buying a commercial catalog for data governance.&lt;/p&gt;\n\n&lt;p&gt;Honestly, &lt;em&gt;A LOT&lt;/em&gt; of effort was spent on this development, but at the end of the day, the custom platform isn&amp;#39;t differentiating, and there&amp;#39;s still a lot of manual work needed to connect to the data to run compute on it. Another half of the data community recently decided that they needed a simper out of the box solution, and decided to PoC Databricks, and they&amp;#39;ll be building out a data lake with the Unity Catalog, nothing custom, just an out of the box implementation. &lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m trying to premptivly think of how these two systems can integrate with each other? I&amp;#39;m very experienced in AWS, but much less-so in Databricks... I can easily imagine how S3 data could be shared with a Databricks consuming account, however, from my understanding, Databricks needs files in it&amp;#39;s data lake to be of type Delta Lake. Does this mean that we should encourage all data producers to store their data as Delta Lake files? Doing the conversion on an as-needed basis might lead to data duplication and data silos.&lt;/p&gt;\n\n&lt;p&gt;Also, what challenges arise from using Unity Catalog alongside a second data catalog? It seems like a bad idea, but maybe a two-state-solution like this could work? I&amp;#39;m curious if anyone else ever worked around a split ecosystem like this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16c4yrt", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16c4yrt/how_easily_would_databricks_integrate_into_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16c4yrt/how_easily_would_databricks_integrate_into_a/", "subreddit_subscribers": 127274, "created_utc": 1694056902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data architect for my company and have recently been pulled in to a dept I've not worked with before that is using ADF pipelines for data ingestion through to our CRM and Analytics dbs.  I noticed that there are tables on a Azure SQL Server that have no primary/foreign keys, and no indexes at all. Is this common practice?  I've looked at the pipelines, and yes there is some use of databricks to create dataframes off of the data in the tables, but I still feel like this might be slightly inefficient. I don't have a big DE background, so I figured I'd check with you all. Common practice because you can gain efficiencies through creating python scripts to pull and manage the data from the SQL dbs?  For clarification, these are not temp tables. These are production tables attempting to manage millions of records a day.", "author_fullname": "t2_2v1p3nx2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Un-indexed Azure SQL Server tables used in ADF Pipelines (Is this common?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16c3ezp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694052533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data architect for my company and have recently been pulled in to a dept I&amp;#39;ve not worked with before that is using ADF pipelines for data ingestion through to our CRM and Analytics dbs.  I noticed that there are tables on a Azure SQL Server that have no primary/foreign keys, and no indexes at all. Is this common practice?  I&amp;#39;ve looked at the pipelines, and yes there is some use of databricks to create dataframes off of the data in the tables, but I still feel like this might be slightly inefficient. I don&amp;#39;t have a big DE background, so I figured I&amp;#39;d check with you all. Common practice because you can gain efficiencies through creating python scripts to pull and manage the data from the SQL dbs?  For clarification, these are not temp tables. These are production tables attempting to manage millions of records a day.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Architect", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16c3ezp", "is_robot_indexable": true, "report_reasons": null, "author": "No-Current-7884", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16c3ezp/unindexed_azure_sql_server_tables_used_in_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16c3ezp/unindexed_azure_sql_server_tables_used_in_adf/", "subreddit_subscribers": 127274, "created_utc": 1694052533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data engineer setting up dbt for a group of analysts and data scientists. The naming conventions for transformational layers suggested in the dbt docs are:\n\n1. staging\n2. intermediate\n3. marts\n\nTbh, I don't really like these names. Especially staging -- its generic and conflicts with the term staging as a reference to the environment. In past projects (that did not use dbt), I used different naming conventions, but were functionally the same as dbt's in terms of organization:\n\n1. crbo (common reporting business objects)\n   1. this could could be prefixed as stripe\\_crbo, and is almost always suffixed with something like \\_transactions or \\_refunds\n2. core\n   1. also can be prefixed or suffixed in the same manner as crbo\n3. any name that describes the data set, and then its aggregation level\n   1. i.e. stripe\\_transactions\\_by\\_country\\_plan\n\nHow do you structure your dbt layers and what naming conventions do you use? I will probably start out with dbt's conventions, but am interested in other approaches!", "author_fullname": "t2_1p505jz8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Layer Naming Conventions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bulr0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694031133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data engineer setting up dbt for a group of analysts and data scientists. The naming conventions for transformational layers suggested in the dbt docs are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;staging&lt;/li&gt;\n&lt;li&gt;intermediate&lt;/li&gt;\n&lt;li&gt;marts&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Tbh, I don&amp;#39;t really like these names. Especially staging -- its generic and conflicts with the term staging as a reference to the environment. In past projects (that did not use dbt), I used different naming conventions, but were functionally the same as dbt&amp;#39;s in terms of organization:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;crbo (common reporting business objects)\n\n&lt;ol&gt;\n&lt;li&gt;this could could be prefixed as stripe_crbo, and is almost always suffixed with something like _transactions or _refunds&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;core\n\n&lt;ol&gt;\n&lt;li&gt;also can be prefixed or suffixed in the same manner as crbo&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;any name that describes the data set, and then its aggregation level\n\n&lt;ol&gt;\n&lt;li&gt;i.e. stripe_transactions_by_country_plan&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;How do you structure your dbt layers and what naming conventions do you use? I will probably start out with dbt&amp;#39;s conventions, but am interested in other approaches!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bulr0", "is_robot_indexable": true, "report_reasons": null, "author": "Fredonia1988", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bulr0/dbt_layer_naming_conventions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bulr0/dbt_layer_naming_conventions/", "subreddit_subscribers": 127274, "created_utc": 1694031133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey data engineering,\n\nWanted to reach out to the community to see how other teams are solving UAT / QC process and challenges when using DBT. \n\nBackground:\n\nOne of the main challenges we run into is that we're often developing net new data sources, updating existing based on feature requests, or updating existing based on bugs found by the business/BI team. A frequent problem is that we will onboard a new data source all the way to production and then a month later the business finally reviews the source and requests changes. \n\nWe already have dev and prod targets configured in our DBT profiles and our CI/CD runs with a --target prod. The --target dev is primarily used by devs locally against a different database. Our current branching strategy is feature -&gt; develop -&gt; master with the --target prod only running on master branch.\n\nProposed Ideas:\n\nWe're thinking this could likely be solved by using a feature -&gt; develop -&gt; uat -&gt; master branching strategy. The biggest drawback here is that is a lot of PRs that have to happen in order to get code to production. And because we ultimately need different models in different databases based on the stage of the development lifecycle we're in, this seems like an \"all or nothing\" approach unless we start cherry-picking or commits to include in merges.\n\nAlternative idea: Use the DBT tagging feature to add tags=\\[\"uat\"\\] for models that still need review/sign-off by the business  and then add an --exclude tag:uat from our normal DBT runs. This way we don't have to increase the complexity of our branching process and could easily tell which step a model is in. The UAT models could either be run by using a --select tag:uat from local systems or via CI/CD.\n\nAsk:\n\nSo data engineering, how do you solve this issue? What does your process look like for going to production? How do you (or do you) wait until you get business sign off even if all dev work has been completed? Any thoughts/approaches appreciated!\n\nThanks!", "author_fullname": "t2_dvuofczh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle UAT / business QC with DBT projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bs3z4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694025452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey data engineering,&lt;/p&gt;\n\n&lt;p&gt;Wanted to reach out to the community to see how other teams are solving UAT / QC process and challenges when using DBT. &lt;/p&gt;\n\n&lt;p&gt;Background:&lt;/p&gt;\n\n&lt;p&gt;One of the main challenges we run into is that we&amp;#39;re often developing net new data sources, updating existing based on feature requests, or updating existing based on bugs found by the business/BI team. A frequent problem is that we will onboard a new data source all the way to production and then a month later the business finally reviews the source and requests changes. &lt;/p&gt;\n\n&lt;p&gt;We already have dev and prod targets configured in our DBT profiles and our CI/CD runs with a --target prod. The --target dev is primarily used by devs locally against a different database. Our current branching strategy is feature -&amp;gt; develop -&amp;gt; master with the --target prod only running on master branch.&lt;/p&gt;\n\n&lt;p&gt;Proposed Ideas:&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re thinking this could likely be solved by using a feature -&amp;gt; develop -&amp;gt; uat -&amp;gt; master branching strategy. The biggest drawback here is that is a lot of PRs that have to happen in order to get code to production. And because we ultimately need different models in different databases based on the stage of the development lifecycle we&amp;#39;re in, this seems like an &amp;quot;all or nothing&amp;quot; approach unless we start cherry-picking or commits to include in merges.&lt;/p&gt;\n\n&lt;p&gt;Alternative idea: Use the DBT tagging feature to add tags=[&amp;quot;uat&amp;quot;] for models that still need review/sign-off by the business  and then add an --exclude tag:uat from our normal DBT runs. This way we don&amp;#39;t have to increase the complexity of our branching process and could easily tell which step a model is in. The UAT models could either be run by using a --select tag:uat from local systems or via CI/CD.&lt;/p&gt;\n\n&lt;p&gt;Ask:&lt;/p&gt;\n\n&lt;p&gt;So data engineering, how do you solve this issue? What does your process look like for going to production? How do you (or do you) wait until you get business sign off even if all dev work has been completed? Any thoughts/approaches appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16bs3z4", "is_robot_indexable": true, "report_reasons": null, "author": "I_Blame_DevOps", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bs3z4/how_do_you_handle_uat_business_qc_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bs3z4/how_do_you_handle_uat_business_qc_with_dbt/", "subreddit_subscribers": 127274, "created_utc": 1694025452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do any of your folks have some ideas around how I could attach some data points to my **impression** that \"Delta still 800 pound gorilla, but Iceberg has  momentum, and Hudi is more of a bespoke use case vs. wide adoption\"\n\nI've been googling / chatgpt &amp; claude-ing like crazy and not finding anything really useful to measure true adoption and momentum. There are github stars and commits, but that still is a very tenous connection to real adoption in my mind...\n\nI can't seem to swing a stick these days without hitting some sort of press release about iceberg (Snowflake, Redshift, etc.) and that seems like a leading INDICATOR for adoption, but still not a sure thing...I know I'm not going to get binary with this question, but it still feels too loosey-goosey.\n\n&amp;#x200B;", "author_fullname": "t2_cabpv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Determining Iceberg v. Delta v. Hudi adoption?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16cghib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694094854.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do any of your folks have some ideas around how I could attach some data points to my &lt;strong&gt;impression&lt;/strong&gt; that &amp;quot;Delta still 800 pound gorilla, but Iceberg has  momentum, and Hudi is more of a bespoke use case vs. wide adoption&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been googling / chatgpt &amp;amp; claude-ing like crazy and not finding anything really useful to measure true adoption and momentum. There are github stars and commits, but that still is a very tenous connection to real adoption in my mind...&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t seem to swing a stick these days without hitting some sort of press release about iceberg (Snowflake, Redshift, etc.) and that seems like a leading INDICATOR for adoption, but still not a sure thing...I know I&amp;#39;m not going to get binary with this question, but it still feels too loosey-goosey.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16cghib", "is_robot_indexable": true, "report_reasons": null, "author": "JudgingYouThisSecond", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16cghib/determining_iceberg_v_delta_v_hudi_adoption/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16cghib/determining_iceberg_v_delta_v_hudi_adoption/", "subreddit_subscribers": 127274, "created_utc": 1694094854.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_mbuz666f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PostgreSQL Bigger != Better", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_16ccnx7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/i7r-hqAAwVBRiCAju2_g24Teu8ZLI8xOVMocvIaLqLU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694083821.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "epsio.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.epsio.io/blog/postgresql-bigger-better", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fYgbjVe5AQgHOpo5Q-8w_KqpXwrPWFoszbhIDK5W_-k.jpg?auto=webp&amp;s=09671ea5df115485b7fc92f02e16bd28a6d297f2", "width": 2401, "height": 1255}, "resolutions": [{"url": "https://external-preview.redd.it/fYgbjVe5AQgHOpo5Q-8w_KqpXwrPWFoszbhIDK5W_-k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fbdfd82efe983f2bb57183ac9957be96e68b301b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fYgbjVe5AQgHOpo5Q-8w_KqpXwrPWFoszbhIDK5W_-k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5c92fbe8b6643cb094fbd2bb56d7773264efa1d1", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/fYgbjVe5AQgHOpo5Q-8w_KqpXwrPWFoszbhIDK5W_-k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ecc8827dbd65aee484b5be92d534cd1d563f441", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/fYgbjVe5AQgHOpo5Q-8w_KqpXwrPWFoszbhIDK5W_-k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac2b22bc7cb5492a2ca1e7bf8d4a51e2905dd367", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/fYgbjVe5AQgHOpo5Q-8w_KqpXwrPWFoszbhIDK5W_-k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4007386a2704ce65441f7c1cf2c30b9f15177f58", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/fYgbjVe5AQgHOpo5Q-8w_KqpXwrPWFoszbhIDK5W_-k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=152fd43fbe21c72d5f2a50eaacb7bbc464f2fe55", "width": 1080, "height": 564}], "variants": {}, "id": "Far3Q5O4VaIR6aOnR3lM0C8HT_ZB-uLJdxi095KBHUU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16ccnx7", "is_robot_indexable": true, "report_reasons": null, "author": "dkgs19982", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ccnx7/postgresql_bigger_better/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.epsio.io/blog/postgresql-bigger-better", "subreddit_subscribers": 127274, "created_utc": 1694083821.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I was asked to create an Audit Trail on the Master table where all the information about a specific product is stored , so I used a trigger to capture all the changes that were made on the table , the trigger was working perfectly in the testing environment but in production during the night time something happened and all the process were slowed and stopped , the db owner deleted the trigger which in turn deleted all the log history of the trigger, so no I am unable to figure out why that happened, can you guys please guide me", "author_fullname": "t2_4p2t0v0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using a trigger not the way to capture Audit Trail?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16c7ouo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694065538.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I was asked to create an Audit Trail on the Master table where all the information about a specific product is stored , so I used a trigger to capture all the changes that were made on the table , the trigger was working perfectly in the testing environment but in production during the night time something happened and all the process were slowed and stopped , the db owner deleted the trigger which in turn deleted all the log history of the trigger, so no I am unable to figure out why that happened, can you guys please guide me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16c7ouo", "is_robot_indexable": true, "report_reasons": null, "author": "omghag18", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16c7ouo/using_a_trigger_not_the_way_to_capture_audit_trail/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16c7ouo/using_a_trigger_not_the_way_to_capture_audit_trail/", "subreddit_subscribers": 127274, "created_utc": 1694065538.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to implement my first data pipeline. I have 7 python scripts that extract and clean api data, then upload to sql server. I\u2019m trying to run these scripts every evening. Should I use Airflow or Alteryx to facilitate this?", "author_fullname": "t2_t45bb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alteryx or airflow for simple API pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bt2oe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694027665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to implement my first data pipeline. I have 7 python scripts that extract and clean api data, then upload to sql server. I\u2019m trying to run these scripts every evening. Should I use Airflow or Alteryx to facilitate this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16bt2oe", "is_robot_indexable": true, "report_reasons": null, "author": "giantdickinmyface", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bt2oe/alteryx_or_airflow_for_simple_api_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bt2oe/alteryx_or_airflow_for_simple_api_pipeline/", "subreddit_subscribers": 127274, "created_utc": 1694027665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone here found a viable way to do any type of cost comparisons between data platforms? I\u2019m trying to provide the roughest of estimates for AZ synapse analytics and Snowflake so we can decide on one of them, but everything is abstracted to SCUs or Snowflake credits. I get what both are conceptually, but have no idea how to apply it to even the roughest of cost estimation models.", "author_fullname": "t2_bsjfy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analytics platforms cost comparisons", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16cloh6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694107813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone here found a viable way to do any type of cost comparisons between data platforms? I\u2019m trying to provide the roughest of estimates for AZ synapse analytics and Snowflake so we can decide on one of them, but everything is abstracted to SCUs or Snowflake credits. I get what both are conceptually, but have no idea how to apply it to even the roughest of cost estimation models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16cloh6", "is_robot_indexable": true, "report_reasons": null, "author": "screwuapple", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16cloh6/analytics_platforms_cost_comparisons/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16cloh6/analytics_platforms_cost_comparisons/", "subreddit_subscribers": 127274, "created_utc": 1694107813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to choose a laptop for my work. I want to get a MacBook, but I need some tools unavailable on MacOS.\n\nHas anyone installed and used SSMS, VS (with SQL Server data tools), and Tabular Editor 3 on a MacBook M1/M2 using Parallels? How was your experience? Do you suggest it?\n\nAre there alternative solutions, or should I get a Windows laptop?", "author_fullname": "t2_bw3dor88", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSMS, VS &amp; Tabular Editor 3 MacBook M1/2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16clihq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694107264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to choose a laptop for my work. I want to get a MacBook, but I need some tools unavailable on MacOS.&lt;/p&gt;\n\n&lt;p&gt;Has anyone installed and used SSMS, VS (with SQL Server data tools), and Tabular Editor 3 on a MacBook M1/M2 using Parallels? How was your experience? Do you suggest it?&lt;/p&gt;\n\n&lt;p&gt;Are there alternative solutions, or should I get a Windows laptop?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16clihq", "is_robot_indexable": true, "report_reasons": null, "author": "pydatadriven", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16clihq/ssms_vs_tabular_editor_3_macbook_m12/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16clihq/ssms_vs_tabular_editor_3_macbook_m12/", "subreddit_subscribers": 127274, "created_utc": 1694107264.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to establish a db that includes a company(s), users from the company, and reports generated where people from the company can also work on the same report. I have the users and report tables thought out. Should I have the reports model include the company_id, the users table include the company_id, and the company table have a company_id, and company name?", "author_fullname": "t2_ezjteqa7g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help establishing a new db.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ciz8j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694101002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to establish a db that includes a company(s), users from the company, and reports generated where people from the company can also work on the same report. I have the users and report tables thought out. Should I have the reports model include the company_id, the users table include the company_id, and the company table have a company_id, and company name?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ciz8j", "is_robot_indexable": true, "report_reasons": null, "author": "Adept_Explorer_7714", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ciz8j/need_help_establishing_a_new_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ciz8j/need_help_establishing_a_new_db/", "subreddit_subscribers": 127274, "created_utc": 1694101002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I use activ batch for the workload automation in my organization. I used a lot sql agent but not activ batch. I searched for resources to learn this but can't find any resources. Please anybody share the resources if you have used in your job or career. Thanks a lot in advance.", "author_fullname": "t2_5prh8z7e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advonsys activbatch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16chsiz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694098151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use activ batch for the workload automation in my organization. I used a lot sql agent but not activ batch. I searched for resources to learn this but can&amp;#39;t find any resources. Please anybody share the resources if you have used in your job or career. Thanks a lot in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16chsiz", "is_robot_indexable": true, "report_reasons": null, "author": "Mrmjix", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16chsiz/advonsys_activbatch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16chsiz/advonsys_activbatch/", "subreddit_subscribers": 127274, "created_utc": 1694098151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been planning to give AWS exams for certifications, although the price of it is just too high for me as a student. If anyone can help me with any sort of discount or promo codes I would highly appreciate it.", "author_fullname": "t2_7wq5o3nd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Certification Fee", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16cgkfi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694095061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been planning to give AWS exams for certifications, although the price of it is just too high for me as a student. If anyone can help me with any sort of discount or promo codes I would highly appreciate it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16cgkfi", "is_robot_indexable": true, "report_reasons": null, "author": "Little-Physics-1646", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16cgkfi/aws_certification_fee/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16cgkfi/aws_certification_fee/", "subreddit_subscribers": 127274, "created_utc": 1694095061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI have a project for which I have to set up a live web scraper for a couple of websites, establish an ETL pipeline, and automate the data preprocessing to get it all into a defined format (the data from the different websites comes in different formats).\n\nI want to use open source frameworks and tools, and the solution must be scalable. Would appreciate suggestions and advice.\n\nI am considering Apache NiFi. Thoughts on this?\n\nThanks in advance :)", "author_fullname": "t2_gaxqj6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up ETL pipelines and data preprocessing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16cc57w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694082009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I have a project for which I have to set up a live web scraper for a couple of websites, establish an ETL pipeline, and automate the data preprocessing to get it all into a defined format (the data from the different websites comes in different formats).&lt;/p&gt;\n\n&lt;p&gt;I want to use open source frameworks and tools, and the solution must be scalable. Would appreciate suggestions and advice.&lt;/p&gt;\n\n&lt;p&gt;I am considering Apache NiFi. Thoughts on this?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16cc57w", "is_robot_indexable": true, "report_reasons": null, "author": "yipra97", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16cc57w/setting_up_etl_pipelines_and_data_preprocessing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16cc57w/setting_up_etl_pipelines_and_data_preprocessing/", "subreddit_subscribers": 127274, "created_utc": 1694082009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys,\nIs there a way to find out who executed which query at a workspace level? Is there any logs which can be read into a dataframe to extract this  information? I can see the users in query history tab within the UI so is it possible pull this info into a dataframe?\nThank you!", "author_fullname": "t2_f1s7yw3kw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks - Identify who executed the query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16c61uw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694060162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys,\nIs there a way to find out who executed which query at a workspace level? Is there any logs which can be read into a dataframe to extract this  information? I can see the users in query history tab within the UI so is it possible pull this info into a dataframe?\nThank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16c61uw", "is_robot_indexable": true, "report_reasons": null, "author": "fusebox12345", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16c61uw/databricks_identify_who_executed_the_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16c61uw/databricks_identify_who_executed_the_query/", "subreddit_subscribers": 127274, "created_utc": 1694060162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say I have 50 tables which each need 2 tasks in a workflow. I currently have a initial bulk load workflow with the 100 tasks required. How/would this change with a near real time requirement? Workflow for each table? Continuously run the workflow I already have? Just trying to understand what best practice is here. My apologies in advance if these are dumb questions. I don't have much experience with orchestration. Thanks", "author_fullname": "t2_feymqzcjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Workflow Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16bxhcj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694037536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say I have 50 tables which each need 2 tasks in a workflow. I currently have a initial bulk load workflow with the 100 tasks required. How/would this change with a near real time requirement? Workflow for each table? Continuously run the workflow I already have? Just trying to understand what best practice is here. My apologies in advance if these are dumb questions. I don&amp;#39;t have much experience with orchestration. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16bxhcj", "is_robot_indexable": true, "report_reasons": null, "author": "TheConSpooky", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bxhcj/databricks_workflow_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16bxhcj/databricks_workflow_question/", "subreddit_subscribers": 127274, "created_utc": 1694037536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j68228u1s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Developing an Elo Based, Data-Driven Ranking System for 2v2 Multiplayer Games", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_16bvy7e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9gfc6HTFq8-nNnNMoMJBOeCQVc9o1ZcyzxfdIWTYyMk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1694034075.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@lazarekolebka/developing-an-elo-based-data-driven-ranking-system-for-2v2-multiplayer-games-7689f7d42a53", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?auto=webp&amp;s=c0bd3da4d3611b3e1117c469f22addc41b5df10a", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec62a37864fa95077a312b7028165478361ca38f", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ff7a20bd2e5b26d2ac66339264d6e921c7162479", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=16b4d444ad60041d878c8baeb1c319b3897d2ab9", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd5f2bac301ffdd333d8cb2e53464935edc756b2", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e7d8b8fd5b708813c03f8c9e45b64c30f461dd5", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/hHBXFuPqYtYEmlpBVtFMA3Am5JvJ17qZ8gXVRRF8BRA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=52f1bddb3d8d712d063ab62ad3c33e4053ef987a", "width": 1080, "height": 720}], "variants": {}, "id": "LatcCAz20Bz3Y65ssYat9EM3WANhjc5gqXqo7blclXA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16bvy7e", "is_robot_indexable": true, "report_reasons": null, "author": "Bulky-Violinist7187", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16bvy7e/developing_an_elo_based_datadriven_ranking_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@lazarekolebka/developing-an-elo-based-data-driven-ranking-system-for-2v2-multiplayer-games-7689f7d42a53", "subreddit_subscribers": 127274, "created_utc": 1694034075.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have reached out to Joe Reis on a number of occasions to discuss all things data engineering, wanted to share that experience hasn't been wholly positive. Most discussions just ended up in buy my book it's the bible, or attend my event. Appreciate time is money in a capitalistic society but he's far from the authority on Data Engineering. Keen to hear other people's thoughts on this.", "author_fullname": "t2_eq09r4vmp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My Experience with Joe Reis is that he's only in it for book sales or conference seats", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16cgbp7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.16, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1694094444.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have reached out to Joe Reis on a number of occasions to discuss all things data engineering, wanted to share that experience hasn&amp;#39;t been wholly positive. Most discussions just ended up in buy my book it&amp;#39;s the bible, or attend my event. Appreciate time is money in a capitalistic society but he&amp;#39;s far from the authority on Data Engineering. Keen to hear other people&amp;#39;s thoughts on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16cgbp7", "is_robot_indexable": true, "report_reasons": null, "author": "fackingirish", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16cgbp7/my_experience_with_joe_reis_is_that_hes_only_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16cgbp7/my_experience_with_joe_reis_is_that_hes_only_in/", "subreddit_subscribers": 127274, "created_utc": 1694094444.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}