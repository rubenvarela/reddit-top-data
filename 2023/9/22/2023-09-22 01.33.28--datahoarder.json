{"kind": "Listing", "data": {"after": "t3_16oskj0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been wanting to announce this here for a while now, but if you do download stuff from the website please don't download a lot of stuff at once!\n\n&amp;#x200B;\n\n**This is ExternalTube**, an early, read-only, YouTube archive/preservation project \\[made by my friend IdioticSniper\\] with a lot of lost, normal and rare videos scraped from the old YouTube CDNs. We've gotten some amazingly rare stuff, like old videos with almost 2 views at-most, videos from the user \"videos\" \\[their channel was completely wiped by YouTube themselves\\] and lost-until-now videos from the user \"steve\". More than 100 CDNs have been scraped and the archive is currently at 120GB, with the hard-drive hosting the files and everything supporting 3TB.\n\nI recommend you check it out on an Adobe Flash-compatible browser, it's amazing!\n\nhttps://preview.redd.it/8ff1il0d8lpb1.png?width=521&amp;format=png&amp;auto=webp&amp;s=64625e8b53793741b6543a9f9c2b6a68431767e4\n\nQuick warning though: Since this is from the old YouTube CDNs, those are all in FLV quality and use the old YouTube Flash player from 2005.\n\nSo what are you waiting for? **Dig through the archives.**\n\n[**https://exttube.snippr.win**](https://exttube.snippr.win)\n\n&amp;#x200B;\n\n**EDIT:** The source code has been released! [**https://github.com/IdioticSniper/externaltube**](https://github.com/IdioticSniper/externaltube)\n\n**EDIT 2: 150 UPVOTES!** Okay, I quite honestly didn't expect this to **get stickied** and be a \"**post-of-the-day**\", but thank you all! **Please also thank** [**u/aaaahhmyballs**](https://www.reddit.com/u/aaaahhmyballs/) **(A.K.A. website dev and owner) for making all this** (except me, andry6702, I just made the Reddit post lmao). Y'all are awesome!!!", "author_fullname": "t2_7a9ymbww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ExternalTube - A YouTube archive project that ranges with videos from 2005-2009.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"8ff1il0d8lpb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 125, "x": 108, "u": "https://preview.redd.it/8ff1il0d8lpb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3f9d765ff6f49d0bdc003ad0091b991393d75ac4"}, {"y": 251, "x": 216, "u": "https://preview.redd.it/8ff1il0d8lpb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8df8bbb2d6e089ad8481051195db925ddd1e5157"}, {"y": 372, "x": 320, "u": "https://preview.redd.it/8ff1il0d8lpb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b7982594d2f006755d988b6e3ec7b05481d1623"}], "s": {"y": 606, "x": 521, "u": "https://preview.redd.it/8ff1il0d8lpb1.png?width=521&amp;format=png&amp;auto=webp&amp;s=64625e8b53793741b6543a9f9c2b6a68431767e4"}, "id": "8ff1il0d8lpb1"}}, "name": "t3_16ocljy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 272, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 272, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uIbBDETLrN3yy7Qy8Ajyf14E5-clV7VqsL0TPzdiYVs.jpg", "edited": 1695321896.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695293113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been wanting to announce this here for a while now, but if you do download stuff from the website please don&amp;#39;t download a lot of stuff at once!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;This is ExternalTube&lt;/strong&gt;, an early, read-only, YouTube archive/preservation project [made by my friend IdioticSniper] with a lot of lost, normal and rare videos scraped from the old YouTube CDNs. We&amp;#39;ve gotten some amazingly rare stuff, like old videos with almost 2 views at-most, videos from the user &amp;quot;videos&amp;quot; [their channel was completely wiped by YouTube themselves] and lost-until-now videos from the user &amp;quot;steve&amp;quot;. More than 100 CDNs have been scraped and the archive is currently at 120GB, with the hard-drive hosting the files and everything supporting 3TB.&lt;/p&gt;\n\n&lt;p&gt;I recommend you check it out on an Adobe Flash-compatible browser, it&amp;#39;s amazing!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8ff1il0d8lpb1.png?width=521&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=64625e8b53793741b6543a9f9c2b6a68431767e4\"&gt;https://preview.redd.it/8ff1il0d8lpb1.png?width=521&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=64625e8b53793741b6543a9f9c2b6a68431767e4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Quick warning though: Since this is from the old YouTube CDNs, those are all in FLV quality and use the old YouTube Flash player from 2005.&lt;/p&gt;\n\n&lt;p&gt;So what are you waiting for? &lt;strong&gt;Dig through the archives.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://exttube.snippr.win\"&gt;&lt;strong&gt;https://exttube.snippr.win&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; The source code has been released! &lt;a href=\"https://github.com/IdioticSniper/externaltube\"&gt;&lt;strong&gt;https://github.com/IdioticSniper/externaltube&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT 2: 150 UPVOTES!&lt;/strong&gt; Okay, I quite honestly didn&amp;#39;t expect this to &lt;strong&gt;get stickied&lt;/strong&gt; and be a &amp;quot;&lt;strong&gt;post-of-the-day&lt;/strong&gt;&amp;quot;, but thank you all! &lt;strong&gt;Please also thank&lt;/strong&gt; &lt;a href=\"https://www.reddit.com/u/aaaahhmyballs/\"&gt;&lt;strong&gt;u/aaaahhmyballs&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(A.K.A. website dev and owner) for making all this&lt;/strong&gt; (except me, andry6702, I just made the Reddit post lmao). Y&amp;#39;all are awesome!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16ocljy", "is_robot_indexable": true, "report_reasons": null, "author": "Peak_Environmental", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16ocljy/externaltube_a_youtube_archive_project_that/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/16ocljy/externaltube_a_youtube_archive_project_that/", "subreddit_subscribers": 703060, "created_utc": 1695293113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for app recommendations (self-hosted is ok).  I've used podcast addict for about a decade but I've always been on the lookout for a service that has an android and windows app, preferably with the ability to sync progress between the two.  \n\nPie in the sky wishlist would be the ability to import/export listening stats and subscribed podcasts.", "author_fullname": "t2_92cxd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Podcast Hoarders here?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ouph4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695338438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for app recommendations (self-hosted is ok).  I&amp;#39;ve used podcast addict for about a decade but I&amp;#39;ve always been on the lookout for a service that has an android and windows app, preferably with the ability to sync progress between the two.  &lt;/p&gt;\n\n&lt;p&gt;Pie in the sky wishlist would be the ability to import/export listening stats and subscribed podcasts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "56 TB RAW", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16ouph4", "is_robot_indexable": true, "report_reasons": null, "author": "felopez", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/16ouph4/any_podcast_hoarders_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16ouph4/any_podcast_hoarders_here/", "subreddit_subscribers": 703060, "created_utc": 1695338438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has the title suggest, I am in need of a \u201cproper \u201cprogram to move the roughly 100 GB of videos, photos and texts from my iPhone to my computer, I have done this in the past several times, but I don\u2019t remember what program I have used as that computer is no longer operational.\n\nI\u2019ll also add that , whenever I embark on these, let\u2019s call them adventures, there are several problems that seem to keep cropping up when I am either finding  programs for this particular purpose, or when I am attempting to transfer the data.\nThe most common problem, drive pretty much had them all of my computers with every idevice I\u2019ve ever had , is that the PC does not seem to recognize that my iPhone is plugged in , or it will be plugged in, and I will hear the notifications, but I can\u2019t access the phone from the program.\n\nAny all suggestions are welcome! I just want to find a way to make this as painless as possible, it already takes long enough moving all of the data from one device to another.", "author_fullname": "t2_8mz3m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best program for iPhone mass data transfer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16osfex", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695332834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has the title suggest, I am in need of a \u201cproper \u201cprogram to move the roughly 100 GB of videos, photos and texts from my iPhone to my computer, I have done this in the past several times, but I don\u2019t remember what program I have used as that computer is no longer operational.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ll also add that , whenever I embark on these, let\u2019s call them adventures, there are several problems that seem to keep cropping up when I am either finding  programs for this particular purpose, or when I am attempting to transfer the data.\nThe most common problem, drive pretty much had them all of my computers with every idevice I\u2019ve ever had , is that the PC does not seem to recognize that my iPhone is plugged in , or it will be plugged in, and I will hear the notifications, but I can\u2019t access the phone from the program.&lt;/p&gt;\n\n&lt;p&gt;Any all suggestions are welcome! I just want to find a way to make this as painless as possible, it already takes long enough moving all of the data from one device to another.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16osfex", "is_robot_indexable": true, "report_reasons": null, "author": "atomicwater", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16osfex/best_program_for_iphone_mass_data_transfer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16osfex/best_program_for_iphone_mass_data_transfer/", "subreddit_subscribers": 703060, "created_utc": 1695332834.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, as in title. How do You put your psychical book into computer? I want to do so mostly to be able to \"grep\" over my book collections but it also feel handy as ripping CD so I can access my bookshelf from remote location. \n\n&amp;#x200B;\n\nSo that's a question as I hoard ton of papier lately", "author_fullname": "t2_11am0r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you digitalize Your books?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16og3dj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695303265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, as in title. How do You put your psychical book into computer? I want to do so mostly to be able to &amp;quot;grep&amp;quot; over my book collections but it also feel handy as ripping CD so I can access my bookshelf from remote location. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So that&amp;#39;s a question as I hoard ton of papier lately&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "16og3dj", "is_robot_indexable": true, "report_reasons": null, "author": "wytrzeszcz", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16og3dj/how_do_you_digitalize_your_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16og3dj/how_do_you_digitalize_your_books/", "subreddit_subscribers": 703060, "created_utc": 1695303265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I've been struggling with this for the past hour. I have a video I want to archive from a website that no longer exists. I'm pretty sure Internet Archived it because I was able to play the video once (and then it stopped working). The video I'm trying to download is a flash video but it loads up using Ruffle. How can I download this video: [https://web.archive.org/web/20121005125354/http://debatevision.com/videos/122/spartan-debate-institute-2011-how-to-learn-by-watching](https://web.archive.org/web/20121005125354/http://debatevision.com/videos/122/spartan-debate-institute-2011-how-to-learn-by-watching) ", "author_fullname": "t2_7zjm3ef7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download a flash video from Internet Archive (not YouTube).", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o2p74", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695259731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been struggling with this for the past hour. I have a video I want to archive from a website that no longer exists. I&amp;#39;m pretty sure Internet Archived it because I was able to play the video once (and then it stopped working). The video I&amp;#39;m trying to download is a flash video but it loads up using Ruffle. How can I download this video: &lt;a href=\"https://web.archive.org/web/20121005125354/http://debatevision.com/videos/122/spartan-debate-institute-2011-how-to-learn-by-watching\"&gt;https://web.archive.org/web/20121005125354/http://debatevision.com/videos/122/spartan-debate-institute-2011-how-to-learn-by-watching&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16o2p74", "is_robot_indexable": true, "report_reasons": null, "author": "Curious_Loomer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16o2p74/how_to_download_a_flash_video_from_internet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16o2p74/how_to_download_a_flash_video_from_internet/", "subreddit_subscribers": 703060, "created_utc": 1695259731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Tl;dr: I want convert VHS tapes to digital on my PC, but my VCR only has Scart connectors.\n\nI've got a few dozen tapes I want to digitize. I've found excellent tutorials online, including this one: [https://www.youtube.com/watch?v=tk-n7IlrXI4](https://www.youtube.com/watch?v=tk-n7IlrXI4)\n\nBut the capture cards recommended in that tutorial have RCA connectors. My VCR (Panasonic NV-FJ622F-S) has Scart connectors. It seems I have two options: buy a high quality capture card like the  I-O Data GV-USB2 and buy a Scart to RCA adaptor; or buy a high quality capture card with a Scart connector. I don't know of any such devices, however.\n\nAfter perusing a forum online I ascertained that Scart might be superior to RCA because the output signal is RGB, while the older RCA connectors only offer a composite output signal. I'm not looking to spend $100s or $1000s on this project. I just want something that will help me convert my VHS tapes in a decent quality. ", "author_fullname": "t2_l3d1z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digitizing VHS tapes using a Scart VCR", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oll55", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695316961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tl;dr: I want convert VHS tapes to digital on my PC, but my VCR only has Scart connectors.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a few dozen tapes I want to digitize. I&amp;#39;ve found excellent tutorials online, including this one: &lt;a href=\"https://www.youtube.com/watch?v=tk-n7IlrXI4\"&gt;https://www.youtube.com/watch?v=tk-n7IlrXI4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;But the capture cards recommended in that tutorial have RCA connectors. My VCR (Panasonic NV-FJ622F-S) has Scart connectors. It seems I have two options: buy a high quality capture card like the  I-O Data GV-USB2 and buy a Scart to RCA adaptor; or buy a high quality capture card with a Scart connector. I don&amp;#39;t know of any such devices, however.&lt;/p&gt;\n\n&lt;p&gt;After perusing a forum online I ascertained that Scart might be superior to RCA because the output signal is RGB, while the older RCA connectors only offer a composite output signal. I&amp;#39;m not looking to spend $100s or $1000s on this project. I just want something that will help me convert my VHS tapes in a decent quality. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/owshpG4G9cycG-VzEtF7JGb8rIOuiaxTUk90rW3jHMg.jpg?auto=webp&amp;s=0ff299b36e896de2fdf217212f436a70a15afbfe", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/owshpG4G9cycG-VzEtF7JGb8rIOuiaxTUk90rW3jHMg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ffea08f3d3bdf4b64409656e08c3165cea80207", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/owshpG4G9cycG-VzEtF7JGb8rIOuiaxTUk90rW3jHMg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc036c6e225ed6da22fc67406488ae642e2534f8", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/owshpG4G9cycG-VzEtF7JGb8rIOuiaxTUk90rW3jHMg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f1141f261dff0ea1a2752ecfe9c40fe0b133f3f", "width": 320, "height": 240}], "variants": {}, "id": "TNw3I0ufneO6qTbuzfxsAwXvakKOQwPKYRAirzVNYxc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16oll55", "is_robot_indexable": true, "report_reasons": null, "author": "Steebee_Weebee", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16oll55/digitizing_vhs_tapes_using_a_scart_vcr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16oll55/digitizing_vhs_tapes_using_a_scart_vcr/", "subreddit_subscribers": 703060, "created_utc": 1695316961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m working on a archival project for a certain youtuber, he spams his community tab all day with stuff and I\u2019d like a efficient way for me to archive it. \n\nIs there any software or script that will automatically scan and take screenshots or something of the community posts? I\u2019d prefer fully automatic as I\u2019ll probably forget eventually but if not that\u2019s alright. \n\nI don\u2019t know how to code or anything so the terminal and python stuff really confuses me. But I will learn if I have to.", "author_fullname": "t2_g9dhzldt6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can anyone help me find a way to automatically archive Youtube Community Post?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o9yg9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695283266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m working on a archival project for a certain youtuber, he spams his community tab all day with stuff and I\u2019d like a efficient way for me to archive it. &lt;/p&gt;\n\n&lt;p&gt;Is there any software or script that will automatically scan and take screenshots or something of the community posts? I\u2019d prefer fully automatic as I\u2019ll probably forget eventually but if not that\u2019s alright. &lt;/p&gt;\n\n&lt;p&gt;I don\u2019t know how to code or anything so the terminal and python stuff really confuses me. But I will learn if I have to.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16o9yg9", "is_robot_indexable": true, "report_reasons": null, "author": "YendoZakari", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16o9yg9/can_anyone_help_me_find_a_way_to_automatically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16o9yg9/can_anyone_help_me_find_a_way_to_automatically/", "subreddit_subscribers": 703060, "created_utc": 1695283266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "(Originally posted here: https://unix.stackexchange.com/q/757117)\n\nIf we do\n\n    rsync -a --link-dest=dest1 src dest2\n\nThen rsync will only copy files in `src` which have been modified compared to `dest1`. However, even if only one byte in a 1GB file is modified, the entire 1GB will be copied.\n\nHow to change the command so that\n\n 1. Old versions of the backup are all kept;\n 2. Only the delta of each file is copied;\n 3. We can restore any version of any files;\n 4. We can, in some way, remove the old backups while keeping newer versions?\n\n(This is sort of like snapshots in filesystems like ZFS.)", "author_fullname": "t2_nw8uzkwp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most space-efficient way of using rsync to do incremental back up, saving only deltas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o4ee9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695264425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Originally posted here: &lt;a href=\"https://unix.stackexchange.com/q/757117\"&gt;https://unix.stackexchange.com/q/757117&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;If we do&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;rsync -a --link-dest=dest1 src dest2\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then rsync will only copy files in &lt;code&gt;src&lt;/code&gt; which have been modified compared to &lt;code&gt;dest1&lt;/code&gt;. However, even if only one byte in a 1GB file is modified, the entire 1GB will be copied.&lt;/p&gt;\n\n&lt;p&gt;How to change the command so that&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Old versions of the backup are all kept;&lt;/li&gt;\n&lt;li&gt;Only the delta of each file is copied;&lt;/li&gt;\n&lt;li&gt;We can restore any version of any files;&lt;/li&gt;\n&lt;li&gt;We can, in some way, remove the old backups while keeping newer versions?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;(This is sort of like snapshots in filesystems like ZFS.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bK4oXKNpgxoE53LCEzd4qyFMVNZmmo1f8nfKJ6xAfwY.jpg?auto=webp&amp;s=4d3a3fdcce8a0b6f57eff17914edd41d6cba223d", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/bK4oXKNpgxoE53LCEzd4qyFMVNZmmo1f8nfKJ6xAfwY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=101ef2d5c1c11cd19dc570a3de92b1d42dd4619f", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/bK4oXKNpgxoE53LCEzd4qyFMVNZmmo1f8nfKJ6xAfwY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a374f1faf62ab4af6a8201f9deb3ea9b18c78dc5", "width": 216, "height": 216}], "variants": {}, "id": "Eu27KCJ9gAHAoJ1FuuHqnnJTGcSqDhwVFQAcFhKk4cc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16o4ee9", "is_robot_indexable": true, "report_reasons": null, "author": "spherical_shell", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16o4ee9/most_spaceefficient_way_of_using_rsync_to_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16o4ee9/most_spaceefficient_way_of_using_rsync_to_do/", "subreddit_subscribers": 703060, "created_utc": 1695264425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying again because I am unsure if I worded my last post correctly.\n\nI had a full 10TB drive, connected to an app that I use daily/frequently.\n\nAgain, I will never judge anyone (much less anyone on here) for what they collect, we all want to preserve the things we love/passionate about, and that should never be justified or judged.\n\nI have been in this position before, where I was using a drive, it was almost reaching capacity, and I had to purchase a new one, clone it, wait until that finished, eject the old one, rename the new one the exact same as the old drive, load up the app, and pray it thinks, it is the exact same drive, just with larger capacity than before, I have had to do this twice with my music collected, when I was using iTunes, I went from a 2TB to a 3TB, and then a 3TB to a 6TB.\n\nSo, this time around I had two options, either.\n\n1. Take one of my existing, almost full, larger capacity drives, go through it, offload what I want to keep and what can be (potentially...gasp...deleted) archive everything that is left onto my DS1821+ (currently 60TB free and I want it to stay that way or as long as possible) which if I did this with a 12-14-16TB drive I would have left me 48-46-44TB and the thought of that gave me heart palpitations and panic attacks. Then erase the drive fully, start the cloning process, then test it, and if it works, wipe the 10TB (yay free 10TB) and continue from there or\n\n2. In a cost-of-living crisis/inflation on the rise, find the cheapest 18TB I could, start the cloning process (once it finishes, I'll have 8TB free) erase the 10TB (yay free 10TB) and continue using the app.\n\nSo reluctantly I went with option 2, and don't know logically, if these above were the only two options I had, or if I have made the right decision?\n\nMy goal was to always continue using the app, so I knew the inevitability would be to clone the drive, but to what size was the concern, especially hoping that naturally 8TB free, would theoretically last longer than 6-4-or2TB.\n\nAnd then having the free 10TB I can use to archive my TV shows or something like that.\n\nDid I do the right thing? or was there another option I had not considered.\n\n ", "author_fullname": "t2_c7lcxyz2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying again, if it was you in my shoes, what would you have done.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16ovq2v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695341128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying again because I am unsure if I worded my last post correctly.&lt;/p&gt;\n\n&lt;p&gt;I had a full 10TB drive, connected to an app that I use daily/frequently.&lt;/p&gt;\n\n&lt;p&gt;Again, I will never judge anyone (much less anyone on here) for what they collect, we all want to preserve the things we love/passionate about, and that should never be justified or judged.&lt;/p&gt;\n\n&lt;p&gt;I have been in this position before, where I was using a drive, it was almost reaching capacity, and I had to purchase a new one, clone it, wait until that finished, eject the old one, rename the new one the exact same as the old drive, load up the app, and pray it thinks, it is the exact same drive, just with larger capacity than before, I have had to do this twice with my music collected, when I was using iTunes, I went from a 2TB to a 3TB, and then a 3TB to a 6TB.&lt;/p&gt;\n\n&lt;p&gt;So, this time around I had two options, either.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Take one of my existing, almost full, larger capacity drives, go through it, offload what I want to keep and what can be (potentially...gasp...deleted) archive everything that is left onto my DS1821+ (currently 60TB free and I want it to stay that way or as long as possible) which if I did this with a 12-14-16TB drive I would have left me 48-46-44TB and the thought of that gave me heart palpitations and panic attacks. Then erase the drive fully, start the cloning process, then test it, and if it works, wipe the 10TB (yay free 10TB) and continue from there or&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;In a cost-of-living crisis/inflation on the rise, find the cheapest 18TB I could, start the cloning process (once it finishes, I&amp;#39;ll have 8TB free) erase the 10TB (yay free 10TB) and continue using the app.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So reluctantly I went with option 2, and don&amp;#39;t know logically, if these above were the only two options I had, or if I have made the right decision?&lt;/p&gt;\n\n&lt;p&gt;My goal was to always continue using the app, so I knew the inevitability would be to clone the drive, but to what size was the concern, especially hoping that naturally 8TB free, would theoretically last longer than 6-4-or2TB.&lt;/p&gt;\n\n&lt;p&gt;And then having the free 10TB I can use to archive my TV shows or something like that.&lt;/p&gt;\n\n&lt;p&gt;Did I do the right thing? or was there another option I had not considered.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16ovq2v", "is_robot_indexable": true, "report_reasons": null, "author": "massivlybored", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16ovq2v/trying_again_if_it_was_you_in_my_shoes_what_would/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16ovq2v/trying_again_if_it_was_you_in_my_shoes_what_would/", "subreddit_subscribers": 703060, "created_utc": 1695341128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am using PowerRename (from Windows PowerToys) but it's VERY slow when you try to mass edit big lists of folders with lots of files inside them. I have a folders with 1000 subfolders (several thousands of files) and if I try to open PowerRename, it takes like 30 minutes to OPEN, let alone allow me to do anything.\n\nIs there any way I can have a faster and preview-friendly way of editing in bulk with regular expressions?", "author_fullname": "t2_49efs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fastest, most reliable mass folder name editor for Windows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ou2hv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695336760.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using PowerRename (from Windows PowerToys) but it&amp;#39;s VERY slow when you try to mass edit big lists of folders with lots of files inside them. I have a folders with 1000 subfolders (several thousands of files) and if I try to open PowerRename, it takes like 30 minutes to OPEN, let alone allow me to do anything.&lt;/p&gt;\n\n&lt;p&gt;Is there any way I can have a faster and preview-friendly way of editing in bulk with regular expressions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16ou2hv", "is_robot_indexable": true, "report_reasons": null, "author": "inhalingsounds", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16ou2hv/fastest_most_reliable_mass_folder_name_editor_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16ou2hv/fastest_most_reliable_mass_folder_name_editor_for/", "subreddit_subscribers": 703060, "created_utc": 1695336760.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to archive some accounts but my main downloader by furyutei is dead, so now im using WFDownloader instead. and im having issues with it not downloading everything  \n\n\nhttps://preview.redd.it/nnjirvin8opb1.png?width=270&amp;format=png&amp;auto=webp&amp;s=10f97a7f15495911c46bdadf76f1d0c619c67af4\n\nhttps://preview.redd.it/w06kox1t8opb1.png?width=686&amp;format=png&amp;auto=webp&amp;s=7691411d743e9b2801127f9e3d17dc7c010d79e6\n\n[I may be missing something but im really stumped, maybe its a issue with twitter and it will be fixed but incase its a fixable issue currently, i should try to get some assistance. if anyone can help id be grateful! ](https://preview.redd.it/8nu443sv8opb1.png?width=525&amp;format=png&amp;auto=webp&amp;s=23851a59867674a63f1b8f008fe8efcb334401c0)", "author_fullname": "t2_3893dy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WFDownloader not downloading entire twitter account", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 27, "top_awarded_type": null, "hide_score": false, "media_metadata": {"nnjirvin8opb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 21, "x": 108, "u": "https://preview.redd.it/nnjirvin8opb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f0e055e211f0c4a1eeda0878d01c512d56ca8da"}, {"y": 42, "x": 216, "u": "https://preview.redd.it/nnjirvin8opb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f13d66118f9d3aed3c278f48147d2099a9d088ce"}], "s": {"y": 53, "x": 270, "u": "https://preview.redd.it/nnjirvin8opb1.png?width=270&amp;format=png&amp;auto=webp&amp;s=10f97a7f15495911c46bdadf76f1d0c619c67af4"}, "id": "nnjirvin8opb1"}, "8nu443sv8opb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 20, "x": 108, "u": "https://preview.redd.it/8nu443sv8opb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=07de83cd9b7aa69382e9b16d6ae4f9c27baab23d"}, {"y": 41, "x": 216, "u": "https://preview.redd.it/8nu443sv8opb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5c3c7e5fe10109ae5a553bb14cc4878012b98652"}, {"y": 61, "x": 320, "u": "https://preview.redd.it/8nu443sv8opb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6303c6a7652131a9de9699b2ba7cc7aea88f3901"}], "s": {"y": 101, "x": 525, "u": "https://preview.redd.it/8nu443sv8opb1.png?width=525&amp;format=png&amp;auto=webp&amp;s=23851a59867674a63f1b8f008fe8efcb334401c0"}, "id": "8nu443sv8opb1"}, "w06kox1t8opb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 17, "x": 108, "u": "https://preview.redd.it/w06kox1t8opb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f1e55d208544058c833422979713284f768619c"}, {"y": 35, "x": 216, "u": "https://preview.redd.it/w06kox1t8opb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d97be36fa3f914f15d62363f5a62f9d744bff0e8"}, {"y": 52, "x": 320, "u": "https://preview.redd.it/w06kox1t8opb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9cc690fabe8658c38bdea59e0716f8a32aecdfb2"}, {"y": 104, "x": 640, "u": "https://preview.redd.it/w06kox1t8opb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2553ae6ab3f8e69bf9157511137911fc3a94d157"}], "s": {"y": 112, "x": 686, "u": "https://preview.redd.it/w06kox1t8opb1.png?width=686&amp;format=png&amp;auto=webp&amp;s=7691411d743e9b2801127f9e3d17dc7c010d79e6"}, "id": "w06kox1t8opb1"}}, "name": "t3_16or0ag", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TdL0NJCrb-2JQjaJg-2KaNRSyNpNkOjKGyqiFXDGEVU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695329528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to archive some accounts but my main downloader by furyutei is dead, so now im using WFDownloader instead. and im having issues with it not downloading everything  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nnjirvin8opb1.png?width=270&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=10f97a7f15495911c46bdadf76f1d0c619c67af4\"&gt;https://preview.redd.it/nnjirvin8opb1.png?width=270&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=10f97a7f15495911c46bdadf76f1d0c619c67af4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/w06kox1t8opb1.png?width=686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7691411d743e9b2801127f9e3d17dc7c010d79e6\"&gt;https://preview.redd.it/w06kox1t8opb1.png?width=686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7691411d743e9b2801127f9e3d17dc7c010d79e6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8nu443sv8opb1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23851a59867674a63f1b8f008fe8efcb334401c0\"&gt;I may be missing something but im really stumped, maybe its a issue with twitter and it will be fixed but incase its a fixable issue currently, i should try to get some assistance. if anyone can help id be grateful! &lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16or0ag", "is_robot_indexable": true, "report_reasons": null, "author": "Taylorkingct", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16or0ag/wfdownloader_not_downloading_entire_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16or0ag/wfdownloader_not_downloading_entire_twitter/", "subreddit_subscribers": 703060, "created_utc": 1695329528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "In 2020, I grabbed off of ebay an LSI 9207-8i (HP 9205-8i) SATA/SAS PCI-E 3.0 expander card for $45. It was flashed to IT mode and flashed to remove the MPT Bios and I have been using it to simply add more SATA HDDs to my large Fractal cases.\n\nI see they are still available, now for $60.\n\nIs this still the best option to get, or is there a newer/superior recommendation?\n\nAs well, back when I got this I had seen some recommend adding this very small noctua fan to keep the card cooler, which I did with good results: [https://www.amazon.com/gp/product/B07DXRNYNX/ref=ppx\\_yo\\_dt\\_b\\_search\\_asin\\_title?ie=UTF8&amp;psc=1](https://www.amazon.com/gp/product/B07DXRNYNX/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1)\n\nIs that still recommended as well?\n\nThanks for the help!", "author_fullname": "t2_cmfdu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the current best PCIe SATA expansion card for home server use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16onge4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695321252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In 2020, I grabbed off of ebay an LSI 9207-8i (HP 9205-8i) SATA/SAS PCI-E 3.0 expander card for $45. It was flashed to IT mode and flashed to remove the MPT Bios and I have been using it to simply add more SATA HDDs to my large Fractal cases.&lt;/p&gt;\n\n&lt;p&gt;I see they are still available, now for $60.&lt;/p&gt;\n\n&lt;p&gt;Is this still the best option to get, or is there a newer/superior recommendation?&lt;/p&gt;\n\n&lt;p&gt;As well, back when I got this I had seen some recommend adding this very small noctua fan to keep the card cooler, which I did with good results: &lt;a href=\"https://www.amazon.com/gp/product/B07DXRNYNX/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;amp;psc=1\"&gt;https://www.amazon.com/gp/product/B07DXRNYNX/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;amp;psc=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Is that still recommended as well?&lt;/p&gt;\n\n&lt;p&gt;Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16onge4", "is_robot_indexable": true, "report_reasons": null, "author": "filmguy123", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16onge4/what_is_the_current_best_pcie_sata_expansion_card/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16onge4/what_is_the_current_best_pcie_sata_expansion_card/", "subreddit_subscribers": 703060, "created_utc": 1695321252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_102ft9mv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I extract the data from a 1.5 TB, WD 15NMVW external hard drive? There are no docking stations that I can find that micro b can fit into", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_16omj6d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/StNwFko1HBRa5WjqVKbcr67tEdVLmpaj1e2XHGDEuVE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695319044.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/sxkkb01xdnpb1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/sxkkb01xdnpb1.jpg?auto=webp&amp;s=e5b0a7efa4af20460ab05a58f0208841e6f8cd2b", "width": 4032, "height": 3024}, "resolutions": [{"url": "https://preview.redd.it/sxkkb01xdnpb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=26484f046022b0a9f6efb0ff8bd403729d2f0687", "width": 108, "height": 81}, {"url": "https://preview.redd.it/sxkkb01xdnpb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d19544cb9f79eb6d75bd5a655ed12327a5338e95", "width": 216, "height": 162}, {"url": "https://preview.redd.it/sxkkb01xdnpb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4226477910a7ad27c7a7f56dc242bab6d686bf20", "width": 320, "height": 240}, {"url": "https://preview.redd.it/sxkkb01xdnpb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a063e3cafe2a7485d77754dca90eb275682427ef", "width": 640, "height": 480}, {"url": "https://preview.redd.it/sxkkb01xdnpb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b8c95f362427d67bd84e33eea7d278973ece83c1", "width": 960, "height": 720}, {"url": "https://preview.redd.it/sxkkb01xdnpb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5709fa609a28667725e3900cb4a9c17198673f38", "width": 1080, "height": 810}], "variants": {}, "id": "iCOzFCvbkUbHYt9N04bOHwhWfmyhmGHVi5l5zSBnlxU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16omj6d", "is_robot_indexable": true, "report_reasons": null, "author": "bomb_adrenaline", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16omj6d/how_can_i_extract_the_data_from_a_15_tb_wd_15nmvw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/sxkkb01xdnpb1.jpg", "subreddit_subscribers": 703060, "created_utc": 1695319044.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm interested in backing up my photo collection to backblaze b2b (as it seems the best value supplier)\n\nI am considering using rclone and crypt as it seems like the best solution.\n\nThe query is this, what is the best workflow to approach this from ?  I have some historic bitrot and I want to ensure that duplicate photos are deleted but the files themselves are not bit identical as picasa and myself have edited the exif metadata.  So only the underlying image data should be checked.\n\nHas anyone else approached this ?  Filesystem deduplication doesn't seem to be the answer.\n\nI have been thinking about using exiftool to export the sha1 to an xmp sidecar, and use python to identify duplicate checksums.  Is this a good approach ?\n\nAs an aside, I have also exported my data from google takeout in case my local data lacks integrity, but that has its exifdata stripped and the images resized I believe.\n\nAny advice would be appreciated.  Cheers.", "author_fullname": "t2_1w5zqtt8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backing up and deduplicating digital camera photos to the cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oarur", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695286407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m interested in backing up my photo collection to backblaze b2b (as it seems the best value supplier)&lt;/p&gt;\n\n&lt;p&gt;I am considering using rclone and crypt as it seems like the best solution.&lt;/p&gt;\n\n&lt;p&gt;The query is this, what is the best workflow to approach this from ?  I have some historic bitrot and I want to ensure that duplicate photos are deleted but the files themselves are not bit identical as picasa and myself have edited the exif metadata.  So only the underlying image data should be checked.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else approached this ?  Filesystem deduplication doesn&amp;#39;t seem to be the answer.&lt;/p&gt;\n\n&lt;p&gt;I have been thinking about using exiftool to export the sha1 to an xmp sidecar, and use python to identify duplicate checksums.  Is this a good approach ?&lt;/p&gt;\n\n&lt;p&gt;As an aside, I have also exported my data from google takeout in case my local data lacks integrity, but that has its exifdata stripped and the images resized I believe.&lt;/p&gt;\n\n&lt;p&gt;Any advice would be appreciated.  Cheers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16oarur", "is_robot_indexable": true, "report_reasons": null, "author": "simonmcnair", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16oarur/backing_up_and_deduplicating_digital_camera/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16oarur/backing_up_and_deduplicating_digital_camera/", "subreddit_subscribers": 703060, "created_utc": 1695286407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CD-Drives and Ferrite Cores: Are they necessary?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o5q90", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_118v1j", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "Cd_collectors", "selftext": "I have a set of external DVD-drives, and they came with USB Mini cables, which all the cables have ferrite cores.\n\nI picked up some second hand DVD-drives that didn't come with USB Mini  cables. I bought some spare cables, and recently picked up more ferrite cores.\n\n&amp;#x200B;\n\n**Question:**\n\nDo external portable CD-Drives need a ferrite core, if all it's doing is reading/writing data? Or would it be fine to use USB mini cables without the ferrite core?\n\nAlso, the USB Mini cables I picked up are pretty cheap.\n\nBut I don't know how noise would interfere with any CD or DVD extraction. Is noise or interference  possible in my case?", "author_fullname": "t2_118v1j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CD-Drives and Ferrite Cores: Are they necessary?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/Cd_collectors", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16o5o6l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695268267.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Cd_collectors", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a set of external DVD-drives, and they came with USB Mini cables, which all the cables have ferrite cores.&lt;/p&gt;\n\n&lt;p&gt;I picked up some second hand DVD-drives that didn&amp;#39;t come with USB Mini  cables. I bought some spare cables, and recently picked up more ferrite cores.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Do external portable CD-Drives need a ferrite core, if all it&amp;#39;s doing is reading/writing data? Or would it be fine to use USB mini cables without the ferrite core?&lt;/p&gt;\n\n&lt;p&gt;Also, the USB Mini cables I picked up are pretty cheap.&lt;/p&gt;\n\n&lt;p&gt;But I don&amp;#39;t know how noise would interfere with any CD or DVD extraction. Is noise or interference  possible in my case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "49431bd4-437f-11ec-97c0-c2be14adce00", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2uxz7", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#646d73", "id": "16o5o6l", "is_robot_indexable": true, "report_reasons": null, "author": "nPrevail", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/Cd_collectors/comments/16o5o6l/cddrives_and_ferrite_cores_are_they_necessary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/Cd_collectors/comments/16o5o6l/cddrives_and_ferrite_cores_are_they_necessary/", "subreddit_subscribers": 33799, "created_utc": 1695268267.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1695268449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Cd_collectors", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/Cd_collectors/comments/16o5o6l/cddrives_and_ferrite_cores_are_they_necessary/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "16o5q90", "is_robot_indexable": true, "report_reasons": null, "author": "nPrevail", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_16o5o6l", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16o5q90/cddrives_and_ferrite_cores_are_they_necessary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/Cd_collectors/comments/16o5o6l/cddrives_and_ferrite_cores_are_they_necessary/", "subreddit_subscribers": 703060, "created_utc": 1695268449.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to get 8TB capacity in RAID 10 with some fault tolerance and better performance to get closer to utilizing 10GBe network locally on laptop and desktop which have 10Gbe connections coming..\n\nI want to try to keep budget as low as possible and it adds up with drives.\n\n[https://www.newegg.com/p/pl?N=100011693 600545605 600038519&amp;Order=1](https://www.newegg.com/p/pl?N=100011693%20600545605%20600038519&amp;Order=1)\n\nThat link above should give list of SATA SSDs 4TB sorted by lowest price.\n\nAre the KingSpec, Silicon Power, and TeamGroup ones fine and reliable for $143 and lower, or are they cheap for a reason meaning they have bad reliability compared to Samsung, WD, Crucial and SanDisk?\n\nOr are they just cheaper for having less performance than the name brands, but still should have fine reliability?\u00a0\n\nNot concerned about less performance as the performance will still be tons multiple times better than the best HDD and buying 4 of them for RAID 10 I want a good deal. Unless performance tanks so much that you get way under 350MB/s which is unlikely as it seems most SSDs even low end ones are often in the 400s MB/s and high end ones upper 500s with much better IOPs", "author_fullname": "t2_w5qns", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are these 4TB SSDs fine for a Raid 10 array in a NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16orez0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695330756.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695330500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to get 8TB capacity in RAID 10 with some fault tolerance and better performance to get closer to utilizing 10GBe network locally on laptop and desktop which have 10Gbe connections coming..&lt;/p&gt;\n\n&lt;p&gt;I want to try to keep budget as low as possible and it adds up with drives.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.newegg.com/p/pl?N=100011693%20600545605%20600038519&amp;amp;Order=1\"&gt;https://www.newegg.com/p/pl?N=100011693 600545605 600038519&amp;amp;Order=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;That link above should give list of SATA SSDs 4TB sorted by lowest price.&lt;/p&gt;\n\n&lt;p&gt;Are the KingSpec, Silicon Power, and TeamGroup ones fine and reliable for $143 and lower, or are they cheap for a reason meaning they have bad reliability compared to Samsung, WD, Crucial and SanDisk?&lt;/p&gt;\n\n&lt;p&gt;Or are they just cheaper for having less performance than the name brands, but still should have fine reliability?\u00a0&lt;/p&gt;\n\n&lt;p&gt;Not concerned about less performance as the performance will still be tons multiple times better than the best HDD and buying 4 of them for RAID 10 I want a good deal. Unless performance tanks so much that you get way under 350MB/s which is unlikely as it seems most SSDs even low end ones are often in the 400s MB/s and high end ones upper 500s with much better IOPs&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16orez0", "is_robot_indexable": true, "report_reasons": null, "author": "Tigers2349", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16orez0/are_these_4tb_ssds_fine_for_a_raid_10_array_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16orez0/are_these_4tb_ssds_fine_for_a_raid_10_array_in_a/", "subreddit_subscribers": 703060, "created_utc": 1695330500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, I hope everyone is doing well.\n\nI am currently looking to build a NAS and I am researching the best way to go about it. For the psychical equipment I have decided to use a NetApp Disk Shelf and my Dell Server. I took a look at Synology's products and felt it wasn't worth the price tag. \n\nHowever the software side of things is where it gets a little murky for me. I know TrueNAS and UnRAID are great pieces of software and I am also aware that, (As big of a pain in the neck that it is) Windows Storage Spaces also has a somewhat dedicated userbase.\n\nHere is what I am planning on doing with my NAS as time goes on\n\n* I am going to be storing photographs, video files, documents, Movies, VM Snapshots, pretty much everything under the sun.\n* Initially I want to install the minimum amount of drives needed to have Double-Parity (I guess in this case 5)\n* As I do not have all of the money currently, I will be expanding my NAS to roughly 100TB and then expand it as needed from there. (100TB is just a estimate number)\n* Ideally I would like to use the least amount of drives possible to achieve said number but this isn't a huge requirement.\n* I want a dedicated cache drive (or two) for faster writes as I will be writing large files to the array regularly. \n\n&amp;#x200B;\n\nCurrently I am leaning most towards using Windows Storage Spaces for a few reasons\n\n* The host machine is already running Windows Server 2022\n* I am more familiar with Windows\n* Storage Spaces seems the most modular, working with Disks of different types and capacities.\n* **Capability to use a cache drive\\*\\*\\***\n\nHowever I have found that WSS is rather stupid when in Parity/Dual-Parity mode. I am not opposed getting dirty in PowerShell in order to get what I want. But when it comes to reliability I have heard power outages causing the entire pool to be lost. (I will have a UPS  but that's not my point) I have also read that in the past there was \"tiering\" in WSS where faster storage would be written to first in order to get better performance. I have not found a single article in my research that claims that this feature actually worked correctly and users were able to get a performance \"gain\" from it. More recently, in Windows Server 2022, Microsoft introduced Storage Bus Cache. However due to this feature being so new, I have been unable find much about people using this feature. Since I currently do not have to hardware to test it myself, I cannot verify if this works as intended. **If anyone has had any use with this feature, please let me know how it worked for you!!**\n\n&amp;#x200B;\n\nMy Second Option is TrueNAS. While I have heard great things about this software and ZFS as a whole, I do have some concerns with it.\n\n* I will not be able to easily expand a volume/pool when I want to add more disks to it. I don't want to have 3 different drive pools to get to my 100TB number because i couldn't afford everything at once.\n\nMy third option would be UnRAID, This does sound like it is the best of both worlds, allowing flexibility in expanding my arrays (As long as the Parity Drives are in check) and allowing a cache drive to work as intended. I do not even mind that there is a price tag attached. My only concern is, that since my Dell T7920 Server is already running Windows and has Hyper-V VMs on it, I would have to run UnRAID in a VM, Doable sure. But it just doesn't sound practical at all.\n\nI have also explored software like StableBit DrivePool I haven't really made a determination on them as of yet, so any insight would be appreciated.\n\n&amp;#x200B;\n\nI hope I explained myself and my situation well enough that some people on here can give me some advice as to what would best fit my needs going forward. If you have any questions feel free to ask them and I will do my best to answer them!!! Thank you!!", "author_fullname": "t2_122wez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to decide what storage pool software to use. UnRAID, TrueNAS, or Windows Storage Spaces.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oqw9c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695329273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I hope everyone is doing well.&lt;/p&gt;\n\n&lt;p&gt;I am currently looking to build a NAS and I am researching the best way to go about it. For the psychical equipment I have decided to use a NetApp Disk Shelf and my Dell Server. I took a look at Synology&amp;#39;s products and felt it wasn&amp;#39;t worth the price tag. &lt;/p&gt;\n\n&lt;p&gt;However the software side of things is where it gets a little murky for me. I know TrueNAS and UnRAID are great pieces of software and I am also aware that, (As big of a pain in the neck that it is) Windows Storage Spaces also has a somewhat dedicated userbase.&lt;/p&gt;\n\n&lt;p&gt;Here is what I am planning on doing with my NAS as time goes on&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I am going to be storing photographs, video files, documents, Movies, VM Snapshots, pretty much everything under the sun.&lt;/li&gt;\n&lt;li&gt;Initially I want to install the minimum amount of drives needed to have Double-Parity (I guess in this case 5)&lt;/li&gt;\n&lt;li&gt;As I do not have all of the money currently, I will be expanding my NAS to roughly 100TB and then expand it as needed from there. (100TB is just a estimate number)&lt;/li&gt;\n&lt;li&gt;Ideally I would like to use the least amount of drives possible to achieve said number but this isn&amp;#39;t a huge requirement.&lt;/li&gt;\n&lt;li&gt;I want a dedicated cache drive (or two) for faster writes as I will be writing large files to the array regularly. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Currently I am leaning most towards using Windows Storage Spaces for a few reasons&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The host machine is already running Windows Server 2022&lt;/li&gt;\n&lt;li&gt;I am more familiar with Windows&lt;/li&gt;\n&lt;li&gt;Storage Spaces seems the most modular, working with Disks of different types and capacities.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Capability to use a cache drive**\\&lt;/strong&gt;*&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;However I have found that WSS is rather stupid when in Parity/Dual-Parity mode. I am not opposed getting dirty in PowerShell in order to get what I want. But when it comes to reliability I have heard power outages causing the entire pool to be lost. (I will have a UPS  but that&amp;#39;s not my point) I have also read that in the past there was &amp;quot;tiering&amp;quot; in WSS where faster storage would be written to first in order to get better performance. I have not found a single article in my research that claims that this feature actually worked correctly and users were able to get a performance &amp;quot;gain&amp;quot; from it. More recently, in Windows Server 2022, Microsoft introduced Storage Bus Cache. However due to this feature being so new, I have been unable find much about people using this feature. Since I currently do not have to hardware to test it myself, I cannot verify if this works as intended. &lt;strong&gt;If anyone has had any use with this feature, please let me know how it worked for you!!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My Second Option is TrueNAS. While I have heard great things about this software and ZFS as a whole, I do have some concerns with it.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I will not be able to easily expand a volume/pool when I want to add more disks to it. I don&amp;#39;t want to have 3 different drive pools to get to my 100TB number because i couldn&amp;#39;t afford everything at once.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My third option would be UnRAID, This does sound like it is the best of both worlds, allowing flexibility in expanding my arrays (As long as the Parity Drives are in check) and allowing a cache drive to work as intended. I do not even mind that there is a price tag attached. My only concern is, that since my Dell T7920 Server is already running Windows and has Hyper-V VMs on it, I would have to run UnRAID in a VM, Doable sure. But it just doesn&amp;#39;t sound practical at all.&lt;/p&gt;\n\n&lt;p&gt;I have also explored software like StableBit DrivePool I haven&amp;#39;t really made a determination on them as of yet, so any insight would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I hope I explained myself and my situation well enough that some people on here can give me some advice as to what would best fit my needs going forward. If you have any questions feel free to ask them and I will do my best to answer them!!! Thank you!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16oqw9c", "is_robot_indexable": true, "report_reasons": null, "author": "Dominator211", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16oqw9c/trying_to_decide_what_storage_pool_software_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16oqw9c/trying_to_decide_what_storage_pool_software_to/", "subreddit_subscribers": 703060, "created_utc": 1695329273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Have a new (to me) 16bay100TB SuperMicro dual Xeon E5-2695 v2,  Quadro P4000, 198GB ECC Ram, 10Gb NIC server just waiting for a new OS.  Which one?  Have just all of the listed ones but Unraid and wanted to see what everyone thought would be the best for a home server/NAS/Docker Arr apps/Plex all-in-one headless server.  Don't run many VM's, have other hardware for that right now. Have run Ubuntu server and Windows 2022 servers in the past and may start one or two up if the need arises. \n\nI know it's overkill city, but it fell in my lap and I hate to piece it out.  Just needed to move HD's from my retired NAS.", "author_fullname": "t2_ba6cz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unraid, TrueNAS Scale, Proxmox, OpenMediaVault: Which one?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16opa4s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695325488.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have a new (to me) 16bay100TB SuperMicro dual Xeon E5-2695 v2,  Quadro P4000, 198GB ECC Ram, 10Gb NIC server just waiting for a new OS.  Which one?  Have just all of the listed ones but Unraid and wanted to see what everyone thought would be the best for a home server/NAS/Docker Arr apps/Plex all-in-one headless server.  Don&amp;#39;t run many VM&amp;#39;s, have other hardware for that right now. Have run Ubuntu server and Windows 2022 servers in the past and may start one or two up if the need arises. &lt;/p&gt;\n\n&lt;p&gt;I know it&amp;#39;s overkill city, but it fell in my lap and I hate to piece it out.  Just needed to move HD&amp;#39;s from my retired NAS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16opa4s", "is_robot_indexable": true, "report_reasons": null, "author": "macfound32", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16opa4s/unraid_truenas_scale_proxmox_openmediavault_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16opa4s/unraid_truenas_scale_proxmox_openmediavault_which/", "subreddit_subscribers": 703060, "created_utc": 1695325488.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have 24 - 2TB SSDs and am unclear if the Dell PowerVault MD1220 will allow for all of them? The spec sheet only mentions 1.6TB SSDs, but I think thats what the option was to ship with? The Max capacity says \" Maximum capacity (per enclosure) Up to 48TB when using 24 x 2TB NL-SAS 2.5\u201d HDDs \" but does that apply to SSDs as well or only Nearline?\n\n[https://i.dell.com/sites/csdocuments/Shared-Content\\_data-Sheets\\_Documents/en/storage-powervault-md1220-specsheet.pdf](https://i.dell.com/sites/csdocuments/Shared-Content_data-Sheets_Documents/en/storage-powervault-md1220-specsheet.pdf)\n\n&amp;#x200B;", "author_fullname": "t2_i8t2x35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dell PowerVault MD1220 SSD Capacity?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16onzzr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695322494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 24 - 2TB SSDs and am unclear if the Dell PowerVault MD1220 will allow for all of them? The spec sheet only mentions 1.6TB SSDs, but I think thats what the option was to ship with? The Max capacity says &amp;quot; Maximum capacity (per enclosure) Up to 48TB when using 24 x 2TB NL-SAS 2.5\u201d HDDs &amp;quot; but does that apply to SSDs as well or only Nearline?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.dell.com/sites/csdocuments/Shared-Content_data-Sheets_Documents/en/storage-powervault-md1220-specsheet.pdf\"&gt;https://i.dell.com/sites/csdocuments/Shared-Content_data-Sheets_Documents/en/storage-powervault-md1220-specsheet.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16onzzr", "is_robot_indexable": true, "report_reasons": null, "author": "fluce13", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16onzzr/dell_powervault_md1220_ssd_capacity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16onzzr/dell_powervault_md1220_ssd_capacity/", "subreddit_subscribers": 703060, "created_utc": 1695322494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title", "author_fullname": "t2_ldb4c05u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are servermonkey refurbished Dell HDDs good for the cheap price?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16omalt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695318468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16omalt", "is_robot_indexable": true, "report_reasons": null, "author": "sexpusa", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16omalt/are_servermonkey_refurbished_dell_hdds_good_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16omalt/are_servermonkey_refurbished_dell_hdds_good_for/", "subreddit_subscribers": 703060, "created_utc": 1695318468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I would like to store complete copies of BluRay disks as ISO or BDMV folders (for playback with menu support on an Oppo player), but then also rip these disks into a compatible format for a PLEX library (mp4 or mkv, depending on which method would result in the most efficient dedup). \n\nIs there any viable deduplication strategy for such a setup, so that it would only require the storage space of approximately that single ISO/BDMV, and no additional storage space for the mp4/mkv files? \n\nWould block level dedup like offered by ZFS be viable for this? Are there better options?", "author_fullname": "t2_1nu5vo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deduplication options for movie library.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ofxls", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695302848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to store complete copies of BluRay disks as ISO or BDMV folders (for playback with menu support on an Oppo player), but then also rip these disks into a compatible format for a PLEX library (mp4 or mkv, depending on which method would result in the most efficient dedup). &lt;/p&gt;\n\n&lt;p&gt;Is there any viable deduplication strategy for such a setup, so that it would only require the storage space of approximately that single ISO/BDMV, and no additional storage space for the mp4/mkv files? &lt;/p&gt;\n\n&lt;p&gt;Would block level dedup like offered by ZFS be viable for this? Are there better options?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16ofxls", "is_robot_indexable": true, "report_reasons": null, "author": "elecsys", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16ofxls/deduplication_options_for_movie_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16ofxls/deduplication_options_for_movie_library/", "subreddit_subscribers": 703060, "created_utc": 1695302848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for budget 3x SSD RAID 1 for Synology NAS, but i am afraid that drives will die at same time.\n\nWhat if i bought one 870 EVO (TLC) and 2x cheap 870 QVO (QLC)? Are there any possible issues with this setup ?\nMaybe even take one TLC SSD from different manufacturer.", "author_fullname": "t2_60vpu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2x QVO and 1x EVO in RAID 1 ? (Samsung 870)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oengr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695300213.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695299391.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for budget 3x SSD RAID 1 for Synology NAS, but i am afraid that drives will die at same time.&lt;/p&gt;\n\n&lt;p&gt;What if i bought one 870 EVO (TLC) and 2x cheap 870 QVO (QLC)? Are there any possible issues with this setup ?\nMaybe even take one TLC SSD from different manufacturer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16oengr", "is_robot_indexable": true, "report_reasons": null, "author": "Harrierx", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16oengr/2x_qvo_and_1x_evo_in_raid_1_samsung_870/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16oengr/2x_qvo_and_1x_evo_in_raid_1_samsung_870/", "subreddit_subscribers": 703060, "created_utc": 1695299391.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I've been planning to buy a Crucial MX500 for data storage but when I check the 1-star reviews on amazon a lot of reviews say that they failed as a system drive. So, I would like to know if it's good for data storage. Thanks.", "author_fullname": "t2_s017aqbz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Crucial MX500 for Data Storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16odnnq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695296587.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;ve been planning to buy a Crucial MX500 for data storage but when I check the 1-star reviews on amazon a lot of reviews say that they failed as a system drive. So, I would like to know if it&amp;#39;s good for data storage. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "0B", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16odnnq", "is_robot_indexable": true, "report_reasons": null, "author": "-KasaneTeto-", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/16odnnq/crucial_mx500_for_data_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16odnnq/crucial_mx500_for_data_storage/", "subreddit_subscribers": 703060, "created_utc": 1695296587.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I'm the manager of a new educative Workspace Fundamentals, per my knowledge all Google Drive storage is now pooled, no unlimited anymore which is fine for us, however I was migrating some data to shared drives and I noticed even after 48 hours that Shared Drives space was not pooled or showing up on the storage bar, is this normal? It seems to be unlimited or am I doing something wrong? Checking also the subscriptions and billing part as we don't want any overcharges but I don't see any \"surprise\" charges.  \n\n\nI don't have any intention on using more than 100Tb, but just curious about it.\n\n&amp;#x200B;\n\nThank you all!\n\nhttps://preview.redd.it/k52spzieikpb1.png?width=2003&amp;format=png&amp;auto=webp&amp;s=81182cd2242462d178815f508695d0ae743642d6", "author_fullname": "t2_11x0y2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shared Drives not part as Pooled Storage? Google Drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 42, "top_awarded_type": null, "hide_score": false, "media_metadata": {"k52spzieikpb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 32, "x": 108, "u": "https://preview.redd.it/k52spzieikpb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a04b233c13b2c51fd83bac625f8be0f0ade506f"}, {"y": 65, "x": 216, "u": "https://preview.redd.it/k52spzieikpb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c953ff63ee88f290c6f0813713738012b441fca2"}, {"y": 97, "x": 320, "u": "https://preview.redd.it/k52spzieikpb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=77912a1ff9c27283a90314ed51e8ffa22c46da17"}, {"y": 194, "x": 640, "u": "https://preview.redd.it/k52spzieikpb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=714062537d226855060b049ba48cd369214c27ac"}, {"y": 291, "x": 960, "u": "https://preview.redd.it/k52spzieikpb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=53d167726462d7c006c0999f3d55b0e71bd0de74"}, {"y": 328, "x": 1080, "u": "https://preview.redd.it/k52spzieikpb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=852dc131265fbaa6a0bea4a3e3c6cdeccabec429"}], "s": {"y": 609, "x": 2003, "u": "https://preview.redd.it/k52spzieikpb1.png?width=2003&amp;format=png&amp;auto=webp&amp;s=81182cd2242462d178815f508695d0ae743642d6"}, "id": "k52spzieikpb1"}}, "name": "t3_16oa7np", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1LCn13ikVyKDN7PrXK2lqBmTOEwOjemV9Lk1xhf_amY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695284238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I&amp;#39;m the manager of a new educative Workspace Fundamentals, per my knowledge all Google Drive storage is now pooled, no unlimited anymore which is fine for us, however I was migrating some data to shared drives and I noticed even after 48 hours that Shared Drives space was not pooled or showing up on the storage bar, is this normal? It seems to be unlimited or am I doing something wrong? Checking also the subscriptions and billing part as we don&amp;#39;t want any overcharges but I don&amp;#39;t see any &amp;quot;surprise&amp;quot; charges.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have any intention on using more than 100Tb, but just curious about it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you all!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/k52spzieikpb1.png?width=2003&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81182cd2242462d178815f508695d0ae743642d6\"&gt;https://preview.redd.it/k52spzieikpb1.png?width=2003&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81182cd2242462d178815f508695d0ae743642d6&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16oa7np", "is_robot_indexable": true, "report_reasons": null, "author": "alemonpie", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16oa7np/shared_drives_not_part_as_pooled_storage_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16oa7np/shared_drives_not_part_as_pooled_storage_google/", "subreddit_subscribers": 703060, "created_utc": 1695284238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just need to get some information. And it would seem you guys have the most experience.\n\nI recently bought a 14TB WD White Label drive (taken from a WD external enclosure). Within the first 36 hours I wrote about 8TB of data (mostly video files) to it at nominal SATA3 speeds. When I did that, everything seemed fine. But now, two months later, I can no longer access some of the data. The drive keeps dropping off when trying to access it (and then reappears as if everything is fine). My old 10TB WD Purple did that too  a month before completely failing and that was after about several years of 24/7 in the desktop. So I know what the deal is.\n\nThe loss of data is irrelevant. I have a multi-tiered air gapped backup schema on older/smaller drives. The monetary loss hurts me more.  \n\nTherefore are WD White Label (external) drives trash? Or did I stumble upon a fluke?\n\nI've never come upon this issue with older and smaller Purple, Blue, Green or Black drives when writing up to 90% of formatted capacity at once. Or with Seagate or Toshiba drives. Up to and including 10TB in capacity. Therefore....\n\nCan you all recommend me a heavy duty SATA hard drive at 12TB or more in capacity. I need to write at least 8TB to it in chunks of about 200-400GB within the first 36 hours.  After that, if it can stay spinning 24/7 with little to no data written (maybe a 500GB a year), for at least 5 years, I'll be happy.\n\n(Sure I could be spinning multiple sub 10TB drives in a JBOD or RAID setup. But I'd rather have a single primary drive for everything).", "author_fullname": "t2_b9vj9w10", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are WD external drives trash? 12TB+ drive recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oskj0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695333156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just need to get some information. And it would seem you guys have the most experience.&lt;/p&gt;\n\n&lt;p&gt;I recently bought a 14TB WD White Label drive (taken from a WD external enclosure). Within the first 36 hours I wrote about 8TB of data (mostly video files) to it at nominal SATA3 speeds. When I did that, everything seemed fine. But now, two months later, I can no longer access some of the data. The drive keeps dropping off when trying to access it (and then reappears as if everything is fine). My old 10TB WD Purple did that too  a month before completely failing and that was after about several years of 24/7 in the desktop. So I know what the deal is.&lt;/p&gt;\n\n&lt;p&gt;The loss of data is irrelevant. I have a multi-tiered air gapped backup schema on older/smaller drives. The monetary loss hurts me more.  &lt;/p&gt;\n\n&lt;p&gt;Therefore are WD White Label (external) drives trash? Or did I stumble upon a fluke?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve never come upon this issue with older and smaller Purple, Blue, Green or Black drives when writing up to 90% of formatted capacity at once. Or with Seagate or Toshiba drives. Up to and including 10TB in capacity. Therefore....&lt;/p&gt;\n\n&lt;p&gt;Can you all recommend me a heavy duty SATA hard drive at 12TB or more in capacity. I need to write at least 8TB to it in chunks of about 200-400GB within the first 36 hours.  After that, if it can stay spinning 24/7 with little to no data written (maybe a 500GB a year), for at least 5 years, I&amp;#39;ll be happy.&lt;/p&gt;\n\n&lt;p&gt;(Sure I could be spinning multiple sub 10TB drives in a JBOD or RAID setup. But I&amp;#39;d rather have a single primary drive for everything).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16oskj0", "is_robot_indexable": true, "report_reasons": null, "author": "MVmikehammer", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16oskj0/are_wd_external_drives_trash_12tb_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16oskj0/are_wd_external_drives_trash_12tb_drive/", "subreddit_subscribers": 703060, "created_utc": 1695333156.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}