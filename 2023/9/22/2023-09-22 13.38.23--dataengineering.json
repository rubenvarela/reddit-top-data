{"kind": "Listing", "data": {"after": "t3_16p5zwo", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like every new trend in Data it has become the norm for people to invent terms. This one is the latest one that people are talking about.\n\nSchema management is not new and nobody in a team does so frequent changes that you need a GitHub like infrastructure to settle on a schema.\n\nIf we want versioning, as you would do in a REST world with breaking changes, that\u2019s a better approach and more formalized way to do change. I feel like the principles of SWE are jammed into the data world with no forethought.\n\nThis concept has been made way too complicated than it needs to be. I think we need some level setting here. \n\nWhat do you think?", "author_fullname": "t2_fy6g58f7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data contracts is a buzzword", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16on32m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695320376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like every new trend in Data it has become the norm for people to invent terms. This one is the latest one that people are talking about.&lt;/p&gt;\n\n&lt;p&gt;Schema management is not new and nobody in a team does so frequent changes that you need a GitHub like infrastructure to settle on a schema.&lt;/p&gt;\n\n&lt;p&gt;If we want versioning, as you would do in a REST world with breaking changes, that\u2019s a better approach and more formalized way to do change. I feel like the principles of SWE are jammed into the data world with no forethought.&lt;/p&gt;\n\n&lt;p&gt;This concept has been made way too complicated than it needs to be. I think we need some level setting here. &lt;/p&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16on32m", "is_robot_indexable": true, "report_reasons": null, "author": "WarriorData", "discussion_type": null, "num_comments": 51, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16on32m/data_contracts_is_a_buzzword/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16on32m/data_contracts_is_a_buzzword/", "subreddit_subscribers": 129806, "created_utc": 1695320376.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have 4 YOE. Current company is great but I\u2019ve plateau\u2019d. Is there a list of employers in the states that is heavily focused on Python, Docker, AWS Snowflake, Airflow? I\u2019ve had experience with many technologies and languages. Over the years I learned that I LOVE writing Python or anything Python related. Any way I can find employers that seek stack heavily focused on Python, AWS, Snowflake, Airflow, Docker? Preferably in the U.S?\nWhich data engineering titles would help me find what I\u2019m looking for? Or how can I find such roles/employers?", "author_fullname": "t2_83p02r6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good Companies that use: Python, AWS, Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oocc3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695323297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have 4 YOE. Current company is great but I\u2019ve plateau\u2019d. Is there a list of employers in the states that is heavily focused on Python, Docker, AWS Snowflake, Airflow? I\u2019ve had experience with many technologies and languages. Over the years I learned that I LOVE writing Python or anything Python related. Any way I can find employers that seek stack heavily focused on Python, AWS, Snowflake, Airflow, Docker? Preferably in the U.S?\nWhich data engineering titles would help me find what I\u2019m looking for? Or how can I find such roles/employers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16oocc3", "is_robot_indexable": true, "report_reasons": null, "author": "1337codethrow", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16oocc3/good_companies_that_use_python_aws_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16oocc3/good_companies_that_use_python_aws_snowflake/", "subreddit_subscribers": 129806, "created_utc": 1695323297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When I migrated to Snowflake my costs went up like 5x. Maybe more.", "author_fullname": "t2_j13q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone migrating away from Snowflake and back to AWS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p1v2j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695360099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I migrated to Snowflake my costs went up like 5x. Maybe more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p1v2j", "is_robot_indexable": true, "report_reasons": null, "author": "Fitbot5000", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p1v2j/anyone_migrating_away_from_snowflake_and_back_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p1v2j/anyone_migrating_away_from_snowflake_and_back_to/", "subreddit_subscribers": 129806, "created_utc": 1695360099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just found out my boss used ChatGPT to write our department goals for data engineering. Mainly because she doesn\u2019t actually understand technology at all.\n\nFor 2024 our ChatGPT overlord demands we:\n\n* Implement an Enterprise data lake\n* Move all existing applications to AWS\n* Implement a master data management solution\n* Implement a self-service model with easy-to-use data catalogs\n* Implement an AI solution for self-service (is ChatGPT trying to reproduce?!!!)\n\nTeam size? 4 engineers.\n\nAnyone else dealing with this kind of craziness?", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ChatGPT for goals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16okwxv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695315146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just found out my boss used ChatGPT to write our department goals for data engineering. Mainly because she doesn\u2019t actually understand technology at all.&lt;/p&gt;\n\n&lt;p&gt;For 2024 our ChatGPT overlord demands we:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Implement an Enterprise data lake&lt;/li&gt;\n&lt;li&gt;Move all existing applications to AWS&lt;/li&gt;\n&lt;li&gt;Implement a master data management solution&lt;/li&gt;\n&lt;li&gt;Implement a self-service model with easy-to-use data catalogs&lt;/li&gt;\n&lt;li&gt;Implement an AI solution for self-service (is ChatGPT trying to reproduce?!!!)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Team size? 4 engineers.&lt;/p&gt;\n\n&lt;p&gt;Anyone else dealing with this kind of craziness?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16okwxv", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16okwxv/chatgpt_for_goals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16okwxv/chatgpt_for_goals/", "subreddit_subscribers": 129806, "created_utc": 1695315146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working as a data analyst but for the past 6 months I have been working on building out data infrastructure for our team. \n\nNamely being the point person for our team to start using dbt, AWS, and more Data Science/Data Modeling type work.\n\nDo you think it would be fair for me to request a job title change from my boss?", "author_fullname": "t2_hhc0wpglp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Title Change Data Analyst \u2014&gt; Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ov9k2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695339885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working as a data analyst but for the past 6 months I have been working on building out data infrastructure for our team. &lt;/p&gt;\n\n&lt;p&gt;Namely being the point person for our team to start using dbt, AWS, and more Data Science/Data Modeling type work.&lt;/p&gt;\n\n&lt;p&gt;Do you think it would be fair for me to request a job title change from my boss?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16ov9k2", "is_robot_indexable": true, "report_reasons": null, "author": "Unable-Barracuda6775", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ov9k2/job_title_change_data_analyst_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ov9k2/job_title_change_data_analyst_engineer/", "subreddit_subscribers": 129806, "created_utc": 1695339885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to start learning how to use sql through python along with pandas, etc. I was hoping you guys could tell me whats the most common option for this in the industry as of now. I saw some say SQLalchemy.", "author_fullname": "t2_8mn3m0sl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tool/framework to pick up for SQL in python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16or26x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695329653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to start learning how to use sql through python along with pandas, etc. I was hoping you guys could tell me whats the most common option for this in the industry as of now. I saw some say SQLalchemy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16or26x", "is_robot_indexable": true, "report_reasons": null, "author": "PurpVan", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16or26x/best_toolframework_to_pick_up_for_sql_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16or26x/best_toolframework_to_pick_up_for_sql_in_python/", "subreddit_subscribers": 129806, "created_utc": 1695329653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently saw a job for a Golang Data Engineer and I was surprised because I don\u2019t remember hearing about any libraries in Go for handling data like how Python has Pandas, Polars, etc. If you\u2019ve used Go for data engineering how are you using it? \n\nI\u2019m a Data Engineer and I wanna learn Go for backend development but if theres some libraries like Polars or Pandas in Go for dealing with data-frames I would love to learn that as well.", "author_fullname": "t2_8wpw0e1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Golang in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16omrba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695319588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently saw a job for a Golang Data Engineer and I was surprised because I don\u2019t remember hearing about any libraries in Go for handling data like how Python has Pandas, Polars, etc. If you\u2019ve used Go for data engineering how are you using it? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m a Data Engineer and I wanna learn Go for backend development but if theres some libraries like Polars or Pandas in Go for dealing with data-frames I would love to learn that as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16omrba", "is_robot_indexable": true, "report_reasons": null, "author": "khaili109", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16omrba/golang_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16omrba/golang_in_data_engineering/", "subreddit_subscribers": 129806, "created_utc": 1695319588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any good recommendations on learning how to design a data architecture for a data science project or an enterprise?", "author_fullname": "t2_61mc4jcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Designing a Data Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16or0j9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695329543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any good recommendations on learning how to design a data architecture for a data science project or an enterprise?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16or0j9", "is_robot_indexable": true, "report_reasons": null, "author": "prtkkr", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16or0j9/designing_a_data_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16or0j9/designing_a_data_architecture/", "subreddit_subscribers": 129806, "created_utc": 1695329543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it better to have all your data in one database or break it into multiple databases for each stage like staging and marts? I could see it being easier to deploy different environments within the same snowflake account with one database. Or should I have one database for raw then another one for staging/analytics? Any recommendations?", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Database Design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oymfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695349598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it better to have all your data in one database or break it into multiple databases for each stage like staging and marts? I could see it being easier to deploy different environments within the same snowflake account with one database. Or should I have one database for raw then another one for staging/analytics? Any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16oymfs", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16oymfs/snowflake_database_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16oymfs/snowflake_database_design/", "subreddit_subscribers": 129806, "created_utc": 1695349598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've gained experience with Google Cloud Platform (GCP) and explored its fundamental services. However, I've noticed that many job opportunities in this field prefer or require AWS expertise. As a result, I've recently set up a free tier AWS account.\n\nPreviously, I had been using GCP with the free credits provided by Google for three months, following tutorials tailored for GCP. This made my experience smooth and easy. The challenge now is that AWS is entirely new to me. I'm seeking guidance on finding resources or platforms where I can discover equivalent tools and services to what I used in GCP.\n\nI'm not completely shifting from GCP to AWS; rather, I want to gain a similar level of exposure to AWS to enhance my qualifications. Any advice on this transition would be greatly appreciated.", "author_fullname": "t2_430i2d0p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switch from GCP to AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16omzke", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695320144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve gained experience with Google Cloud Platform (GCP) and explored its fundamental services. However, I&amp;#39;ve noticed that many job opportunities in this field prefer or require AWS expertise. As a result, I&amp;#39;ve recently set up a free tier AWS account.&lt;/p&gt;\n\n&lt;p&gt;Previously, I had been using GCP with the free credits provided by Google for three months, following tutorials tailored for GCP. This made my experience smooth and easy. The challenge now is that AWS is entirely new to me. I&amp;#39;m seeking guidance on finding resources or platforms where I can discover equivalent tools and services to what I used in GCP.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not completely shifting from GCP to AWS; rather, I want to gain a similar level of exposure to AWS to enhance my qualifications. Any advice on this transition would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16omzke", "is_robot_indexable": true, "report_reasons": null, "author": "Blanco04", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16omzke/switch_from_gcp_to_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16omzke/switch_from_gcp_to_aws/", "subreddit_subscribers": 129806, "created_utc": 1695320144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I received an offer for an interview with a company for an Engineer position after I showcased my portfolio to them, i.e. my Github with sources of my work and full documentation. They offered to let me interview... but I have to do a live coding interview. I asked for more information before moving forward, and was given almost nothing to help me prepare. \n\n\nThe Round 1 Telephonic is a live coding round format:\n\n \n\nOne SQL problem  \u00e0 20 Minutes (They can use postgres/sql lite to solve SQL problem)\n\nOne programming problem \u00e0 20 minutes. (They can use Java/Python programming language to solve)\n\n\nQ &amp; A \u00e0 5 minutes", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone ever been asked to attend an interview with a \"coding exercise\" where they tell you almost nothing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oun5e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695347471.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695338266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I received an offer for an interview with a company for an Engineer position after I showcased my portfolio to them, i.e. my Github with sources of my work and full documentation. They offered to let me interview... but I have to do a live coding interview. I asked for more information before moving forward, and was given almost nothing to help me prepare. &lt;/p&gt;\n\n&lt;p&gt;The Round 1 Telephonic is a live coding round format:&lt;/p&gt;\n\n&lt;p&gt;One SQL problem  \u00e0 20 Minutes (They can use postgres/sql lite to solve SQL problem)&lt;/p&gt;\n\n&lt;p&gt;One programming problem \u00e0 20 minutes. (They can use Java/Python programming language to solve)&lt;/p&gt;\n\n&lt;p&gt;Q &amp;amp; A \u00e0 5 minutes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16oun5e", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16oun5e/has_anyone_ever_been_asked_to_attend_an_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16oun5e/has_anyone_ever_been_asked_to_attend_an_interview/", "subreddit_subscribers": 129806, "created_utc": 1695338266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Question:** What are some low-code time series manipulation tools available for excel users who can't be trusted to author production ready transformations using SQL or Python?  \n\n\nI'm a software dev building tools for a financial data analyst team. The analyst team has asked me to build a tool for self-serve authoring of time series transformations so analysts can author hundreds of these transformations that power dashboards we deliver to customers.\n\nI plan on using open-source only modern data stack tools, dbt in particular, as I don't see a point in reinventing the wheel. The main obstacle however is my team doesn't want to use SQL or Python as our analysts can't be trusted to code. They complain that SQL is too verbose and ugly for time series, and python is powerful enough that analyst-authored code has introduced production bugs previously. Our analysts are more like excel analysts. Thus i'm looking for low-code tools that would make panel data manipulations (time series &amp; cross-sectional gap fills, bucketing, panel data) fairly trivial to author **and** productionize.  \n\n\nI did some research on the following approaches:\n\n* **Semantic Layers**: This is promising as it compiles to SQL, but it's not clear from documentation if they implement time series features well yet.\n* **Custom DSL**: Instead of a YAML like DSL in semantic layers, I could write a simple lisp-like DSL for manipulating time series that feels more like excel while not allowing advanced features in python. This gives me most control over the prettiness of the language while also giving me the freedom to compile to either SQL or Python or wtv in the future. Our data sets are small enough that we don't need the SQL optimizer necessarily.\n* **Dashboard w/ Code Gen**: It might be possible for a tool like superset to generate the SQL visually instead.\n* **Raw SQL**: I've been holding my ground that data analysts should be expected to write SQL, as it was literally designed for analysts to do business transformations. The syntax is ugly and can get fairly advanced, but it has a proven track record for several decades, with a large ecosystem of tooling. I don't know if I have the level of influence to change an entire org though. I will keep trying to push this concern to higher level mgmt as i think upskilling your workforce while eliminating engineering work is a big win overall.", "author_fullname": "t2_2naya68b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What simplified language to use for time series (panel) data manipulation in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ojf6c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695311474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What are some low-code time series manipulation tools available for excel users who can&amp;#39;t be trusted to author production ready transformations using SQL or Python?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a software dev building tools for a financial data analyst team. The analyst team has asked me to build a tool for self-serve authoring of time series transformations so analysts can author hundreds of these transformations that power dashboards we deliver to customers.&lt;/p&gt;\n\n&lt;p&gt;I plan on using open-source only modern data stack tools, dbt in particular, as I don&amp;#39;t see a point in reinventing the wheel. The main obstacle however is my team doesn&amp;#39;t want to use SQL or Python as our analysts can&amp;#39;t be trusted to code. They complain that SQL is too verbose and ugly for time series, and python is powerful enough that analyst-authored code has introduced production bugs previously. Our analysts are more like excel analysts. Thus i&amp;#39;m looking for low-code tools that would make panel data manipulations (time series &amp;amp; cross-sectional gap fills, bucketing, panel data) fairly trivial to author &lt;strong&gt;and&lt;/strong&gt; productionize.  &lt;/p&gt;\n\n&lt;p&gt;I did some research on the following approaches:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Semantic Layers&lt;/strong&gt;: This is promising as it compiles to SQL, but it&amp;#39;s not clear from documentation if they implement time series features well yet.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Custom DSL&lt;/strong&gt;: Instead of a YAML like DSL in semantic layers, I could write a simple lisp-like DSL for manipulating time series that feels more like excel while not allowing advanced features in python. This gives me most control over the prettiness of the language while also giving me the freedom to compile to either SQL or Python or wtv in the future. Our data sets are small enough that we don&amp;#39;t need the SQL optimizer necessarily.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dashboard w/ Code Gen&lt;/strong&gt;: It might be possible for a tool like superset to generate the SQL visually instead.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Raw SQL&lt;/strong&gt;: I&amp;#39;ve been holding my ground that data analysts should be expected to write SQL, as it was literally designed for analysts to do business transformations. The syntax is ugly and can get fairly advanced, but it has a proven track record for several decades, with a large ecosystem of tooling. I don&amp;#39;t know if I have the level of influence to change an entire org though. I will keep trying to push this concern to higher level mgmt as i think upskilling your workforce while eliminating engineering work is a big win overall.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ojf6c", "is_robot_indexable": true, "report_reasons": null, "author": "shuaibot", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ojf6c/what_simplified_language_to_use_for_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ojf6c/what_simplified_language_to_use_for_time_series/", "subreddit_subscribers": 129806, "created_utc": 1695311474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w07bcus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming Databases: Everything You Wanted to Know", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "name": "t3_16osaaa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GR1WqvPIIOLXNrUUsA-uQuEaf6gcpeSwaZmjjPetkBs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695332501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "risingwave.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.risingwave.com/blog/streaming-databases-everything-you-wanted-to-know/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?auto=webp&amp;s=b7e4dcc20f21eb08532be4f44bcb2afd83a3598f", "width": 2560, "height": 1600}, "resolutions": [{"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da32a5d91a292bffb6d119660b638c9e590567e6", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7353fe828c48d0f4db3187b3034190456a3dec1f", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdb2196149d90817950922af7342f2f08e62780a", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e8121c7e1905cecc6c23cc5660a7c426e5bb4ae2", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=62cc77e8c5b58f12cdccc038f3cd16ebdf21ced9", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=60b72dad7898861032ecfe89103ca54e6a5d96e2", "width": 1080, "height": 675}], "variants": {}, "id": "hFCRvLcZo01Mnja6CckUi4AgVCLM2MZOpfNWBh4FS5U"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16osaaa", "is_robot_indexable": true, "report_reasons": null, "author": "yingjunwu", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16osaaa/streaming_databases_everything_you_wanted_to_know/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.risingwave.com/blog/streaming-databases-everything-you-wanted-to-know/", "subreddit_subscribers": 129806, "created_utc": 1695332501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nin my current org, we are looking to revamp our data products. We're looking at new ELT tools as well as doing a bit of a \"reset\" for the standard approach to building data pipelines. \n\nI am struggling with pitching what I perceive as the \"modern\" solution.\n\nCurrent state:\n\nWe have an in house tool, something like Airflow, to control the workflow orchestration. For the actual code, typically my colleagues (data engineers) are writing often opaque PHP+SQL scripts. You know, scripts the create the joys of reviewing what's going wrong in 10,000+ lines of code that one colleague wrote (who happens to be on holiday when it breaks)\n\nProposed state:\n\nEssentially I am pitching that we use a [mage.ai](https://mage.ai) like solution, and build our pipelines with python+dbt+sql.\n\nI foresee [Mage.ai](https://Mage.ai) (or something similar like Airbyte) giving me the visibility over the pipeline (assuming we use granular code blocks; \\~1 for each transformation step); compared to the currently opaque PHP scripts my colleagues are producing.\n\nThen, I expect workflow orchestration to be vastly improved, thanks to dbt run. Which lets us slowly abandon the in house tool we've built up over the years.\n\nCaveat:\n\nI have come into my current \"junior manager\" role, from an analyst background, so I am quite happy to listen to the more experienced data engineering colleagues, regarding best practice.\n\nThat being said, even with less experience, I can't understand the reasoning for continuing our current path. So many of our php pipelines contain arrays being tortured to death;.....where I would just drop the data into the database and transform it in SQL. It sometimes feels like little(big) protected islands for the developer are being made, every time one of these opaque behemoth scripts is released. \n\nIs this something you have encountered in your teams? Am I on the right path? Am I fighting the good fight? If not, why?\n\n\\*We generally don't have any \\*special\\* requirements for our pipelines (streaming, big data, real time....etc)....it's often a \"regular\" ingest (from prod API) -&gt; transform -&gt; (BI) datamart type pipeline.", "author_fullname": "t2_20h89ytj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modernizing data pipelines......organizational blockers or am I just a noob?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p4ztz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695372162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;in my current org, we are looking to revamp our data products. We&amp;#39;re looking at new ELT tools as well as doing a bit of a &amp;quot;reset&amp;quot; for the standard approach to building data pipelines. &lt;/p&gt;\n\n&lt;p&gt;I am struggling with pitching what I perceive as the &amp;quot;modern&amp;quot; solution.&lt;/p&gt;\n\n&lt;p&gt;Current state:&lt;/p&gt;\n\n&lt;p&gt;We have an in house tool, something like Airflow, to control the workflow orchestration. For the actual code, typically my colleagues (data engineers) are writing often opaque PHP+SQL scripts. You know, scripts the create the joys of reviewing what&amp;#39;s going wrong in 10,000+ lines of code that one colleague wrote (who happens to be on holiday when it breaks)&lt;/p&gt;\n\n&lt;p&gt;Proposed state:&lt;/p&gt;\n\n&lt;p&gt;Essentially I am pitching that we use a &lt;a href=\"https://mage.ai\"&gt;mage.ai&lt;/a&gt; like solution, and build our pipelines with python+dbt+sql.&lt;/p&gt;\n\n&lt;p&gt;I foresee &lt;a href=\"https://Mage.ai\"&gt;Mage.ai&lt;/a&gt; (or something similar like Airbyte) giving me the visibility over the pipeline (assuming we use granular code blocks; ~1 for each transformation step); compared to the currently opaque PHP scripts my colleagues are producing.&lt;/p&gt;\n\n&lt;p&gt;Then, I expect workflow orchestration to be vastly improved, thanks to dbt run. Which lets us slowly abandon the in house tool we&amp;#39;ve built up over the years.&lt;/p&gt;\n\n&lt;p&gt;Caveat:&lt;/p&gt;\n\n&lt;p&gt;I have come into my current &amp;quot;junior manager&amp;quot; role, from an analyst background, so I am quite happy to listen to the more experienced data engineering colleagues, regarding best practice.&lt;/p&gt;\n\n&lt;p&gt;That being said, even with less experience, I can&amp;#39;t understand the reasoning for continuing our current path. So many of our php pipelines contain arrays being tortured to death;.....where I would just drop the data into the database and transform it in SQL. It sometimes feels like little(big) protected islands for the developer are being made, every time one of these opaque behemoth scripts is released. &lt;/p&gt;\n\n&lt;p&gt;Is this something you have encountered in your teams? Am I on the right path? Am I fighting the good fight? If not, why?&lt;/p&gt;\n\n&lt;p&gt;*We generally don&amp;#39;t have any *special* requirements for our pipelines (streaming, big data, real time....etc)....it&amp;#39;s often a &amp;quot;regular&amp;quot; ingest (from prod API) -&amp;gt; transform -&amp;gt; (BI) datamart type pipeline.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?auto=webp&amp;s=c6069afb05bd5a0610454eff205b0de0f4c7cef2", "width": 1524, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa478f4ea779b8ac72061216cb8a8cda7e655638", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1f4194503a2dbbc777d91bf043f284a7ce85b6d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db595a6de5fa4afca5771571c913b2f01f4119b7", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2a94dbbee373405a6a78a29452b9490eba644bb", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7f0b65ba6c15688f3916734aae188f2390a6042", "width": 960, "height": 503}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d7425076e002cc0e60bfaa4c1a94925f12856bd8", "width": 1080, "height": 566}], "variants": {}, "id": "9-9fQOf4CbozRQb-4LsE4XBYETAYFLqieboBGJfHCFI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p4ztz", "is_robot_indexable": true, "report_reasons": null, "author": "Lalagabor", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p4ztz/modernizing_data_pipelinesorganizational_blockers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p4ztz/modernizing_data_pipelinesorganizational_blockers/", "subreddit_subscribers": 129806, "created_utc": 1695372162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Some background:\nI currently work for a small startup (30 employees) and we are starting to mature our data infrastructure (postgres -&gt; AWS DMS-&gt; Redshift -&gt;DBT-&gt;Redshift-&gt;PowerBi). \nWe store a large amount of information in JSONs which can sometimes be &gt;100k characters long, suprassing the 65k Byte limit for Amazon Redshift data limits for a single cell. \nThese JSONs are paramount to our reporting - what options are there to be able to utilise this json without it being truncated in redshift? We don't have a dedicated data engineer and so don't want to create too much infra to upkeep. \nIs the only option to create some custom python scripts to abstract certain keys from the JSON?\nCheers!", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading Large JSON into Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16p8no9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695384692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some background:\nI currently work for a small startup (30 employees) and we are starting to mature our data infrastructure (postgres -&amp;gt; AWS DMS-&amp;gt; Redshift -&amp;gt;DBT-&amp;gt;Redshift-&amp;gt;PowerBi). \nWe store a large amount of information in JSONs which can sometimes be &amp;gt;100k characters long, suprassing the 65k Byte limit for Amazon Redshift data limits for a single cell. \nThese JSONs are paramount to our reporting - what options are there to be able to utilise this json without it being truncated in redshift? We don&amp;#39;t have a dedicated data engineer and so don&amp;#39;t want to create too much infra to upkeep. \nIs the only option to create some custom python scripts to abstract certain keys from the JSON?\nCheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16p8no9", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p8no9/loading_large_json_into_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p8no9/loading_large_json_into_redshift/", "subreddit_subscribers": 129806, "created_utc": 1695384692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have data that looks like this:\n\n&amp;#x200B;\n\n|Row|Hashed IP Address|Cookie|Persistent User ID|\n|:-|:-|:-|:-|\n|1|A|1||\n|2|B|1|DD|\n|3|B|2||\n|4|C|3|DD|\n|5|E|4|EE|\n|6||5|EE|\n\nTo improve attribution, we're trying to leverage three data points to find unique users: a hashed IP address, a tracking cookie, and a persistent user ID sent from the backend when a user is logged in.\n\nThe essential theory is that, if any data point shows up more than once, we know that's the same user. \n\nSo, by looking at the data, I know that rows 1 - 4 are the same person and that 5 - 6 are another person. What I don't know if how to tell a computer how to figure that out.\n\nI know other people have tackled this challenge before, but I can't find anything on it. Anyone know of any articles or videos on this challenge?\n\n*Note: There are other challenges here, like dealing with the possibility that multiple people get assigned the same IP, (e.g: through mobile networks or VPNs) or log in to multiple accounts, but we'll deal with one problem at a time here.*", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tracking one user with three datapoints", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p6fgo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695377680.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695377497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have data that looks like this:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Row&lt;/th&gt;\n&lt;th align=\"left\"&gt;Hashed IP Address&lt;/th&gt;\n&lt;th align=\"left\"&gt;Cookie&lt;/th&gt;\n&lt;th align=\"left\"&gt;Persistent User ID&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;B&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;DD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;B&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;C&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;DD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;td align=\"left\"&gt;E&lt;/td&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;EE&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;6&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;td align=\"left\"&gt;EE&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;To improve attribution, we&amp;#39;re trying to leverage three data points to find unique users: a hashed IP address, a tracking cookie, and a persistent user ID sent from the backend when a user is logged in.&lt;/p&gt;\n\n&lt;p&gt;The essential theory is that, if any data point shows up more than once, we know that&amp;#39;s the same user. &lt;/p&gt;\n\n&lt;p&gt;So, by looking at the data, I know that rows 1 - 4 are the same person and that 5 - 6 are another person. What I don&amp;#39;t know if how to tell a computer how to figure that out.&lt;/p&gt;\n\n&lt;p&gt;I know other people have tackled this challenge before, but I can&amp;#39;t find anything on it. Anyone know of any articles or videos on this challenge?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Note: There are other challenges here, like dealing with the possibility that multiple people get assigned the same IP, (e.g: through mobile networks or VPNs) or log in to multiple accounts, but we&amp;#39;ll deal with one problem at a time here.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16p6fgo", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p6fgo/tracking_one_user_with_three_datapoints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p6fgo/tracking_one_user_with_three_datapoints/", "subreddit_subscribers": 129806, "created_utc": 1695377497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working for a company where there pipelines are hosted on Google Cloud Run in a Flask application and orchestrated via Airflow. This model already existed when I joined, but it doesn't seem ideal to me.  Some of these scripts are very complex and use a lot of selenium to collect data.  Others are simpler pipelines. What do you recommend, refactor and move everything to Airflow or is there a better solution?", "author_fullname": "t2_96jab4in", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to host my pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p6bjh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695377099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working for a company where there pipelines are hosted on Google Cloud Run in a Flask application and orchestrated via Airflow. This model already existed when I joined, but it doesn&amp;#39;t seem ideal to me.  Some of these scripts are very complex and use a lot of selenium to collect data.  Others are simpler pipelines. What do you recommend, refactor and move everything to Airflow or is there a better solution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p6bjh", "is_robot_indexable": true, "report_reasons": null, "author": "T0ny_Corleone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p6bjh/where_to_host_my_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p6bjh/where_to_host_my_pipelines/", "subreddit_subscribers": 129806, "created_utc": 1695377099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all first time poster little while lurker.\n\nI see alot of people talk about snowflake and AWS as these seem to be very popular. I don't know if there is a competitive edge or it's just happens to be the go to tool in this industry. Can I ask if many of you out there use Google Cloud Platform and Big Query or a combination of on premises and GCP or different cloud providers?", "author_fullname": "t2_4r6g4urx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p66k3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695376617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all first time poster little while lurker.&lt;/p&gt;\n\n&lt;p&gt;I see alot of people talk about snowflake and AWS as these seem to be very popular. I don&amp;#39;t know if there is a competitive edge or it&amp;#39;s just happens to be the go to tool in this industry. Can I ask if many of you out there use Google Cloud Platform and Big Query or a combination of on premises and GCP or different cloud providers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p66k3", "is_robot_indexable": true, "report_reasons": null, "author": "ljsmith970", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p66k3/gcp_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p66k3/gcp_bigquery/", "subreddit_subscribers": 129806, "created_utc": 1695376617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \n\nI am looking for advise /reassurance on how to change jobs into a Data Engineering role within Europe. \n\nI have worked in marketing for the last 4 years, but have been hating it. In my roles, I have been gravitating towards data and tech more than the marketing elements. Recently, i was made redundant due to the company collapsing. So, I am taking this opportunity to switch. I looked into bootcamps, but I can't really afford the price of \u20ac7,000. I can probably pay about \u20ac1,000 for learning etc (gotta keep bread on the table). \n\n\nI have taken some courses in the past in python. I am not versed in object oriented programming, but have a very good understanding of the fundamentals and have created a few NLP projects. And I also have a very low level understanding of SQL and mongoDB.\n\nI am wondering if anyone else has also switched careers in the European zone and what your experience was in changing? If there are books, courses or certs that would help the switch?\n\nAlso, Is it in 2023 a bad time to change careers? (I keep being told by my friends that it's the wrong time ) \ud83d\ude2d\n\nAlso a bit off topic, but would a scrum cert benefit the role?\n\n\nAny help would be appreciated \ud83d\udc4d", "author_fullname": "t2_7lu47gxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is A European Role Change To DE hard?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p5t70", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695375237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I am looking for advise /reassurance on how to change jobs into a Data Engineering role within Europe. &lt;/p&gt;\n\n&lt;p&gt;I have worked in marketing for the last 4 years, but have been hating it. In my roles, I have been gravitating towards data and tech more than the marketing elements. Recently, i was made redundant due to the company collapsing. So, I am taking this opportunity to switch. I looked into bootcamps, but I can&amp;#39;t really afford the price of \u20ac7,000. I can probably pay about \u20ac1,000 for learning etc (gotta keep bread on the table). &lt;/p&gt;\n\n&lt;p&gt;I have taken some courses in the past in python. I am not versed in object oriented programming, but have a very good understanding of the fundamentals and have created a few NLP projects. And I also have a very low level understanding of SQL and mongoDB.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if anyone else has also switched careers in the European zone and what your experience was in changing? If there are books, courses or certs that would help the switch?&lt;/p&gt;\n\n&lt;p&gt;Also, Is it in 2023 a bad time to change careers? (I keep being told by my friends that it&amp;#39;s the wrong time ) \ud83d\ude2d&lt;/p&gt;\n\n&lt;p&gt;Also a bit off topic, but would a scrum cert benefit the role?&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated \ud83d\udc4d&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16p5t70", "is_robot_indexable": true, "report_reasons": null, "author": "Representative_Two37", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p5t70/is_a_european_role_change_to_de_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p5t70/is_a_european_role_change_to_de_hard/", "subreddit_subscribers": 129806, "created_utc": 1695375237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am getting data from on premise sharepoint. Users put data into folder and I need to process them.  \nSo far I didnt found a solution how on premise sharepoint can notify that file was uploaded into folder, so I am getting folder content using sharepoint API and once there is a file, I process it and move it to another folder.\n\nIt is just set of python codes that handle file processing. \n\nHow would you schedule processing? Now it is cron job on a centos server. \n\nI can use openshift, or we are also running Airflow, but I dont think Airflow would be good match for running tasks this frequently.\n\nMany thanks for your ideas!", "author_fullname": "t2_46l1zqd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to deploy and run job that needs to be run every 2 minutes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p59ze", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695373246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am getting data from on premise sharepoint. Users put data into folder and I need to process them.&lt;br/&gt;\nSo far I didnt found a solution how on premise sharepoint can notify that file was uploaded into folder, so I am getting folder content using sharepoint API and once there is a file, I process it and move it to another folder.&lt;/p&gt;\n\n&lt;p&gt;It is just set of python codes that handle file processing. &lt;/p&gt;\n\n&lt;p&gt;How would you schedule processing? Now it is cron job on a centos server. &lt;/p&gt;\n\n&lt;p&gt;I can use openshift, or we are also running Airflow, but I dont think Airflow would be good match for running tasks this frequently.&lt;/p&gt;\n\n&lt;p&gt;Many thanks for your ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p59ze", "is_robot_indexable": true, "report_reasons": null, "author": "pyzo_ryzo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p59ze/where_to_deploy_and_run_job_that_needs_to_be_run/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p59ze/where_to_deploy_and_run_job_that_needs_to_be_run/", "subreddit_subscribers": 129806, "created_utc": 1695373246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a bunch of US/Non US addresses which I need to standardize according to USPS format. Can you guys recommend me best way to do it.  Is there any Python package that I can rely?  \nOr Is there any paid/free third party endpoint where I can make a call to validate.  Google gave me [https://www.smarty.com/](https://www.smarty.com/) but I am not sure about other. Do you guys have any recommended vendor?", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Address verification and standardization tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p1smy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695359860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bunch of US/Non US addresses which I need to standardize according to USPS format. Can you guys recommend me best way to do it.  Is there any Python package that I can rely?&lt;br/&gt;\nOr Is there any paid/free third party endpoint where I can make a call to validate.  Google gave me &lt;a href=\"https://www.smarty.com/\"&gt;https://www.smarty.com/&lt;/a&gt; but I am not sure about other. Do you guys have any recommended vendor?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?auto=webp&amp;s=5633dd3efd6979a9f065870210ba32bdb450edaf", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec1290b11636f849df11bcd2a9574db1d2de71ee", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c44367f44736bde57c7cf902bf409d31522ea58", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6307f712ef2da9365026f5339c461ddbf092005", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdfeaaa5bb4f017311064d21d8af2f904255001b", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a4249baf92f99963d4f1ec5e66bf2c47be2a5579", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=837281c129c8a69e452296dea9ef616ba85f177c", "width": 1080, "height": 567}], "variants": {}, "id": "RgGgH5XSBWSNjWbkX2NvcezqdVDW9PhVpc-BnehunSc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16p1smy", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p1smy/address_verification_and_standardization_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p1smy/address_verification_and_standardization_tools/", "subreddit_subscribers": 129806, "created_utc": 1695359860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "can some one explain how to ingest downloaded data from kaggle incrementally using python. you can use aws glue  and store data in s3 ?", "author_fullname": "t2_8vmwwx5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16onc3z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695320978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;can some one explain how to ingest downloaded data from kaggle incrementally using python. you can use aws glue  and store data in s3 ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16onc3z", "is_robot_indexable": true, "report_reasons": null, "author": "lifealtering111", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16onc3z/data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16onc3z/data_ingestion/", "subreddit_subscribers": 129806, "created_utc": 1695320978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_agd3b25og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple personal finance data engineering project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_16omopv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/qAfYIWzd97XHeRbSSy9Bti07lV5qAIIu4eeql_n5Kr8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695319410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/sam-wright-1/personal-finance-automation", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?auto=webp&amp;s=2bfb8cac5bfb9409447c65c5b2e8e5944295f808", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3fd795baaad4e5c64facc3c9e2d1f59c66386a7d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c4831f914f843e0eb0f349575c65e58636bd0af", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8614c9e967090b21dd6049af4d683c83388492f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fef6f995a2c998faba20c8c794cb30b76ba5fb4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=170e686546300424996ce707e93c461e87788af2", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d51f9a396d2b197fe1be96da7ed54ada899634f", "width": 1080, "height": 540}], "variants": {}, "id": "p3Ivo_RgfDav9G-934-9Ik7yNegAsq2eYHKKp5kjpvA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "16omopv", "is_robot_indexable": true, "report_reasons": null, "author": "data-partner", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16omopv/simple_personal_finance_data_engineering_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/sam-wright-1/personal-finance-automation", "subreddit_subscribers": 129806, "created_utc": 1695319410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi  I'm new to data engineering.\n\nI want to devise a system that loads tabular data (10-20 columns with 10\\^9/10\\^10/ -billions of data records), that will be ingest from eternal API.\n\nThe system should do processing (filtering subset of the rows and make basic computation analysis)\n\nHow to approach this problem? and what are the recommenced tools\n\nDoes best practice of DB like Spark or Snowflake will be sufficient for that task?\n\nand if there are any good resources to gain more knowledge- it will be awesome!\n\nI'm looking for the best of: latency in inference, easy to develop my system, easy to maintain, minimum server requirement, easy and basic monitor.\n\nThanks!\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_5831rdhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to approach designing a novel system that loads tabular data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oiax5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695308771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi  I&amp;#39;m new to data engineering.&lt;/p&gt;\n\n&lt;p&gt;I want to devise a system that loads tabular data (10-20 columns with 10^9/10^10/ -billions of data records), that will be ingest from eternal API.&lt;/p&gt;\n\n&lt;p&gt;The system should do processing (filtering subset of the rows and make basic computation analysis)&lt;/p&gt;\n\n&lt;p&gt;How to approach this problem? and what are the recommenced tools&lt;/p&gt;\n\n&lt;p&gt;Does best practice of DB like Spark or Snowflake will be sufficient for that task?&lt;/p&gt;\n\n&lt;p&gt;and if there are any good resources to gain more knowledge- it will be awesome!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for the best of: latency in inference, easy to develop my system, easy to maintain, minimum server requirement, easy and basic monitor.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16oiax5", "is_robot_indexable": true, "report_reasons": null, "author": "Expensive_Breakfast6", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16oiax5/how_to_approach_designing_a_novel_system_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16oiax5/how_to_approach_designing_a_novel_system_that/", "subreddit_subscribers": 129806, "created_utc": 1695308771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://reddit.com/r/dataengineering/s/ZigMuAZRPP\n\nIn what order do you recommend acquiring the skills?", "author_fullname": "t2_jpnf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Follow up question to the 80/20 Pareto skills question.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p5zwo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695375932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://reddit.com/r/dataengineering/s/ZigMuAZRPP\"&gt;https://reddit.com/r/dataengineering/s/ZigMuAZRPP&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In what order do you recommend acquiring the skills?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p5zwo", "is_robot_indexable": true, "report_reasons": null, "author": "kiwifruta", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p5zwo/follow_up_question_to_the_8020_pareto_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p5zwo/follow_up_question_to_the_8020_pareto_skills/", "subreddit_subscribers": 129806, "created_utc": 1695375932.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}