{"kind": "Listing", "data": {"after": "t3_16onect", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like every new trend in Data it has become the norm for people to invent terms. This one is the latest one that people are talking about.\n\nSchema management is not new and nobody in a team does so frequent changes that you need a GitHub like infrastructure to settle on a schema.\n\nIf we want versioning, as you would do in a REST world with breaking changes, that\u2019s a better approach and more formalized way to do change. I feel like the principles of SWE are jammed into the data world with no forethought.\n\nThis concept has been made way too complicated than it needs to be. I think we need some level setting here. \n\nWhat do you think?", "author_fullname": "t2_fy6g58f7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data contracts is a buzzword", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16on32m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695320376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like every new trend in Data it has become the norm for people to invent terms. This one is the latest one that people are talking about.&lt;/p&gt;\n\n&lt;p&gt;Schema management is not new and nobody in a team does so frequent changes that you need a GitHub like infrastructure to settle on a schema.&lt;/p&gt;\n\n&lt;p&gt;If we want versioning, as you would do in a REST world with breaking changes, that\u2019s a better approach and more formalized way to do change. I feel like the principles of SWE are jammed into the data world with no forethought.&lt;/p&gt;\n\n&lt;p&gt;This concept has been made way too complicated than it needs to be. I think we need some level setting here. &lt;/p&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16on32m", "is_robot_indexable": true, "report_reasons": null, "author": "WarriorData", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16on32m/data_contracts_is_a_buzzword/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16on32m/data_contracts_is_a_buzzword/", "subreddit_subscribers": 129769, "created_utc": 1695320376.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have 4 YOE. Current company is great but I\u2019ve plateau\u2019d. Is there a list of employers in the states that is heavily focused on Python, Docker, AWS Snowflake, Airflow? I\u2019ve had experience with many technologies and languages. Over the years I learned that I LOVE writing Python or anything Python related. Any way I can find employers that seek stack heavily focused on Python, AWS, Snowflake, Airflow, Docker? Preferably in the U.S?\nWhich data engineering titles would help me find what I\u2019m looking for? Or how can I find such roles/employers?", "author_fullname": "t2_83p02r6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good Companies that use: Python, AWS, Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oocc3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695323297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have 4 YOE. Current company is great but I\u2019ve plateau\u2019d. Is there a list of employers in the states that is heavily focused on Python, Docker, AWS Snowflake, Airflow? I\u2019ve had experience with many technologies and languages. Over the years I learned that I LOVE writing Python or anything Python related. Any way I can find employers that seek stack heavily focused on Python, AWS, Snowflake, Airflow, Docker? Preferably in the U.S?\nWhich data engineering titles would help me find what I\u2019m looking for? Or how can I find such roles/employers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16oocc3", "is_robot_indexable": true, "report_reasons": null, "author": "1337codethrow", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16oocc3/good_companies_that_use_python_aws_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16oocc3/good_companies_that_use_python_aws_snowflake/", "subreddit_subscribers": 129769, "created_utc": 1695323297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working as a data analyst but for the past 6 months I have been working on building out data infrastructure for our team. \n\nNamely being the point person for our team to start using dbt, AWS, and more Data Science/Data Modeling type work.\n\nDo you think it would be fair for me to request a job title change from my boss?", "author_fullname": "t2_hhc0wpglp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Title Change Data Analyst \u2014&gt; Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ov9k2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695339885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working as a data analyst but for the past 6 months I have been working on building out data infrastructure for our team. &lt;/p&gt;\n\n&lt;p&gt;Namely being the point person for our team to start using dbt, AWS, and more Data Science/Data Modeling type work.&lt;/p&gt;\n\n&lt;p&gt;Do you think it would be fair for me to request a job title change from my boss?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16ov9k2", "is_robot_indexable": true, "report_reasons": null, "author": "Unable-Barracuda6775", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ov9k2/job_title_change_data_analyst_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ov9k2/job_title_change_data_analyst_engineer/", "subreddit_subscribers": 129769, "created_utc": 1695339885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just found out my boss used ChatGPT to write our department goals for data engineering. Mainly because she doesn\u2019t actually understand technology at all.\n\nFor 2024 our ChatGPT overlord demands we:\n\n* Implement an Enterprise data lake\n* Move all existing applications to AWS\n* Implement a master data management solution\n* Implement a self-service model with easy-to-use data catalogs\n* Implement an AI solution for self-service (is ChatGPT trying to reproduce?!!!)\n\nTeam size? 4 engineers.\n\nAnyone else dealing with this kind of craziness?", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ChatGPT for goals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16okwxv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695315146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just found out my boss used ChatGPT to write our department goals for data engineering. Mainly because she doesn\u2019t actually understand technology at all.&lt;/p&gt;\n\n&lt;p&gt;For 2024 our ChatGPT overlord demands we:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Implement an Enterprise data lake&lt;/li&gt;\n&lt;li&gt;Move all existing applications to AWS&lt;/li&gt;\n&lt;li&gt;Implement a master data management solution&lt;/li&gt;\n&lt;li&gt;Implement a self-service model with easy-to-use data catalogs&lt;/li&gt;\n&lt;li&gt;Implement an AI solution for self-service (is ChatGPT trying to reproduce?!!!)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Team size? 4 engineers.&lt;/p&gt;\n\n&lt;p&gt;Anyone else dealing with this kind of craziness?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16okwxv", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16okwxv/chatgpt_for_goals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16okwxv/chatgpt_for_goals/", "subreddit_subscribers": 129769, "created_utc": 1695315146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am storing over 200gb of data on mysql, I am considering  to migrate it into a more effective dbms (or just keep using mysql?):\n- Data durability is optional (this table is defered from another source of truth so I can regeneration it).\n- Not in-memory db (I can not afford to have 200GB memory)\n- Read instensive, infrequently batched update.\n- the query is very simple (select * from table where key1 &gt; key2 and key3 &gt; const ) . don't need to join multiple table or doing aggregation \n- Support range query, index\n- Effective data storage (I don't want it to turn into a 1TB data)\n- Easy to deploy (only need to run on single node via docker)", "author_fullname": "t2_bju0j9jyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choose me a database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocikg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695292819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am storing over 200gb of data on mysql, I am considering  to migrate it into a more effective dbms (or just keep using mysql?):\n- Data durability is optional (this table is defered from another source of truth so I can regeneration it).\n- Not in-memory db (I can not afford to have 200GB memory)\n- Read instensive, infrequently batched update.\n- the query is very simple (select * from table where key1 &amp;gt; key2 and key3 &amp;gt; const ) . don&amp;#39;t need to join multiple table or doing aggregation \n- Support range query, index\n- Effective data storage (I don&amp;#39;t want it to turn into a 1TB data)\n- Easy to deploy (only need to run on single node via docker)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ocikg", "is_robot_indexable": true, "report_reasons": null, "author": "chu_nghia_nam_thang", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocikg/choose_me_a_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ocikg/choose_me_a_database/", "subreddit_subscribers": 129769, "created_utc": 1695292819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to start learning how to use sql through python along with pandas, etc. I was hoping you guys could tell me whats the most common option for this in the industry as of now. I saw some say SQLalchemy.", "author_fullname": "t2_8mn3m0sl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tool/framework to pick up for SQL in python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16or26x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695329653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to start learning how to use sql through python along with pandas, etc. I was hoping you guys could tell me whats the most common option for this in the industry as of now. I saw some say SQLalchemy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16or26x", "is_robot_indexable": true, "report_reasons": null, "author": "PurpVan", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16or26x/best_toolframework_to_pick_up_for_sql_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16or26x/best_toolframework_to_pick_up_for_sql_in_python/", "subreddit_subscribers": 129769, "created_utc": 1695329653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any good recommendations on learning how to design a data architecture for a data science project or an enterprise?", "author_fullname": "t2_61mc4jcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Designing a Data Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16or0j9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695329543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any good recommendations on learning how to design a data architecture for a data science project or an enterprise?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16or0j9", "is_robot_indexable": true, "report_reasons": null, "author": "prtkkr", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16or0j9/designing_a_data_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16or0j9/designing_a_data_architecture/", "subreddit_subscribers": 129769, "created_utc": 1695329543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently saw a job for a Golang Data Engineer and I was surprised because I don\u2019t remember hearing about any libraries in Go for handling data like how Python has Pandas, Polars, etc. If you\u2019ve used Go for data engineering how are you using it? \n\nI\u2019m a Data Engineer and I wanna learn Go for backend development but if theres some libraries like Polars or Pandas in Go for dealing with data-frames I would love to learn that as well.", "author_fullname": "t2_8wpw0e1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Golang in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16omrba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695319588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently saw a job for a Golang Data Engineer and I was surprised because I don\u2019t remember hearing about any libraries in Go for handling data like how Python has Pandas, Polars, etc. If you\u2019ve used Go for data engineering how are you using it? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m a Data Engineer and I wanna learn Go for backend development but if theres some libraries like Polars or Pandas in Go for dealing with data-frames I would love to learn that as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16omrba", "is_robot_indexable": true, "report_reasons": null, "author": "khaili109", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16omrba/golang_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16omrba/golang_in_data_engineering/", "subreddit_subscribers": 129769, "created_utc": 1695319588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3i0xn3gy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Motherduck is now free for anyone to sign up for", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_16ofd3b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/d_Y3RLZe_KhTsbyenw-vB_8759rZH3XEztzDvH0aVwk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695301324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "motherduck.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://motherduck.com/blog/motherduck-open-for-all-with-series-b/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?auto=webp&amp;s=8e4e22764c7020df061de59726b1fae1cc0fb7cb", "width": 3200, "height": 1672}, "resolutions": [{"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2636a7232b1fe7525a4d5b1251afa5094b5fa5de", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3903f2fd35776377e83f5d2bf5f292b42daf81c4", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fb04ac59ca8b94c744f0c29a0ca7dbc6a9da4818", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da889478bbaa7a40a819fc79e6ddba2c729545c5", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c3ce0a20370d11fef048e9f7fc5503706c8ba38b", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/gWMYrIO5uCu-uuA1a-yzVlpFD6n2u0JFCkgMgl059d4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=69a2759f51a49739e14139f0efc138621dbc9d46", "width": 1080, "height": 564}], "variants": {}, "id": "kv9OeUq5aTu0KwswK_YR0kPeKOsiIRZ02HxYtgwWFKc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16ofd3b", "is_robot_indexable": true, "report_reasons": null, "author": "ddanieltan", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ofd3b/motherduck_is_now_free_for_anyone_to_sign_up_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://motherduck.com/blog/motherduck-open-for-all-with-series-b/", "subreddit_subscribers": 129769, "created_utc": 1695301324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When I migrated to Snowflake my costs went up like 5x. Maybe more.", "author_fullname": "t2_j13q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone migrating away from Snowflake and back to AWS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p1v2j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695360099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I migrated to Snowflake my costs went up like 5x. Maybe more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p1v2j", "is_robot_indexable": true, "report_reasons": null, "author": "Fitbot5000", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p1v2j/anyone_migrating_away_from_snowflake_and_back_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p1v2j/anyone_migrating_away_from_snowflake_and_back_to/", "subreddit_subscribers": 129769, "created_utc": 1695360099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've gained experience with Google Cloud Platform (GCP) and explored its fundamental services. However, I've noticed that many job opportunities in this field prefer or require AWS expertise. As a result, I've recently set up a free tier AWS account.\n\nPreviously, I had been using GCP with the free credits provided by Google for three months, following tutorials tailored for GCP. This made my experience smooth and easy. The challenge now is that AWS is entirely new to me. I'm seeking guidance on finding resources or platforms where I can discover equivalent tools and services to what I used in GCP.\n\nI'm not completely shifting from GCP to AWS; rather, I want to gain a similar level of exposure to AWS to enhance my qualifications. Any advice on this transition would be greatly appreciated.", "author_fullname": "t2_430i2d0p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switch from GCP to AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16omzke", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695320144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve gained experience with Google Cloud Platform (GCP) and explored its fundamental services. However, I&amp;#39;ve noticed that many job opportunities in this field prefer or require AWS expertise. As a result, I&amp;#39;ve recently set up a free tier AWS account.&lt;/p&gt;\n\n&lt;p&gt;Previously, I had been using GCP with the free credits provided by Google for three months, following tutorials tailored for GCP. This made my experience smooth and easy. The challenge now is that AWS is entirely new to me. I&amp;#39;m seeking guidance on finding resources or platforms where I can discover equivalent tools and services to what I used in GCP.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not completely shifting from GCP to AWS; rather, I want to gain a similar level of exposure to AWS to enhance my qualifications. Any advice on this transition would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16omzke", "is_robot_indexable": true, "report_reasons": null, "author": "Blanco04", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16omzke/switch_from_gcp_to_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16omzke/switch_from_gcp_to_aws/", "subreddit_subscribers": 129769, "created_utc": 1695320144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it better to have all your data in one database or break it into multiple databases for each stage like staging and marts? I could see it being easier to deploy different environments within the same snowflake account with one database. Or should I have one database for raw then another one for staging/analytics? Any recommendations?", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Database Design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oymfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695349598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it better to have all your data in one database or break it into multiple databases for each stage like staging and marts? I could see it being easier to deploy different environments within the same snowflake account with one database. Or should I have one database for raw then another one for staging/analytics? Any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16oymfs", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16oymfs/snowflake_database_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16oymfs/snowflake_database_design/", "subreddit_subscribers": 129769, "created_utc": 1695349598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Question:** What are some low-code time series manipulation tools available for excel users who can't be trusted to author production ready transformations using SQL or Python?  \n\n\nI'm a software dev building tools for a financial data analyst team. The analyst team has asked me to build a tool for self-serve authoring of time series transformations so analysts can author hundreds of these transformations that power dashboards we deliver to customers.\n\nI plan on using open-source only modern data stack tools, dbt in particular, as I don't see a point in reinventing the wheel. The main obstacle however is my team doesn't want to use SQL or Python as our analysts can't be trusted to code. They complain that SQL is too verbose and ugly for time series, and python is powerful enough that analyst-authored code has introduced production bugs previously. Our analysts are more like excel analysts. Thus i'm looking for low-code tools that would make panel data manipulations (time series &amp; cross-sectional gap fills, bucketing, panel data) fairly trivial to author **and** productionize.  \n\n\nI did some research on the following approaches:\n\n* **Semantic Layers**: This is promising as it compiles to SQL, but it's not clear from documentation if they implement time series features well yet.\n* **Custom DSL**: Instead of a YAML like DSL in semantic layers, I could write a simple lisp-like DSL for manipulating time series that feels more like excel while not allowing advanced features in python. This gives me most control over the prettiness of the language while also giving me the freedom to compile to either SQL or Python or wtv in the future. Our data sets are small enough that we don't need the SQL optimizer necessarily.\n* **Dashboard w/ Code Gen**: It might be possible for a tool like superset to generate the SQL visually instead.\n* **Raw SQL**: I've been holding my ground that data analysts should be expected to write SQL, as it was literally designed for analysts to do business transformations. The syntax is ugly and can get fairly advanced, but it has a proven track record for several decades, with a large ecosystem of tooling. I don't know if I have the level of influence to change an entire org though. I will keep trying to push this concern to higher level mgmt as i think upskilling your workforce while eliminating engineering work is a big win overall.", "author_fullname": "t2_2naya68b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What simplified language to use for time series (panel) data manipulation in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ojf6c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695311474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What are some low-code time series manipulation tools available for excel users who can&amp;#39;t be trusted to author production ready transformations using SQL or Python?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a software dev building tools for a financial data analyst team. The analyst team has asked me to build a tool for self-serve authoring of time series transformations so analysts can author hundreds of these transformations that power dashboards we deliver to customers.&lt;/p&gt;\n\n&lt;p&gt;I plan on using open-source only modern data stack tools, dbt in particular, as I don&amp;#39;t see a point in reinventing the wheel. The main obstacle however is my team doesn&amp;#39;t want to use SQL or Python as our analysts can&amp;#39;t be trusted to code. They complain that SQL is too verbose and ugly for time series, and python is powerful enough that analyst-authored code has introduced production bugs previously. Our analysts are more like excel analysts. Thus i&amp;#39;m looking for low-code tools that would make panel data manipulations (time series &amp;amp; cross-sectional gap fills, bucketing, panel data) fairly trivial to author &lt;strong&gt;and&lt;/strong&gt; productionize.  &lt;/p&gt;\n\n&lt;p&gt;I did some research on the following approaches:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Semantic Layers&lt;/strong&gt;: This is promising as it compiles to SQL, but it&amp;#39;s not clear from documentation if they implement time series features well yet.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Custom DSL&lt;/strong&gt;: Instead of a YAML like DSL in semantic layers, I could write a simple lisp-like DSL for manipulating time series that feels more like excel while not allowing advanced features in python. This gives me most control over the prettiness of the language while also giving me the freedom to compile to either SQL or Python or wtv in the future. Our data sets are small enough that we don&amp;#39;t need the SQL optimizer necessarily.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dashboard w/ Code Gen&lt;/strong&gt;: It might be possible for a tool like superset to generate the SQL visually instead.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Raw SQL&lt;/strong&gt;: I&amp;#39;ve been holding my ground that data analysts should be expected to write SQL, as it was literally designed for analysts to do business transformations. The syntax is ugly and can get fairly advanced, but it has a proven track record for several decades, with a large ecosystem of tooling. I don&amp;#39;t know if I have the level of influence to change an entire org though. I will keep trying to push this concern to higher level mgmt as i think upskilling your workforce while eliminating engineering work is a big win overall.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ojf6c", "is_robot_indexable": true, "report_reasons": null, "author": "shuaibot", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ojf6c/what_simplified_language_to_use_for_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ojf6c/what_simplified_language_to_use_for_time_series/", "subreddit_subscribers": 129769, "created_utc": 1695311474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I received an offer for an interview with a company for an Engineer position after I showcased my portfolio to them, i.e. my Github with sources of my work and full documentation. They offered to let me interview... but I have to do a live coding interview. I asked for more information before moving forward, and was given almost nothing to help me prepare. \n\n\nThe Round 1 Telephonic is a live coding round format:\n\n \n\nOne SQL problem  \u00e0 20 Minutes (They can use postgres/sql lite to solve SQL problem)\n\nOne programming problem \u00e0 20 minutes. (They can use Java/Python programming language to solve)\n\n\nQ &amp; A \u00e0 5 minutes", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone ever been asked to attend an interview with a \"coding exercise\" where they tell you almost nothing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oun5e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695347471.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695338266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I received an offer for an interview with a company for an Engineer position after I showcased my portfolio to them, i.e. my Github with sources of my work and full documentation. They offered to let me interview... but I have to do a live coding interview. I asked for more information before moving forward, and was given almost nothing to help me prepare. &lt;/p&gt;\n\n&lt;p&gt;The Round 1 Telephonic is a live coding round format:&lt;/p&gt;\n\n&lt;p&gt;One SQL problem  \u00e0 20 Minutes (They can use postgres/sql lite to solve SQL problem)&lt;/p&gt;\n\n&lt;p&gt;One programming problem \u00e0 20 minutes. (They can use Java/Python programming language to solve)&lt;/p&gt;\n\n&lt;p&gt;Q &amp;amp; A \u00e0 5 minutes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16oun5e", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16oun5e/has_anyone_ever_been_asked_to_attend_an_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16oun5e/has_anyone_ever_been_asked_to_attend_an_interview/", "subreddit_subscribers": 129769, "created_utc": 1695338266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_74fdrilk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PyJaws v0.1.7: A Pythonic way of Declaring Databricks Jobs and Workflows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocnvb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1695293333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "pypi.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://pypi.org/project/pyjaws", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16ocnvb", "is_robot_indexable": true, "report_reasons": null, "author": "j0selit0342", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocnvb/pyjaws_v017_a_pythonic_way_of_declaring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://pypi.org/project/pyjaws", "subreddit_subscribers": 129769, "created_utc": 1695293333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w07bcus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming Databases: Everything You Wanted to Know", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "name": "t3_16osaaa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GR1WqvPIIOLXNrUUsA-uQuEaf6gcpeSwaZmjjPetkBs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695332501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "risingwave.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.risingwave.com/blog/streaming-databases-everything-you-wanted-to-know/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?auto=webp&amp;s=b7e4dcc20f21eb08532be4f44bcb2afd83a3598f", "width": 2560, "height": 1600}, "resolutions": [{"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da32a5d91a292bffb6d119660b638c9e590567e6", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7353fe828c48d0f4db3187b3034190456a3dec1f", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdb2196149d90817950922af7342f2f08e62780a", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e8121c7e1905cecc6c23cc5660a7c426e5bb4ae2", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=62cc77e8c5b58f12cdccc038f3cd16ebdf21ced9", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=60b72dad7898861032ecfe89103ca54e6a5d96e2", "width": 1080, "height": 675}], "variants": {}, "id": "hFCRvLcZo01Mnja6CckUi4AgVCLM2MZOpfNWBh4FS5U"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16osaaa", "is_robot_indexable": true, "report_reasons": null, "author": "yingjunwu", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16osaaa/streaming_databases_everything_you_wanted_to_know/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.risingwave.com/blog/streaming-databases-everything-you-wanted-to-know/", "subreddit_subscribers": 129769, "created_utc": 1695332501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are trying to read a big chunk of small files from several buckets in S3 with Spark. The objective is to merge those files into one and write it in another s3 bucket. \n\n&amp;#x200B;\n\nThe code for read:\n\n    val parquetFiles = Seq(\"s3a://...\", \"s3a://.....\" ..)\n    val df = spark.read.format(\"parquet\").load(parquetFiles:_*)\n\n It takes about 10 minutes to execute the following query:\n\n    df.coalesce(1).write.format(\"parquet\").save(\"s3://...\")\n\nWhile a `df.count()` it takes about 2 minutes (which is also not ok, I guess). \n\nWe've tried changing a lot of configurations from `hadoop.fs.s3a`, but no combination seems to alleviate the time. We cannot clearly understand which task is delaying the execution, but from Spark UI we have seen that not much CPU or Memory is consumed. \n\nMy assumption is that HTTP calls to S3 are getting too expensive. But I am not sure. \n\nHas anyone experienced similar issues? \n\nHave you solved them with conf or is it just a known problem? \n\n&amp;#x200B;\n\nThank you!\n\n&amp;#x200B;", "author_fullname": "t2_ffabopog", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reading small files from S3 with Spark is slow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocm37", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695293163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are trying to read a big chunk of small files from several buckets in S3 with Spark. The objective is to merge those files into one and write it in another s3 bucket. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The code for read:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;val parquetFiles = Seq(&amp;quot;s3a://...&amp;quot;, &amp;quot;s3a://.....&amp;quot; ..)\nval df = spark.read.format(&amp;quot;parquet&amp;quot;).load(parquetFiles:_*)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;It takes about 10 minutes to execute the following query:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df.coalesce(1).write.format(&amp;quot;parquet&amp;quot;).save(&amp;quot;s3://...&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;While a &lt;code&gt;df.count()&lt;/code&gt; it takes about 2 minutes (which is also not ok, I guess). &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve tried changing a lot of configurations from &lt;code&gt;hadoop.fs.s3a&lt;/code&gt;, but no combination seems to alleviate the time. We cannot clearly understand which task is delaying the execution, but from Spark UI we have seen that not much CPU or Memory is consumed. &lt;/p&gt;\n\n&lt;p&gt;My assumption is that HTTP calls to S3 are getting too expensive. But I am not sure. &lt;/p&gt;\n\n&lt;p&gt;Has anyone experienced similar issues? &lt;/p&gt;\n\n&lt;p&gt;Have you solved them with conf or is it just a known problem? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ocm37", "is_robot_indexable": true, "report_reasons": null, "author": "paolapardo", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocm37/reading_small_files_from_s3_with_spark_is_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ocm37/reading_small_files_from_s3_with_spark_is_slow/", "subreddit_subscribers": 129769, "created_utc": 1695293163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a bunch of US/Non US addresses which I need to standardize according to USPS format. Can you guys recommend me best way to do it.  Is there any Python package that I can rely?  \nOr Is there any paid/free third party endpoint where I can make a call to validate.  Google gave me [https://www.smarty.com/](https://www.smarty.com/) but I am not sure about other. Do you guys have any recommended vendor?", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Address verification and standardization tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p1smy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695359860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bunch of US/Non US addresses which I need to standardize according to USPS format. Can you guys recommend me best way to do it.  Is there any Python package that I can rely?&lt;br/&gt;\nOr Is there any paid/free third party endpoint where I can make a call to validate.  Google gave me &lt;a href=\"https://www.smarty.com/\"&gt;https://www.smarty.com/&lt;/a&gt; but I am not sure about other. Do you guys have any recommended vendor?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?auto=webp&amp;s=5633dd3efd6979a9f065870210ba32bdb450edaf", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec1290b11636f849df11bcd2a9574db1d2de71ee", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c44367f44736bde57c7cf902bf409d31522ea58", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6307f712ef2da9365026f5339c461ddbf092005", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdfeaaa5bb4f017311064d21d8af2f904255001b", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a4249baf92f99963d4f1ec5e66bf2c47be2a5579", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=837281c129c8a69e452296dea9ef616ba85f177c", "width": 1080, "height": 567}], "variants": {}, "id": "RgGgH5XSBWSNjWbkX2NvcezqdVDW9PhVpc-BnehunSc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16p1smy", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p1smy/address_verification_and_standardization_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p1smy/address_verification_and_standardization_tools/", "subreddit_subscribers": 129769, "created_utc": 1695359860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "can some one explain how to ingest downloaded data from kaggle incrementally using python. you can use aws glue  and store data in s3 ?", "author_fullname": "t2_8vmwwx5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16onc3z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695320978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;can some one explain how to ingest downloaded data from kaggle incrementally using python. you can use aws glue  and store data in s3 ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16onc3z", "is_robot_indexable": true, "report_reasons": null, "author": "lifealtering111", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16onc3z/data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16onc3z/data_ingestion/", "subreddit_subscribers": 129769, "created_utc": 1695320978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_agd3b25og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple personal finance data engineering project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_16omopv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/qAfYIWzd97XHeRbSSy9Bti07lV5qAIIu4eeql_n5Kr8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695319410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/sam-wright-1/personal-finance-automation", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?auto=webp&amp;s=2bfb8cac5bfb9409447c65c5b2e8e5944295f808", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3fd795baaad4e5c64facc3c9e2d1f59c66386a7d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c4831f914f843e0eb0f349575c65e58636bd0af", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8614c9e967090b21dd6049af4d683c83388492f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fef6f995a2c998faba20c8c794cb30b76ba5fb4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=170e686546300424996ce707e93c461e87788af2", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d51f9a396d2b197fe1be96da7ed54ada899634f", "width": 1080, "height": 540}], "variants": {}, "id": "p3Ivo_RgfDav9G-934-9Ik7yNegAsq2eYHKKp5kjpvA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "16omopv", "is_robot_indexable": true, "report_reasons": null, "author": "data-partner", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16omopv/simple_personal_finance_data_engineering_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/sam-wright-1/personal-finance-automation", "subreddit_subscribers": 129769, "created_utc": 1695319410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi  I'm new to data engineering.\n\nI want to devise a system that loads tabular data (10-20 columns with 10\\^9/10\\^10/ -billions of data records), that will be ingest from eternal API.\n\nThe system should do processing (filtering subset of the rows and make basic computation analysis)\n\nHow to approach this problem? and what are the recommenced tools\n\nDoes best practice of DB like Spark or Snowflake will be sufficient for that task?\n\nand if there are any good resources to gain more knowledge- it will be awesome!\n\nI'm looking for the best of: latency in inference, easy to develop my system, easy to maintain, minimum server requirement, easy and basic monitor.\n\nThanks!\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_5831rdhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to approach designing a novel system that loads tabular data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oiax5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695308771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi  I&amp;#39;m new to data engineering.&lt;/p&gt;\n\n&lt;p&gt;I want to devise a system that loads tabular data (10-20 columns with 10^9/10^10/ -billions of data records), that will be ingest from eternal API.&lt;/p&gt;\n\n&lt;p&gt;The system should do processing (filtering subset of the rows and make basic computation analysis)&lt;/p&gt;\n\n&lt;p&gt;How to approach this problem? and what are the recommenced tools&lt;/p&gt;\n\n&lt;p&gt;Does best practice of DB like Spark or Snowflake will be sufficient for that task?&lt;/p&gt;\n\n&lt;p&gt;and if there are any good resources to gain more knowledge- it will be awesome!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for the best of: latency in inference, easy to develop my system, easy to maintain, minimum server requirement, easy and basic monitor.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16oiax5", "is_robot_indexable": true, "report_reasons": null, "author": "Expensive_Breakfast6", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16oiax5/how_to_approach_designing_a_novel_system_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16oiax5/how_to_approach_designing_a_novel_system_that/", "subreddit_subscribers": 129769, "created_utc": 1695308771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just published an article on how to setup Zero-ETL integration between an Amazon Aurora database and Redshift.\n\nWith the Zero-ETL feature, you do not need to bother about setting up complex data pipelines in your organisation.\n\n[https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift](https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift)", "author_fullname": "t2_k6fldwtv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How you can set up Zero-ETL data pipeline between Amazon Aurora and Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16of8ak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695300955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just published an article on how to setup Zero-ETL integration between an Amazon Aurora database and Redshift.&lt;/p&gt;\n\n&lt;p&gt;With the Zero-ETL feature, you do not need to bother about setting up complex data pipelines in your organisation.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift\"&gt;https://gbengaoni.com/blog/Setup-Zero-ETL-Integration-with-AWS-RDS-Aurora-and-Redshift&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?auto=webp&amp;s=43854de3c536f86fe8edad9982c3583ab380ad09", "width": 681, "height": 370}, "resolutions": [{"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecdcf6fcf7adb0a98d8aa69dc115ea125539065f", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d049f2c1df9c1ec82935472ae7fbeb1ccbd6e8d", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6807b4b492aac7f299db9cd405289e68a67fe5f8", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/zl7ZUKRM8e-nimR3c9TbMU5MmwLCxkjfdDqokqpAIME.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ee0039bae6a8d6f694eb79595479c445690476e", "width": 640, "height": 347}], "variants": {}, "id": "C3ODgW2FQeNZnu5nJxonIMIBLmXRwIgb3c7YtEsw3sM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16of8ak", "is_robot_indexable": true, "report_reasons": null, "author": "gbxnga", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16of8ak/how_you_can_set_up_zeroetl_data_pipeline_between/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16of8ak/how_you_can_set_up_zeroetl_data_pipeline_between/", "subreddit_subscribers": 129769, "created_utc": 1695300955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, \n\nI am wondering If someone of you used open source DVT from google github  - [professional-services-data-validator](https://github.com/GoogleCloudPlatform/professional-services-data-validator).\n\nI need to validate data exported from differences system. I have one folder, where all files are stored and via pipelines are loaded to the BigQuery. I can easily transform these files to the .csv and run the validation. But the problem is with the Filesystem connection. Filesystem connection has argument \"file path\" and connection is created only for one file.\n\nMy question is, with your experience, Do you think its better to create new connection for every file (thousands files) or use python script that will transform and load data to the one file work\\_file.csv that is connected and after the validation delete the file content then take another file and transform and load data to the work\\_file.csv...\n\nI do not have enough experience with these technologies and BI processes so I would be glad for every point you have. \n\nThank you very much.  \nM\n\n&amp;#x200B;", "author_fullname": "t2_79da9w8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Validation via professional-services-data-validator from Google", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ocb07", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695292088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;I am wondering If someone of you used open source DVT from google github  - &lt;a href=\"https://github.com/GoogleCloudPlatform/professional-services-data-validator\"&gt;professional-services-data-validator&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I need to validate data exported from differences system. I have one folder, where all files are stored and via pipelines are loaded to the BigQuery. I can easily transform these files to the .csv and run the validation. But the problem is with the Filesystem connection. Filesystem connection has argument &amp;quot;file path&amp;quot; and connection is created only for one file.&lt;/p&gt;\n\n&lt;p&gt;My question is, with your experience, Do you think its better to create new connection for every file (thousands files) or use python script that will transform and load data to the one file work_file.csv that is connected and after the validation delete the file content then take another file and transform and load data to the work_file.csv...&lt;/p&gt;\n\n&lt;p&gt;I do not have enough experience with these technologies and BI processes so I would be glad for every point you have. &lt;/p&gt;\n\n&lt;p&gt;Thank you very much.&lt;br/&gt;\nM&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?auto=webp&amp;s=e21a2042737dc223e67be7ea27e40fc2236d12ee", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c766345a5f83563a2848746e21a43c0ad01a8c7e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80407217416d454d1f32b3b0c1fe41d4ba9dbf87", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ab68f2323e57915f688870703aa3b7827922c10", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3167be568b95344dbbaa19877c37d4bfffb3e688", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f988a35b7c0f9d7945888db9d4ec0e48eddb82d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/lo_K8gKyMl9vMm_BzGB50oCa2GG11EJSo-z5FzbpdZs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e44e34cb74d2a495765f7b1cc9148eed809bb28", "width": 1080, "height": 540}], "variants": {}, "id": "Y2KMmNmoflDB_TA4VN4tScfKspEdT0Y_43XDwIeYUzA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16ocb07", "is_robot_indexable": true, "report_reasons": null, "author": "Illustrious_Union216", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ocb07/data_validation_via/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ocb07/data_validation_via/", "subreddit_subscribers": 129769, "created_utc": 1695292088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nMy company's data analysis workflow relies solely on Google Cloud's Looker. Currently, we manually export data from the company's main system into an Excel file and then upload it to Looker to demonstrate the charts and visualizations. However, this process is slow due to the large amounts of data. Therefore, I have been tasked with creating an automated process from scratch to analyze the data and display visualizations similar to Looker's interface, but with more advanced data analysis tools like Python, Pandas, Jupyter, or SQL to make the system faster.\n\nI already have experience using Python, Pandas, Jupyter, and SQL in previous projects, but I need suggestions on how to utilize these tools for my current project. I am also willing to learn new skills if it will help me complete the project successfully.", "author_fullname": "t2_jkvo7dqy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need assistance in developing an automated analysis system. Can you help me with that?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16osu1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695333757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company&amp;#39;s data analysis workflow relies solely on Google Cloud&amp;#39;s Looker. Currently, we manually export data from the company&amp;#39;s main system into an Excel file and then upload it to Looker to demonstrate the charts and visualizations. However, this process is slow due to the large amounts of data. Therefore, I have been tasked with creating an automated process from scratch to analyze the data and display visualizations similar to Looker&amp;#39;s interface, but with more advanced data analysis tools like Python, Pandas, Jupyter, or SQL to make the system faster.&lt;/p&gt;\n\n&lt;p&gt;I already have experience using Python, Pandas, Jupyter, and SQL in previous projects, but I need suggestions on how to utilize these tools for my current project. I am also willing to learn new skills if it will help me complete the project successfully.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16osu1j", "is_robot_indexable": true, "report_reasons": null, "author": "CanSecret8665", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16osu1j/i_need_assistance_in_developing_an_automated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16osu1j/i_need_assistance_in_developing_an_automated/", "subreddit_subscribers": 129769, "created_utc": 1695333757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m working for a financial institution as an azure data factory developer, i have been migrating all the workflows from informatica to ADF but my company doesn\u2019t approve mapping dataflows and asked me to execute everything in pipeline level. But I\u2019m stuck at a point where i need to create a sequence generator for a particular mapping, any thoughts on how can we do that without using mapping data flows", "author_fullname": "t2_fiazaz05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering - issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16onect", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695321118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m working for a financial institution as an azure data factory developer, i have been migrating all the workflows from informatica to ADF but my company doesn\u2019t approve mapping dataflows and asked me to execute everything in pipeline level. But I\u2019m stuck at a point where i need to create a sequence generator for a particular mapping, any thoughts on how can we do that without using mapping data flows&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16onect", "is_robot_indexable": true, "report_reasons": null, "author": "akki06085", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16onect/data_engineering_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16onect/data_engineering_issue/", "subreddit_subscribers": 129769, "created_utc": 1695321118.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}